<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | arxiv-smry: Papers at bullet speed</title><meta name=keywords content><meta name=description content="Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."><meta name=author content="Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie"><link rel=canonical href=https://niclaswue.github.io/arxiv-smry/posts/2301.00808/><link crossorigin=anonymous href=/arxiv-smry/assets/css/stylesheet.ebc76a19328fd90bea134236d0972fb4509b5ed219346f30467a25e3ba7ec5a2.css integrity="sha256-68dqGTKP2QvqE0I20JcvtFCbXtIZNG8wRnol47p+xaI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/arxiv-smry/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://niclaswue.github.io/arxiv-smry/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://niclaswue.github.io/arxiv-smry/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://niclaswue.github.io/arxiv-smry/favicon-32x32.png><link rel=apple-touch-icon href=https://niclaswue.github.io/arxiv-smry/apple-touch-icon.png><link rel=mask-icon href=https://niclaswue.github.io/arxiv-smry/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"><meta property="og:description" content="Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."><meta property="og:type" content="article"><meta property="og:url" content="https://niclaswue.github.io/arxiv-smry/posts/2301.00808/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-02T18:59:31+00:00"><meta property="article:modified_time" content="2023-01-02T18:59:31+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"><meta name=twitter:description content="Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://niclaswue.github.io/arxiv-smry/posts/"},{"@type":"ListItem","position":3,"name":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders","item":"https://niclaswue.github.io/arxiv-smry/posts/2301.00808/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders","name":"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders","description":"Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper.","keywords":[],"articleBody":"Link to paper The full paper is available here.\nPaper Content Introduction Building on research breakthroughs in earlier decades, the field of visual recognition has ushered in a new era of large-scale visual representation learning. Three main factors influence the performance of a visual representation learning system: the neural network architecture, the training method, and the data used for training. Convolutional neural network architectures (ConvNets) have been used for feature learning and enabled a wide range of vision applications. The transformer architecture has gained popularity due to its strong scaling behavior. Self-supervised pre-training with pretext objectives has become popular for visual representation learning. Masked autoencoders have brought success in masked language modeling to the vision domain. Combining the design elements of architectures and self-supervised learning frameworks can be challenging with ConvNeXt. ConvNeXt V2 is proposed to address the issue of feature collapse, and has improved performance across various tasks. Related Work ConvNets were first introduced in the 1980s and have been improved over time. Supervised training on the Im-ageNet dataset has been used to discover many of these improvements. Self-supervised pre-text tasks such as rotation prediction and colorization have been used for architecture search. ConvNeXt has been used to demonstrate that pure ConvNets can be as scalable as vision transformers. ConvNeXt V2 model has been developed, powered by self-supervised learning, to provide a boost in performance. Masked autoencoders are a self-supervised learning strategy which has been adapted for use with ConvNets. MCMAE has used convolutional blocks as input tokenizers. There are no pretrained models that show self-supervised learning can improve upon the best ConvNeXt supervised results. Fully Convolutional Masked Autoencoder Approach is conceptually simple and runs in a fully convolutional manner Model predicts missing parts given context Masks randomly remove 60% of 32 x 32 patches from the original image Encoder uses ConvNeXt with sparse convolution Decoder uses single ConvNeXt block with 512 dimensions Computes mean squared error between reconstructed and target images Evaluates effectiveness with end-to-end fine-tuning performance Pre-training provides better initialization than random baseline Global Response Normalization Introduce Global Response Normalization (GRN) with ConvNeXt architecture Feature collapse observed in dimension-expansion MLP layers in ConvNeXt Feature cosine distance analysis to quantify feature collapse GRN includes global feature aggregation, feature normalization, and feature calibration GRN incorporated in ConvNeXt V2 model GRN mitigates feature collapse issue GRN improves representation quality without additional parameter overhead GRN compared with other normalization layers and feature gating methods GRN important to keep in both pre-training and fine-tuning ImageNet Experiments FCMAE pre-training framework and ConvNeXt V2 architecture designed to make masked-based self-supervised pre-training successful Synergizing of designs provides strong foundation for scaling Comparisons to previous masked auto-encoder methods Strong model scaling behavior, improved performance over supervised baseline Sets new state-of-the-art accuracy using publicly available data Transfer Learning Experiments Evaluating the impact of co-design on transfer learning performance Comparing ConvNeXt V1 + supervised and ConvNeXt V2 + FC-MAE Comparing with Swin transformer models pre-trained with SimMIM Object detection and segmentation on COCO Gradual improvement from V1 to V2 Model benefits from better initialization with FCMAE ConvNeXt V2 pre-trained on FCMAE outperforms Swin transformer counterparts Semantic segmentation on ADE20K Similar trend to object detection experiments Final model significantly improves over V1 counterparts On par with Swin transformer in base and large model regimes Outperforms Swin in huge model regime Conclusion ConvNeXt V2 is a new ConvNet model family It is designed for self-supervised learning It can improve performance of pure ConvNets on various tasks, like ImageNet classification, COCO object detection, and ADE20K segmentation Uses mixup, cutmix, drop path, head init, and EMA End-to-end IN-1K fine-tuning setting for Atto, Femto, Pico, and Nano models Complete comparisons with V1 Tables 14 and 15 present detailed experiment-level comparisons between ConvNeXt V1 and V2. Table 14 shows ImageNet-1K fine-tuning results using eight models. Performance is improved when architecture is upgraded from V1 to V2 and self-supervised learning framework FCMAE is used. Table 15 compares the performance of the Base V2 model with the baseline ResNet-50 model on downstream tasks. Further Analyses Sparse convolution used in FCMAE framework to block flow of information and facilitate pre-training Computational and memory efficiency improved during pre-training Benchmark experiments conducted to understand pre-training efficiency Class selectivity index analysis conducted on FCMAE pre-trained weights Distribution of class selectivity index closely matched between V1 and V2 in early stages, but diverges in deep layers V2 tends to include more class-generic features than V1 Feature normalization not preceded by global aggregation, both operations work together to make GRN effective Additional Experiments Masking ratios of 0.5 to 0.7 produce the best results with a masking ration of 0.6 being the highest Performance declines when removing or leaving 90% of the input information Comparing the performance of two dominant self-supervised learning approaches: contrastive learning and masked image modeling FCMAE leads to better representation quality than MoCo V3 and also outperforms the supervised baseline ","wordCount":"811","inLanguage":"en","datePublished":"2023-01-02T18:59:31Z","dateModified":"2023-01-02T18:59:31Z","author":{"@type":"Person","name":"Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://niclaswue.github.io/arxiv-smry/posts/2301.00808/"},"publisher":{"@type":"Organization","name":"arxiv-smry: Papers at bullet speed","logo":{"@type":"ImageObject","url":"https://niclaswue.github.io/arxiv-smry/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://niclaswue.github.io/arxiv-smry accesskey=h title="arxiv-smry: Papers at bullet speed (Alt + H)">arxiv-smry: Papers at bullet speed</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://niclaswue.github.io/arxiv-smry>Home</a>&nbsp;»&nbsp;<a href=https://niclaswue.github.io/arxiv-smry/posts/>Posts</a></div><h1 class=post-title>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</h1><div class=post-description>Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper.</div><div class=post-meta><span title='2023-01-02 18:59:31 +0000 UTC'>January 2, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;811 words&nbsp;·&nbsp;Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#link-to-paper aria-label="Link to paper">Link to paper</a></li><li><a href=#paper-content aria-label="Paper Content">Paper Content</a><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#related-work aria-label="Related Work">Related Work</a></li><li><a href=#fully-convolutional-masked-autoencoder aria-label="Fully Convolutional Masked Autoencoder">Fully Convolutional Masked Autoencoder</a></li><li><a href=#global-response-normalization aria-label="Global Response Normalization">Global Response Normalization</a></li><li><a href=#imagenet-experiments aria-label="ImageNet Experiments">ImageNet Experiments</a></li><li><a href=#transfer-learning-experiments aria-label="Transfer Learning Experiments">Transfer Learning Experiments</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#complete-comparisons-with-v1 aria-label="Complete comparisons with V1">Complete comparisons with V1</a></li><li><a href=#further-analyses aria-label="Further Analyses">Further Analyses</a></li><li><a href=#additional-experiments aria-label="Additional Experiments">Additional Experiments</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=link-to-paper>Link to paper<a hidden class=anchor aria-hidden=true href=#link-to-paper>#</a></h1><p>The full paper is available <a href=https://arxiv.org/abs/2301.00808>here</a>.</p><h1 id=paper-content>Paper Content<a hidden class=anchor aria-hidden=true href=#paper-content>#</a></h1><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><ul><li>Building on research breakthroughs in earlier decades, the field of visual recognition has ushered in a new era of large-scale visual representation learning.</li><li>Three main factors influence the performance of a visual representation learning system: the neural network architecture, the training method, and the data used for training.</li><li>Convolutional neural network architectures (ConvNets) have been used for feature learning and enabled a wide range of vision applications.</li><li>The transformer architecture has gained popularity due to its strong scaling behavior.</li><li>Self-supervised pre-training with pretext objectives has become popular for visual representation learning.</li><li>Masked autoencoders have brought success in masked language modeling to the vision domain.</li><li>Combining the design elements of architectures and self-supervised learning frameworks can be challenging with ConvNeXt.</li><li>ConvNeXt V2 is proposed to address the issue of feature collapse, and has improved performance across various tasks.</li></ul><h2 id=related-work>Related Work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h2><ul><li>ConvNets were first introduced in the 1980s and have been improved over time.</li><li>Supervised training on the Im-ageNet dataset has been used to discover many of these improvements.</li><li>Self-supervised pre-text tasks such as rotation prediction and colorization have been used for architecture search.</li><li>ConvNeXt has been used to demonstrate that pure ConvNets can be as scalable as vision transformers.</li><li>ConvNeXt V2 model has been developed, powered by self-supervised learning, to provide a boost in performance.</li><li>Masked autoencoders are a self-supervised learning strategy which has been adapted for use with ConvNets.</li><li>MCMAE has used convolutional blocks as input tokenizers.</li><li>There are no pretrained models that show self-supervised learning can improve upon the best ConvNeXt supervised results.</li></ul><h2 id=fully-convolutional-masked-autoencoder>Fully Convolutional Masked Autoencoder<a hidden class=anchor aria-hidden=true href=#fully-convolutional-masked-autoencoder>#</a></h2><ul><li>Approach is conceptually simple and runs in a fully convolutional manner</li><li>Model predicts missing parts given context</li><li>Masks randomly remove 60% of 32 x 32 patches from the original image</li><li>Encoder uses ConvNeXt with sparse convolution</li><li>Decoder uses single ConvNeXt block with 512 dimensions</li><li>Computes mean squared error between reconstructed and target images</li><li>Evaluates effectiveness with end-to-end fine-tuning performance</li><li>Pre-training provides better initialization than random baseline</li></ul><h2 id=global-response-normalization>Global Response Normalization<a hidden class=anchor aria-hidden=true href=#global-response-normalization>#</a></h2><ul><li>Introduce Global Response Normalization (GRN) with ConvNeXt architecture</li><li>Feature collapse observed in dimension-expansion MLP layers in ConvNeXt</li><li>Feature cosine distance analysis to quantify feature collapse</li><li>GRN includes global feature aggregation, feature normalization, and feature calibration</li><li>GRN incorporated in ConvNeXt V2 model</li><li>GRN mitigates feature collapse issue</li><li>GRN improves representation quality without additional parameter overhead</li><li>GRN compared with other normalization layers and feature gating methods</li><li>GRN important to keep in both pre-training and fine-tuning</li></ul><h2 id=imagenet-experiments>ImageNet Experiments<a hidden class=anchor aria-hidden=true href=#imagenet-experiments>#</a></h2><ul><li>FCMAE pre-training framework and ConvNeXt V2 architecture designed to make masked-based self-supervised pre-training successful</li><li>Synergizing of designs provides strong foundation for scaling</li><li>Comparisons to previous masked auto-encoder methods</li><li>Strong model scaling behavior, improved performance over supervised baseline</li><li>Sets new state-of-the-art accuracy using publicly available data</li></ul><h2 id=transfer-learning-experiments>Transfer Learning Experiments<a hidden class=anchor aria-hidden=true href=#transfer-learning-experiments>#</a></h2><ul><li>Evaluating the impact of co-design on transfer learning performance</li><li>Comparing ConvNeXt V1 + supervised and ConvNeXt V2 + FC-MAE</li><li>Comparing with Swin transformer models pre-trained with SimMIM</li><li>Object detection and segmentation on COCO</li><li>Gradual improvement from V1 to V2</li><li>Model benefits from better initialization with FCMAE</li><li>ConvNeXt V2 pre-trained on FCMAE outperforms Swin transformer counterparts</li><li>Semantic segmentation on ADE20K</li><li>Similar trend to object detection experiments</li><li>Final model significantly improves over V1 counterparts</li><li>On par with Swin transformer in base and large model regimes</li><li>Outperforms Swin in huge model regime</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><ul><li>ConvNeXt V2 is a new ConvNet model family</li><li>It is designed for self-supervised learning</li><li>It can improve performance of pure ConvNets on various tasks, like ImageNet classification, COCO object detection, and ADE20K segmentation</li><li>Uses mixup, cutmix, drop path, head init, and EMA</li><li>End-to-end IN-1K fine-tuning setting for Atto, Femto, Pico, and Nano models</li></ul><h2 id=complete-comparisons-with-v1>Complete comparisons with V1<a hidden class=anchor aria-hidden=true href=#complete-comparisons-with-v1>#</a></h2><ul><li>Tables 14 and 15 present detailed experiment-level comparisons between ConvNeXt V1 and V2.</li><li>Table 14 shows ImageNet-1K fine-tuning results using eight models.</li><li>Performance is improved when architecture is upgraded from V1 to V2 and self-supervised learning framework FCMAE is used.</li><li>Table 15 compares the performance of the Base V2 model with the baseline ResNet-50 model on downstream tasks.</li></ul><h2 id=further-analyses>Further Analyses<a hidden class=anchor aria-hidden=true href=#further-analyses>#</a></h2><ul><li>Sparse convolution used in FCMAE framework to block flow of information and facilitate pre-training</li><li>Computational and memory efficiency improved during pre-training</li><li>Benchmark experiments conducted to understand pre-training efficiency</li><li>Class selectivity index analysis conducted on FCMAE pre-trained weights</li><li>Distribution of class selectivity index closely matched between V1 and V2 in early stages, but diverges in deep layers</li><li>V2 tends to include more class-generic features than V1</li><li>Feature normalization not preceded by global aggregation, both operations work together to make GRN effective</li></ul><h2 id=additional-experiments>Additional Experiments<a hidden class=anchor aria-hidden=true href=#additional-experiments>#</a></h2><ul><li>Masking ratios of 0.5 to 0.7 produce the best results with a masking ration of 0.6 being the highest</li><li>Performance declines when removing or leaving 90% of the input information</li><li>Comparing the performance of two dominant self-supervised learning approaches: contrastive learning and masked image modeling</li><li>FCMAE leads to better representation quality than MoCo V3 and also outperforms the supervised baseline</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://niclaswue.github.io/arxiv-smry/posts/writer/><span class=title>Next »</span><br><span>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders on twitter" href="https://twitter.com/intent/tweet/?text=ConvNeXt%20V2%3a%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders&url=https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f&title=ConvNeXt%20V2%3a%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders&summary=ConvNeXt%20V2%3a%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders&source=https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f&title=ConvNeXt%20V2%3a%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders on whatsapp" href="https://api.whatsapp.com/send?text=ConvNeXt%20V2%3a%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders%20-%20https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders on telegram" href="https://telegram.me/share/url?text=ConvNeXt%20V2%3a%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders&url=https%3a%2f%2fniclaswue.github.io%2farxiv-smry%2fposts%2f2301.00808%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://niclaswue.github.io/arxiv-smry>arxiv-smry: Papers at bullet speed</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>