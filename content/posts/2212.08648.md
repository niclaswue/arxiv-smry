---
title: "Connecting Permutation Equivariant Neural Networks and Partition Diagrams"
date: 2022-12-16T18:48:54.000Z
author: "Edward Pearce-Crump"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08648v2.webp" # image path/url
    alt: "Connecting Permutation Equivariant Neural Networks and Partition Diagrams" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08648)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08648).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/connecting-permutation-equivariant-neural).

# Abstract
- The Schur-Weyl duality between the partition algebra and the symmetric group results in a stronger theoretical foundation for characterising all of the possible permutation equivariant neural networks.
- In doing so, we unify two separate bodies of literature, and correct some of the major results that are now widely quoted by the machine learning community.
- We find a basis of matrices for the learnable, linear, permutation equivariant layer functions between such tensor power spaces in the standard basis of $M_n$ by using an elegant graphical representation of a basis of set partitions for the partition algebra and its related vector spaces.
- Also, we show how we can calculate the number of weights that must appear in these layer functions by looking at certain paths through the McKay quiver for $M_n$.
- Finally, we describe how our approach generalises to the construction of neural networks that are equivariant to local symmetries.

# Paper Content

## Introduction
- Permutation equivariant neural networks were born out of the desire to be able to learn from data that has, at its core, permutation symmetry.
- For example, sets and graphs are data structures that exhibit such symmetry.
- Indeed, the labelling of the elements of a set or of the nodes of a graph is arbitrary; hence, any function that is learned from data that has permutation symmetry should not depend on how the data is labelled.
- Despite there being a number of papers on this research topic, such as Zaheer et al. (2017), Hartford et al. (2018), Maron et al. (2019), Thiede et al. (2020), Finzi et al. (2021), and Pan and Kondor (2022), whose details we explore further in Section 9, we believe that two major issues remain: 1. Some of the early major results that are now widely quoted by the more recent papers in the field are missing key technical details whose exclusion significantly impacts the correct construction of these neural networks.
- The approaches that have been taken to prove the major results are all rather different, which suggests that there may be one overarching set of reasons as to why all of these results are true.
- In this paper, we suggest a new approach.
- We show how the representation theory of the partition algebra, developed by pure mathematicians, can be used to construct permu-tation equivariant neural networks whose layers are some tensor power of the permutation representation M n of dimension n of the symmetric group S n .
- These neural networks learn from data that exhibit relationships up to any order.
- For example, the individual elements of a set exhibit first order relationships, whereas the nodes of a graph exhibit second order relationships.
- In making the connection between the partition algebra and these permutation equivariant neural networks, we create a strong theoretical foundation that we believe resolves the two major issues stated above.
- In particular, we show how the construction of these neural networks is founded upon a number of combinatorial objects, which overall makes for a rather satisfying theory.
- The main contributions of this paper are as follows: 1. We are the first to show how the combinatorics underlying the partition algebra and its related vector spaces, adapted from the Schur-Weyl duality that exists between the partition algebra and the symmetric group, provides the theoretical background for constructing permutation equivariant neural networks when the layers are some tensor power of the permutation representation M n of the symmetric group S n .
- In particular, we find a basis of matrices for the learnable, linear, permutation equivariant layer functions between such tensor power spaces in the standard basis of M n by using an elegant graphical representation of a basis of set partitions for the partition algebra and its related vector spaces.
- We derive a number of combinatorial constructions that can be used to calculate the number of weights that must appear in the learnable, linear layer functions of the permutation equivariant neural networks in question.
- In particular, we show how the McKay quiver for the representation M n of the symmetric group S n combined with the fact that S n is in Schur-Weyl duality with a tensor power centraliser algebra, End Sn (M ⊗k n ), enables us to calculate the number of weights.
- We generalise this diagrammatical approach to show how to construct neural networks that are equivariant to local symmetries.
- We suggest that Schur-Weyl duality is a powerful mathematical concept that could be used to characterise other group equivariant neural networks beyond those considered in this paper.

## Preliminaries

### A Word on Notation
- The symmetric group of permutations of n objects is denoted by S n .
- In most cases, we let [n] represent the set {1, . . . , n}, although at times we use the same notation to refer to the 1-dimensional trivial representation of S n .
- The tensor product of two permutations is taken over the field C, unless otherwise stated.
- The context should make clear what is being referred to, although if there is any doubt, we use the word shape before [n] to make clear that we are referring to the representation and not the set of n elements.

### Representations of the Symmetric Group and its Branching Rules
- A representation of S n is a choice of vector space V over C and a group homomorphism ρ : S n → GL(V ) (2.1)
- We often abuse our terminology by calling V a representation of S n , even though the representation is technically the homomorphism ρ. When the homomorphism ρ needs to be emphasised alongside its vector space V , we will use the notation (V, ρ). We will also refer to such V as S n -modules, again abusing our notation slightly.
- Technically, V is a module over the group algebra C[S n ] and not the group. However, because this is done elsewhere in the literature, we will do the same in this paper.
- Ultimately, these are all interchangeable ways of referring to the same concept.
- For more details on the basics of representation theory, see any of Goodman and Wallach (2009), Ceccherini-Silberstein et al. (2010), or Segal (2014).
- It is well known that the representation theory of the symmetric group can be formulated using a combinatorial approach that is intimately connected to the partitions of some nonnegative integer n and their associated Young frames.
- A partition λ of n, denoted by λ n, is defined to be a tuple (λ 1 , λ 2 , . . . , λ k ) of nonnegative integers such that 1. λ 1 ≥ λ 2 ≥ . . . ≥ λ k > 0 and 2. We say that such a partition consists of k parts, and we define its length to be k.
- We denote the set of all partitions of n by Λ(S n ).
- As a matter of convention, when a partition has parts whose values are the same, instead of writing the parts separately, we group them together using power notation.
- For each partition λ n we associate a Young frame of shape λ, that is, a diagram consisting of k rows, where row i consists of λ i boxes (by convention, the row number increases downwards).
- We will denote a Young frame of shape λ also by λ, with the context making clear what is being referred to.
- Figure 1a) shows the Young frame for the partition (4, 2 2 ) 8.
- For each partition λ n we can assign to its corresponding Young frame of shape λ a set of coordinates, with the rows being numbered from top to bottom (with starting index 1) and the columns from left to right (also with starting index 1).
- We say that the box at coordinates (i, j) is removable if there is no box at the locations (i + 1, j) and (i, j + 1).
- We say that the box at coordinates (i, j) is addable if, when i = 1, j = λ 1 + 1, else, when i > 1, λ i = j − 1 < λ i−1 .
- Figure 1b) shows the removable (X) and addable (*) boxes for the Young frame of shape (4, 2 2 ) 8.
- For each partition λ n, a Young tableau of shape λ is a bijection between the set {1, 2, . . . , n} and the boxes of the Young frame of shape λ.
- We represent such a bijection diagrammatically by filling in the boxes of the Young frame with the numbers 1, 2, . . . , n such that each number appears in exactly one box.
- Figure 2: a) A Young tableau for...

### A Special Isomorphism between Two Representations of S n .
- The symmetric group is represented by the permutation representation M [n−1,1] .
- The permutation representation is extended linearly by the action of the symmetric group on the basis {v a | a ∈ [n]} .
- The k-tensor power of the permutation representation is the representation of the symmetric group that is given by (2.9) and is extended linearly.

### Permutation Equivariant Neural Networks
- permutation equivariant neural networks are constructed by alternately composing linear and non-linear permutation equivariant maps between representations of the symmetric group.
- We define permutation equivariance below: Definition 1
- A map φ : V → W is said to be permutation equivariant if ] for all g ∈ S n and v ∈ V.
- We denote the set of all linear permutation equivariant maps between V and W by Hom Sn (V, W ).
- When V = W , we write this set as End Sn (V ).
- It can be shown that Hom Sn (V, W ) is a vector space over C, and that End Sn (V ) is an algebra over C.
- A special case of permutation equivariant is permutation invariance: Definition 2
- The map φ given in Definition 1 is said to be permutation invariant if ρ W is defined to be the d-dimensional trivial representation of S n .
- Remark 3
- Since the d-dimensional trivial representation of S n can be decomposed into a direct sum of d irreducible representations of S n , each of which is the irreducible 1dimensional trivial representation of S n , we choose in the following to focus on permutation invariant functions that map to the 1-dimensional trivial representation C.
- We can now define the type of neural network that is the focus of this paper: such that the l th layer function is a map of representations of S n of a learnable, linear, permutation equivariant function φ l : (V l−1 , ρ l−1 ) → (V l , ρ l ) together with a fixed, non-linear activation function σ l : (V l , ρ l ) → (V l , ρ l ) such that
- 1. σ l is a permutation equivariant map, as in (2.11), and
- 2. σ l acts pointwise (after a basis has been chosen for each copy of V l in σ l .)
- We focus on the learnable, linear, permutation equivariant functions in this paper, since the non-linear functions are fixed.
- The entire neural network f NN is itself a permutation equivariant function because it can be shown that the composition of any number of permutation equivariant functions is itself permutation equivariant.
- One way of making a neural network of the form given in Definition 4 permutation invariant is by choosing the representation in the final layer to be the d-dimensional trivial representation of S n .
- Figure 4: Permutation Equivariant Neural Networks.
- Remark 6
- Figure 4 highlights the equivariance property of a permutation equivariant neural network. Specifically, the figure shows that if some v 0 ∈ V 0 is passed through the neural network (as represented by the left hand dotted box) with output equal to some v L := f NN (v 0 ) ∈ V L , then if ρ 0 (g)[v 0 ], for any g ∈ S n , is passed through the same neural network (as represented by the right hand dotted box), the output will be that is, the neural network respects the action of the symmetric group on both its input and its output.
- We assume throughout most of the paper that the feature dimension for all of our representations is one. This is because the symmetric group does not act on the feature space in most of the cases that we consider.
- We relax this assumption in Section 7.3.

## How Walking on a Graph results in a Tensor Power Representation
- module is the same as the quiver associated with the subgroup of the finite group . The McKay quiver associated with the S n -module V is a directed graph with potentially multiple arrows and loops: the elements of Q are called nodes, the elements of Q are called arrows, and if α is an arrow, then s(α) is called its start vertex and e(α) is called its end vertex.
- module is a directed graph with potentially multiple arrows and loops: the elements of Q are called nodes, the elements of Q are called arrows, and if α is an arrow, then s(α) is called its start vertex and e(α) is called its end vertex. The McKay quiver associated with the S n -module V is a quiver consisting of a set of nodes whose elements are the partitions λ n corresponding to the irreducible S n -modules {S λ n | λ ∈ Λ(S n )} over C, and a set of arrows where, for λ, µ ∈ Λ(S n ), there are α λ,µ arrows from λ to µ in Q V (S n ) if the irreducible S n -module S µ n appears with multiplicity α λ,µ in the decomposition of S λ n ⊗ V into irreducible S n -modules, that is, if If α λ,µ = α µ,λ , then the arrows between λ and µ ∈ Q V (S n ) are drawn without arrowheads, that is, there are α λ,µ undirected edges between λ and µ in Q V (S n ).
- module is a quiver consisting of a set of nodes whose elements are the partitions λ n corresponding to the irreducible S -modules {S λ n | λ ∈ Λ(S )} over C, and a set of arrows where, for λ, µ ∈ Λ(S ), there are α λ,µ arrows from λ to µ in Q V (S ) if the irreducible S -module S µ n appears with multiplicity α λ,µ in the decomposition of S ⊗ V into irreducible S -modules, that is, if If α λ,µ = α µ,λ , then the arrows between λ and µ ∈ Q V (S ) are drawn without arrowheads, that is, there are α λ,µ undirected edges between λ and µ in Q V (S ).
- module is a quiver consisting of a set of nodes whose elements are the partitions λ n corresponding to the irreducible S -modules {S λ n | λ ∈ Λ(S )} over C, and a set of arrows where, for λ, µ ∈ Λ(S ), there are α λ,µ arrows from λ to µ in Q V (S ) if the irreducible S -module S µ n appears with multiplicity α λ,µ in the decomposition of S ⊗ V into irreducible S -modules, that is, if If α λ,µ = α µ,λ ,...

## Schur-Weyl Duality and Centraliser Algebras
- module by using the McKay quiver for M , and how to calculate the multiplicities of each irreducible appearing in its decomposition. However, we can go further in understanding the structure of M by delving briefly into the theory of semisimple algebras. The most important result required is the Double Centraliser Theorem (see Stevens, 2016), which says: where U are all the simple modules of A and W are all the simple modules of B. Choosing A = C[S ], the group algebra of S , which is a semisimple algebra by Maschke's Theorem, and V = M , which is a finite dimensional vector space over C of dimension n , we see that B = End Sn (M ⊗k ) is a semisimple algebra and, most importantly, since that is, that all the simple modules ) can be indexed by the partitions of n, and so S is in Schur-Weyl duality with its centraliser algebra on M ⊗k , End Sn (M ⊗k ). Hence we see that the multiplicity of the irreducible S -module S in M ⊗k (see (3.11)) is really the dimension of the the irreducible End Sn (M ⊗k )-module Z , and so Z acts as the multiplicity space for S in the decomposition of M ⊗k into irreducibles of S . We have also shown that the McKay quiver for M n can be used to decompose M ⊗k n as an S n -module.
- module. We have also shown that the McKay quiver for M can be used to decompose M ⊗k as an -module. The Double Centraliser Theorem says that the centraliser of the centraliser of a semisimple algebra is semisimple.

## Understanding Permutation Equivariant Neural Networks through the Irreducibles of the Symmetric Group
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.
- module S λ n in M ⊗k n (see (3.11)). The number of walks of steps from the node [n] to the node λ in Q Mn (S n ). Mn , the th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom Theorem 10 The following are the same: The multiplicity of the irreducible S n -module S λ n in M ⊗k n (see (3.11)). 2. The number of walks of k steps from the node [n] to the node λ in Q Mn (S n ). Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). As before, this is no surprise given that, from (2.19), Hom However, in the remainder of this paper, we take a different approach, one that shows how a series of basic combinatorial diagrams gives us the form of the weight matrices in the standard basis of M n , for each of the cases presented in this section.

### The Number of Weights in a k-order Permutation Equivariant Layer Function
- The number of weights in a k-order permutation equivariant layer function is equal to the number of walks of length 2k that start and end at a node in the network.
- This number is the sum of the number of walks of length 2k that start and end at every node in the network.
- Therefore, the number of weights in a k-order permutation equivariant layer function is equal to the 2k th power of the adjacency matrix for the network.

### The Number of Weights in a k-order Permutation Invariant Layer Function
- The number of weights in a k-order permutation invariant layer function, dim Hom Sn (M ⊗k n , M ⊗0 n ), is equal to the number of walks of length k that start and end at (5.1)
- The multiplicity of the irreducible S n -module S [n] n in M ⊗k n . Mn , the k th power of the adjacency matrix A Mn for Q Mn (S n ). (5.2)
- By the definition of such a function, we see that dim Hom Sn (M ⊗k n , M ⊗0 n ) = m [n] k (5.3)
- Consequently, by Theorem 10, we also have that dim Hom Sn (M ⊗k n , M ⊗0 n ) is equal to 1.

## The Partition Algebra and its Consequences for k-order Permutation Equivariant Layer Functions
- Constructing the partition algebra P k (n)
- The orbit basis of P k (n) is the set of all bijective diagrams that correspond to partitions of a set of 2k elements
- Calculating the standard basis matrices of End Sn (M ⊗k n ) is possible by considering certain orbit basis diagrams of P k (n)

### The Partition Algebra, P k (n)
- There are a number of different possible choices for how to label these vertices, but we make an intentional decision to label the top row by 1, . . . , k and the bottom row by k + 1, . . . , 2k.
- We can create a set partition of [2k] by partitioning it into a number of subsets.
- We call the subsets of a set partition blocks.
- Let Π 2k be the set of all set partitions of [2k].
- It will also be useful to have a way to label the blocks in Π 2k .
- We draw edges between vertices in any manner such that the connected components that appear in the diagram d π correspond to the blocks of π.
- As a result, d π represents the equivalence class of all diagrams with connected components equal to the blocks of π.
- Figure 10 shows the diagram d π corresponding to the partition π given in (6.1).
- Figure 11 gives an example of the concatenation of two diagrams.
- We now define the partition algebra P k (n), for any n ∈ Z ≥1 and k ∈ Z ≥0 .
- Let P 0 (n) = C. Otherwise, define P k (n) to be the C-linear span of the set of all diagrams corresponding to a set partition in Π 2k , with the algebra product defined on the basis elements as follows:
- where c(d π 1 , d π 2 ) is the number of connected components that are removed from the middle row in the concatenation of d π 1 and d π 2 .
- It is clear that this product is associative, and that the algebra has an identity element corresponding to the set partition π.

### The Orbit Basis of P k (n)
- The diagram basis is a basis of P k (n)
- We can construct a basis of P k (n) in the following way: First, we define a partial ordering on the set partitions in Π 2k (for k ∈ Z ≥1 ), denoted by , which states that for all π 1 , π 2 ∈ Π 2k π 1 π 2 if every block of π 1 is contained in a block of π 2
- Then we can define a set of elements in P k (n) indexed by the set partitions of Π 2k , B O := {x π | π ∈ Π 2k }, with respect to the diagram basis as
- To see why the set B O forms a basis of P k (n), first, we form an ordered set of set partitions of Π 2k by ordering the set partitions by the number of blocks that they have from smallest to largest, with any arbitrary ordering allowed for a pair of set partitions that have the same number of blocks. Call this set S 2k
- Then, because the transition matrix between the diagram basis and the set B O , whose rows and columns are indexed (in order) by the ordered set S 2k , is unitriangular, by (6.5), and therefore invertible, we get that B O forms a basis of P k (n)
- We call the set of elements in P k (n) indexed by the set partitions of Π 2k the orbit basis.
- For more details on the orbit basis, specifically for how to express an orbit basis element as a linear combination of diagram basis elements, see Benkart and Halverson (2019a,b)

## Constructing Other Permutation Invariant and Equivariant Layer Functions through Related Partition Diagrams
- The partition algebra P k (n) can be adapted to find the weight matrices for other permutation equivariant layer functions.
- For each case, we construct a vector space consisting of some combinatorial diagrams whose orbit basis can be mapped onto a basis of the vector space of the permutation equivariant layer functions in question.
- Since many of the details are similar to those found in the previous section, we only present the key ideas for each case.
- Functions: Hom Sn (M ⊗k n , M ⊗l n )
- For l, k ∈ Z ≥0 , consider the set [l + k] having l + k elements, and define Π l+k to be the set of all set partitions of [l + k].
- Let Π l+k,n be the subset of Π l+k having at most n blocks.
- As before, the size of Π l+k is B(l + k), and the size of Π l+k,n is B(l + k, n).
- We can form a vector space from the C-linear span of a set of diagrams indexed by the elements of Π l+k , where each diagram in the set has two rows of vertices and edges between vertices such that there are 1. l black nodes on the top row, labelled by 1, . . . , l 2. k black nodes on the bottom row, labelled by l + 1, . . . , l + k, and 3. the edges between the vertices correspond to the connected components of the set partition that indexes the diagram.
- Denote this vector space by P l k (n).
- By construction, it has dimension B(l + k).
- As before, we call the basis described here the diagram basis.
- Note that the partition algebra P k (n) of Section 6.1 is P k k (n) as defined here.
- We can construct another basis, called the orbit basis, in much the same way as before: we have a partial ordering on Π l+k , , and we define a set of elements indexed by the set
- As before we can show that this set is a basis by considering the transition matrix between the diagram basis and this new set and seeing that it is invertible.
- We draw orbit basis diagrams with white nodes.
- For l, k ∈ Z ≥0 , n ∈ Z ≥1 , we have that is a basis of Hom Sn (M ⊗k n , M ⊗l n ), and so
- The critical idea for our purposes is that, for l, k ∈ Z ≥0 , n ∈ Z ≥1 , we can set up a map of vector spaces Φ l k,n : P l k (n) → Hom Sn (M ⊗k n , M ⊗l n ) by defining it on the orbit basis of P l k (n) and extending it linearly, as follows:
- X π if π has n or fewer blocks 0 if π has more than n blocks.

### Adding Features and Biases
- The standard basis matrices for the spaces under consideration are found in Remark 7.
- Including bias terms in the layer functions of a permutation equivariant neural network is harder, but it can be done.
- To find the matrix form of c, all we need to do is find the basis elements of Hom Sn (M ⊗0 n , M ⊗l n ). But we saw how to do this in Section 7.1, where we now set k = 0.

## A Generalisation to Layer Functions that are Equivariant to a Product of Symmetric Groups
- The permutation equivariant neural networks model local symmetries in data.
- The layers in these networks are the (external) tensor product of some k r -order tensor power of the permutation representation M nr of dimension n r of the symmetric group S nr .
- These permutation equivariant neural networks can be generalized to any number of groups with their accompanying representations.
- The map f : from the input space to the output space is well defined because the Hom-space given in (8.4) is isomorphic to Hom Sn r (M ⊗kr nr , M ⊗lr nr ) .
- The dimension of the subspace given in (8.9) is less than (and can be significantly less than) the dimension of the Hom-space given in (8.10).

## Related Work
- The combinatorial representation theory of the partition algebras has been developed over time by pure mathematicians
- Martin (1990Martin ( , 1994Martin ( , 1996)) was the first to construct the surjective algebra homomorphism seen in Section 6 between the partition algebra P k (n) and the tensor power centraliser algebra End Sn (M ⊗k n ).
- Jones (1994) was the first to construct the surjective algebra homomorphism seen in Section 6 between the partition algebra P k (n) and the tensor power centraliser algebra End Sn (M ⊗k n ).
- The development of much of the theory, however, has been led by Benkart and Halverson.
- Benkart and Moon (2018) wrote a paper on the topic of walking on graphs and its connection to the centraliser algebra.
- Benkart (2014) studied how the McKay quiver relates to Schur-Weyl duality.
- Benkart ( 2016) also looked at how to find the McKay quiver for representations of a number of groups using character theory.
- Halverson and Ram (2005) wrote a seminal paper on the partition algebra, with an emphasis on finding the simple modules of P k (n) in the case where P k (n) is semisimple.
- Halverson (2001) also studied the characters of the partition algebra.
- Bowman et al. (2013) looked at how the partition algebra can be used to describe the Kronecker coefficients coming from the tensor product of two Specht modules.
- The papers written by Benkart and Halverson (2019a,b) and Benkart et al. (2017), however, are especially important in the context of this paper as they show how the partition algebra can be used to construct the invariant theory of the symmetric group, with an emphasis on understanding the kernel of Jones' surjective algebra homomorphism when n < 2k.
- Halverson (2019) summarised the main results that appeared in these papers.
- Permutation equivariant neural networks, on the other hand, have been a hot topic of study in the artificial intelligence community.
- Zaheer et al. (2017) introduced the first permutation equivariant neural network, called Deep Sets, for learning from sets in a permutation equivariant manner.
- Sets of size n can be modelled as a 1-order tensor of the permutation representation of dimension n of S n .
- Hartford et al. (2018) considered permutation equivariant neural networks that learn the interactions between any number of sets; we saw in Section 8.1 that our method is able to recover their results.
- Maron et al. (2019) were the first to study the problem of classifying all of the linear permutation equivariant and invariant neural network layers, with their motivation coming from learning relations between the nodes of graphs.
- For example, the adjacency matrix of a graph with n nodes can be viewed as a 2-order tensor of the permutation representation of dimension n of S n .
- They characterised all of the learnable, linear, permutation equivariant layer functions in Hom Sn (M ⊗k n , M ⊗l n ) in the practical cases (specifically, when n ≥ k + l).
- They used some equalities involving Kronecker products to find a number of fixed point equations which they solved to find a basis, in tensor form, for the layer functions under consideration.
- In fact, the equations they find, in representation theoretic terminology, are actually the equations of the invariant subspace for M ⊗l n , for any l, and in obtaining the basis, they repeat, seemingly without knowledge, the construction given in Jones (1994).
- Our paper is the closest to Maron et al. (2019) in terms of the learnable, linear, permutation equivariant layer functions that are the focus of their study; however, our method for calculating a basis for the layer functions in Hom Sn (M...

## Conclusion
- We are the first to show how the combinatorial representation theory of the partition algebra and its related vector spaces provides the theoretical background for understanding permutation equivariant neural networks when the layers are some tensor power of the permutation represention of dimension n of S n .
- In the first half of the paper, we showed how the McKay quiver for this special representation allows us to decompose any tensor power of the representation into irreducibles of S n ; furthermore, using the fact that S n (technically, C[S n ]) is in Schur-Weyl duality with the centraliser algebra End Sn (M ⊗k n ) on M ⊗k n , we saw that the multiplicities of the irreducibles appearing in this decomposition are actually the dimensions of all of the simple modules of the centraliser algebra.
- We saw how this gave us a number of different ways of calculating the number of weights that appear in the layer functions of the permutation equivariant neural networks in question.
- In the second half of the paper, we looked at the problem of actually calculating the form of the layer functions themselves in the standard matrix basis. We saw how the orbit basis of the partition algebra and its related vector spaces allowed us to find this basis. We were also able to generalise this diagrammatic approach to layer functions that were equivariant to local symmetries, and we considered layer functions that are equivariant across the feature dimension for all of the tensor spaces.
- The critical idea underlying all of these results is that, as a result of the surjection of algebras Φ k,n : P k (n) → End Sn (M ⊗k n ), we see that S n is actually in Schur-Weyl duality with the partition algebra P k (n), and so, ultimately, in this paper, we have used the theory of the partition algebra to understand the structure of permutation equivariant neural networks when the layers are some tensor power of the permutation represention of dimension n of S n .
