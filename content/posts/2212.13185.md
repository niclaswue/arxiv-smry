---
title: "Fully Differentiable RANSAC"
date: 2022-12-26T15:13:13.000Z
author: "Tong Wei, Yash Patel, Jiri Matas, Daniel Barath"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13185v1.webp" # image path/url
    alt: "Fully Differentiable RANSAC" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13185)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13185).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/fully-differentiable-ransac).

# Abstract
- Fully differentiable $\nabla$-RANSAC predicts the inlier probabilities of the input data points
- Exploits the predictions in a guided sampler to estimate the model parameters (e.g., fundamental matrix) and its quality
- Propagates the gradients through the entire procedure
- The random sampler in $\nabla$-RANSAC is based on a clever re-parametrization strategy, i.e. the Gumbel Softmax sampler

# Paper Content

## Introduction
- A direct optimization on the test-time evaluation metric has shown to be beneficial in deep learning for many vision tasks
- In such cases, training with a surrogate of the metric has shown to be effective.
- Examples include surrogate losses of average precision and recall@k for image retrieval [15,17,58,62], perceptual loss for image compression [54,55], intersection-over-union loss for object detection [56,64,87], edit distance loss for scene text recognition [56,57], etc.
- However, a differentiable version of RANSAC [28] still does not exist, prohibiting a direct optimization on the evaluation metrics for several tasks that are utmost important in computer vision and robots.
- Robust estimation is a fundamental component in vision pipelines, including relative pose estimation [23], wide baseline matching [48,49,60], multi-model fitting [36,59], image-based localization [13], motion segmentation [77], and pose graph initialization of Structure-from-Motion (SfM) algorithms [67,69].
- While several robust estimators have been proposed throughout the years [33,34,44,85], randomized hypothesize-and-verify approaches, like RANSAC [28] and its recent variants [6][7][8]37], have become the most widely used methods due to their robustness, simplicity, and efficiency.
- RANSAC repeatedly selects random minimal subsets of the input data and fits a model, e.g., a 3D plane to three points or fundamental matrix to seven point correspondences. The model quality is then computed as the cardinality of its support, i.e., the number of points that fall closer than a manually set inlier threshold.
- RANSAC terminates when the probability of finding an all-inlier sample falls below a threshold, controlled by a confidence parameter typically set to 0.95 or 0.99.
- Finally, the model with the highest quality, polished, e.g., by least-squares fitting on all inliers, is returned.

## Fully Differentiable RANSAC
- Algorithmic components of the proposed ∇-RANSAC make it capable of end-to-end training
- A pipeline employing ∇-RANSAC is in Fig. 2
- The input is a set of tentative point correspondences, possibly equipped with extra information from the detector and matcher, e.g., feature orientation, scale, affine shape, and matching confidence
- An initial step of the algorithm is based on consensus learning via exploiting the pruning block from the recent [89]
- Instead of using the weights to iteratively remove outliers as in [89], we consider them initial inlier probabilities that are fed into the subsequent fully differentiable RANSAC module, ∇-RANSAC. Similarly to plain RANSAC, the proposed ∇-RANSAC is an iterative random sampling of m data points, model estimation via a minimal solver, and model scoring
- The devil is, however, in the details. The sampler is the Gumbel Softmax Sampler that exploits the input probabilities as guidance in a randomized manner while propagating the gradients.

### Gumbel Softmax Sampler
- The first step of ∇-RANSAC is hypothesis generation which requires sampling m data points from a set of n total samples.
- The sampling distribution is either governed by the importance scores for each sample or follows a uniform distribution.
- The standard sampling operation is either non-differentiable or has sparse gradients that are not useful for training.
- To make randomized importance score sampling differentiable for RANSAC, we extend the Gumbel-Softmax with the straight through trick to multiple samples.
- The sampling is performed on a distribution that is governed by the importance scores for each samples.

### Differentiable Minimal Solver
- Minimal solvers are a fundamental part of RANSAC-like hypothesize-and-verify approaches
- Most minimal solvers commonly used in computer vision are actually differentiable after taking each algorithmic component into careful considerations
- For essential matrix estimation, there are a number of 5PC solvers proposed throughout the years
- The steps of [72] are as follows: creating the coefficient matrix from the input points, decomposing it by SVD, solving a linear system of equations by, e.g., the Moore-Penrose pseudoinverse, and finally, recovering the linear combination of the null vectors via the Eigen decomposition of the action matrix
- Even though both SVD [84] and Eigen decompositions [70] have differentiable solutions, we found the Eigen decomposition to be unstable for gradient propagation
- The steps of the solver of Nister [53] are as follows: creating the coefficient matrix from the input correspondences, decomposing it by SVD, solving a linear system of equations, solving a set of polynomial equations, and finally, basic arithmetic operations
- We implemented a differentiable polynomial solver based on Sturm sequences [30] and, also, one using companion matrices
- While both algorithms work well, the companion matrix-based solution is the fastest
- We apply this solver in the proposed ∇-RANSAC

### Trainable Quality Function
- In RANSAC, the quality of an estimated model is calculated as the cardinality of its support, i.e., the inlier number.
- Since RANSAC, a number of algorithms [4,7,8,78] have improved the performance by better modelling the noise both in the inliers and outliers.
- However, all such methods perform a best model selection step based on the maximal quality which renders the procedure non-differentiable.
- Some works [13] tackle with this problem by employing soft probabilistic hypothesis selection.
- Other methods [86,89] combine classification loss with regression and geometry-induced losses [14] to reason about the quality of the estimated model and inlier assignments.
- Instead of the above solutions, we exploit all models { θi } t i=1 estimated during the fixed t ∈ N >0 iterations.
- In each iteration, the estimated model is compared to the ground truth and its implied loss is calculated, e.g., as the relative pose error in case of essential matrix estimation.
- The minimized loss, in case of relative pose estimation, is L pose calculated as the average over all models as follows:
- where the upper index refers to the rotation (R ∈ SO(3)) and translation components (t ∈ R 3 ) of the pose, θ is the ground truth, and functions R and t return the rotation and translation errors, respectively.
- Functions Besides L pose we employ other widely used objectives too.
- Given the probabilities Pi = {p i j } n j=1 that model θ i implies in the ith iteration (p i j ∈ [0, 1]), we calculate classification loss L inlier w.r.t. the inlier probabilities P = {p j } n j de-termined by the ground truth model.
- Probabilities {p j } n j=1 can be calculated from the point-to-model residuals making certain assumptions about the noise.
- In our algorithm, we follow the RANSAC assumption and p i ∈ {0, 1} is a binary mask.
- The implied classification loss is as follows:
- Also, we compute a loss L epi summing the average symmetric epipolar errors as follows:
- where I is the inlier set selected by the ground truth model.
- The overall loss minimized in our training procedure is the linear combination of the above ones as follows:
- where w α , w β , w γ ∈ R are the weighting parameters.

### Training and Testing Details
- Input of ∇-RANSAC is a set of correspondences obtained by any feature detector and matcher.
- The number of matches is fixed to 2000.
- We choose the best 2000 ones, based on the matching score, if we are given more.
- In case of having fewer correspondences, we fill up the missing values with zeros.
- We extract local and global features from the correspondences by a consensus learning block as backbone [89].
- We integrate over the extracted geometric information, e.g., scale, and orientation of the local descriptors to help learn the qualities of correspondences.
- Training. First, we apply a 1000 epoch-long weight initialization procedure that minimizes the Kullback-Leibler divergence [83] of the predicted and target probabilities [14].
- Along with the initialized weights, the gradient clipping [19] technique is used to avoid exploding gradients, make the training stable and accelerate the convergence.
- In the F matrix case, we normalize the point coordinates for consensus learning, and use the original unnormalized ones in minimal solvers.
- We train the pipeline using the 8PC algorithm for F estimation with a fixed 1000 iterations.
- For E matrices, we use the 5PC algorithm and 100 iterations.
- The entire training pipeline is implemented in PyTorch.
- Inference. Since in test time, the back-propagation of the gradients is not required, we equip ∇-RANSAC with stateof-the-art components.
- The end-to-end trained model provides importance scores to the Gumbel Softmax Sampler.
- For selecting the best model, we use the MAGSAC++ [7] model quality function, marginalizing over a range of noise scales.
- We also apply an inner RANSAC-based local optimization [42] to further improve the accuracy.
- Also, we perform the Levenberg-Marquardt [51] numerical optimization minimizing the pose error on all inliers as a final step.

## Experimental Results
- We test epipolar geometry estimation on 13 scenes from the training set of the CVPR IMW 2020 PhotoTourism benchmark
- We train and validate the method on scene St. Peter's Square consisting a total of 4950 image pairs
- We compare ∇-RANSAC on fundamental (Section 3.1) and essential matrix (Section 3.2) estimation, to classical robust estimators, i.e., RANSAC [28], LMEDS [66], or their recent alternatives, like GC-RANSAC [6], EAS [27], MAGSAC [8], and MAGSAC++ [7]
- We re-implemented RANSAC in PyTorch and made it easy to connect with other modules in an end-to-end fashion

### Fundamental Matrix Estimation
- Fundamental matrix estimation is performed on 12000 image pairs from the PhotoTourism dataset.
- The proposed ∇-RANSAC achieves the most accurate Method AUC@5 • AUC@10 • AUC@20 • Time (ms) LMEDS [66] 0.24 0.30 0.37 27 RSC [28] 0.26 0.32 0.40 88 GC-RSC [6] 0.33 0.37 0.42 175 MSC [8] 0.37 0.42 0.47 239 MSC++ [7] 0.37 0.42 0.47 113 EAS [27] 0.24 0.28 0.34 325 OANet [88] 0 results.
- Compared to the second best method, NG-RANSAC, ∇-RANSAC runs at comparable speed and achieves better results on all but one scene.
- The most efficient method is LMeDS achieving 11.61% lower F1 score than ∇-RANSAC.

### Essential Matrix Estimation
- We evaluate our method for E estimation with the same train, validation, and test scenes as F estimation.
- The correspondences are normalized by the intrinsic matrices.
- We use the differentiable 5PC algorithm when training on essential matrix estimation as it leads to more stable solutions than the 8PC solver in our experiments.
- We train the method end-to-end for 10 epochs with weight initialization and gradient clipping.
- The iteration number is fixed to 100.
- To evaluate the estimated essential matrix, we decompose the E matrix to rotation and translation, calculate their errors R , t and report the maximum max( R , t ).
- We calculate the Area Under the Recall curve (AUC) thresholded at 5 • , 10 • and 20 • [14].
- The AUC scores, averaged over all scenes, are reported in Table 3.
- The highest AUC scores, at all thresholds, are achieved by ∇-RANSAC.
- For example, its AUC@5 • score is higher than that of the second best methods (i.e., MAGSAC and MAGSAC++) by AUC 3 points.
- It has comparable run-times to other less accurate alternatives.
- The results of minimal solvers are reported in Table 4.
- The highest AUC scores, at all thresholds, are achieved by the five-point algorithm, confirming the necessity of use better minimal solvers instead of the 8PC algorithm.
- The effect of the weight initialization with the Kullback-Leibler divergence [83] with different minimal solvers on essential matrix estimation is shown in the left plot of Fig. 5.
- For both minimal solvers, the initialization clearly improves the final accuracy.
- Training with the 5PC solver without performing initialization leads to the same results as training with 8PC with initialization.
- The best results are obtained by the 5PC solver with pre-trained weights. This again confirms the importance of using other solvers than the 8PC algorithm.
- The right plot of Fig. 5 shows different sampling weights.
- For this plot, we only used scene Sacre Coeur as test set.
- Uniform sampler is basically the results of plain RANSAC.
- Gumbel Sampler with SNN ratios requires no training. It uses the pre-calculated SNN ratios to guide the sampling.
- Finally, the effect of training with different loss functions is shown.
- On this dataset and problem, the epipolar error leads to the best results.
- An advantage of the fully differentiable RANSAC is that it allows the RANSAC pipeline to be optimized for testtime evaluation metrics such as pose error, inlier classification, and epipolar error (see Sec. 2.3).
