---
title: "OPT: Open Pre-trained Transformer Language Models"
date: 2022-05-02T17:49:50.000Z
author: "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2205-01068v4.webp" # image path/url
    alt: "OPT: Open Pre-trained Transformer Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.01068)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.01068).


# Abstract
- Large language models can be effectively replicated without significant capital
- Open Pre-trained Transformers (OPT) provide decoder-only pre-trained models for 125M-175B parameters, which are comparable to GPT-3 while requiring only 1/7th the carbon footprint to develop
- The logbook detailing the infrastructure challenges faced, as well as code for experimenting with all of the released models, is being released.

# Paper Content

## Introduction
- Large language models (LLMs) can generate text and perform zero-and few-shot learning
- LLMs are currently limited to only a few highly resourced labs
- We are releasing a suite of decoderonly pre-trained transformers that we aim to fully and responsibly share with interested researchers
- Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale
- The entire AI community -academic researchers, civil society, policymakers, and industry -must work together to develop clear guidelines around responsible AI in general and responsible LLMs in particular

## Method

### Models
- Transformer language models can have up to 175 billion parameters
- The models were designed to be transparent and to reduce the risk of training instability
- The models were based off of work done by Brown et al. in 2020

### Training Setup
- For weight initialization, we use a normal distribution with zero mean and standard deviation of 0.006
- Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers
- All bias terms are initialized as 0, and all models are trained with ReLU activation and a sequence length of 2048
- We use an AdamW optimizer (Loshchilov and Hutter, 2017) with (β 1 , β 2 ) set to (0.9, 0.95), and weight decay of 0.1
- We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens
- A number of mid-flight changes to LR were also required (see Section 2.5)

### Pre-training Corpus
- The pre-training corpus contains a concatenation of datasets used in RoBERTa (Liu et al., 2019b), the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020;Roller et al., 2021).
- All corpora were previously collected or filtered to contain predominantly English text, but a small amount of non-English data is still present within the corpus via CommonCrawl.
- We removed duplicated documents across all datasets by filtering out documents via Min-hashLSH (Rajaraman and Ullman, 2011) with a Jaccard similarity ≥ .95.
- We found the Pile was particularly full of duplicate documents, and advise future researchers using the Pile to perform additional de-duplication processing.
- We tokenize all corpora using the GPT-2 byte level BPE tokenizer (Sennrich et al., 2016;Radford et al., 2019;Brown et al., 2020).
- Our final corpus contains roughly 180B tokens.
- RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021.
- This CC-News v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).
- The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, Hack-erNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia.
- Other subsets of the Pile were eliminated as we found they increased the risk of instabilities, as measured by tendency to cause spikes in gradient norms at the 1.3B scale, or were otherwise deemed unsuitable.
- All subsets went through additional ad-hoc whitespace normalization.
- PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021).
- To convert the conversational trees into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.

### Training Efficiency
- OPT-175B was trained on 992 GPUs using Fully Sharded Data Parallel (Artetxe et al., 2021)
- The model weights were kept in FP16, but the Adam state was in FP32
- To avoid underflows, dynamic loss scaling was used

### Training Processes
- Hardware failures caused at least 35 manual restarts and the cycling of over 100 hosts over the course of 2 months.
- Loss divergences were also an issue in our training run.
- When the loss diverged, we found that lowering the learning rate and restarting from an earlier checkpoint allowed for the job to recover and continue training.
- We noticed a correlation between loss divergence, our dynamic loss scalar crashing to 0, and the l 2 -norm of the activations of the final layer spiking.
- These observations led us to pick restart points for which our dynamic loss scalar was still in a "healthy" state (≥ 1.0), and after which our activation norms would trend downward instead of growing unboundedly.
- Our empirical LR schedule is shown in Figure 1.
- Early in training, we also noticed that lowering gradient clipping from 1.0 to 0.3 helped with stability; see our released logbook for exact details.
- Figure 2 shows our validation loss with respect to training iterations.

## Evaluations

### Prompting & Few-Shot
- We evaluate our model on 16 standard NLP tasks
- We follow GPT-3 by using their prompts and overall experimental setup
- We compare primarily to GPT-3, having aimed to re-implement their evaluation settings, but include reported performance of other LLMs on a per-task basis when available
- For the Winograd Schema Challenge (WSC) task in the SuperGLUE benchmark, we follow (Brown et al., 2020) and formulate the task as multiple choice questions, which is known to affect performance
- Zero-shot Overall average zero-shot performance across all 14 tasks may be seen in Figure 3
- Overall, we see our average performance follows the trend of GPT-3. However, performance can vary radically across the tasks: for a full breakdown, see Appendix A
- Chinchilla (Hoffmann et al., 2022) and Gopher (Rae et al., 2021) perform roughly consistently with others for their parameter sizes, while PaLM (Chowdhery et al., 2022) generally performs better across all settings, even when controlling for number of parameters
- We speculate the high performance of PaLM comes predominantly from higher quality and diversity of pre-training data

### Dialogue
- LLMs are known to be an integral component of modern dialogue models
- OPT-175B outperforms the alsounsupervised Reddit 2.7B model on all tasks
- OPT-175B performs competitively with the fully supervised BlenderBot 1 model, especially in the ConvAI2 dataset
- OPT-175B is generalizing well across multiple PersonaChat-like datasets

## Bias & Toxicity Evaluations
- OPT-175B has been found to be less accurate than other hate speech detection algorithms
- The benchmarks used to evaluate OPT-175B are not representative of the real world
- OPT-175B has been found to be less accurate than other hate speech detection algorithms
- The benchmarks used to evaluate OPT-175B are not representative of the real world
- OPT-175B has been found to be less accurate than other hate speech detection algorithms
- The benchmarks used to evaluate OPT-175B are not representative of the real world
- The benchmarks used to evaluate OPT-175B are not representative of the real world
- The benchmarks used to evaluate OPT-175B are not representative of the real world
- The benchmarks used to evaluate OPT-175B are not representative of the real world

### Hate Speech Detection
- OPT-175B is better than Davinci at identifying whether or not English statements are racist or sexist
- The better performance is likely due to the addition of safety controls and the presence of unmoderated social media discussions in the pre-training dataset

### CrowS-Pairs
- CrowS-Pairs is a masked language model that was developed by Nangia et al. (2020)
- When compared to Davinci, OPT-175B appears to exhibit more stereotypical biases in almost all categories except for religion
- This is likely due to differences in training data; Nangia et al. (2020) showed that the Pushshift.io Reddit corpus has a higher incidence rate for stereotypes and discriminatory text than other corpora (e.g. Wikipedia)
- Given this is a primary data source for OPT-175B, the model may have learned more discriminatory associations, which directly impacts its performance on CrowS-Pairs.

### StereoSet
- Lieber et al. (2021) used StereoSet to measure stereotypical bias across 4 categories: profession, gender, religion, and race
- Davinci and OPT-175B both outperform in the areas of profession and race, while OPT-175B outperforms in the areas of Gender and Religion
- OPT-175B performs better across the board on the SS metric, while Davinci generally outperforms on the LMS metric

### RealToxicityPrompts
- The OPT-175B model has a tendency to generate toxic language
- The OPT-175B model is more toxic than other models
- The inclusion of unmoderated social media texts in the pre-training corpus raises model familiarity with, and therefore propensity to generate and detect, toxic text

### Dialogue Safety Evaluations
- OPT-175B is a model that is similar to the Reddit 2.7B model in terms of performance
- OPT-175B has better performance when it is compared to the Reddit 2.7B model in terms of safety
- Future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile.

## Limitations
- The OPT-175B model performs similarly to other released models
- The OPT-175B model is prone to generating toxic language and reinforcing harmful stereotypes
- Mitigations for toxicity and biases are needed in order to make the model safe for use in real world applications

## Conclusion
- OPT is a collection of auto-regressive language models ranging in size from 125M to 175B parameters
- Training details, evaluation performance, and characterization of behaviors with respect to bias, toxicity, and hate speech are described
- Guidelines for responsible LLMs are discussed

## A Additional Evaluations

## B Contributions

## D Model Card
- OPT-175B was developed by Meta AI
- OPT-175B was released on May 3, 2022
- OPT-175B described in this paper is version 1.0.0
- OPT-175B is a large decoder-only transformer language model
- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to 175B. See the Data Card (Appendix C) for information about training data and Section 2.2 -2.5 for information about the training process.
- Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository.
- License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license.
- Where to send questions or comments about the model: Please contact the corresponding authors {susanz,roller,namangoyal}@fb.com for any questions or comments.

### D.3 Data, Limitations, and Recommendations
- Data selection for training: Training data for OPT-175B was selected based on a combination of breadth and availability.
- Data selection for evaluation: Evaluations in this paper were chosen to provide comparable performance assessments relative to similar scale models in the literature.
- Limitations: Like other large language models for which the diversity (or lack thereof) of training data induces downstream impact on the quality of our model, OPT-175B has limitations in terms of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and hallucination.
- Recommendations for future work: See Section 6 for more about our Considerations for Release, including a discussion of potential avenues of research enabled by opening our model to more of the research community. We hope that the release of OPT-175B, as well as information around our model training process, will increase open science around both large language models in specific and natural language processing and deep learning in general.

## E Sample Model Outputs
- For all sample outputs, the initial prompt is given in bold and the remainder is the continuation.
- We found that lowering learning rate was helpful for avoiding instabilities.
- Figure 5: RealToxicityPompts. OPT-175B is more likely to generate toxic responses than either Davinci or PaLM. Consistent with prior work, toxicity rates increase as prompt toxicity increases.
- Figure 6: Zero-shot NLP Evaluations. Full evaluations on all 16 NLP tasks, with comparisons where available. We find that across most tasks, GPT-3 models and OPT models perform similarly, but some tasks display highly erratic behavior.
- Figure 7: Multishot-shot NLP Evaluations. Full evaluations on all 16 NLP tasks, with comparisons to the GPT-3 reported performance. As with zero-shot, performance is roughly similar for most tasks, with some tasks demonstrating erratic behavior.
- Figure 8: Poetry generation.We have observed the model can write entertaining poetry on topics such as dodos, samosas, and performance reviews. However, we struggled to get the model to observe rhyme or meter.
- Figure 9: Conversation generation. OPT-175B adopts a patriotic personality when prompted as the Statue of Liberty. However, the model also devolves into somewhat simple and linguistically repetitive generations further into the conversation.
- Figure 10: Basic few-shot translation example. OPT was not intentionally trained to be multilingual, but we found anecdotally it has limited success with simple translations in German, Spanish, French, and Chinese.
- Figure 11: Paper writing example. Prompting with "1. Introduction" generally yielded more interesting results compared to prompting with "Abstract." Our prompt here was inspired by the first sentence of the seminal ResNet work (He et al., 2016).
- Figure 12: Arithmetic. We observe mistakes when extending from addition to other operations.
- Figure 13: Python programming. Simply switching out a variable name can alter the generated output.
