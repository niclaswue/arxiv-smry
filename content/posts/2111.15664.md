---
title: "OCR-free Document Understanding Transformer"
date: 2021-11-30T18:55:19.000Z
author: "Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2111-15664v5.webp" # image path/url
    alt: "OCR-free Document Understanding Transformer" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2111.15664)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2111.15664).


# Abstract
- Understanding document images (e.g., invoices) is a core but challenging task
- current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs
- although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process
- to address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer
- Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy
- in addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut

# Paper Content

## Introduction
- Reading document images to extract information
- OCR-dependent methods have problems such as high computational cost and poor generalization ability
- Post-OCR correction module is not practical
- We propose a novel OCR-free approach that is based on a Transformer
- We show Donut can be easily extended to a multi-lingual setting

## Method

### Preliminary: background
- There have been various VDU methods to understand and extract essential information from the semi-structured documents
- Earlier VDU attempts have been done with OCR-independent visual backbones, but the performances are limited
- Later, with the remarkable advances of OCR, various OCR-dependent VDU models have been proposed
- Most state-of-the-arts use both powerful OCR engines and large-scale real document image data for a model pre-training
- Although they showed remarkable advances in recent years, extra effort is required to ensure the performance of an entire VDU model by using the off-the-shelf OCR engine

### Document Understanding Transformer
- Donut is an end-to-end VDU model for general understanding of document images
- The architecture of Donut is quite simple, which consists of a Transformer-based visual encoder and textual decoder modules
- Donut does not rely on any modules related to OCR functionality but uses a visual encoder for extracting features from a given document image
- The textual decoder maps the derived features into a sequence of subword tokens to construct a desired structured format (e.g., JSON)
- Each model component is Transformer-based, and thus the model is trained easily in an end-to-end manner
- The overall process of Donut is illustrated in Figure 3

### Pre-training
- A low-resolution input image
- A high-resolution input image
- A low-resolution output image
- A high-resolution output image
- A low-resolution text image
- A high-resolution text image
- A low-resolution token image
- A high-resolution token image
- A low-resolution context image
- A high-resolution context image
- A low-resolution next token image
- A high-resolution next token image
- A low-resolution loss function
- A high-resolution loss function
- A low-resolution cross entropy
- A high-resolution cross entropy
- A low-resolution error
- A high-resolution error
- A low-resolution prediction
- A high-resolution prediction
- A low-resolution loss
- A high-resolution loss
- A low-resolution prediction error
- A high-resolution prediction error
- A low-resolution prediction loss
- A high-resolution prediction loss
- A low-resolution text error
- A high-resolution text error
- A low-resolution token error
- A high-resolution token error
- A low-resolution context error
- A high-resolution context error
- A low-resolution next token error
- A high-resolution next token error
- A high-resolution loss
- A high-resolution loss
- A high-resolution cross entropy
- A high-resolution cross entropy
- A high-resolution error
- A high-resolution error
- A high-resolution prediction
- A high-resolution prediction
- A high-resolution prediction error
- A high-resolution prediction loss
- A high-resolution prediction loss
- A low-resolution text error
- A high-resolution text error
- A low-resolution token error
- A high-resolution token error
- A low-resolution context error
- A high-resolution context error
- A low-resolution next token error
- A high-resolution next token error
- A low-resolution loss
- A low-resolution loss
- A low-resolution cross entropy
- A low-resolution cross entropy
- A low-resolution error
- A low-resolution error
- A low-resolution prediction
- A low-resolution prediction
- A low-resolution prediction error
- A low-resolution prediction loss
- A low-resolution prediction loss
- A low-resolution text error
- A high-resolution text error
- A low-resolution token error
- A high-resolution token error
- A low-resolution context error
- A high-resolution context error
- A low-resolution next token error
- A high-resolution next token error
- A high-resolution loss
- A high-resolution loss
- A high-resolution cross entropy
- A high-resolution cross entropy
- A high-resolution error
- A high-resolution error
- A high-resolution prediction
- A high-resolution prediction
- A high-resolution prediction error
- A high-resolution prediction loss

### Fine-tuning
- After the model learns how to read, in the application stage (i.e., fine-tuning), we teach the model how to understand the document image.
- As shown in Figure 3, we interpret all downstream tasks as a JSON prediction problem.
- The decoder is trained to generate a token sequence that can be converted into a JSON that represents the desired output information.
- For example, in the document classification task, the decoder is trained to generate a token sequence [START class][memo][END class] which is 1-to-1 invertible to a JSON {"class": "memo"}.

## Experiments and Analyses
- Donut fine-tuning results on three VDU applications on six different datasets
- The samples are shown in Figure 5.

### Downstream Tasks and Datasets
- The model can distinguish across different types of documents
- The model can extract document information (IE)
- The model can predict the answer to a question from a document
- The model can generate the answer to a question by setting the question as a starting prompt

### Setups
- We use Swin-B [40] as a visual encoder of Donut with slight modification
- We set the layer numbers and window size as {2, 2, 14, 2} and 10
- In further consideration of the speed-accuracy trade-off, we use the first four layers of BART as a decoder
- As explained in Section 2.3, we train the multi-lingual Donut using the 2M synthetic and 11M IIT-CDIP scanned document images
- We pre-train the model for 200K steps with 64 A100 GPUs and a mini-batch size of 196
- We use Adam [30] optimizer, the learning rate is scheduled and the initial rate is selected from 1e-5 to 1e-4
- The input resolution is set to 2560Ã—1920 and a max length in the decoder is set to 1536
- All fine-tuning results are achieved by starting from the pre-trained multi-lingual model
- Some hyperparameters are adjusted at fine-tuning and in ablation studies
- We fine-tune the model while monitoring the edit distance over token sequences
- The speed of Donut is measured on a P40 GPU, which is much slower than A100

### Experimental Results
- Document classification: Donut outperforms the state-of-the-art general-purpose VDU models such as LayoutLMv2 and LayoutLM.
- Document information extraction: Donut achieves competitive scores with the baselines that are dependent on external OCR engines.
- Document visual question answering: Donut shows promising results without OCR.

### Further Studies
- Testing different pre-training tasks for VDUs
- Transformer-based architecture outperforms others
- Attention mechanism can avoid the matter in architectural design

### Optical Character Recognition
- Recent trends of OCR study involve using deep learning models to decode text from images.
- Methods used to decode text from images include CNNs and component-level approaches.

### Visual Document Understanding

## Conclusions
- Donut directly maps an input document image into a desired structured output
- Unlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion
- We also propose a synthetic document image generator, SynthDoG, to alleviate the dependency on large-scale real document images
- Our extensive experiments and analysis on both external public benchmarks and private internal service datasets show higher performance and better costeffectiveness of the proposed method

### A.3 Details of Document Information Extraction
- Information extraction is an arduous task
- Previous works have only focused on extracting several pre-defined key information
- In this section, for explanation purposes, we show some sample images (which are the raw input of the IE pipeline) with the output of Donut
- We go beyond the previous works by considering (c) also.

### A.4 Details of Model Training Scheme and Output Format
- The Transformer encoder-decoder architecture is used
- The teacher-forcing training scheme is used
- The cost increased rapidly when using a larger dataset, e.g., RVL-CDIP or DocVQA
- Using an efficient attention mechanism may avoid the problem

### A.6 Preliminary Experiments in Smaller Resources
- Donut was pre-trained on smaller data and fewer GPUs
- Donut is able to generalize better than other deep learning models
- Donut is able to learn faster than other deep learning models
- Donut is able to learn from smaller data sets
- Donut was pre-trained on smaller data and fewer GPUs
- Donut is able to generalize better than other deep learning models
- Donut is able to learn faster than other deep learning models
- Donut is able to learn from smaller data sets

### A.7 Details of OCR-dependent Baseline Models
- Conventional backbones, such as LayoutLM and LayoutLMv2, perform downstream VDU tasks by extracting text from a given image and converting it to a sequence of text tokens
- The goal of document IE is to extract the structured information from a given semistructured document image
- Text detection is conducted to obtain text locations and each box is passed to the recognizer to comprehend characters
- The proposed end-to-end model, Donut, outperforms the recent OCR-dependent VDU models in memory, time cost and accuracy
- Performances on visual document IE are shown in Figure 2 and more results on various VDU tasks are available at Section 3
- Donut is trained without any supervision for the localization
- Donut achieves state-of-the-art performance with reasonable speed and model size efficiency
