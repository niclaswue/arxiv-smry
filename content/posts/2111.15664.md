---
title: "OCR-free Document Understanding Transformer"
date: 2021-11-30T18:55:19.000Z
author: "Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2111-15664v5_bo4jj-qnm.jpg" # image path/url
    alt: "OCR-free Document Understanding Transformer" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2111.15664)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2111.15664).


# Abstract
- Understanding document images (e.g., invoices) is a core but challenging task
- Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs
- Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process.
- To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer.
- As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss).
- Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.

# Paper Content

## Introduction
- Reading text from images
- OCR-dependent approach has critical problems
- Off-the-shelf OCR methods rarely have flexibility dealing with different languages or domain changes
- Pre-training phase enables the utilization of synthetic data
- Extensive experiments and analyses are conducted on both public benchmarks and private industrial datasets

## Method

### Preliminary: background
- There have been various VDU methods to understand and extract essential information from the semi-structured documents
- Earlier VDU attempts have been done with OCR-independent visual backbones, but the performances are limited
- Later, with the remarkable advances of OCR and BERT, various OCR-dependent VDU models have been proposed
- Most state-of-the-arts use both powerful OCR engines and large-scale real document image data for a model pre-training
- Although they showed remarkable advances in recent years, extra effort is required to ensure the performance of an entire VDU model by using the off-the-shelf OCR engine

### Document Understanding Transformer
- Donut is an end-to-end VDU model for general understanding of document images
- The architecture of Donut is quite simple, which consists of a Transformer-based visual encoder and textual decoder modules
- Donut does not rely on any modules related to OCR functionality but uses a visual encoder for extracting features from a given document image
- The textual decoder maps the derived features into a sequence of subword tokens to construct a desired structured format (e.g., JSON)
- Each model component is Transformer-based, and thus the model is trained easily in an end-to-end manner
- The overall process of Donut is illustrated in Figure 3

### Pre-training
- A low-resolution input image
- A high-resolution input image
- A low-resolution output image
- A high-resolution output image
- A low-resolution synthesized document
- A high-resolution synthesized document
- A low-resolution synthesized document label
- A high-resolution synthesized document label
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document text label
- A high-resolution synthesized document text label
- A low-resolution synthesized document text region
- A high-resolution synthesized document text region
- A low-resolution synthesized document image region
- A high-resolution synthesized document image region
- A low-resolution synthesized document text region
- A high-resolution synthesized document text region
- A low-resolution synthesized document image region
- A high-resolution synthesized document image region
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized document text
- A high-resolution synthesized document text
- A low-resolution synthesized document image
- A high-resolution synthesized document image
- A low-resolution synthesized...

### Fine-tuning
- After the model learns how to read, in the application stage (i.e., fine-tuning), we teach the model how to understand the document image.
- As shown in Figure 3, we interpret all downstream tasks as a JSON prediction problem.
- The decoder is trained to generate a token sequence that can be converted into a JSON that represents the desired output information.
- For example, in the document classification task, the decoder is trained to generate a token sequence [START class][memo][END class] which is 1-to-1 invertible to a JSON {"class": "memo"}.

## Experiments and Analyses
- Donut fine-tuning results on three VDU applications on six different datasets
- The samples are shown in Figure 5.

### Downstream Tasks and Datasets
- Document Classification: To see whether the model can distinguish across different types of documents, we test a classification task.
- Document Information Extraction: To see the model fully understands the complex layouts and contexts in documents, we test document information extraction (IE) tasks on various real document images including both public benchmarks and real industrial datasets. In this task, the model aims to map each document to a structured form of information that is consistent with the target ontology or database schema.
- Document Visual Question Answering: To validate the further capacity of the model, we conduct a document visual question answering task (DocVQA). In this task, a document image and question pair is given and the model predicts the answer for the question by capturing both visual and textual information within the image.
- DocVQA: The dataset is from Document Visual Question Answering competition 4 and consists of 50K questions defined on more than 12K documents.

### Setups
- We use Swin-B [40] as a visual encoder of Donut with slight modification
- We set the layer numbers and window size as {2, 2, 14, 2} and 10
- In further consideration of the speed-accuracy trade-off, we use the first four layers of BART as a decoder
- As explained in Section 2.3, we train the multi-lingual Donut using the 2M synthetic and 11M IIT-CDIP scanned document images
- We pre-train the model for 200K steps with 64 A100 GPUs and a mini-batch size of 196
- We use Adam [30] optimizer, the learning rate is scheduled and the initial rate is selected from 1e-5 to 1e-4
- The input resolution is set to 2560×1920 and a max length in the decoder is set to 1536
- All fine-tuning results are achieved by starting from the pre-trained multi-lingual model
- Some hyperparameters are adjusted at fine-tuning and in ablation studies
- We fine-tune the model while monitoring the edit distance over token sequences
- The speed of Donut is measured on a P40 GPU, which is much slower than A100

### Experimental Results
- Document Classification: Donut outperforms other general-purpose VDU models such as LayoutLMv2 and BIO-tagging-based named entity recognition.
- Document Information Extraction: Donut extracts key information from images with low resource usage and stable performance.
- Document Visual Question Answering: Donut achieves competitive scores with baselines that are dependent on external OCR engines.

### Further Studies
- Testing different pre-training tasks for VDUs
- Transformer-based backbones outperform others on both datasets
- Images with many tiny texts show more robust performances with a larger input resolution

### Optical Character Recognition
- Recent trends of OCR study involve using deep learning models to decode text from images.
- Deep learning models are typically used to decode text from images in two steps: first, a detector predicts text areas in the image, and second, a recognizer recognizes all characters in the predicted text areas.
- Early detection methods used CNNs to predict local segments and apply heuristics to merge them.
- Later, region proposal and bounding box regression based methods were proposed.
- Recently, focusing on the homogeneity and locality of texts, component-level approaches were proposed.
- Many modern text recognizer share a similar approach that can be interpreted into a combination of several common deep modules.

### Visual Document Understanding
- Classification of document type is a core step towards automated document processing
- Early methods treated the problem as a general image classification, so various CNNs were tested
- Recently, with BERT, the methods based on a combination of CV and NLP were widely proposed
- As a common approach, most methods rely on an OCR engine to extract texts; then the OCR-ed texts are serialized into a token sequence; finally they are fed into a language model (e.g., BERT) with some visual features if available.
- Although the idea is simple, the methods showed remarkable performance improvements and became a main trend in recent years
- Document IE covers a wide range of real applications
- Most recent models take the output of OCR as their input.
- However, the methods work in an extractive manner by their nature, and there are some concerns for the question whose answer does not appear in the given image.
- To tackle the concerns, generation-based methods have also been proposed.

## Conclusions
- Donut directly maps an input document image into a desired structured output
- Unlike conventional methods, Donut does not depend on OCR and can easily be trained in an end-to-end fashion
- We also propose a synthetic document image generator, SynthDoG, to alleviate the dependency on large-scale real document images
- Our extensive experiments and analysis on both external public benchmarks and private internal service datasets show higher performance and better costeffectiveness of the proposed method

### A.3 Details of Document Information Extraction
- Information extraction is an arduous task
- In this section, we show some sample images (which are the raw input of the IE pipeline) with the output of Donut
- The task is complex, but its interface is simple

### A.4 Details of Model Training Scheme and Output Format
- The Transformer encoder-decoder architecture
- The Transformer library is used for the BERT, BROS, LayoutLMv2, and WYVERN baselines
- The major hyperparameters, such as the initial learning rate and number of epochs, are adjusted by monitoring the scores on the validation set
- The OCR-dependent VDU backbone baselines (LayoutLM and LayoutLMv2) are available in Appendix A.7

### A.6 Preliminary Experiments in Smaller Resources
- Donut was pre-trained on smaller data and fewer GPUs
- Donut is able to generalize better than other deep learning models
- Donut is able to learn faster than other deep learning models
- Donut is able to learn from smaller data sets
- Donut was pre-trained on smaller data and fewer GPUs
- Donut is able to generalize better than other deep learning models
- Donut is able to learn faster than other deep learning models
- Donut is able to learn from smaller data sets

### A.7 Details of OCR-dependent Baseline Models
- The conventional backbones perform downstream VDU tasks
- The output of the OCR engine is used as input features of the backbone
- The difference in each task depends on a slight modification on the input sequence or on the utilization of the output vectors
