---
title: "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale"
date: 2022-12-18T14:36:07.000Z
author: "Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, Dan Roth"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09095v1.webp" # image path/url
    alt: "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09095)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09095).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/rethinking-the-role-of-scale-for-in-context).

# Abstract
- Language models have been shown to perform better with an increase in scale
- on a wide variety of tasks via the in-context learning paradigm
- In this paper, we investigate the hypothesis that the ability of a large language model to
- Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case:
- We find substantial overlap in the set of attention heads (un)important for in-context learning across
- We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B

# Paper Content

## Introduction
- Large language models (LLMs) based on the Transformer architecture (Vaswani et al., 2017) pre-trained using self-supervision on web-scale textual corpora have revolutionized the field of natural language processing (NLP).
- At larger scales, these models demonstrate remarkable emergent (Wei et al., 2022) prowess in performing a wide variety of tasks without any form of fine-tuning, via the zero/few-shot incontext learning paradigm (Brown et al., 2020).
- In this paradigm (depicted in Figure 1), LLMs are prompted to generate output text conditioned on a few (or zero) in-context examples that form solved "input-output" pairs along with a query input.
- How in-context learning works has been an open question since its advent and recent studies (Xie et al., 2021;Garg et al., 2022;Olsson et al., 2022;Min et al., 2022b) have begun scratching the surface toward better understanding the paradigm.
- In this paper, we empirically address the following key question: Are all LLM components really needed to perform in-context learning?
- The first way we address the aforementioned question is through the lens of task-specific importance scores and structured pruning (Li et al., 2016;Molchanov et al., 2016;Anwar et al., 2017) of components underlying modern LLMs, which are primarily stacks composed of multiple highdimensional self-attention blocks that form multiheaded attention and densely activated feed forward networks (FFNs).
- We find that important attention heads are primarily clustered in the intermediate layers and important FFNs are primarily in later (31+) layers ( §4).
- We find that the ability to perform zero/few-shot in-context learning on almost all of a variety of 14 NLP datasets/tasks stays nearly intact when up to 70% (∼15.7B parameters in OPT-66B) of the attention heads are removed ( §5.1).
- The attention heads that are (un)important for in-context learning also seem to overlap across tasks ( §6.1) and shots ( §6.2), and pruning of attention heads based on a "universal" importance order computed using all 14 datasets generalizes to varying degrees on out-ofdistribution datasets ( §6.1.2).
- These observations indicate that a common task-agnostic subset of the attention heads are responsible for in-context learning.
- We also find that only up to 20% of the FFNs (∼8.5B parameters) can be removed with minimal decline in zero/few-shot in-context learning performance ( §5.2), indicating the importance of FFNs toward in-context learning.
- In addition, when accounting for the interplay between attention heads and FFNs by jointly pruning both based on their individual importance orders, the newly obtained inflection points to in-context learning performance do not deviate much from the inflection points obtained via standalone pruning ( §5.3).
- The second way we address the aforementioned question is by quantifying the capacity of all attention heads in OPT-66B to perform a subset of task-agnostic primitive operations associated with in-context learning, namely, prefix matching and copying: explicitly searching for a prior occurrence of the current token in-context and copying over its suffix.
- Elhage et al. (2021) and Olsson et al. (2022) developed a mathematical framework to reverse-engineer a Transformer and also find such heads, termed induction heads, and explored the hypothesis that such heads drive in-context learning with model sizes up to 13B parameters in a mostly task-agnostic fashion.
- Using this framework, we compute task-agnostic scores for prefix matching and copying for each attention head and find that a small set of heads in OPT-66B have nontrivial scores for both primitives ( §6.3).
- Qualitative inspection and quantitative analyses show that these heads overlap (to varying degrees) with the ones identified earlier to be important for in-context learning via our set of 14 NLP datasets/tasks, suggesting that induction heads are capable of more sophisticated behaviors associated with in-context learning such as latent concept matching but are not the only heads with such capabilities ( §6.3.1).

## Background & Methods
- The Open Pretrained Transformer (OPT) model is used for the experiments
- In-context learning is described and the mathematical formulation of induction heads by Olsson et al. (2022) is used
- Adaptation of oracle and gradient-based importance score formulations for in-context learning is described

### Open Pre-trained Transformer (OPT)
- OPT is a suite of language models of varying sizes that are aimed at serving as open replicas of GPT-3.
- The largest openly accessible model from this suite is OPT-66B with 66 billion parameters, which was also the largest publicly available dense decoderonly language model at the time of our experiments.
- Hence, we chose OPT-66B for our study.
- Architecture: Consider a tokenized input sentence to OPT, X ∈ R N ×de , where N is the number of tokens in the sentence and d e is the embedding dimension.
- The input is processed by multiple decoder layers consisting of multi-headed attention (MHA) blocks, layer norm (LN) and feed forward networks (FFN), followed by a final FFN to produce logits over the output vocabulary.
- The decoder layers can be formally expressed as follows: where z 1 = X, and (1) & (2) are the residual connections corresponding to the MHA and FFN in layer >= 1 respectively.
- OPT-66B was pre-trained with a maximum sequence length of 2048 and embedding dimension d e = 9216.
- In an MHA block, H attention heads are applied in parallel to the input and their outputs are concatenated.
- In OPT-66B, there are H = 72 attention heads of dimension d h = 128 in every layer .
- An individual attention head h in layer consists of three learnable matrices, W h k , W h q , W h v ∈ R de×d h , all unique to the head, such that it applies selfattention A h (.) on the input, where d h = d e /H.
- Formally, for input M in layer : where σ is the softmax function and W o ∈ R de×de is a learnable output matrix unique to the MHA block in layer .
- To ensure OPT is auto-regressive, the output of s h (.) is masked to prevent the dependence of the hidden state of the token i, z i ∈ R de , on the tokens that lie to the right of it in indices {i + 1, . . . , N }.
- To remove a head h in layer in practice, we set A h (M) to be the zero matrix in Equation ( 3). This implies that W h k , W h q , W h v can be entirely removed, and the corresponding d h rows in W o can also be removed.
- In total, there are 4608 attention heads across 64 layers in OPT-66B that constitute 21.7B of the total 66B parameters.
- FFN: Each layer consists of a feed forward network (FFN) parameterized by a high-dimensional projection matrix, W 1 ∈ R de×d followed by a low-dimensional projection matrix, W 2 ∈ R d×de where d = 36864 for OPT-66B.
- Formally, for input M in layer : where ReLU is the rectified linear unit activation function and LN is the layer norm.
- To remove an FFN in layer in practice, we set FFN (M) to be the zero matrix in Equation ( 6). This implies W 1 , W 2 and the layer norm LN (.) for the FFN can be entirely removed.
- In total, FFNs constitute 43.4B parameters in OPT-66B.

### In-Context Learning & Induction Heads
- In-context learning is a new paradigm of learning that relies on a few (or zero) examples to help the model perform tasks.
- In-context learning is task-agnostic, meaning that the model can learn to perform tasks without being explicitly primed with task descriptions.
- In-context learning is based on the ability of a model to better predict tokens later in the context than the tokens earlier.
- In-context learning is based on the ability of a model to perform induction heads, which are heads that are capable of performing task-agnostic primitive operations.
- To quantify the relative contributions of model components to in-context learning, scores are computed.

## Experimental Setup
- We perform our experiments with OPT-66B on a variety of 14 NLP datasets/tasks.
- For consistency in the evaluation metric, we report accuracy on all tasks.
- Our choice of datasets and metric is in line with that of Meta AI in the OPT paper (Zhang et al., 2022).
- The datasets include ARC Easy and Challenge (Clark et al., 2018) and OpenBookQA (Mihaylov et al., 2018) for advanced questionanswering, HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020) and Winogrande (Sakaguchi et al., 2021) for various forms of commonsense reasoning, and the following datasets from the standard SuperGLUE benchmark (Wang et al., 2019): BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, and WSC.
- For a subset of experiments involving evaluation for out-of-distribution generalization, we also use 2 additional datasets: MathQA (Amini et al., 2019) and LAMBADA (Paperno et al., 2016).
- We use a modified version of Eleuther AI's lmevaluation-harness framework (Gao et al., 2021) for our experiments.

## Importance Scores for OPT-66B
- The importance scores for all attention heads and feed forward networks in OPT-66B were found to be important for performing zero-shot and fewshot (1-shot and 5-shot) in-context learning.
- The importance scores were found to be highest for the attention head that was focused on the target object the most.

### Attention Heads
- Figure 3 depicts heatmaps of the head importance scores averaged across all tasks (as described in §2.3.2) in the zero-shot, one-shot and five-shot settings.
- Task-specific heatmaps are provided in Appendix A.1.
- We observe that the important attention heads are primarily clustered in the intermediate layers of OPT-66B in both the task-averaged and task-specific cases.
- We also observe some overlap in the most important attention heads across the different zero/few-shot settings.

### Feed Forward Networks
- We compute oracle importance scores for 64 FFNs in OPT-66B
- We observe that in the zero-shot and one-shot settings, the removal of any FFN in the early (1-30) layers of OPT-66B either gives comparable or better performance for a vast majority of tasks.
- In the five-shot setting however, both the early and later layers seem to have important FFNs for most tasks.
- We also generally observe high variance in FFN importance scores in later layers.

## Iterative Pruning
- The ability to in-context learn-perform a task is not uniformly spread across all of OPT-66B's underlying components
- We can remove multiple attention heads and FFNs with minimal decline in task performance

### Removing Attention Heads
- Sort all attention heads in OPT-66B in ascending order by importance score
- Remove attention heads in an iterative fashion, 10% at a time, and re-evaluate task performance after each removal
- Figure 5 depicts the resulting accuracy trends for each task and the trends when averaged across all tasks
- We observe that the average accuracy across tasks does not change much up until ∼70% of the attention heads are removed
- Some oddities include tasks such as WSC and CB, wherein we observe that the zero-shot accuracy actually increases after the removal of 70% of the attention heads

### Removing FFNs
- Sort all FFNs in OPT-66B in ascending order by importance score
- Remove FFNs in an iterative fashion from the start of this order, 10% at a time
- Observe that in the zero-shot setting, the average accuracy across tasks does not change up until ∼20% of the FFNs are removed
- For some tasks such as PIQA, Winogrande and RTE, the accuracy does not change even if 30% of the FFNs (∼13B of the 66B parameters) are removed
- The inflection point after which we observe a sharp decline in accuracy changes to 10% for the few-shot settings

### Combined Removal of Heads & FFNs
- The average accuracy of all tasks on joint iterative removal of attention heads and FFNs, 10% at a time, based on their respective importance scores in the zero and few-shot settings.
- We observe that the removal of 70% of the attention heads (∼15.7B parameters) and 20% of the FFNs (∼8.5B parameters) leads to a mere 5% absolute drop in the average zero-shot accuracy.
- In the one-shot setting, the absolute drop in accuracy is 6% on removing 70% of the attention heads and 10% of the FFNs.
- In the five-shot setting, the ab-solute drop in accuracy is 4% on removing 60% of the attention heads and 20% of the FFNs.

## Detailed Analysis of Attention Heads
- In this section, we perform a detailed analysis of the attention heads in OPT-66B, given that incontext learning is auto-regressive in nature and attention heads explicitly encode cross-token interactions.
- We perform cross-task analyses to understand whether various tasks share (un)important attention heads.
- We also perform cross-shot analyses to study whether the (un)important attention heads for a task are shared across the zero-shot and few-shot settings.
- We finally quantify the capacity of the attention heads to perform task-agnostic induction operations (defined in §2.2) and study correlations with task-specific importance scores.

### Cross-Task Analysis
- We assess overlap in (un)important attention heads across tasks by sorting task-specific head importance scores to get head importance rankings and computing the Spearman's rank correlation coefficient (SRCC) between the rankings for every pair of tasks in the zero-shot and few-shot settings.
- We also sort the task-aggregate head importance scores to get the aggregate ranking and compute the SRCC against the ranking for every constituent task.
- All pairwise correlations for the zero and one-shot settings are depicted in Figure 8, and depicted for the five-shot setting in Appendix A.2. In both zero and few-shot settings, we observe statistically significant (p < 0.01) positive correlations in the head importance rankings for every pair of tasks, as well as between every task's ranking and the aggregate ranking. This indicates that the set of (un)important attention heads are clustered together across tasks.
- We also observe seemingly lower magnitude SRCC values between every task and ReCoRD, a long reading comprehension task which requires commonsense reasoning, indicating the amount of head overlap is proportionally lower.
- We now investigate how well head importance rankings generalize across tasks by studying accuracy trends for a task when pruning using its own head importance ranking as well as using different head importance rankings.
- We perform this investigation for two sets of tasks. The first set of tasks we investigate were used to compute the aggregate ranking: COPA, Winogrande and ReCoRD. For each of these 3 tasks, we use the self-ranking, aggregate ranking and the rankings from the tasks which share the highest and lowest SRCC with them based on Figure 8.
- For instance, we see that ReCoRD has the highest and lowest SRCCs with RTE and OpenBookQA respectively in the zero-shot setting, so we consider the impact of pruning based on their head  importance rankings on ReCoRD accuracy. Figures 9a, 9b and 9c depict the accuracy trends for these 3 tasks in the zero-shot setting. We observe that the accuracy on all 3 tasks when pruning using the rankings described is almost unaffected up to the 50% mark. We then observe a sharp decline in accuracy on COPA and Winogrande when the model is pruned to the 70% mark using the ranking identified via ReCoRD, the task with the lowest SRCC (0.13) with both COPA and Winogrande in Figure 8a. This indicates that even if the rankings vary between ReCoRD and COPA/Winogrande (as reflected in the low magnitude of the SRCC score), the set of attention heads important for zero-shot learning with ReCoRD are important for COPA/Winogrande too.
- To further verify this, we calculated and found 71% and 76% overlap between the top 30% important attention heads for ReCoRD-COPA and ReCoRD-Winogrande respectively. We also surprisingly observe that the accuracy on ReCoRD at the 70% pruning mark is better using the aggregate ranking than using the ranking for ReCoRD itself.
- We observe similar trends in the few-shot settings as well, depicted in Appendix A.3.

### Cross-Shot Analysis
- The attention heads that are identified to be (un)important for a task are shared across the different zero and few-shot settings
- The SRCC is higher for rankings within the few-shot setting (0.41 between 1-shot and 5-shot) than for rankings across the zero and few-shot settings

### Induction Heads in OPT-66B
- We look for induction heads in OPT-66B by quantifying the capacity of all attention heads to perform prefix matching and copying using random input sequences in a task-agnostic fashion, following the definition and algorithms by Olsson et al. (2022) discussed in §2.2 and Appendix A.4.
- Figures 10a and 10b depict the prefix matching and copying score heatmaps respectively for OPT-66B. We observe that a small subset of attention heads in OPT-66B have high prefix matching scores, located in the upper layers (31+) of the model.
- On the other hand, there are a relatively larger number of attention heads with high copying scores, although the vast majority of these are also located in the upper layers (41+).
- When seen in conjunction, these observations indicate that there is a sparse set of attention heads that are capable of performing both primitive operations and thus can be deemed plausible induction heads.
- We previously ( §4.1) computed task-specific and task-aggregated importance scores for attention heads in the zero-shot and few-shot in-context learning settings. By virtue of being important for the challenging downstream tasks we consider, these attention heads are capable of sophisticated and latent behaviors associated with in-context learning that go well beyond the basic primitives of explicit prefix matching and copying.
- We now study whether induction heads overlap with attention heads deemed important for our chosen tasks. A qualitative comparison of the heatmaps in Figure 10 against the heatmaps in Figure 3 indicate that induction heads do overlap with task-aggregated important attention heads.
- To better facilitate this comparison, we first formalize the total capacity of a model to perform prefix matching (or copying) to be the sum of the respective scores for individual attention heads in the model. We then investigate how much of this capacity is retained when attention heads are pruned in the order of least important heads first.
- Figure 11 depicts this comparison. We observe that much of the total prefix matching  score is retained when 20% of the least important heads are removed, with the slope of decline becoming sharp only after the 40% pruning mark. This indicates that unimportant heads also have low prefix matching scores.
- We also observe that the prefix matching scores are generally higher for heads important for few-shot in-context learning than for heads important for zero-shot learning. On the other hand, we observe across the zero-shot and few-shot settings that the total copying score retained on pruning attention heads rapidly and consistently declines, indicating that even unimportant heads have a non-trivial capacity to perform copying.

## Related Work
- There has been an interest in effectively leveraging the in-context learning paradigm
- Several works have also focused on analyzing and interpreting how attention works
- Vig and Belinkov (2019) performed a study on GPT-2 small with Wikipedia sentences, finding that attention targets different parts of speech at different layer depths and aligns with dependency relations most strongly in the middle layers.
- Tenney et al. (2019) showcase that BERT encodes the classical NLP pipeline in an interpretable way across layers.
- There are works relying on different formulations for head importance, such as layer-wise relevance propagation (Voita et al., 2019), gradient-based importance and oracle knock-off importance (Michel et al., 2019), with small task-specific trained models and report the existence of specialized heads.
- Given the recent trend of increasing model scale (Lieber et al., 2021;Chowdhery et al., 2022;Smith et al., 2022;Rae et al., 2021;Hoffmann et al., 2022) toward tuning-free general-purpose language models that exhibit emergent in-context learning abilities, we draw and build on prior work to understand just how much scale is really needed and/or used for in-context learning downstream, an aspect somewhat eclipsed by the focus on the pre-training loss curve in scaling laws (Hoffmann et al., 2022).
- It is also worth noting that some of our empirical observations rely on a simple greedy approach to training-free pruning since our focus was not to optimally prune a language model with respect to performing in-context learning.

## Conclusion & Future Work
- In this paper, we studied the efficacy of attention heads and feed forward networks (FFNs) in a large language model (OPT-66B) in performing in-context learning in both task-specific and task-agnostic settings.
- We observed that while in-context learning may have emerged via self-supervised pre-training at scale, only a core nucleus of attention heads and FFNs seem to be important for in-context learning across a wide variety of downstream tasks.
- We observed that a small set of attention heads have the capacity to perform taskagnostic primitive induction operations associated with in-context learning, namely prefix matching and copying.
- We also saw that these induction heads overlap with task-specific important attention heads to varying degrees, indicating that induction heads are capable of more sophisticated forms of in-context learning and reinforcing claims (Olsson et al., 2022) about the generality of induction heads.
- Overall, our in-context learning-centric observations complement recent work (Hoffmann et al., 2022) in indicating that large language models may be under-trained and motivate several interesting directions for future work.
