---
title: "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
date: 2022-04-16T03:12:30.000Z
author: "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2204-07705v3.webp" # image path/url
    alt: "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2204.07705)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2204.07705).


# Abstract
- Super-NaturalInstructions is a benchmark of 1,616 diverse NLP tasks and their expert-written instructions
- 76 distinct task types are included
- Tk-Instruct is a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples)
- Generalization is a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes

# Paper Content

## Introduction
- The NLP community has witnessed great progress in building models for generalization to unseen tasks via in-context instructions
- A successful model is expected to use the provided instructions (including task definition and demonstration examples) to output responses to a pool of evaluation instances
- In this paper, we construct a meta-dataset (i.e., dataset of datasets; Triantafillou et al., 2019) that consists of a wide variety of NLP tasks with their instructions, and train a model that can perform a new task given the instruction, outperforming InstructGPT (which uses 16× more parameters)
- Our dataset, SUPER-NATURALINSTRUCTIONS (SUP-NATINST for short), is a large benchmark of 1,616 NLP tasks and their natural language instructions. It brings in a diverse variety of tasks-76 broad task types spanning 55 different languages. Each task is paired up with an instruction that consists of the task definition for mapping an input text to a task output and several examples for demonstrating the desired or undesired output (see Fig. 1 as an example task). These tasks and their instructions are contributed by 88 NLP practitioners, in response to our public call. These contributions are consolidated after several rounds of peer-review and crowdsourced feedback to ensure quality.
- Having this diverse and large-scale data enables us to carefully split the tasks into training and test sets and systematically study how state-of-the-art methods perform on them.
- According to human evaluation, Tk-INSTRUCT generates responses at least as well as the ground truth for 77% of the testing instances

## Related Work
- Language instructions can be used to define goals for tasks
- This benchmark includes 26 different tasks and more than 100,000 instructions
- This benchmark can be used to study multi-task learning more broadly

### Evaluation Setup
- An evaluation split of unseen tasks into evaluation and supervision tasks
- For evaluation tasks, we fix a manuallyselected collection of 12 categories that represent 154 tasks
- For an efficient evaluation, we sample a maximum of 100 instances for each task, which results in 15,310 testing instances in total
- The remaining tasks are used for training models
- For cross-lingual cross-task generalization, we divide our evaluation tasks into English and X-lignual tracks
- To the best of our knowledge, this is the first study in cross-lingual cross-task generalization

### Baselines and Existing Models
- Heuristic baselines
- Copying Demo Output
- Copying Instance Input
- Off-the-shelf pretrained language models
- Upper bound estimates
- Experimental Results

### Overall Results
- Table 3 summarizes the results of the study
- The study used the same input encoding as the most effective instructional elements
- The study found that instruction-tuned models perform better than untuned LM counterparts and heuristic baselines
- There is a sizable gap between the generalization of instruction-based models and the supervised training approach

### Human Evaluation
- Automatic metrics are only an approximation of human judgments
- We conduct a human evaluation to confirm the findings so far
- The resulting human evaluation metric indicates how often model predictions were rated as at least as good as our ground truth labels
- The theoretical upper bound of this metric is 100% when the model is rated at least as good as the ground truth for all the instances

## Further Analysis
- We conduct further analysis to understand the important factors for models to generalize across tasks.
- Due to the computational cost, this analysis is done on the English track and using the T5-3B checkpoint, except for the experiments on model sizes.

### Scaling Trends of Generalization
- The generalization performance of Tk-INSTRUCT improves with the number of training tasks used
- The generalization performance grows log-linearly with the number of tasks used
- A large number of training instances does not help generalization
- We then vary the number of instances per task that are used for finetuning
- While the conventional wisdom in supervised learning is that more training instances usually helps, tuning larger models with instructions consistently lead to gains
- We study the effect of model scaling by initializing Tk-INSTRUCT from different sizes of pretrained T5 checkpoints
- We found that increasing the model sizes consistently bring significant improvement (log-linearly with parameter size)

### Instructing with Different Elements
- Benefit of different instructional elements
- As shown in Fig. 1, SUP-NATINST provides multiple elements for instructing a task
- We train multiple models with different combinations of these elements
- The diagonal cells of Table 4 show the performance of our models when trained and evaluated on a particular instruction encoding

## Conclusion
- We construct a large-scale benchmark consisting of a diverse set of NLP tasks and their instructions.
- This benchmark can serve as a rich playground for training or evaluation of models that can generalize to unseen tasks by following instructions.
- Furthermore, we train Tk-INSTRUCT using this data, and demonstrate its capability to perform unseen tasks to a surprising extent.
- We provide extensive analysis to understand the important factors for such generalization.
- We hope our data and model will facilitate future work towards more general-purpose models.

## Limitations
- The distributions of task types in the paper suffer from skews
- The proposed benchmark is biased towards English
- Output diversity is underrepresented in the paper's data
- Crowdsourcing is used to collect feedback on the quality of the collected instructions
- The paper limits its crowdworkers to predominantly English-speaking countries and those who have finished over 1K HITs with an approval rating of over 99%.

## B Crowdsourcing Human Judgements of Generation Quality
- We perform a crowdsourcing experiment on Amazon Mechanical Turk (AMT) to assess the quality of the generated responses of models
- The annotation interface is shown in Fig. 7
- It is essentially the same template used for the quality assessment of the dataset ( §A)
- For each instance, we obtain annotations from an annotator as to whether they prefer either response over the other or they would rate them equally ("tie")
- The model receives a credit of 1.0 if the worker favors the model's prediction at least as well as the ground truth label (otherwise, the model would receive a credit of 0.0)
- The overall accuracy score for the model is computed by averaging instance-level scores
- To reduce the costs, the human evaluation of our models is done on 60 randomly selected tasks (about half of our evaluation tasks), and on 10 random instances of each task
- Since it is non-trivial to find non-English speaking crowdworkers (Pavlick et al., 2014), this evaluation was restricted to English language tasks
- Therefore, since our task is focused on English tasks, we required workers to be based in a country with a population predominantly of native English speakers (USA, Canada, UK, and Australia) and have completed at least 5000 HITs with ≥99% assignment approval rate

## C Instruction Schema
- Our instruction schema is based on that of NATINST (Mishra et al., 2022b)
- We simplify it to make data collection easier
- Our DEFINITION field serves as the union of Mishra et al.'s DEFINITION, THINGS TO AVOID, and EMPHASIS & CAUTION
- We drop their TITLE and PROMPT as their content is most often covered by DEFINITION

## D Model Implementation Details
- We use T5 for training our Tk-INSTRUCT, estimating the performance of the supervised approach and conducting analysis.
- Our experiments that finetune the T5-11B model are conducted based on the Google's T5 library9 and we use their T5.1.1.xxl checkpoint10 by default, which is pre-trained only on C4.11
- These experiments are run on Google V3-256 TPUs using a batch size of 1,048,576 tokens (1,024 examples), a constant learning rate of 1e-5 and a total of 1000 steps.
- Each training run takes 4 hours to complete.
- Our analyses that use T5 models smaller than 11B parameters are conducted based on Huggingface's transformers library and model checkpoints 12 (Wolf et al., 2020) on GPU machines.
- When fine-tuning models, we train them for two epochs with a batch size of 16 and a constant learning rate of 1e-5.
- The maximum input length is set to 1024, and the maximum output length is set to 128.
- These experiments are conducted with 8 A100 GPUs with 48GB GPU memory per each.
- We use DeepSpeed13 for model parallelization, with bfloat16 precision enabled to save the GPU mem-ory.
- Each training run takes 6 hours to complete.

## E Evaluation Metrics
- We adopt ROUGE-L as our automatic evaluation metric in this work
- However, it remains a question for how much ROUGE-L can reflect model's performance on different tasks
- Although we cannot test ROUGE-L's correlation with each task-specific metric of the tasks included in our data, we do investigate whether ROUGE-L can be used for classification tasks
- Fig. 9 plots the ROUGE-L scores and accuracy of several models on different types of tasks
- These task types are usually regarded as classification tasks and have very short ground truth output
- We can see that for all these task types, the trend of ROUGE-L correlates well with the trend of accuracy

## F Distribution of Tasks
- SUP-NATINST provides the annotation for categorizing tasks along three different dimensions: task type, language, and domain
- Fig. 10 shows the distribution of tasks among these three dimensions
- This meta-information can be used to study model's generalization ability in different senses

## G Evaluation Tasks
- Table 5: Task categories and tasks included in each
- Table 6: Task 1: Fetching a web page
- Table 7: Task 2: Parsing a web page
- Table 8: Task 3: Manipulating a web page
- Table 9: Task 4: Generating a web page
- Table 10: Task 5: Uploading a file
- Table 11: Task 6: Downloading a file
- Table 12: Task 7: Checking a web page for errors
- Table 13: Task 8: Updating a web page
- Table 14: Task 9: Deleting a web page
- Table 15: Task 10: Checking a web page for validity
- Table 16: Task 11: Generating a web page from scratch
- Table 17: Task 12: Checking a web page for plagiarism

## H Performance Improvement per Evaluation Task
- Tk-INSTRUCT generalizes better than other methods on a variety of task types.
- Tk-INSTRUCT performs better than other methods on most task types.
- Tk-INSTRUCT performs better than other methods on all task types.
- Tk-INSTRUCT performs better than other methods on most task types with a large number of training instances.
