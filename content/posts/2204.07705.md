---
title: "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
date: 2022-04-16T03:12:30.000Z
author: "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2204-07705v3.webp" # image path/url
    alt: "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2204.07705)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2204.07705).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/benchmarking-generalization-via-in-context).

# Abstract
- Super-NaturalInstructions is a benchmark of 1,616 diverse NLP tasks and their expert-written instructions
- 76 distinct task types are included
- Tk-Instruct is a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples)
- Generalization is a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes

# Paper Content

## Introduction
- Tk-INSTRUCT is a generative model for transforming task inputs given declarative in-context instructions (task definition or kshot examples)
- Tk-INSTRUCT outperforms InstructGPT by 9.9 ROUGE-L points when evaluated on 119 unseen English tasks, and the multilingual variant mTk-INSTRUCT outperforms InstructGPT by 13.3 points on 35 non-English tasks
- According to human evaluation, Tk-INSTRUCT generates responses at least as well as the ground truth for 77% of the testing instances

## Related Work
- Language instructions are a versatile way of defining goals, which is why they have been studied in the context of a variety of applications, such as instructions in grounded environments (Shridhar et al., 2020;Stepputtis et al., 2020;Min et al., 2022b;Weir et al., 2022) and database commands (Kim et al., 2020).
- Recent literature has been motivated by building models that are generalizable across a variety of NLP tasks, when prompted with either a few examples (Ye and Ren, 2021;Bragg et al., 2021) or language definitions (Efrat and Levy, 2020;Weller et al., 2020;Zhong et al., 2021;Mishra et al., 2022b,a;Parmar et al., 2022).
- Our work is related to the existing benchmarks in this line of work, as delineated in Table 1 along various dimensions.
- Our benchmark extends NATINST (Mishra et al., 2022) with 26× more tasks and greater variety of task types (Fig. 2).
- While CROSSFIT (Ye et al., 2021) focuses on benchmarking with a few in-context examples, our benchmark also offers task instructions.
- Concurrent to our work, PROMPTSOURCE (Bach et al., 2022) is another benchmark of tasks and their language instructions (prompts).
- An important distinction between this benchmark and ours is the phrasing of the task definitions: while PROMPTSOURCE task definitions are relatively concise, our task definitions are collected with the intention of providing a complete definition of each task and therefore are longer (24 tokens vs. 56 tokens on average; Table 1).
- More recently, BIG-BENCH (Srivastava et al., 2022) introduces a collection of 204 tasks and also provides short task descriptions and input prefixes that can be used for prompting LMs. With little overlap to our collection of tasks, they focus more on finding challenging tasks that can be used to test different behaviors of current LMs. Nevertheless, we believe that all these efforts in collecting different tasks as well as the task instructions are complementary, and the community will benefit from considering different benchmarks.
- Finally, the well-adopted InstructGPT model (Ouyang et al., 2022) is partially enabled by a large dataset of prompts that are collected via various synthetic data augmentation which, unfortunately, is not publicly available.

### Evaluation Setup
- An evaluation split of SUP-NATINST into two subsets: one for evaluation and the other for supervision
- For evaluation tasks, we fix a manuallyselected collection of 12 categories that represent 154 tasks
- The large variety of tasks in SUP-NATINST enables us to choose a diverse set of tasks for evaluation -such as those at word, sentence, and document levels, covering both classification and generation formats
- Appendix G lists our evaluation tasks with examples for representative tasks
- For an efficient evaluation, we sample a maximum of 100 instances for each task, which results in 15,310 testing instances in total
- The remaining tasks are used for training models
- 6 Divided Tracks for English and X-lignual Tasks
- For English cross-task generalization (119 tasks) and the other for cross-lingual cross-task generalization (35 tasks)
- To the best of our knowledge, this is the first study in cross-lingual cross-task generalization (i.e., generalization to unseen tasks in different languages)
- Fig. 11 and Fig. 12 in the appendix contain the evaluation tasks for each track
- Evaluation Metrics
- ROUGE-L (Lin, 2004) for reporting aggregated performance results
- We show that the ranking from this metric correlates well with accuracy for classification tasks in Appendix E
- We also conduct a human evaluation in §6.2

### Baselines and Existing Models
- Evaluates heuristics and off-the-shelf pretrained language models
- Estimates an upper bound on generalization
- Tests on English and non-English tasks

### Overall Results
- Table 3 summarizes the results of the study
- The study used the same input encoding as the most effective instructional elements
- The study found that instruction-tuned models perform better than untuned LM counterparts and heuristic baselines
- There is a sizable gap between the generalization of instruction-based models and the supervised training approach

### Human Evaluation
- Automatic metrics are only an approximation of human judgments
- We conduct a human evaluation to confirm the findings so far
- The resulting human evaluation metric indicates how often model predictions were rated as at least as good as our ground truth labels
- The theoretical upper bound of this metric is 100% when the model is rated at least as good as the ground truth for all the instances

## Further Analysis
- We conduct further analysis to understand the important factors for models to generalize across tasks.
- Due to the computational cost, this analysis is done on the English track and using the T5-3B checkpoint, except for the experiments on model sizes.

### Scaling Trends of Generalization
- The generalization performance of Tk-INSTRUCT improves with the number of tasks used for training
- The generalization performance grows log-linearly with the number of tasks used for training
- A large number of training instances does not help generalization
- We then vary the number of instances per task that are used for finetuning and found that increasing the model sizes consistently bring significant improvement (log-linearly with parameter size)

### Instructing with Different Elements
- Benefit of different instructional elements
- As shown in Fig. 1, SUP-NATINST provides multiple elements for instructing a task
- We train multiple models with different combinations of these elements
- The diagonal cells of Table 4 show the performance of our models when trained and evaluated on a particular instruction encoding

## Conclusion
- We construct a large-scale benchmark consisting of a diverse set of NLP tasks and their instructions.
- This benchmark can serve as a rich playground for training or evaluation of models that can generalize to unseen tasks by following instructions.
- Furthermore, we train Tk-INSTRUCT using this data, and demonstrate its capability to perform unseen tasks to a surprising extent.
- We provide extensive analysis to understand the important factors for such generalization.
- We hope our data and model will facilitate future work towards more general-purpose models.
- While the presented data offers a notable variety (e.g., diverse task types), its underlying distributions suffer from skews which should be addressed in future work (see Appendix F).
- On language diversity, the proposed benchmark is biased towards English.
- On output diversity, the collected tasks are generally still skewed to short responses, which might reflect the distribution of the available tasks in the field. This under-representation of the longtail of tasks poses a challenge for building generalpurpose models in the future.
- We hope future work addresses such distributional imbalances.
- Furthermore, we see natural extensions of the instructionfollowing setup here in the context of other modalities such as vision or speech.
- Automatic evaluation of models' performance is another challenge, considering the diverse set of tasks in our benchmark, and many of them being open-ended generation tasks.
- We use ROUGE-L as an aggregated metric in this paper and find it as a good proxy for the overall performance of the mod-els, aligning well with human evaluation. However, there are specific tasks for which ROUGE-L might not serve as an effective proxy of quality (such as rewriting tasks or error correction tasks where copying the input can result in a high ROUGE-L score).
- We hope these issues will be addressed with the development of more powerful evaluation metrics for text generation.
- In terms of computing power, we have experimented with models that were accessible to us and have made the resulting models publicly available.
- We also acknowledge that there are larger models that we were not able to train due to the limitations of our computational budget.
- We use Amazon Mechanical Turk (AMT) to crowdsource feedback on the quality of the collected instructions.
- We limit our crowdworkers to predominantly English-speaking countries (USA, UK, Canada, and Australia), and to those who have finished over 1K HITs with an approval rating of over 99%.
- Fig. 6 shows the crowdsourcing template used for collecting crowdworker feedback on our instructions.
- We show the instructions (the task definition, along with positive and negative examples) followed by forms for their feedback.
- We allow the crowdworkers to give us a qualitative measure of their perceived quality as well as text boxes for more concrete items (such as typos or phrasings that may benefit from more clear articulation).
- For each task, we solicit the feedback of 3 crowdworkers and then use this feedback to improve the task definitions or the examples for each task.
- We perform a crowdsourcing experiment on Amazon Mechanical Turk (AMT) to assess the quality of the generated responses of models.
- Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instances.
- The annotation interface is shown in Fig. 7.
- It is essentially the same template used for the quality assessment of the dataset ( §A), except that here the crowdworkers are shown a pair of responses for each instancesthe reference text (from our benchmark) and the one generated by the model-turning the task into a comparative evaluation.
- For each instance, we obtain annotations from an...
