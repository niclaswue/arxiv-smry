---
title: "RT-1: Robotics Transformer for Real-World Control at Scale"
date: 2022-12-13T18:55:15.000Z
author: "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-06817v1.webp" # image path/url
    alt: "RT-1: Robotics Transformer for Real-World Control at Scale" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.06817)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.06817).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/rt-1-robotics-transformer-for-real-world).

# Abstract
- Modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets
- One key to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures
- We present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties

# Paper Content

## INTRODUCTION
- RT-1 takes images and natural language instructions and outputs discretized base and arm actions.
- Despite its size (35M parameters), it does this at 3 Hz, due to its efficient yet high-capacity architecture: a FiLM (Perez et al., 2018) conditioned EfficientNet (Tan & Le, 2019), a TokenLearner (Ryoo et al., 2021), and a Transformer (Vaswani et al., 2017).
- RT-1's large-scale, real-world training (130k demonstrations) and evaluation (3000 real-world trials) show impressive generalization, robustness, and ability to learn from diverse data.
- Figure 1: A high-level overview of RT-1's architecture, dataset, and evaluation. The two main challenges lie in assembling the right dataset and designing the right model.
- While data collection and curation is often the "unsung hero" of many large-scale machine learning projects (Radford et al., 2021;Ramesh et al., 2021), this is especially true in robotics, where datasets are often robot-specific and gathered manually (Dasari et al., 2019;Ebert et al., 2021).
- At the same time, the tasks in the dataset should be sufficiently well-connected to enable generalization, such that the model can discover the patterns between structural similar tasks and perform new tasks that combine those patterns in novel ways.
- We utilize a dataset that we gathered over the course of 17 months with a fleet of 13 robots, containing ∼130k episodes and over 700 tasks, and we ablate various aspects of this dataset in our evaluation.
- The second challenge lies in the design of the model itself. Effective robotic multi-task learning requires a high capacity model, and Transformer (Vaswani et al., 2017) models excel in this regard, particularly when it is necessary to learn many tasks conditioned, as in our case, on language instructions.
- However, robotic controllers must also be efficient enough to run in real time, which presents a major challenge for Transformers in particular.
- We propose a novel architecture that we call RT-1 (Robotics Transformer 1), which by encoding high-dimensional inputs and outputs, including camera images, instructions and motor commands into compact token representations to be used by the Transformer, allows for efficient inference at runtime to make real-time control feasible.
- Our contribution is the RT-1 model and experiments with this model on a large and broad dataset of real-world robotic tasks.
- Our experiments not only demonstrate that RT-1 can exhibit significantly improved generalization and robustness compared to prior techniques, but also evaluate and ablate many design choices in both the model and in the composition of the training set.
- Our results show that RT-1 can perform over 700 training instructions at 97% success rate, and can generalize to new tasks, distractors, and backgrounds 25%, 36% and 18% better than the next best baseline, respectively.
- This level of performance allows us to execute very long-horizon tasks in the SayCan (Ahn et al., 2022) framework, with as many as 50 stages.

## Preprint 2 RELATED WORK
- Transformer-based policies for robotic control are proposed
- The focus of the work is on generalizable and robust real-world robotic manipulation at scale
- Existing works on real-world Transformer-based robotic manipulation focus on efficiently learning tasks from a set of demonstrations per task
- Behavior Transformer (Shafiullah et al., 2022) and Gato (Reed et al., 2022) advocate for training a single model on large-scale robotic and non-robotic datasets
- Multi-task robotic learning has also been approached from the perspective of learning to reach goals
- Prior works in robotics have also focused on collecting datasets containing demonstrations or trials that illustrate a variety of different tasks

## PRELIMINARIES
- Robot learning: We aim to learn robot policies to solve language-conditioned tasks from vision.
- Formally, we consider a sequential decision-making environment.
- At timestep t = 0, the policy π is presented with a language instruction i and an initial image observation x 0 .
- The policy produces an action distribution π(• | i, x 0 ) from which an action a 0 is sampled and applied to the robot.
- This process continues, with the policy iteratively producing actions a t by sampling from a learned distribution π(• | i, {x j } t j=0 ) and applying those actions to the robot.
- The interaction ends when a termination condition is achieved.
- At the end of an episode, the agent will be given a binary reward r ∈ {0, 1} indicating whether the robot performed the instruction i.
- The goal is to learn a policy π that maximizes the average reward, in expectation over a distribution of instructions, starting states x 0 , and transition dynamics.
- Transformers. RT-1 uses a Transformer (Vaswani et al., 2017) to parameterize the policy π.
- Generally speaking, a Transformer is a sequence model mapping an input sequence {ξ h } H h=0 to an output sequence {y k } K k=0 using combinations of self-attention layers and fully-connected neural networks.
- While Transformers were originally designed for text sequences, where each input ξ j and output y k represents a text token, they have been extended to images (Parmar et al., 2018) as well as other modalities (Lee et al., 2022a;Reed et al., 2022).
- As detailed in the next section, we parameterize π by first mapping inputs i, {x j } t j=0 to a sequence {ξ h } H h=0 and action outputs a t to a sequence {y k } K k=0 before using a Transformer to learn the mapping {ξ h } H h=0 → {y k } K k=0 .
- Imitation learning. Imitation learning methods train the policy π on a dataset D of demonstrations (Pomerleau, 1988;Zhang et al., 2018;Jang et al., 2021).
- Specifically, we assume access to a dataset of episodes, all of which are successful (i.e., have a final reward of 1).
- We learn π using behavioral cloning (Pomerleau, 1988), which optimizes π by minimizing the negative log-likelihood of actions a t given the images and language instructions.

## SYSTEM OVERVIEW
- The goal of this work is to build and demonstrate a general robot learning system that can absorb large amounts of data and generalize effectively.
- We use mobile manipulators from Everyday Robots3, which have a 7 degree-of-freedom arm, a two-fingered gripper, and a mobile base (see Fig. 2 (d)).
- To collect data and evaluate our method, we use three kitchen-based environments: two real office kitchens and a training environment modelled off these real kitchens.
- The training environment, shown in Fig. 2 (a), consists of partial counters and is constructed for large scale data collection.
- The two real environments, shown in Fig. 2 (b, c), have similar counter tops to the training environment, but vary in lighting, background, and full kitchen geometry (e.g., there may be a cabinet instead of a drawer or a sink may be visible).
- We evaluate the performance of our policies across these different environments, measuring the policy's performance and ability to generalize.
- Our training data consists of human-provided demonstrations, and we annotate each episode with a textual description of the instruction that the robot just performed.
- The instructions usually contain a verb and one or more nouns describing the target objects.
- To group these instructions together, we split them into a number of skills (e.g., verbs such as "pick", "open" or "place upright") and objects (e.g., nouns such as "coke can", "apple", or "drawer").
- We describe the details of our data collection strategy at scale in Sec.
- Our largest dataset contains over 130k individual demonstrations constituting over 700 distinct task instructions using a large variety of objects (see Fig. 2 (f)).
- We describe the details of the data collected in Sec.
- One of the main contributions of our system is the network architecture, Robotics Transformer 1 (RT-1), an efficient model that can absorb large amounts of data, effectively generalize, and output actions at real-time rates for practical robotic control.
- RT-1 takes a short sequence of images and a natural language instruction as input and outputs an action for the robot at each time step.
- To this end, the architecture (shown in Fig. 1a) leverages several elements: first the images and text are processed via an ImageNet pretrained convolutional network (Tan & Le, 2019) conditioned on a pretrained embedding of the instruction via FiLM (Perez et al., 2018), followed by a Token Learner (Ryoo et al., 2021) to compute a compact set of tokens, and finally a Transformer (Vaswani et al., 2017) to attend over these tokens and produce discretized action tokens.
- The actions consist of seven dimensions for the arm movement (x, y, z, roll, pitch, yaw, opening of the gripper), three dimensions for base movement (x, y, yaw) and a discrete dimension to switch between three modes: controlling the arm, the base, or terminating the episode.
- RT-1 performs closed-loop control and commands actions at 3 Hz until it either yields a "terminate" action or hits a pre-set time step limit.

## RT-1: ROBOTICS TRANSFORMER
- Tokenize images, text, and actions
- Discuss RT-1 model architecture
- Achieve runtime speed required for real-time control
- Describe data collection procedure

## MODEL
- Image tokenization
- Instruction tokenization
- TokenLearner
- Transformer
- Action tokenization
- Inference speed

## DATA
- The goal of the paper is to build a system that exhibits high performance, generalization to new tasks, and robustness to distractors and backgrounds.
- The paper collected a large, diverse dataset of robot trajectories.
- The dataset consists of ∼130k robot demonstrations, collected with a fleet of 13 robots over the course of 17 months.

## Skills and instructions.
- RT-1 is able to perform over 700 language instructions in multiple realistic office kitchen environments
- The skills were chosen to demonstrate multiple behaviors with many objects
- The system is easily extendable

## Preprint

## EXPERIMENTS
- RT-1 can learn to perform a large number of instructions, as well as to generalize in zero shot to new tasks, objects and environments
- Can we push the resulting model even further by incorporating heterogeneous data sources, such as simulated data or data from different robots?
- How do various methods generalize to long-horizon robotic scenarios?
- What are the important and practical decisions in the design of the model and how do they affect performance and generalization?
- We evaluate the success rate in experiments to measure performance on training instructions, generalization to unseen instructions, robustness to backgrounds and distractors, and performance in long-horizon scenarios

## EXPERIMENTAL SETUP
- Evaluated RT-1 with a set of mobile manipulators from Everyday Robots in three environments: two real office kitchens and a training environment modelled off these real kitchens.
- The training environment, shown in Fig. 2 (a), consists of partial counters while the two real environments, shown in Fig. 2 (b, c), have similar counter tops to the training environment, but vary in lighting, background, and full kitchen geometry (e.g., there may be a cabinet instead of a drawer or a sink may be visible).
- The policies are evaluated for performance on training tasks as well as generalization to new tasks, robustness to unseen environments, and performance when chained together for long-horizon tasks, as detailed below.
- Seen task performance. To evaluate performance on seen instructions, we evaluate performance on instructions sampled from the training set. Note, however, that this evaluation still involves varying the placement of objects and other factors of the setup (e.g., time of day, robot position), requiring the skills to generalize to realistic variability in the environment. In all, we test over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for picking out of and placing objects into drawers.
- Unseen tasks generalization. To evaluate generalization to unseen tasks, we test 21 novel, unseen instructions. These instructions are distributed across skills and objects. This ensures that at least some instances of each object and skill were present in the training set but they will be combined in novel ways.
- Robustness. To evaluate robustness, we perform 30 real-world tasks for distractor robustness and 22 tasks for background robustness. The background robustness was tested by evaluating in new kitchens (which have different lighting and background visuals) and with different counter surfaces (e.g., a patterned table cloth).
- Long-horizon scenarios. We also evaluate generalization to more realistic long-horizon scenarios, which each require executing a sequence of skills. The goal of this evaluation is to combine multiple generalization axes such as new tasks, objects, environments and test the overall generalization capabilities in realistic settings. These evaluations consist of 15 long-horizon instructions in two real kitchens, which require executing sequences of skills consisting of ∼ 10 distinct steps, with each step of roughly comparable scope as the training instructions. These steps are obtained automatically from higher level instructions, such as "how would you throw away all the items on the table?" by using the SayCan system (Ahn et al., 2022), as described in detail in Section 6.4 and Appendix D.3.

## CAN RT-1 LEARN TO PERFORM A LARGE NUMBER OF INSTRUCTIONS, AND TO GENERALIZE TO NEW TASKS, OBJECTS AND ENVIRONMENTS?
- RT-1 outperforms the prior models significantly
- RT-1 is able to generalize to novel instructions
- RT-1 is robust to distractors and backgrounds

## CAN WE PUSH THE RESULTING MODEL FURTHER BY INCORPORATING HETEROGENEOUS
- RT-1 is able to incorporate and learn from vastly different data sources and improve from such data without sacrificing its original-tasks performance across the varied tasks inherent in this data.
- RT-1 is able to absorb both real and simulation data without losing performance.
- RT-1 is able to absorb data from different robots and improve performance.

## HOW DO VARIOUS METHODS GENERALIZE LONG-HORIZON ROBOTIC SCENARIOS?
- RT-1 is the best method for planning and executing ultra-long horizon tasks in a new kitchen
- The success rate of long-horizon tasks decreases exponentially with the length of the task
- SayCan-RT1 is able to plan and execute ultra-long horizon tasks with as many as 50 steps
- Data diversity is more essential than data quantity

## CONCLUSIONS, LIMITATIONS AND FUTURE WORK
- We presented Robotics Transformer 1, RT-1, a robot learning method that can effectively absorb large amounts of data and scales with data quantity and diversity.
- We trained RT-1 on a large dataset of demonstrations containing over 130k episodes collected over the course of 17 months with 13 robots.
- In our broad set of experiments, we demonstrated that our method that can perform over 700 instructions at 97% success rate and effectively generalize to new tasks, objects and environments better than previously published baselines.
- We also demonstrated that RT-1 can successfully absorb heterogeneous data from simulation and other robot morphologies without sacrificing original-tasks performance and while improving generalization to new scenarios.
- Lastly, we showed how this level of performance and generalization allowed us to execute very long-horizon tasks in the SayCan (Ahn et al., 2022) framework, with as many as 50 steps.
- While RT-1 presents a promising step towards large-scale robot learning with an data-absorbent model, it comes with a number of limitations. First, it is an imitation learning method, which inherits the challenges of that class of approaches such as the fact that it may not be able to surpass the performance of the demonstrators.
- Second, the generalization to new instructions is limited to the combinations of previously seen concepts and RT-1 is not yet able to generalize to a completely new motion that has not been seen before.
- Lastly, our method is presented on a large but not very dexterous set of manipulation tasks. We plan to continue extending the set of instructions that RT-1 enables and generalizes to address this challenge.
- As we explore future directions for this work, we hope to scale the number of robot skills faster by developing methods that allow non-experts to train the robot via directed data collection and model prompting. While the current version of RT-1 is fairly robust especially to distractor objects, its robustness to backgrounds and environments could be further improved by greatly increasing the environment diversity.
- We also hope to improve the reaction speeds and context retention of RT-1 through scalable attention and memory.
- To allow the research community to build on top of this work, we have open-sourced the code for RT-14.
