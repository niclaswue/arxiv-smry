---
title: "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection"
date: 2023-01-02T18:07:44.000Z
author: "Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A. Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, Zongwei Zhou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-00785v2_4xdYjuj_0.jpg" # image path/url
    alt: "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00785)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00785).


# Abstract
- An increasing number of public datasets have shown a marked clinical impact
- Each of the datasets is small, partially labeled, and rarely investigates severe tumor subjects
- Current models are limited to segmenting specific organs/tumors, which can not be extended to novel domains and classes
- To tackle these limitations, we introduce embedding learned from Contrastive Language-Image Pre-training (CLIP)
- To segmentation models, dubbed the CLIP-Driven Universal Model
- The Universal Model can better segment 25 organs and 6 types of tumors by exploiting the semantic relationship between abdominal structures
- The model is developed from an assembly of 14 datasets with 3,410 CT scans and evaluated on 6,162 external CT scans from 3 datasets
- We achieve the state-of-the-art results on Beyond The Cranial Vault (BTCV)

# Paper Content

## Introduction
- Annotation of medical images has led to advances in medical imaging
- There is a perception that medical imaging datasets are too small to develop robust AI models
- One of the reasons for this perception is that medical imaging datasets are often used in isolation, and that only a small proportion of them contain annotated tumors
- We have assembled 14 publicly available datasets and demonstrated the clinical impact of AI performance, including model generalizability, transferability, and robustness to different types of datasets.

## Related Work
- Partial label problem occurs when training AI models on a combination of different datasets due to their inconsistent label taxonomy
- To exploit the partial labels, several approaches have been investigated
- These studies have the following limitations: (1) due to the small scale of the dataset assembly, the potential of assembling datasets was not convincing. Their performance was similar to dataset-specific models and was not evaluated on the official benchmark. (2) due to the one-hot labels, the semantic relationship between organs and tumors was discarded.

## Methodology

### Background
- Problem definition
- There are two groups of solutions to the partial label problem
- Solution #1 [11,18,25,51,51,58,69,77] aims to solve where the prediction is one-hot encoding with length k
- Solution #2 [29,73,84] where w k is an one-hot vector to indicate which class to be predicted
- According to Zhang et al. [73], both solutions have similar segmentation performance, whereas #2 is computationally more efficient
- However, both solutions rely on one-hot labels, sharing two limitations. First, they dismiss the semantic and anatomical relationship between organs and tumors. Second, they are inappropriate in segmenting various subtypes of tumors (and novel classes). To address these limitations, we modify w k in Solution #2 to CLIP embedding and introduce in-depth

### CLIP-Driven Universal Model
- CLIP embedding is used to generate the CLIP embedding for each organ and tumor
- The text branch first generates the CLIP embedding for each organ and tumor, and then the vision branch takes both CT scans and CLIP embedding to predict the segmentation mask
- There is a computerized tomography in this computerized tomography

### Assembling Public Datasets
- To fully explore the potential of the proposed Universal Model in multi-organ segmentation and tumor detection, we have thus far assembled a total of 3,410 CT scans from 14 public datasets.
- However, it is non-trivial to assemble partially labeled datasets due to several challenges, such as inconsistent label protocols and the long-tail problem.
- To alleviate the long-tail problem, we utilize data augmentation.

## Experiments & Results
- A total of 14 public datasets consisting of 3,410 CT scans are assembled
- The CT scans in LiTS [3], KiTS [24], and MSD Pancreas [1] contain tumors in liver, kidney and pancreas, respectively
- These scans are used to compute the sensitivity of tumor detection
- To perform an alternative check of specificity, we use CHAOS [60] and Pancreas-CT [49]
- Universal Model achieves a higher specificity than most baseline models, while maintaining a compelling sensitivity
- High specificity is clinically important because it reveals that Universal Model does not predict many false positives

### Organ Segmentation on BTCV and MSD
- We provide the leading solution in Beyond The Cranial Vault (BTCV), surpassing the previous methods by a large margin.
- Specifically, Table 3 compares Universal Model with other methods in the validation set of BTCV, offering at least 3.5% improvements over the second best.
- Besides, Figure 3 present detailed comparisons of 5-hold cross validation on the MSD dataset.
- 4 https://monai.io/ shows manual annotations have inter-rater and intra-rater variance [27], particularly in segmentation tasks, because some of the organs' boundaries are blurry and ambiguous.
- We assess the quality of pseudo labels predicted by Universal Model and manual annotation performed by human experts.
- 17 CT scans in BTCV have been annotated by two independent groups of radiologists from different institutes (not test server labels).
- As a result, each CT scan is associated with AI prediction, and two human annotations (Dr1 and Dr2).
- Figure 4 presents their mutual DSC scores, i.e., AI↔Dr1, AI↔Dr2, and Dr1↔Dr2.
- We find the DSC between AI and humans is slightly larger than the DSC between humans in segmenting 6 types of organs (i.e., spleen, liver, kidney, stomach, and pancreas).

### Tumor Detection on Five Datasets
- Figure 3 demonstrates that Universal Model surpasses Swin UNETR by a large margin in segmenting liver, pancreatic, and colon tumors
- DSC scores cannot faithfully reveal the tumor detection performance because, by default, they are only calculated on abnormal CT scans (with tumors) [26]
- The AI might generate numerous false positives when encountering normal CT scans (that have no tumor) [50]
- Therefore, we also evaluate patient-level Sensitivity and Specificity for detecting the three types of tumors
- To obtain normal CT scans, we adopt the CHAOS and Pancreas-CT datasets because these two datasets provide pathological verification that no tumors are present [49,60]
- Table 4 indicates that Universal Model achieves the highest Specificity of 95%, 95%, and 91% on normal CT scans, while maintaining a compelling Sensitivity of 89%, 92%, and 94% on abnormal CT scans
- Rows 1-3 in Figure 5 depict the prediction of small/medium/large pancreatic tumors; Row 4 shows that Universal Model can precisely segment the pancreas and 5 The quality of 19 other organs and 6 tumors has not been compared with human annotations because there is no publicly available CT scans that have been annotated by multiple independent groups on these objects.

## Intriguing Properties

### Efficiency: FLOPs vs. DSC Scores
- It is clinically important to make AI models faster
- Table 6. Transferability: Fine-tuning performance. Fine-tuning Universal Model significantly outperforms learning from scratch on two downstream datasets (i.e., TotalSegmentator and JHH). Moreover, Universal Model, trained by image segmentation as proxy task, can extract better visual representation-more related to segmentation tasks-than other pre-trained models developed in the medical domain.
- Due to the space, the per-class evaluation of TotalSegmentator and JHH can be found in Appendix Tables 7-10 and Table 11, respectively.

### Generalizability: External Datasets Results
- The key expectation of reliable medical AI models is their generalizability,
- Compared with dataset-specific models, Universal Model was trained on the order of magnitude more diverse CT scans, therefore demonstrating significantly better generalizability (i.e., directly testing the model on external data without adaptation or fine-tuning).
- We conduct the evaluation on a public dataset 3D-IRCADb and a private dataset JHH, which are absolutely not seen in the training and can be regarded as external validation.
- As shown in Table 5, Universal Model substantially outperforms the previous methods on 3D-IRCADb and JHH with a DSC improvement of 5% and 4%, respectively.

### Transferability: Fine-tuning Results
- Universal Model is a powerful pre-training model for segmentation
- Through pre-training by assembly dataset directly, the Universal Model achieves the highest DSC compared with other pre-training methods
- In the To-talSegmentator dataset, the Universal Model achieves a DSC of 86.49%, 89.57%, 94.43% and 88.95% for four tasks

### Extensibility: Incremental Learning Results
- Introducing additional classes (CLIP embeddings) would not affect the segmentation model architecture
- As a result, Universal Model is easily extended to upcoming datasets in the future with novel anatomical structures annotated
- To segment the left and right renal veins, two new CLIP embeddings for the left and right renal veins are concatenated to the original CLIP embeddings
- Another text-driven segmentor for the new classes is also added to the model, and the segmentation output for the new classes is generated by this new segmentor
- The other parameters of the model remain the same.

## Conclusion
- Representing the first attempt to explore the potential of medical AI models in an assembly dataset
- To solve the label inconsistency and orthogonality problem and build up an extensible framework
- Proposing a Universal Model with CLIP embedding to enable a flexible and powerful segmentor
- Demonstrating that CLIP embedding can establish the anatomical relationship and enable the model to be extended to novel organs and tumors

## Appendix: CLIP-Driven Universal Model
- The CLIP-Driven Universal Model is a machine learning model that can learn to predict the outcomes of medical procedures.
- The model was trained on a set of public datasets, including 14 datasets that were collected from real-world medical procedures.
- The model was able to predict the outcomes of medical procedures with high accuracy.
- The model was also able to predict the outcomes of medical procedures with low accuracy when the medical prompt template was used.
- The model was able to learn to predict the outcomes of medical procedures with low accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was used.
- The model was able to learn to predict the outcomes of medical procedures with low accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt template was not used.
- The model was able to learn to predict the outcomes of medical procedures with high accuracy when the medical prompt...

## A. Medical Prompt Template
- To fully explore the effect of templates on CLIP embedding, an experiment is performed in the whole assembly of datasets
- Four text templates are employed to show the context, i.e., "V1: A computerized tomography of a [CLS].", "V2: There is [CLS] in this computerized tomography.", "V3: This computerized tomography has a [CLS].", "V4: A photo of a [CLS]."
- The effectiveness of the prompt template is slightly different from the toy experiment. With increasing organ numbers, templates V1 and V2 still show better performance in encoding the relationship, whereas template V3 would deteriorate the results.
- In addition, a widely used template V4 could also promote the segmentation performance.

## B. Assembly of Datasets
- The 14 publicly available datasets for training and 2 public datasets and 1 largescale private dataset for testing are summarized in Table 1.
- It is non-trivial to assemble datasets annotated from various institutions, as the annotation protocols are inconsistent.
- For these datasets (KiTS, WORD, AbdomenCT-1K, and CT-ORG), which do not distinguish between the left and right organs, we split the organ (Kidney, Adrenal Gland, and Lung) into left part and right part through the script.
- In addition, we have taken the inclusion relation into consideration, e.g., the organ tumor is part of the organ, and the hepatic vessel is inside the liver.
- Fortunately, we formulate each organ segmentation result as a binary mask. Thus, we can organize the segmentation ground truth for these overlapped organs independently in a binary mask manner.
- Pancreas-CT [49] consists of 82 contrast-enhanced abdominal CT volumes. This dataset only provides the pancreas label annotated by an experienced radiologist, and all CT scans have no pancreatic tumor.
- LiTS [3] contains 131 and 70 contrast-enhanced 3-D abdominal CT scans for training and testing, respectively. The data set was acquired by different scanners and protocols at six different clinical sites, with a largely varying in-plane resolution from 0.55 to 1.0 mm and slice spacing from 0.45 to 6.0 mm.
- KiTS [23] includes 210 training cases and 90 testing cases with annotations provided by the University of Minnesota Medical Center. Each CT scan has one or more kidney tumors.
- AbdomenCT-1K [36] consists of 1112 CT scans from five datasets with liver, kidney, spleen, and pancreas annotations. [48] is composed of 140 CT images containing 6 organ classes, which are from 8 different medical centers. Most of the images exhibit liver lesions, both benign and malignant.
- CHAOS [60] provides 20 patients for multi-organ segmentation. All CT scans have no liver tumor. [1] includes liver, lung, pancreas, colon, hepatic vessel, and spleen tasks for a total of 947 CT scans with 4 organs and 5 tumors.
- BTCV [31] consists of 50 abdominal CT scans from metastatic liver cancer patients or post-operative ventral hernia patients. They are collected from the Vanderbilt University Medical Center.
- AMOS22 [28] is the abbreviation of the multi-modality abdominal multi-organ segmentation challenge of 2022. The AMOS dataset contains 500 CT with voxel-level annotations of 15 abdominal organs.
- WORD [35] collects 150 CT scans from 150 patients before the radiation therapy in a single center. All of them are scanned by a SIEMENS CT scanner without appearance enhancement. Each CT volume consists of 159 to 330 slices of 512 × 512 pixels.
- 3D-IRCADb [54] contains 20 venous phase enhanced CT scans. Each CT scan has various annotations, and only annotated organs are tested to validate the model's generalizability.

## C. Implementation Details
- Our data augmentation is implemented in python with MONAI 6
- The orientation of CT scans is changed into specified axcodes
- Isotropic spacing is adopted to re-slice each scan to the same voxel size of 1.5 × 1.5 × 1.5mm 3
- Truncate the intensity in each scan to the range [−175, 250] and linearly normalize them to [0, 1]
- Considering the valid part is part of the whole medical image, we crop only the foreground object based on the images.
- During training, we crop random fixed-sized 96×96×96 regions with the center being a foreground or background voxel based on the predefined ratio.
- Also, we randomly rotate the input patch by 90 degrees and shift intensity with 0.1 offset with 0.1 and 0.2 probability.
- To avoid confusion between the organ in the right and left parts, we do not use mirroring augmentation.
- Text branch. We apply the pre-trained text encoder "ViT-B/32" of the CLIP as the text branch.
- We can extract and store the text features to reduce overhead brought by the text encoder in the training and inference stage since the CLIP embedding only depends on the dictionary, which is fixed.
- Vision branch. We adopt Swin UNETR as a vision encoder. The Swin UNETR consists of 4 attention stages comprising 2 transformer blocks and 5 convolution stages comprising of CNN-based structure. In the attention stage, a patch merging layer is used to reduce the resolution by a factor of 2. Stage 1 consists of a linear embedding layer and transformer blocks that maintain the number of tokens as H 2 × W 2 × D 2 . a patch merging layer groups patches with resolution 2 × 2 × 2 and concatenates them, resulting in a 4C-dimensional feature embedding. A linear layer is then used to down-sample the resolution by reducing the dimension to 2C. The same procedure continues in stages 2, 3, and 4.
- The text-based controller is a single convolutional layer, which takes the CLIP embedding and global pooling feature from the last convolution stages in the vision encoder as input.
- The Dice similarity coefficient (DSC) and Normalized Surface Distance (NSD) are used as measurements for 3D segmentation results.

## D. Additional Evaluations
- Tables 7-10 and 11 show the per-class evaluation of TotalSegmentator and JHH, which validates the transferability of the proposed Universal Model.
- Figure 7 exhibits the contour line comparison among Universal Model and two human experts. We can see the model predictions are roughly similar to human annotation, which validates the effectiveness of the pseudo label generated by our Universal Model.
- Figure 9 and Figure 8 show several kidney and liver tumor cases comparison among the proposed Universal Model and four competitive baseline methods. Our method can not only detect small and big tumors in various organs but also not generate false positives of tumors.

## E. Discussion of Open Challenges
- The first open challenge is the inconsistent annotation protocol.
- The annotation standard is different from institution to institution.
- For example, the aorta annotation is different in AbdomenCT-12organ and other datasets, as shown in Figure 10.
- A part of the upper aorta region is missing in AbdomenCT-12organ, while the annotation is complete in BTCV and AMOS.
- This open challenge requires several experienced radiology experts to re-annotate the CT scans.
- The second challenge is the long tail problem. We count the proportion of each organ and tumor in Figure 11.
- The assembly of datasets has a severe long-tail distribution, which would lead to unsatisfactory performance of tumor classes.
- Mitigating the long-tail distribution would contribute to more robust detection of the tumor.
