---
title: "Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks"
date: 2022-12-19T23:33:21.000Z
author: "Kaiser Sun, Peng Qi, Yuhao Zhang, Lan Liu, William Yang Wang, Zhiheng Huang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09912v1.webp" # image path/url
    alt: "Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09912)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09912).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/tokenization-consistency-matters-for).

# Abstract
- Generative models have been widely applied to solve extractive tasks
- Tokenization inconsistency damages the extractive nature of these tasks
- Fix for tokenization inconsistency leads to performance increase

# Paper Content

## Introduction
- Pretrained sequence-to-sequence models have achieved remarkable success in a wide range of tasks
- Tokenizer is frequently discussed, including different tokenization methods and the model's robustness to different tokenization
- In our work, we identify an issue of tokenization consistency that can affect the performance of seq2seq models
- When a seq2seq task has extractive nature, i.e., parts of the output are extracted from the input text, the desired output can be tokenized differently from how it is tokenized in the input, leading to tokenization inconsistency during model training
- For example, in extractive question answering (QA) task, which takes a context question pair as input and outputs a span of context as answer, the answer might be tokenized differently as its appearance in the given context
- This seemingly minor difference in tokenization at training time can lead to a notable impact on model performance during inference
- We use extractive QA as a case study to identify when tokenization inconsistency happens and propose an simple and effective approach to mitigate this issue -extracting the tokenized answer from the context for training
- When fine-tuning with consistently tokenized instances, the model achieves better in-domain and out-of-domain performance (+0.9% F 1 in-domain and +2.0 % zeroshot out-of-domain), converges faster during training, and is less likely to generate out-ofcontext output (i.e. less likely to hallucinate textually).

## Related Work
- BPE, language-model-based segmentation, and their variants are commonly used as the tokenizers for NLP models
- Recent research has identified these tokenization approaches as sources of poor model generalization
- For example, BPE has been shown to produce less ideal morphological segmentation
- Provilkov et al. (2020) propose BPE-Dropout to stochastically corrupt its segmentation so that it is not produced determinstically
- He et al. (2020) further propose Dynamic Programming Encoding (DPE) by marginalizing output sequence as a latent variable to create better segmentation
- Vilar and Federico (2021) extend BPE to generate morphologically advantageous segmentation for machine translation (MT)
- These approaches either modify model train-ing by using stochastic inputs or by modifying how BPE segmentation is done to improve model robustness and generalization
- In contrast, we propose a simple, deterministic, and effective approach that does not rely on altering the segmentation approach in any way
- Besides these generic approaches, researchers have also investigated domain-specific approaches to improve segmentation on specific texts
- Petrak et al. (2022) propose a new pre-training approach, aim at overcoming the inconsistency of tokenizing numbers and enhancing model's numerical reasoning ability
- Rosenbaum et al. (2022) use "spacejoined tokens" to resolve many string matching anomalies after tokenization that lead to unfair evaluation in semantic parsing
- In contrast, we look at inconsistency in a more general perspective: not only numbers and preceding spaces can be tokenized inconsistently, but also punctuation and their compounds
- Another line of research focuses on character-and byte-level modeling as procedures that are free of tokens
- Li et al. (2021) argue that character-level models better capture morphological phenomena and rare words under low-resource machine translation, while Gaido et al. (2021) show that character-based segmentation can help with alleviating gender bias during translation
- While tokenization-free methods show potential to surpass subword segmentation approaches, we keep our focus to be the consistency issue of subword tokenizations.

## Consistent Tokenization: What It Is and How To Achieve It
- The task takes text x as input and outputs y, where y is a sequence of tokens
- When the task is extractive, there exists two sets of indices, I and J, such that x I = y J
- Inconsistent tokenization could emerge due to the existence of preceding space, numbers, or punctuation when using the BPE tokenizer
- When the input and output are tokenized inconsistently, the task can no longer be solved by simply extracting the output from the input token ids, but requires an additional step from the model to "paraphrase" the input ids into the output token ids that do not exist in the input ids
- Therefore, learning with inconsistently tokenized instances become an inherent predicament for model compared to learning with their consistently tokenized counterparts

## Case Study: Extractive QA with Generative Models
- Extractive QA is an ideal candidate for studying the effect of consistent tokenization
- Recent work has demonstrated that applying generative models to this task leads to great performance gains

### Task Description
- The model is given a question with a context and expected to output a span of the context as an answer.
- Extractive models are typically configured with the question and context as the input, and trained to return start and end indices to indicate the location of the predicted answer in the context.
- To apply a generative model to this task, we simply replace the index prediction task with a task of directly predicting the answer string from the decoder of a seq2seq model.

### Experimental Setup
- Data MRQA (Fisch et al., 2019)  consistent variants on all of SQuAD, Trivi-aQA and NewsQA, with in-domain performance gains of 1.0, 1.1 and 0.6 for the three datasets, respectively (marked by shaded cells).
- A similar observation is made with the EM results.
- One potential explanation for the improvement is that, the task becomes inherently simpler with consistent tokenization.
- We hypothesize that consistent tokenization allows the model to simply extract answers from the context instead of also needing to learn to paraphrase the answer into something that does not exist in the context, as we mentioned in Section 3.
- We will validate this hypothesis when we examine the convergence speed of the models with or without consistent tokenization during training.
- Consistent tokenization also improves zeroshot QA performance on out-of-domain datasets.
- We also examine zero-shot model performance on unseen domains in Table 2.
- We find that on all the OOD datasets, the corresponding F 1 of the consistent model is either comparable or higher than the original model, with the highest gain more than 4%. This implies that training with consistent tokenization systematically improves model generalization beyond better overfitting the training domain.
- The improvement on unseen domain can also be explained by the reduction of difficulty: training with consistent tokenization provides the model with information that the answer must exist within the input, thus the model is no longer required to extensively search the entire vocabulary on a previously unknown domain.
- Training with consistent tokenization leads to faster model convergence, and improved model confidence on the gold answer.
- We present the training curves of models fine-tuned on SQuAD with both versions of tokenization in Figure 2, and include training curves for other datasets in Appendix B.
- Across all the datasets, the consistent models converge faster than the original models. This finding aligns with our discussion that it is easier to solve extractive QA when it can be solved by simply extract the answers at token level.
- To further validate this, we also examine the log perplexity of the gold answer for each instance, and compare the distributional difference between consistent and original models (shown in Figure 3).
- We see that the overall distribution of log perplexity difference leans to the left, suggesting that the model is more confident in generating gold answers when tokenization consistency is enforced during training.
- Training with consistent tokenization leads to less textual hallucination.

## Conclusion
- We identify the issue of tokenization consistency in extractive tasks, propose an easy-to-implement method to guarantee consistent tokenization, and show that the model can be benefited in several aspects when trained with consistent tokenization.
- When the model is trained with consistent tokenization, it obtains a better performance whent training with the input.
- The learning curves with models training on Triv-iaQA and NewsQA are shown in Figure 5 and 6, in both of which consistent model exhibits a faster convergence speed than that of original model.
- Out-of-Domain Datasets Figure 3 shows the log perplexity difference between consistent and original models on in-domain dataset.
- We present an example of log perplexity difference on out-of-domain dataset in Figure 7, using BioASQ as an example.
- Table 4 presents an example of out-of-context answer generated by the model.
- CBS set the base rate for a 30-second advertisement at $5,000,000, a record high price for a Super Bowl ad.
- As of January 26, the advertisements had not yet sold out. CBS mandated that all advertisers purchase a package covering time on both the television and digital broadcasts of the game, meaning that for the first time, digital streams of the game would carry all national advertising in pattern with the television broadcast.
- This would be the final year in a multi-year contract with Anheuser-Busch InBev that allowed the beer manufacturer to air multiple advertisements during the game at a steep discount.
- It was also the final year that Doritos, a longtime sponsor of the game, held its "Crash the Super Bowl" contest that allowed viewers to create their own Doritos ads for a chance to have it aired during the game.
- Nintendo and The Pokémon Company also made their Super Bowl debut, promoting the 20th anniversary of the Pokémon video game and media franchise.
- What does the lawmaker say?
- Answer: Israeli military action in Gaza is comparable to that of German soldiers during the Holocaust
- Prediction: Nazi soldiers during the Holocaust
