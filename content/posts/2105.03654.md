---
title: "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
date: 2021-05-08T09:45:21.000Z
author: "Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2105-03654v3_VdABhBloF.jpg" # image path/url
    alt: "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2105.03654)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2105.03654).


# Abstract
- Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance.
- In many application scenarios, however, such contexts are not available.
- In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query.
- We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence.
- Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions.
- Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.

# Paper Content

## Introduction
- Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models.
- Recent work (Devlin et al., 2019;Yu et al., 2020;Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models.
- senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections .
- President Obama called for eliminating the legislative filibuster last month , which could occur if Democrats retake the Senate .
- Some Republicans say it ' s time to undo a wrong committed by Reid .
- Senate Republicans are considering using the " nuclear option " to end a potential Democratic filibuster and confirm Neil Gorsuch to the Supreme Court .
- Senate Republicans deployed the " nuclear option " on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump ' s nominees .
- Label: Group
- Label: Non Entity
- Retrieved Texts: Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of "democrats" and "republican". However, there are a lot of application scenarios in which document-level contexts are unavailable in practice.
- For example, there are sometimes no available contexts in users' search queries, tweets and short comments in various domains such as social media and E-commerce domains.
- When professional annotators annotate ambiguous named entities in such cases, they usually rely on domain knowledge for disambiguation. This kind of knowledge can often be found through a search engine.
- Moreover, when the annotators are not sure about a certain entity, they are usually encouraged to find related knowledge through a search engine (Wang et al., 2019). Therefore, we believe that NER models can benefit from such a process as well.
- In this paper, we propose to improve NER models by retrieving texts related to the input sentence by an off-the-shelf search engine. We re-rank the retrieved texts according to their semantic relevance to the input sentence and select several top-ranking texts as the external contexts. Consequently, we concatenate the input sentence and external contexts together as a new retrieval-based input view and feed it to the pretrained contextual embedding module, so that the resulting semantic representa-arXiv:2105.03654v3 [cs.CL] 8 Dec 2022 tions of the input tokens can be improved.
- The token representations are then fed into a CRF layer for named entity prediction.

## Framework
- CL allows for a model that is both consistent and learnable
- The architecture of our framework is shown in Figure 2
- The framework can be used to learn representations of data

### Re-ranking
- Given an input sentence as a search query, the search engine returns ranked relevant texts.
- However, the off-the-shelf search engine is highly optimized for a fast speed over a large set of documents, so it may sometimes produce semantically irrelevant results or rank the results using inaccurate relevance scores.
- We propose to apply BERTScore (Zhang et al., 2020) to score the relatedness of each retrieved text to the input sentence.
- BERTScore is a language generation metric that calculates a sum of cosine similarity between token representations of two sentences.
- Therefore, it is more likely that the search query and the retrieved texts have strong semantic relations when BERTScore is large.
- Given the corresponding prenormalized token representations {r 1 , • ternal contexts: where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, "[SEP]" in BERT).

### NER Model
- We solve the NER task as a sequence labeling problem
- We apply a neural model with a CRF layer
- The token representations are fed into a CRF layer to get the conditional probability p θ (y|x)
- Now the loss function in Eq. 2 becomes:

### Cooperative Learning
- The retrieval-based input view meets the requirement of the first scenario for its strong token representations
- However, it does not meet the requirement of the second scenario
- The external contexts are usually significantly longer than the input sentence and a search engine may not meet the latency requirements
- These two issues significantly slow down the prediction speed of the model
- Therefore, it is essential to improve the accuracy of the original input views in a unified model to meet these two scenarios
- We propose two approaches for CL: Token Representations: Stronger token representations usually lead to better accuracy on the task. Therefore, CL constrains the token representations of two input views to be similar. This helps the model learn to predict the token representations with external contexts even if the contexts are not available. In this approach, D is the L 2 norm to represent the distances of the token representations: Label Distributions: Since CL enforces the label predictions of both input views to be similar, a straight-forward approach is constraining the label distributions predicted by the model to be similar with the two input views. In this approach, we use the KL divergence as the function D. Then objective function in Eq. 4 becomes the KL divergence between p θ (y|x, x) and p θ (y|x): With the CRF layer, the loss function is difficult to calculate because the output space of p θ (y|•) is exponential in size. To alleviate this issue, we calculate the KL divergence between the marginal distributions q θ (y i |x, x) and q θ (y i |x) at each position of the sentence to approximate Eq. 6. The marginal distributions can be obtained using the forward-backward algorithm: As mentioned earlier, we do not back-propagate the gradient through p θ (y|x, x). Therefore calculating the KL divergence is equivalent to calculating the cross-entropy loss between q(y|x, x) and q(y|x): Together with the negative log-likelihood losses in Eq. 2, 3, the total loss in training is a summation of label losses and a CL loss: where L CL (θ) can be one of the CL loss in Eq. 5, 3

### Settings
- We use WNUT-16 (Strauss et al., 2016) and WNUT-17 (Derczynski et al., 2017) datasets collected from social media.
- We use the standard split for these datasets.
- We use CBS SciTech News dataset collected by Jia et al. (2019).
- The dataset only contains the test set with the same label set as the CoNLL-03 dataset.
- We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain.
- We collect and annotate an internal dataset from one anonymous E-commerce website.
- The dataset contains 25 named entity labels for goods in short texts.
- We also collect 300,000 unlabeled sentences for semi-supervised training.
- We show the statistics of the datasets in Table 1.
- Annotations of the E-commerce dataset
- We manually labeled the user queries through crowdsourcing from www.aliexpress.com, which is a real-world E-commerce website.
- For each query, we asked one annotator to label the entities and ask another annotator to check the quality.
- After that, we randomly select 10% of the dataset and ask the third annotator to check the accuracy.
- As a result, the overall averaged query-level accuracy is 95%.
- The dataset will not be released due to user privacy.
- We use an internal E-commerce search engine for the E-commerce dataset.
- For the other datasets, we use Google Search as the search engine. Google Search is an off-the-shelf search engine and can simulate the offline search over various domains.
- We use summarized descriptions from the search results as the retrieved texts.
- We filter the retrieved texts that contain any part of the datasets.
- Our reranking module selects top 6 relevant texts as the external contexts of the input sentence and chunk the external contexts if the total sub-token lengths of the input sentence and external contexts exceeds 510.
- For the re-ranking module, we use Roberta-Large (Liu et al., 2019) for token representations which is the default configuration in the code of BERTScore (Zhang et al., 2020).
- For token representations in the NER model, we use pretrained Bio-BERT (Lee et al., 2020) for datasets from the biomedical domain and use XLM-RoBERTa (Conneau et al., 2020) for datasets from other domains.
- We train the NER models for 10 epochs for the datasets in Social Media and Biomedical domains while we train the NER models for 5 epochs for other datasets for efficiency as these datasets have more training sentences.

### Results
- LUKE is a very recent state-of-the-art model on CoNLL-03 NER dataset proposed by Yamada et al. (2020)
- We use the same parameter setting as Yamada et al. (2020) and use a single sentence as the input instead of taking document-level contexts in the dataset as in Yamada et al. (2020) for fair comparison.
- W/O CONTEXT represents training the NER model without external contexts (Eq. 2), which is the baseline of our approaches.
- W/ CONTEXT represents training the NER model with external contexts (Eq. 3).
- CL-L 2 represents minimizing the L 2 distance between token representations (Eq. 5).
- CL-KL represents minimizing the KL divergence (Eq. 8) between CRF output distributions.
- Besides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores.
- During the evaluation, our approaches are evaluated using inputs without external contexts (W/O CONTEXT) and inputs with them (W/ CONTEXT).
- We report the results averaged over 5 runs in our experiments. The results are listed in Table 2.
- With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets. Our approaches significantly outperform the baseline that is trained without external contexts.
- Comparing with LUKE, our approaches and our baseline outperform LUKE in all the cases. The possible reason is that LUKE is pretrained only using long word sequences, which makes the model prone to fail to capture the information of entities based on short sentences.
- For our approaches, with CL, the accuracy can be improved on both input views comparing with W/O CONTEXT and W/ CONTEXT, which shows adding constraints between the two views during training helps the model better utilize the original text information.
- For the two constraints in CL, we find that CL-KL is relatively stronger than CL-L 2 in a majority of the cases.

### Cross-Domain Transfer
- Cross-domain transfer is improved with CL
- The accuracy gap between the two input views is diminished

### Semi-supervised Cooperative Learning
- Cooperative learning can take advantage of large amounts of unlabeled text for further improvement
- We jointly train on the labeled data and unlabeled data in training to form a semi-supervised training manner
- During training, we alternate between minimizing the loss (Eq. 9) for labeled data and the CL loss for unlabeled data (Eq. 4)
- We conduct the experiment on the E-commerce dataset as an example
- Results in Table 4 show that the accuracy of  † represents the approach is significantly (p < 0.05) stronger than the approach without semi-supervised learning with the same input view.

## Analysis
- We use the WNUT-17 dataset in the analysis.
- The WNUT-17 dataset has 17,000 entries.
- The dataset is used to study the performance of different algorithms.

### Comparison of Re-ranking Approaches
- Various re-ranking approaches may affect the token representations of the model.
- We compare our approach with three other re-ranking approaches.
- The first is the ranking from the search engine without any re-ranking approaches.
- The second is reranking through a fuzzy match score.
- The approach has been widely applied in a lot of previous work (Gu et al., 2018;Zhang et al., 2018;Hayati et al., 2018;Xu et al., 2020).
- The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring.
- We train our models (W/ CONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT-17 in Table 5.
- Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05).

### How the Context Quality Affects Accuracy
- The NER model will perform worse when the quality of external contexts varies
- The NER model is more accurate when using external contexts that are more semantically relevant to the input sentence

### Ablation Study
- To show the effectiveness of CL, we conduct three ablation studies
- The first one is training the NER model based on one view and predict on the other
- The second is jointly training both views without the CL loss term (removing L CL (θ) in Eq. 9)
- The final one is using both CL losses to train the model (L CL (θ) = L CL-L 2 (θ) + L CL-KL (θ) in Eq. 9)
- Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts
- However, when the model is trained with the external contexts, the accuracy of the model drops when predicting the inputs without external contexts
- In joint training without CL, the accuracy of the model over inputs without contexts can be slightly improved but the accuracy over inputs with contexts drops, which shows the benefit of adding CL

## Related Work
- Named Entity Recognition has been studied for decades
- Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF to achieve state-of-the-art accuracy
- Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings
- Very recent work (Yu et al., 2020;Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets
- Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018;Zhang et al., 2018;Xu et al., 2020), text generation (Weston et al., 2018;Kim et al., 2020), semantic parsing (Hashimoto et al., 2018;Guo et al., 2019)
- Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens.
- For the re-ranking models, fuzzy match score (Gu et al., 2018;Zhang et al., 2018;Hayati et al., 2018;Xu et al., 2020), attention mechanisms (Cao et al., 2018;Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020;Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts.
- Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts based on pretrained contextual embeddings.
- Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data.
- Sun (2013)  Knowledge Distillation Knowledge distillation (Buciluǎ et al., 2006;Hinton et al., 2015) transfers the knowledge of "teacher" models to smaller "student" models through minimizing the KL divergence of prediction probability distribution between the models.
- In speech recognition (Huang et al., 2018) and natural language processing (Wang et al., 2020a(Wang et al., 2021b)), the marginal probability distribution of the linear-chain CRF layer has been applied to distill the knowledge between teacher models and student models.
- Comparing with these approaches, our approaches train a single unified model instead of transferring the knowledge between two models. We also show that the accuracy of both views can be improved with our approaches, unlike in knowledge distillation only the student model is updated and improved.

## Conclusion
- Our proposed approach, which retrieves related contexts from a search engine, improves the accuracy of the NER model
- Our approach is even stronger than some of the previous approaches utilizing maximal document-level contexts
- Our approach is effective in a semi-supervised training manner
