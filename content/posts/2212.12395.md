---
title: "Detecting Objects with Graph Priors and Graph Refinement"
date: 2022-12-23T15:27:21.000Z
author: "Aritra Bhowmik, Martin R. Oswald, Yu Wang, Nora Baka, Cees G. M. Snoek"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12395v1.webp" # image path/url
    alt: "Detecting Objects with Graph Priors and Graph Refinement" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12395)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12395).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/detecting-objects-with-graph-priors-and-graph).

# Abstract
- The goal of the paper is to detect objects in images by exploiting their interrelationships
- Rather than relying on predefined and labeled graph structures, we infer a graph prior from object co-occurrence statistics
- The key idea of the paper is to model object relations as a function of initial class predictions and co-occurrence priors to generate a graph representation of an image
- We additionally learn the object-relation joint distribution via energy based modeling
- Sampling from this distribution generates a refined graph representation of the image which in turn produces improved detection performance
- Experiments on the Visual Genome and MS-COCO datasets demonstrate our method is detector agnostic, end-to-end trainable, and especially beneficial for rare object classes

# Paper Content

## Introduction
- Object detection is a classical task in computer vision
- Deep convolutional networks and self-attention based transformer networks have been used to improve object detection
- In this paper, we propose a way to represent object-relations in an image and use this representation to improve detection rates
- Our method is detector agnostic, end-to-end trainable, and especially beneficial for rare object classes

## Related Works
- Modern object detection models can be broadly classified into two groups: those based on convolution architectures and those based on transformers.
- In the convolution family, methods like Faster-RCNN [38] and R-FCN [8] are well established. They consists of a region proposal network to generate regions of interest and a classification and bounding box regression module to predict a set of objects.
- YOLO [37] and SSD [30] predict objects directly from convolutional feature maps.
- ObjectBox [49] treats objects as center-points in a shape and size agnostic fashion and allows learning at all scales.
- All these methods typically utilize a pretrained backbone like ResNet [18] for feature extraction.
- Recently, there has been a rise in methods which use an attention mechanism in a transformer network. For instance, ViT [43], SwinTransFormer [32] and MViTv2 [25].
- Carion et al. [5] introduced a detection transformer (DETR) which uses an attention mecha-nism for object detection in an end-to-end manner.
- To speed up convergence and reduce the computational cost for the self-attention on image features, Zhu et al. [50] introduced multi-head deformable attention replacing the dot-product in the attention computation with two linear projections for sampling points and computing their attention weights.
- Nguyen et al. [35] introduced BoxeR which samples boxes instead of points for attention computation, for object detection, instance segmentation and 3D detection.
- Although these object detectors use spatial context information in the image space, they do not explicitly model the relation between (detected) objects.
- Using Faster-RCNN and DETR as representative examples of the convolution and transformer approach to object detection, we demonstrate that explicit object context information improves object detection.
- Object graph representations. Aside from classic object detection, many works have aimed to represent the interactions between objects and scenes.
- Early works [14,15,34] use object relations as a post-processing step on simpler datasets like PASCAL VOC [13].
- Recent works model objects and relations together in a graph structure, e.g., [6,7,22,33] where Xu et al. [48] use a unified conditional random field to model the joint distribution of all the objects and their relations in a scene graph.
- However, this requires ground truth labels for both objects and their relations per image in the dataset.
- To avoid explicit relation annotations, others, e.g., [21,[45][46][47] exploit more general priors like cooccurrence or attributes of object classes to obtain a graph representation of an image.
- Jiang et al. [21] predict the edge values representing the relations between object pairs, whereas Xu et al. [46] propose a form of self-attention to obtain these edges.
- In a follow-up work, Xu et al. [47] introduce a global semantic pool, created using the weights of individual object classifiers, as a relationship prior instead of generating an explicit graph representation.
- To aggregate information from different domains, they also introduce graph transfer learning [45].
- All these works enhance the object feature representation by propagating the relation information via a graph structure.
- Recently, Suhail et al. [40] propose an energy-based learning framework for scene graph generation to learn the joint distribution of objects and their relations together.

## Method

### Graph Priors for Object Detection
- Prior-guided object detection.
- For an input image I, visual features {F i } N 1 of dimension D are extracted via a base object detector for N object proposals.
- These feature vectors along with contextual prior knowledge are used in pairs [21,46] to obtain pair-wise edge relationships, thus creating an edge connectivity matrix E.
- A popular source of prior knowledge on objects is the large scale Visual Genome dataset [23] which consists of pairwise relationship between object classes such as "subject-verb-object" relationships (e.g. walking, swimming), spatial relationships (e.g. at, far, in).
- By considering the most frequent relationship annotations in the dataset and counting the co-occurrence among class pairs, a C × C frequent statistics matrix T is generated.
- Each entry in the matrix is a floating point value between 0 and 1, indicating the likelihood of the object pairs co-occurring in an image in the dataset.
- A graphical representation of the input image is obtained based on the feature vectors and the edge values defined as G=(N , E), where N are the set of nodes and E is the edge connectivity matrix.
- This graph is propagated via some message passing algorithm to calculate refined features which are used to obtain final classification and bounding box regression results.
- novel graph representation.
- We model object relations as a function of initial class predictions instead of directly predicting them from feature vector pairs [21].
- Corresponding to the feature vectors {F i } N 1 , we obtain their classification probabilities from the base detector given by P where P ∈ R N ×C , C being the number of classes in the dataset.
- Given the class prediction matrix P and the prior matrix T , we calculate the edge connectivity matrix as:
- Each term e ij in the matrix E is a result of all possible probabilistic combinations of classes for nodes i and j along with their corresponding co-occurrence prior values.
- By modeling the edge matrix in this manner, we penalize the class predictions jointly with the edge values, the former guiding the correctness of the later. Thus, optimizing for the final task of classification would ensure refined edge values, which in turn guides the end task.
- Our joint task loss function is given by:
- where L CE refers to the cross-entropy loss, P GP is the final classification probability matrix output from our model, P is the initial class predictions of the base detector and P * is the ground truth object labels.
- Optimality is obtained when the initial predictions are the same as the ground truth labels.

### Potential of Graph Priors
- The experiment replaces the set of edges E, which was created from the initial class prediction matrix P with the ground truth relation set E GT .
- This is done by performing IoU-matching for the set N of object features with ground truth labels to assign each node a class type, and then obtaining the ground truth edge between a pair of nodes by looking up the prior knowledge matrix T .
- Thus we create our ground truth graph G GT =(N , E GT ), which optimally captures the underlying object-pair relationships.
- Propagating this graph generates the refined object features which are used for the final classification results.
- As ground truth we rely on the 1,000 (V G 1000 ) and 3,000 (V G 3000 ) most frequent object classes from Visual Genome [23].
- We use two proposal-based object detectors with our graph priors, the transformer-based DETR [5] and convolution-based Faster-RCNN [38].
- Both these methods first identify rectangular regions of interest in an image, extract their corresponding feature encodings and create classification predictions for each such region.
- From Fig. 3, we see that knowing the relations between objects in a scene improves the detection results of the base network by a large margin.
- For the V G 1000 dataset we achieve an 119% relative improvement in mean average precision results for DETR, while for Faster-RCNN, it is around 110%.
- Note that we do not embed the ground truth information in any way other than to use it for looking up the true cooccurrence relation between proposed object-pairs.

### Energy-based Graph Refinement
- Energy-based models represent their joint probability distribution as p θ (x, y)= exp(−E θ (x, y))
- Z(θ) = exp(−E θ (x, y)) is referred to as the normalization constant or partition function and becomes intractable
- In order to learn the parameter θ of the energy-based model, we use a direct log likelihood approach which becomes intractable
- We use Langevin dynamics to sample a graph representation of the image and then train the classification model p φ (z|N , E)
- We optimize the model parameters jointly by minimizing the joint loss function

## Experiments

### Datasets, Evaluation and Implementation
- We use the latest release of the Visual Genome dataset (v1.4)
- We use two sets of target classes, the 1000 most frequent categories and the 3000 most frequent categories, resulting in two settings: VG1000 and VG3000
- We split the 92.9K images of the dataset into 87.9K for training and 5K for testing
- For training Faster-RCNN, we augment the datasets with random horizontal flips and use an image size of 1333×800
- We use stochastic gradient descent as the optimizer and train on 8 GPUs with a mini-batch size of 2.
- We use an initial learning rate of 0.02, reducing it twice by 10; a weight decay of 10 −4 is used along with a momentum of 0.9
- We train the model for 20 epochs with learning rate reductions at the 8th and 11th epoch
- For DETR, we use 6 encoder and decoder layers, 200 output queries, a learning rate of 10 −4 , and train it for 100 epochs on 8 GPUs with a mini-batch size of 2.
- We use a form of graph attention network to propagate the node and edge features jointly, additional details of which will be provided in the supplementary material
- For the network, we use two layers with an input dimension of 1024 and a final output dimension of 256
- We use skip connections in between the layers to propagate feature information
- Sigmoid activation is used in the graph message passing computations
- The final output features of dimension 256 are concatenated with the original features of dimension 1024, and fed into the classification and bounding box regression layers
- We use energy model to calculate the energy of the graph
- The graph representation is pooled into a vector via attention pooling
- We use two linear layers after pooling which converts the features into a scalar energy value
- We use 1000 proposals for our method on FRCNN and 200 object queries for DETR during testing
- We will make trained models and source code publicly available

### State-of-the-art Comparison
- For the comparative evaluation of our method, we first focus on the large-scale detection benchmarks: VG1000 (Visual Genome with 1000 categories) and VG3000 (Visual Genome with 3000 categories).
- For both these settings, we report the results of our method on top of F-RCNN and DETR and compare with state-of-the-art methods modeling object interrelationships.
- As can be seen from Tab. 1, we achieve state-of-the-art results for both datasets on almost all metrics when our method is applied on F-RCNN, be it that SGRN [46] outperforms us slightly on AP L , AR 1 and AR L metrics on the VG3000 dataset.
- We also see a good improvement in our average recall numbers with AR M improving by 6.5 and 5.1 over baseline F-RCNN for VG1000 and VG3000 respectively.
- DETR results are below the results on F-RCNN. This is due to the fact that DETR does not produce region proposals but learns to generate a fixed set of object queries, making it dependant on the number of object instances per image. Nonetheless, we still observe a relative improvement on DETR when our method is applied.
- We also report results of our method based on F-RCNN on the MS-COCO dataset and compare with previously reported results in Tab. 2. We achieve an absolute improvement of 4.2 mAP over F-RCNN, and are either better or comparable to alternatives.
- We do not achieve as much improvements as in the Visual Genome datasets, partly because, the co-occurrence statistics have been directly transferred from the visual genome dataset based on the common classes between the two, and hence, fail to capture the true prior distribution of the object classes in MS-COCO. This leads to our energy model learning an improper objectrelation distribution and hence, we obtain only a mild improvement.
- In Tab. 3, we report the contributions of the components of our method. We see that only using graph priors (GP), we improve consistently on both base detectors, while using graph refinement (GPR), we achieve state-of-the-art.
- The ground truth experiment results are also shown, which are clearly several times higher than the rest of the methods.
- Proposal Number. In Tab. Table 1. State-of-the-art comparison on VG1000 and VG3000. We compare to various baseline methods with two backbone object detectors: Faster-RCNN with feature pyramid networks (F-RCNN) [27] and DETR [5].
- For both methods we applied the proposed graphprior in combination with the proposed energy-based refinement. Methods denoted with an asterisk like F-RCNN* refer to results we obtained by running the original code with default parameters, while the other numbers are the reported values.
- For F-RCNN there is a considerable difference between reported and reproducible numbers.
- Table 2. State-of-the-art comparison on MS-COCO. Despite not having access to the true prior knowledge statistics for this dataset, we already perform comparable to the state-of-the-art.
- Class-specific performance. In Fig. 6, we show the impact of our method on different classes. We plot the classes
- In Tab. Table 1. State-of-the-art comparison on VG1000 and VG3000. We compare to various baseline methods with two backbone object detectors: Faster-RCNN with feature pyramid networks (F-RCNN) [27] and DETR [5].
- For both methods we applied the proposed graphprior in combination with the proposed energy-based refinement. Methods denoted...
