---
title: "HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling"
date: 2023-01-05T18:59:44.000Z
author: "Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O'Toole, Changil Kim"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02238v1.webp" # image path/url
    alt: "HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02238)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02238).


# Abstract

# Paper Content

## Introduction
- Six-degrees-of-freedom (6-DoF) videos allow for free exploration of an environment by giving the users the ability to change their head position (3 degrees of freedom) and orientation (3 degrees of freedom).
- The underlying methodology that drives 6-DoF video is view synthesis: the process of rendering new, unobserved views of an environment-static or dynamic-from a set of posed images or videos.
- Volumetric scene representations such as neural radiance fields [29] and instant neural graphics primitives [30] have recently made great strides toward photorealistic view synthesis for static scenes. While several recent works build dynamic view synthesis pipelines on top of these volumetric representations [14,23,24,33,61], it remains a challenging task to create a 6-DoF video format that can achieve high quality, fast rendering, and a small memory footprint (even given many synchronized video streams from multi-view camera rigs [9,35,44]).
- Existing approaches that attempt to create memory-efficient 6-DoF video can take nearly a minute to render a single megapixel image [23].
- Works that target rendering speed and represent dynamic volumes directly with 3D textures require gigabytes of storage even for short video clips [56].
- While other volumetric methods achieve memory efficiency and speed by leveraging sparse or compressed volume storage for static scenes [11,30], only contemporary work [22,49] addresses the extension of these approaches to dynamic scenes. Moreover, all of the above representations struggle to capture highly view-dependent appearance, such as reflections and refractions caused by non-planar surfaces.
- In this paper, we present HyperReel, a novel 6-DoF video representation that achieves state-of-the-art quality while being memory efficient and real-time renderable at high resolution.
- The first ingredient of our approach is a novel rayconditioned sample prediction network that predicts sparse point samples for volume rendering. In contrast to existing static view synthesis methods that use sample networks [20,31], our design is unique in that it both (1) accelerates volume rendering and at the same time (2) improves rendering quality for challenging view-dependent scenes.
- Second, we introduce a memory-efficient dynamic volume representation that achieves a high compression rate by exploiting the spatio-temporal redundancy of a dynamic scene. Specifically, we extend Tensorial Radiance Fields [11] to compactly represent a set of volumetric keyframes, and capture intermediate frames with trainable scene flow. The combination of these two techniques comprises our high-fidelity 6-DoF video representation, HyperReel.
- We validate the individual components of our approach and our representation as a whole with comparisons to state-of-theart sampling network-based approaches for static scenes as well as 6-DoF video representations for dynamic scenes. Not only does HyperReel outperform these existing works, but it also provides high-quality renderings for scenes with challenging non-Lambertian appearances.

## Related Work
- Image-based rendering techniques use approximate scene geometry to reproject and blend source image content onto novel views
- Recent works leverage the power of deep learning and neural fields to improve image-based rendering from both structured (e.g., light fields [16,21]) and unstructured data
- Neural Radiance Fields are one such 3D scene representation for view synthesis that parameterize the appearance and density of every point in 3D space with a multilayer perceptron (MLP)
- Several works improve the quality of NeRFs by accounting for finite pixels and apertures
- Adaptive Sampling for Neural Volume Rendering reduces the number of volume queries required to render a single ray
- 6-DoF video is an emergent technology that allows users to explore new views within videos
- Systems for 6-DoF video use multiview camera rigs that capture a full 360-degree field of view and use variants of depth-based reprojection for view synthesis at each frame of the video
- 6-DoF from Monocular Captures decouples camera and object motion and relies on natural signal priors provided by neural radiance fields

## Method
- Volume representations like NeRF model the density and appearance of a static scene at every point in the 3D space.
- We can render new views of a static scene with where T (o, x t ) denotes the transmittance from o to x t .
- In practice, we can evaluate Equation 1 using numerical quadrature by taking many sample points along a given ray.

### Sample Networks for Volume Rendering
- Most scenes consist of solid objects whose surfaces lie on a 2D manifold within the 3D scene volume.
- To accelerate volume rendering, we would like to query color and opacity only for points with non-zero w k .
- While most volume representations use importance sampling and pruning schemes that help reduce sample counts, they often require hundreds or even thousands of queries per ray to produce accurate renderings.
- As shown in Figure 2, we use a feed-forward network to predict a set of sample locations x k .
- Specifically, we use a sample prediction network E φ : (o, ω) → (x 1 , . . . , x n ) that maps a ray (o, ω) to the sample points x k for volume  (2).
- We then query the outer products of space-time textures in order to produce per-sample-point appearance features, which are then converted to colors via Equation 10.
- We follow a similar procedure for extracting per-sample-point opacities.
- In this work, we use the Plücker parameterization to represent the ray:
- While most designs for the sample prediction network E φ are possible, giving the network too much flexibility may negatively affect view synthesis quality.
- For example, if (x 1 , . . . , x n ) are completely arbitrary points, then renderings may not appear to be multi-view-consistent.
- To address this problem, we choose to predict the parameters of a set of geometric primitives G 1 , . . . , G n with the sample prediction network, where the primitive parame-ters can vary depending on the input ray.
- To get our sample points, we then intersect the ray with each primitive:
- Above, inter(G k ; o, ω) is a differentiable operation that intersects the ray with the primitive G k .
- In all of our experiments, we use axis-aligned z-planes (for forward-facing scenes) or concentric spherical shells centered at the origin (for all other scenes) as our geometric primitives.
- This approach is constrained in that it produces sample points that initially lie along the ray.
- Further, predicting primitives defined in a global coordinate frame makes the sample signal smooth and easy to interpolate.
- For example, if two distinct rays observe the same point in the scene, then the sample network needs only predict one primitive for both rays (i.e., defining a primitive that passes through the point).
- In contrast, existing works such as NeuSample [12], AdaNeRF [20], and TermiNeRF [38] predict distances or per-segment weights that vary depending on the ray even if these rays observe the same point in the scene.
- Flexible Sampling for Challenging Appearance.
- To grant our samples additional flexibility to better represent challenging view-dependent appearance, we also predict a set of Tanh-activated per-sample-point offsets (e 1 , . . . , e n ), as well as a set of scalar values (δ 1 , . . . , δ n ).
- We convert these scalar values to weights with a sigmoid activation, i.e., (γ(δ 1 ), . . . , γ(δ n )) where γ is the sigmoid operator.
- Specifi-cally, we have:
- where we use (d 1 , . . . , d n ) to denote the final displacement, or "point-offset" added to each point.
- The sample network outputs may appear to be overparameterized and under-constrained. However, the above design is essential for achieving good-quality view synthesis.
- In particular, initializing the scalars (δ 1 , . . . , δ n ) to negative values, where the sigmoid is close to 0, and its gradient is small, implicitly discourages the network from unmasking the point offsets, while still allowing the network to use them as necessary.
- In addition to enabling real-time rendering with low sample counts, one added benefit of our sample network architecture is the improved modeling of complex view-dependent appearance.
- For example, distorted refractions break epipolar geometry and appear to change the depth of the refracted content depending on the viewpoint.
- As illustrated in Figure 2, our sample network, on the other hand, has the flexibility to model sample points that warp depending on viewpoint.

### Keyframe-Based Dynamic Volumes
- Tensorial Radiance Fields (TensoRF)
- Static: uses memory efficient Tensorial Radiance Fields (TensoRF)
- Dynamic: extends TensoRF to a keyframe-based dynamic volume representation
- Recall that TensoRF factorizes a 3D volume as a set of outer products between functions of one or more spatial dimensions
- Above, f j and g j are vector-valued functions with output dimension M , and the operator ' ' computes an element-wise product
- In the original TensoRF work [11], the functions f j and g j are discretized into M different 2D and 1D arrays, respectively
- Further, B j denote linear transforms that map the products of f j and g j to spherical harmonic coefficients
- The color L e (x k , ω) for point x k and direction ω is then given by the dot product of the coefficients A (x k ) and the spherical harmonic basis functions evaluated at ray direction ω
- Similar to appearance, for density, we have: where 1 is a vector of ones, and h j and k j are vectorvalued functions with output dimension M
- Given the color L e (x k , ω) and density σ(x k ) for all sample points {x k } along a ray, we can then make use of Equation 2to render the final color for that ray
- To handle dynamics, we adapt TensoRF to parameterize volumetric "keyframes", or snapshots of a dynamic volume at a set of discrete time steps
- If we denote τ i as the time step corresponding to the i th keyframe, we can write: where the only change from Section 3.2.1 is that g j and k j now depend on time, in addition to one spatial dimension
- We note that the above factorization of the dynamic volume representing all keyframes in a video has a similar memory footprint to a static TensoRF for a single frame, assuming that the number of keyframes is small relative to the resolution of our spatial dimensions
- In particular, if the spatial resolution of our volume is (N x , N y , N z ) and the number of keyframes is N t , then we can store a single component of f 1 with an N x × N y array, and store a single component of g 1 with an N z × N t array
- Because N t N x/y/z , the arrays g j do not contribute significantly to the size of the model
- After querying the keyframe-based dynamic volume with {x k }, the equation for volume rendering is then: where w k = T (o, x k , τ i ) (1 − e −σ(x k ,τi)∆x k ), and τ i is the time step corresponding to the closest keyframe to time τ
- This is effectively the same as Equation 2, except C, x k , w k and L e now depend on the time τ

### Optimization
- We optimize our representation using only the training images
- We apply total variation and 1 sparsity regularization to our tensor components
- The loss is summed over training rays and times, and C GT represents the ground-truth color for a given ray and time
- We only use a subset of all training rays to make the optimization tractable on machines with limited memory

## Experiments
- Implemented in PyTorch
- Used a 6layer, 256-hidden unit MLP with Leaky ReLU activations
- Predicted 32 z-planes as geometric primitives
- Used the same space contraction scheme for unbounded scenes as in mip-NeRF 360
- For all dynamic datasets, used every 4th frame as a keyframe
- For static and dynamic datasets, used a batch size of 16,384 rays for training, an initial learning rate of 0.02 for the parameters of the keyframe-based volume, and an initial learning rate of 0.0075 for our sample prediction network
- Trained all models for 1.5 hours each

### Comparisons on Static Scenes
- The DoNeRF dataset contains six synthetic sequences with images of 800×800 pixel resolution
- Our approach outperforms all baselines in terms of quality and improves the performance of other sampling network schemes by a large margin
- The LLFF dataset contains eight realworld sequences with 1008×756 pixel images
- Our approach outperforms DoNeRF, AdaNeRF, TermiNeRF, and InstantNGP but achieves slightly worse quality than NeRF

### Comparisons on Dynamic Scenes
- The Technicolor light field dataset contains videos of varied indoor environments captured by a time-synchronized 4×4 camera rig.
- We compare HyperReel to Neural 3D Video at full image resolution on five sequences (Birthday, Fabien, Painter, Theater, Trains) from this dataset, each 50 frames long.
- We train Neural 3D Video on each sequence for approximately one week on a machine with 8 NVIDIA V100 GPUs.
- We show in Table 2 that the quality of HyperReel exceeds that of Neural 3D Video [23] while also training in just 1.5 GPU hours per sequence (rather than 1000+ GPU hours for Neural 3D), and rendering far more quickly.
- Neural 3D Video does not provide SSIM and LPIPS scores.
- The accompanying paper does not provide quantitative metrics, while the concurrent NeRF-Player does, so we provide a comparison with NeRFPlayer only.
- The Google Immersive dataset contains light field videos of various indoor and outdoor environments captured by a time-synchronized 46fisheye camera rig.
- We compare our approach to NeRF-Player and select the same seven scenes as NeRFPlayer for evaluation on this dataset (Welder, Flames, Truck, Exhibit, DeepView).
- Our method achieves superior quality, outperforming DeepView by a large margin.
- Further, HyperReel consumes less memory per frame than the Immersive Light Field Video's baked layered mesh representation: 1.2 MB per frame vs. 8.87 MB per frame (calculated from the reported bitrate numbers [9]).
- Their layered mesh can render at more than 100 FPS on commodity hardware, while our approach renders at a little over 4 FPS. However, our approach is entirely implemented in vanilla PyTorch and can be further optimized using custom CUDA kernels or baked into a realtime renderable representation for better performance.

### Ablation Studies
- The optimal number of keyframes for motion within a scene depends on the motion.
- Our dynamic volume representation implicitly trades off between temporal resolution and spatial resolution, as it can use the (x, t), (y, t), and (z, t) components to add either spatial or temporal details.
- We show the performance of our method with different sample prediction network designs, including the performance for a Tiny model (4-layers, 128-hidden-unit MLP with 8 predicted sample points), and Small model (4-layers, 256-hidden-unit MLP with 16 predicted sample points).
- Our Tiny model runs at 18 FPS, and our Small model runs at 9 FPS at megapixel resolution, again without any custom CUDA code.
- Our Tiny model performs reasonably well but achieves worse quality than Neural 3D Video on the Technicolor dataset.
- In contrast, our Small model achieves comparable overall performance to Neural3D-showing that we can still achieve good quality renderings at even higher frame rates.
- We show accompanying qualitative results for these models in Figure 5.
- With and Without Sample Prediction Network.
- With and Without Point Offset.

## Conclusion
- HyperReel is a novel representation for 6-DoF video
- It achieves a balance between high rendering quality, speed, and memory efficiency that sets it apart from existing 6-DoF video representations
- We qualitatively and quantitatively compare our approach to prior and contemporary 6-DoF video representations, showing that HyperReel outperforms each of these works along multiple axes
- Limitations and Future Work
- Our sample prediction network is supervised only by a rendering loss on the training images. This can lead to a reduction in quality for views outside of the convex hull of the training cameras or for scene content that is only observed in a small number of views
- Exploring regularization methods that enable reasonable geometry predictions even for extrapolated views is an important future direction.

## A. Appendix Overview
- In addition to this appendix document, we include a supplemental website which contains a video of a prototype demo of our method running in real-time at high-resolution without any custom CUDA code.
- We emphasize that we plan to release code and data to make our method and results as reproducible as possible.
- Within the appendix, we provide: 1. Additional details regarding training and evaluation for static and dynamic datasets; 2. A more complete description of the mapping from sample prediction network outputs to sample points for both forward facing, and non-forward facing scenes; 3. Additional experimental details regarding our keyframebased volume design.
- Further, we provide a full per-scene breakdown of image metrics for the Technicolor dataset in Table D.1.
- As we do not have direct access to the outputs of other methods on the Google Immersive ( [9]) and Neural 3D Video ( [62]) datasets, we do not provide per-scene breakdowns for these datasets.

### B.2. LPIPS Evaluation Details
- For LPIPS computation, we use the AlexNet LPIPS variant
- For SSIM computation, we use the structural similarity scikit-image library function
- While we believe that this inconsistency makes SSIM scores somewhat less reliable, we still report our aggregated SSIM metrics in the quantitative result tables

## C. Sample Prediction Network Description
- For forward facing scenes, we first convert all rays to normalized device coordinates (NDC) [29], so that the view frustum of a "reference" camera lives within [−1, 1] 3 .
- After mapping a ray with origin o and direction ω to its Plücker parameterization via r = Plücker(o, ω) = ( ω, ω × o) .
- We predict the parameters of a set of planes normal to the z-axis with our embedding network.
- In particular, we predict the parameters of a set of planes normal to the z-axis with our embedding network.
- Figure 1. HyperReel: A novel 6-DoF video representation.
- HyperReel converts synchronized multi-view video streams into a high-fidelity, memory efficient scene representation that can be rendered from novel views and time steps at interactive rates.
- HyperReel's combination of high rendering quality, speed, and compactness sets it apart from existing 6-DoF video representations.
- The upper two rows show 6-DoF (i.e., varying viewpoint and viewing orientation) rendering of dynamic scenes[9,44]; the lower two of static scenes[57,58].
- Figure 2. Overview of HyperReel for static scenes.
- Given a set of images and camera poses, the training objective is to reconstruct the measured color associated with every ray.
- (a) Given an input ray originating at the camera origin o and traveling in direction ω, we first reparameterize the ray using Plücker coordinates.
- (b) A network E φ takes this ray as input and outputs the parameters for a set of geometric primitives {G k } (such as axis-aligned planes and spheres) and displacement vectors {d k }.
- To generate sample points {x k } for volume rendering, we compute the intersections between the ray and the geometric primitives, and add the displacement vectors to the results.
- Predicting geometric primitives has the advantage of making the sample signal smooth and easy to interpolate (see Section 3.1).
- The displacement vectors grant additional flexibility to the sample points, enabling better capture of complex view-dependent appearance.
- Finally, we perform volume rendering via Equation 2 to produce a pixel color and supervise training based on the corresponding observation.
- Figure 3. Extracting sample point appearance in the dynamic setting from our keyframe-based representation.
- (1) We first advect the sample points {x k } at time τ into the nearest keyframe τi, using velocities {v k } outputted from the sample prediction network.
- (2) We then query the outer products of space-time textures in order to produce per-sample-point appearance features, which are then converted to colors via Equation10.
- We follow a similar procedure for extracting per-sample-point opacities.
- Figure 5. Ablations on our sampling network.
- We show close-up results for various sampling networks architectures on two of the Technicolor sequences also shown in Figure 4.
- Figure 6. Rendering speed vs. quality trade-off.
- We show the speed-quality trade-off of our method and others applied to dynamic scenes (left) on the Neural 3D Video dataset[23] and static scenes methods (right) on the DoNeRF dataset[31].
- Dynamic dataset results from our method on each of Technicolor ([44]), Neural 3D Video ([23]), and Google Immersive Video ([9]); Qualitative results and comparisons on view-dependent static scenes from the Shiny Dataset ([58]) and the Stanford Light Field Dataset ([57]); Qualitative comparison to[9].
