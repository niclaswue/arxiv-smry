---
title: "HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling"
date: 2023-01-05T18:59:44.000Z
author: "Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O'Toole, Changil Kim"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02238v1.webp" # image path/url
    alt: "HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02238)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02238).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/hyperreel-high-fidelity-6-dof-video-with-ray).

# Abstract

# Paper Content

## Introduction
- Six-degrees-of-freedom (6-DoF) videos allow for free exploration of an environment by giving the users the ability to change their head position (3 degrees of freedom) and orientation (3 degrees of freedom).
- The underlying methodology that drives 6-DoF video is view synthesis: the process of rendering new, unobserved views of an environment-static or dynamic-from a set of posed images or videos.
- Existing approaches that attempt to create memory-efficient 6-DoF video can take nearly a minute to render a single megapixel image [23].
- Works that target rendering speed and represent dynamic volumes directly with 3D textures require gigabytes of storage even for short video clips [56].
- While other volumetric methods achieve memory efficiency and speed by leveraging sparse or compressed volume storage for static scenes [11,30], only contemporary work [22,49] addresses the extension of these approaches to dynamic scenes.
- All of the above representations struggle to capture highly view-dependent appearance, such as reflections and refractions caused by non-planar surfaces.
- In this paper, we present HyperReel, a novel 6-DoF video representation that achieves state-of-the-art quality while being memory efficient and real-time renderable at high resolution.
- The first ingredient of our approach is a novel rayconditioned sample prediction network that predicts sparse point samples for volume rendering.
- In contrast to existing static view synthesis methods that use sample networks [20,31], our design is unique in that it both (1) accelerates volume rendering and at the same time (2) improves rendering quality for challenging view-dependent scenes.
- Second, we introduce a memory-efficient dynamic volume representation that achieves a high compression rate by exploiting the spatio-temporal redundancy of a dynamic scene. Specifically, we extend Tensorial Radiance Fields [11] to compactly represent a set of volumetric keyframes, and capture intermediate frames with trainable scene flow.
- The combination of these two techniques comprises our high-fidelity 6-DoF video representation, HyperReel.
- We validate the individual components of our approach and our representation as a whole with comparisons to state-of-theart sampling network-based approaches for static scenes as well as 6-DoF video representations for dynamic scenes.
- Not only does HyperReel outperform these existing works, but it also provides high-quality renderings for scenes with challenging non-Lambertian appearances.

## Related Work
- Image-based rendering techniques use approximate scene geometry to reproject and blend source image content onto novel views [10,37,46]
- Recent works leverage the power of deep learning and neural fields [62] to improve image-based rendering from both structured (e.g., light fields [16,21]) and unstructured data [7,50]
- Rather than performing image-based rendering, which requires storing the input images, another approach is to optimize some 3D scene representation augmented with appearance information [41]
- Neural Radiance Fields are one such 3D scene representation for view synthesis [29] that parameterize the appearance and density of every point in 3D space with a multilayer perceptron (MLP)
- Several works improve the quality of NeRFs by accounting for finite pixels and apertures [5,60], by enabling application to unbounded scenes [6,63,64], or by modifying the representation to allow for better reproduction of challenging view-dependent appearances like distorted reflections and refractions [8,17,19,54]
- One can achieve significant training and inference speed improvements by replacing the deep multilayer perceptron with a feature voxel grid in combination with a small neural network [11,30,51] or no network at all [18,63]
- Several other works achieve both fast rendering and memory-efficient storage with tensor factorizations [11], learned appearance codebooks, or quantized volumetric features [52]
- Adaptive Sampling for Neural Volume Rendering reduces the number of volume queries required to render a single ray
- 6-DoF from Monocular Captures is a challenging task; an ideal 6-DoF video format must simultaneously achieve high visual quality, rendering speed, and memory efficiency
- Directly extending recent volumetric methods to dynamic scenes can achieve high quality and rendering speed [56], but at the cost of substantial memory requirements, potentially gigabytes of memory [63] for each video frame.

## Method
- We consider the problem of optimizing a volumetric representation for static view synthesis.
- A function F θ : (x, ω) → (L e (x, ω), σ(x)) maps position x and direction ω along a ray to a color L e (x, ω) and density σ(x).
- We can then render new views of a static scene with where T (o, x t ) denotes the transmittance from o to x t .

### Sample Networks for Volume Rendering
- Most scenes consist of solid objects whose surfaces lie on a 2D manifold within the 3D scene volume.
- To accelerate volume rendering, we would like to query color and opacity only for points with non-zero w k .
- We use a feed-forward network to predict a set of sample locations x k .
- Specifically, we use a sample prediction network E φ : (o, ω) → (x 1 , . . . , x n ) that maps a ray (o, ω) to the sample points x k for volume  (2).
- We then query the outer products of space-time textures in order to produce per-sample-point appearance features, which are then converted to colors via Equation 10.
- We follow a similar procedure for extracting per-sample-point opacities.

### Keyframe-Based Dynamic Volumes
- The Tensorial Radiance Fields (TensoRF) approach is used to represent a static 3D volume.
- In the dynamic case, a keyframe-based dynamic volume representation is used.
- To combine the sampling procedure (Section 3.1) and keyframe-based volume representation (Section 3.2.2), a few additional modifications are required.
- The sampling procedure (Section 3.1), volume representation (Section 3.2.2), and rendering scheme for keyframe-based volumes (Section 3.2.3) comprise our 6-DoF video representation: HyperReel.

### Optimization
- We optimize our representation using only the training images
- We apply total variation and 1 sparsity regularization to our tensor components
- The loss is summed over training rays and times, and C GT represents the ground-truth color for a given ray and time
- We only use a subset of all training rays to make the optimization tractable on machines with limited memory

## Experiments
- Implemented a method in PyTorch
- Used a single NVIDIA RTX 3090 GPU with 24 GB RAM
- Used a 6layer, 256-hidden unit MLP with Leaky ReLU activations for both static and dynamic settings
- Predicted 32 z-planes as our geometric primitives with our ray-conditioned sample prediction network
- For our keyframe-based volume representation, we gave the (x, y) and (z, t) textures eight components each and four components to all other textures
- For both static and dynamic datasets, we used a batch size of 16,384 rays for training, an initial learning rate of 0.02 for the parameters of the keyframe-based volume, and an initial learning rate of 0.0075 for our sample prediction network
- Trained all models for 1.5 hours each

### Comparisons on Static Scenes

### Comparisons on Dynamic Scenes
- The Technicolor light field dataset contains videos of varied indoor environments captured by a time-synchronized 4×4 camera rig.
- We compare HyperReel to Neural 3D Video at full image resolution on five sequences (Birthday, Fabien, Painter, Theater, Trains) from this dataset, each 50 frames long.
- We train Neural 3D Video on each sequence for approximately one week on a machine with 8 NVIDIA V100 GPUs.
- We show in Table 2 that the quality of HyperReel exceeds that of Neural 3D Video [23] while also training in just 1.5 GPU hours per sequence (rather than 1000+ GPU hours for Neural 3D), and rendering far more quickly.
- Neural 3D Video does not provide SSIM and LPIPS scores.
- The accompanying paper does not provide quantitative metrics, while the concurrent NeRF-Player does, so we provide a comparison with NeRFPlayer only.
- Google Immersive Dataset.
- The Google Immersive dataset [9] contains light field videos of various indoor and outdoor environments captured by a time-synchronized 46fisheye camera rig.
- We compare our approach to NeRF-Player and select the same seven scenes as NeRFPlayer for evaluation on this dataset (Welder, Flames, Truck, Exhibit, DeepView Dataset.
- Unfortunately, Google's Immersive Light Field Video [9] does not provide quantitative benchmarks for the performance of their approach in terms of image quality.
- As a proxy, we compare our approach to DeepView [13], the method upon which their representation is built, on the static Spaces dataset in Table 3.
- Our method achieves superior quality, outperforming DeepView by a large margin.
- Further, HyperReel consumes less memory per frame than the Immersive Light Field Video's baked layered mesh representation: 1.2 MB per frame vs. 8.87 MB per frame (calculated from the reported bitrate numbers [9]).
- Their layered mesh can render at more than 100 FPS on commodity hardware, while our approach renders at a little over 4 FPS.
- However, our approach is entirely implemented in vanilla PyTorch and can be further optimized using custom CUDA kernels or baked into a realtime renderable representation for better performance.
- Number of Keyframes.
- In Table 4, we ablate our method on the Technicolor light field dataset with different numbers of keyframes. In general, the optimal number of keyframes depends on the motion within a scene.
- Our dynamic volume representation implicitly trades off between temporal resolution and spatial resolution, as it can use the (x, t), (y, t), and (z, t) components to add either spatial or temporal details.
- Our choice of one keyframe for every four frames strikes a good balance between temporal resolution and spatial reso- lution and achieves the best overall performance (Table 4).
- Sample Network Size.
- We also show the performance of our method with different sample prediction network designs in Table 4, including the performance for a Tiny model (4-layers, 128-hidden-unit MLP with 8 predicted sample points), and Small model (4-layers, 256-hidden-unit MLP with 16 predicted sample points).
- Our Tiny model runs at 18 FPS, and our Small model runs at 9 FPS at megapixel resolution, again without any custom CUDA code.
- Our Tiny model performs reasonably well but achieves worse quality than Neural 3D Video on the Technicolor dataset.
- In contrast,...
