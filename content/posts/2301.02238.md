---
title: "HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling"
date: 2023-01-05T18:59:44.000Z
author: "Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O'Toole, Changil Kim"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-02238v1_QK4iUEjl6.jpg" # image path/url
    alt: "HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02238)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02238).


# Abstract

# Paper Content

## Introduction
- Six-degrees-of-freedom (6-DoF) videos allow for free exploration of an environment by giving the users the ability to change their head position (3 degrees of freedom) and orientation (3 degrees of freedom).
- As such, 6-DoF videos offer immersive experiences with many exciting applications in AR/VR.
- The underlying methodology that drives 6-DoF video is view synthesis: the process of rendering new, unobserved views of an environment-static or dynamic-from a set of posed images or videos.
- Volumetric scene representations such as neural radiance fields [29] and instant neural graphics primitives [30] have recently made great strides toward photorealistic view synthesis for static scenes.
- While several recent works build dynamic view synthesis pipelines on top of these volumetric representations [14,23,24,33,61], it remains a challenging task to create a 6-DoF video format that can achieve high quality, fast rendering, and a small memory footprint (even given many synchronized video streams from multi-view camera rigs [9,35,44]).
- Existing approaches that attempt to create memory-efficient 6-DoF video can take nearly a minute to render a single megapixel image [23].
- Works that target rendering speed and represent dynamic volumes directly with 3D textures require gigabytes of storage even for short video clips [56].
- While other volumetric methods achieve memory efficiency and speed by leveraging sparse or compressed volume storage for static scenes [11,30], only contemporary work [22,49] addresses the extension of these approaches to dynamic scenes.
- Moreover, all of the above representations struggle to capture highly view-dependent appearance, such as reflections and refractions caused by non-planar surfaces.
- In this paper, we present HyperReel, a novel 6-DoF video representation that achieves state-of-the-art quality while being memory efficient and real-time renderable at high resolution.
- The first ingredient of our approach is a novel rayconditioned sample prediction network that predicts sparse point samples for volume rendering.
- In contrast to existing static view synthesis methods that use sample networks [20,31], our design is unique in that it both (1) accelerates volume rendering and at the same time (2) improves rendering quality for challenging view-dependent scenes.
- Second, we introduce a memory-efficient dynamic volume representation that achieves a high compression rate by exploiting the spatio-temporal redundancy of a dynamic scene. Specifically, we extend Tensorial Radiance Fields [11] to compactly represent a set of volumetric keyframes, and capture intermediate frames with trainable scene flow.
- The combination of these two techniques comprises our high-fidelity 6-DoF video representation, HyperReel.
- We validate the individual components of our approach and our representation as a whole with comparisons to state-of-theart sampling network-based approaches for static scenes as well as 6-DoF video representations for dynamic scenes.
- Not only does HyperReel outperform these existing works, but it also provides high-quality renderings for scenes with challenging non-Lambertian appearances. Our system renders at up to 18 frames-per-second at megapixel resolution without using any custom CUDA code.

## Related Work
- Novel View Synthesis is the process of rendering new views of a scene given a set of input posed images.
- Classical image-based rendering techniques use approximate scene geometry to reproject and blend source image content onto novel views [10,37,46].
- Recent works leverage the power of deep learning and neural fields [62] to improve image-based rendering from both structured (e.g., light fields [16,21]) and unstructured data [7,50].
- Rather than performing image-based rendering, which requires storing the input images, another approach is to optimize some 3D scene representation augmented with appearance information [41].
- Examples of such representations include point clouds [1,40], voxel grids [27,32,47], meshes [42,43], or layered representations like multi-plane [13,28,65] or multi-sphere images [9].
- Neural Radiance Fields. NeRFs are one such 3D scene representation for view synthesis [29] that parameterize the appearance and density of every point in 3D space with a multilayer perceptron (MLP).
- While NeRFs enable highquality view synthesis at a small memory cost, they do not lend themselves to real-time rendering.
- To render the color of a ray from a NeRF, one must evaluate and integrate the color and opacity of many points along a ray-necessitating, in the case of NeRF, many hundreds of MLP evaluations per pixel.
- Still, due to its impressive performance for static view synthesis, many recent methods build on NeRFs in the quest for higher visual quality, more efficient training, and faster rendering speed [15,53].
- Several works improve the quality of NeRFs by accounting for finite pixels and apertures [5,60], by enabling application to unbounded scenes [6,63,64], or by modifying the representation to allow for better reproduction of challenging view-dependent appearances like distorted reflections and refractions [8,17,19,54].
- One can achieve significant training and inference speed improvements by replacing the deep multilayer perceptron with a feature voxel grid in combination with a small neural network [11,30,51] or no network at all [18,63].
- Several other works achieve both fast rendering and memory-efficient storage with tensor factorizations [11], learned appearance codebooks, or quantized volumetric features [52].
- Adaptive Sampling for Neural Volume Rendering. Other works aim to improve the speed of volumetric representations by reducing the number of volume queries required to render a single ray.
- Approaches like DoNeRF [31], TermiNeRF [38], and AdaNeRF [20] learn weights for each segment along a ray as a function of the ray itself, and use these weights for adaptive evaluation of the underlying NeRF.
- In doing so, they can achieve near-real-time rendering.
- NeuSample [12] replaces the NeRF coarse network with a module that directly predicts the distance to each sample point along a ray.
- Methods like AutoInt [26], DIVeR [59], and neural light fields [4,25,48] learn integrated opacity and color along a small set of ray segments (or just one segment), requiring only a single network evaluation per segment.
- A key component of our framework is a flexible sampling network, which is among one of the few schemes that both accelerates volume rendering, and also improves volume rendering quality for challenging scenes.

## Method
- Volume representations like NeRF model the density and appearance of a static scene at every point in the 3D space
- We can render new views of a static scene with where T (o, x t ) denotes the transmittance from o to x t
- In practice, we can evaluate Equation 1 using numerical quadrature by taking many sample points along a given ray

### Sample Networks for Volume Rendering
- Most scenes consist of solid objects whose surfaces lie on a 2D manifold within the 3D scene volume.
- To accelerate volume rendering, we would like to query color and opacity only for points with non-zero w k .
- While most volume representations use importance sampling and pruning schemes that help reduce sample counts, they often require hundreds or even thousands of queries per ray to produce accurate renderings.
- As shown in Figure 2, we use a feed-forward network to predict a set of sample locations x k .
- Specifically, we use a sample prediction network E φ : (o, ω) → (x 1 , . . . , x n ) that maps a ray (o, ω) to the sample points x k for volume  (2).
- We then query the outer products of space-time textures in order to produce per-sample-point appearance features, which are then converted to colors via Equation 10.
- We follow a similar procedure for extracting per-sample-point opacities.
- We use the Plücker parameterization to represent the ray: While many designs for the sample prediction network E φ are possible, giving the network too much flexibility may negatively affect view synthesis quality.
- For example, if (x 1 , . . . , x n ) are completely arbitrary points, then renderings may not appear to be multi-view-consistent.
- To address this problem, we choose to predict the parameters of a set of geometric primitives G 1 , . . . , G n with the sample prediction network, where the primitive parame-ters can vary depending on the input ray.
- To get our sample points, we then intersect the ray with each primitive: Above, inter(G k ; o, ω) is a differentiable operation that intersects the ray with the primitive G k .
- In all of our experiments, we use axis-aligned z-planes (for forward-facing scenes) or concentric spherical shells centered at the origin (for all other scenes) as our geometric primitives.
- This approach is constrained in that it produces sample points that initially lie along the ray.
- Further, predicting primitives defined in a global coordinate frame makes the sample signal smooth and easy to interpolate.
- For example, if two distinct rays observe the same point in the scene, then the sample network needs only predict one primitive for both rays (i.e., defining a primitive that passes through the point).
- In contrast, existing works such as NeuSample [12], AdaNeRF [20], and TermiNeRF [38] predict distances or per-segment weights that vary depending on the ray even if these rays observe the same point in the scene.
- Flexible Sampling for Challenging Appearance.
- To grant our samples additional flexibility to better represent challenging view-dependent appearance, we also predict a set of Tanh-activated per-sample-point offsets (e 1 , . . . , e n ), as well as a set of scalar values (δ 1 , . . . , δ n ).
- We convert these scalar values to weights with a sigmoid activation, i.e., (γ(δ 1 ), . . . , γ(δ n )) where γ is the sigmoid operator.
- Specifi-cally, we have: where we use (d 1 , . . . , d n ) to denote the final displacement, or "point-offset" added to each point.
- The sample network outputs may appear to be overparameterized and under-constrained. However, the above design is essential for achieving good-quality view synthesis.
- In particular, initializing the scalars (δ 1 , . . . , δ n ) to negative values, where the sigmoid is close to 0, and its gradient is small, implicitly discourages the network from unmasking the point offsets, while still allowing the network to use them as necessary.
- In addition to enabling real-time rendering with low sample counts, one added benefit of our sample network architecture is the improved modeling of complex view-dependent appearance.
- For example, distorted refractions break epipolar geometry and appear to change the depth of the refracted content depending on the viewpoint.
- As illustrated in Figure 2, our sample network, on the other hand, has the flexibility to model sample points that warp depending on viewpoint.

### Keyframe-Based Dynamic Volumes
- The Tensorial Radiance Fields (TensoRF) approach is used to efficiently sample a 3D scene volume.
- In the static case, the memory efficient Tensorial Radiance Fields (TensoRF) approach is used.
- In the dynamic case, a keyframe-based dynamic volume representation is used.
- To complete our system for 6-DoF video, a few additional modifications are required.

### Optimization
- We optimize our representation using only the training images
- We apply total variation and 1 sparsity regularization to our tensor components
- The loss is summed over training rays and times, and C GT represents the ground-truth color for a given ray and time
- We only use a subset of all training rays to make the optimization tractable on machines with limited memory

## Experiments
- Implemented in PyTorch
- Runs experiments on a single NVIDIA RTX 3090 GPU with 24 GB RAM
- Sample network is a 6layer, 256-hidden unit MLP with Leaky ReLU activations for both static and dynamic settings
- For forward-facing scenes, predicts 32 z-planes as our geometric primitives with ray-conditioned sample prediction network
- In all other settings, predicts the radii of 32 spherical shells centered at the origin
- Gives (x, y) and (z, t) textures eight components each and four components to all other textures
- For both static and dynamic datasets, uses every 4th frame as a keyframe
- For both static and dynamic datasets, uses a batch size of 16,384 rays for training, an initial learning rate of 0.02 for the parameters of the keyframe-based volume, and an initial learning rate of 0.0075 for our sample prediction network
- Trains all models for 1.5 hours each

### Comparisons on Static Scenes
- The DoNeRF dataset contains six synthetic sequences with images of 800×800 pixel resolution.
- Our approach outperforms all baselines in terms of quality and improves the performance of other sampling network schemes by a large margin.
- The LLFF dataset contains eight realworld sequences with 1008×756 pixel images.
- Our approach outperforms DoNeRF, AdaNeRF, TermiNeRF, and InstantNGP but achieves slightly worse quality than NeRF.

### Comparisons on Dynamic Scenes
- The Technicolor light field dataset contains videos of varied indoor environments captured by a time-synchronized 4×4 camera rig
- We compare HyperReel to Neural 3D Video at full image resolution on five sequences from this dataset, each 50 frames long
- We train Neural 3D Video on each sequence for approximately one week on a machine with 8 NVIDIA V100 GPUs
- We show in Table 2 that the quality of HyperReel exceeds that of Neural 3D Video while also training in just 1.5 GPU hours per sequence (rather than 1000+ GPU hours for Neural 3D), and rendering far more quickly
- Neural 3D Video does not provide SSIM and LPIPS scores, while HyperReel does
- The accompanying paper does not provide quantitative metrics, while the concurrent NeRF-Player does, so we provide a comparison with NeRFPlayer only
- The Google Immersive dataset contains light field videos of various indoor and outdoor environments captured by a time-synchronized 46fisheye camera rig
- We compare our approach to NeRF-Player and select the same seven scenes as NeRFPlayer for evaluation on this dataset
- We achieve superior quality, outperforming DeepView by a large margin
- Further, HyperReel consumes less memory per frame than the Immersive Light Field Video's baked layered mesh representation: 1.2 MB per frame vs. 8.87 MB per frame (calculated from the reported bitrate numbers [9]).

### Ablation Studies
- The optimal number of keyframes for motion within a scene depends on the motion
- Our dynamic volume representation implicitly trades off between temporal resolution and spatial resolution
- Our choice of one keyframe for every four frames strikes a good balance between temporal resolution and spatial resolution and achieves the best overall performance
- With and Without Sample Prediction Network
- With and Without Point Offset

## Conclusion
- HyperReel is a novel representation for 6-DoF video
- It combines a ray-conditioned sampling network with a keyframe-based dynamic volume representation
- It achieves a balance between high rendering quality, speed, and memory efficiency that sets it apart from existing 6-DoF video representations
- We qualitatively and quantitatively compare our approach to prior and contemporary 6-DoF video representations, showing that HyperReel outperforms each of these works along multiple axes
- Limitations and Future Work
- Our sample prediction network is supervised only by a rendering loss on the training images. This can lead to a reduction in quality for views outside of the convex hull of the training cameras or for scene content that is only observed in a small number of views
- Exploring regularization methods that enable reasonable geometry predictions even for extrapolated views is an important future direction.

## A. Appendix Overview
- In addition to this appendix document, we include a supplemental website which contains a video of a prototype demo of our method running in real-time at high-resolution without any custom CUDA code.
- We emphasize that we plan to release code and data to make our method and results as reproducible as possible.
- Within the appendix, we provide: 1. Additional details regarding training and evaluation for static and dynamic datasets; 2. A more complete description of the mapping from sample prediction network outputs to sample points for both forward facing, and non-forward facing scenes; 3. Additional experimental details regarding our keyframebased volume design.
- Further, we provide a full per-scene breakdown of image metrics for the Technicolor dataset in Table D.1.
- As we do not have direct access to the outputs of other methods on the Google Immersive ( [9]) and Neural 3D Video ( [62]) datasets, we do not provide per-scene breakdowns for these datasets.

### B.2. LPIPS Evaluation Details
- For LPIPS computation, we use the AlexNet LPIPS variant
- For SSIM computation, we use the structural similarity scikit-image library function
- While we believe that this inconsistency makes SSIM scores somewhat less reliable, we still report our aggregated SSIM metrics in the quantitative result tables

## C. Sample Prediction Network Description
- Convert all rays to normalized device coordinates
- Map a ray with origin and direction to its Plücker parameterization
- Predict the parameters of a set of planes normal to the z-axis with an embedding network
