---
title: "Dexterous Manipulation from Images: Autonomous Real-World RL via Substep Guidance"
date: 2022-12-19T22:50:40.000Z
author: "Kelvin Xu, Zheyuan Hu, Ria Doshi, Aaron Rovinsky, Vikash Kumar, Abhishek Gupta, Sergey Levine"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09902v1.webp" # image path/url
    alt: "Dexterous Manipulation from Images: Autonomous Real-World RL via Substep Guidance" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09902)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09902).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/dexterous-manipulation-from-images-autonomous).

# Abstract
- Complex and contact-rich robotic manipulation tasks present a significant challenge to any control method
- Methods based on reinforcement learning offer an appealing choice for such settings, as they can enable robots to learn to delicately balance contact forces and dexterously reposition objects without strong modeling assumptions
- However, running reinforcement learning on real-world dexterous manipulation systems often requires significant manual engineering
- This negates the benefits of autonomous data collection and ease of use that reinforcement learning should in principle provide
- In this paper, we describe a system for vision-based dexterous manipulation that provides a "programming-free" approach for users to define new tasks and enable robots with complex multi-fingered hands to learn to perform them through interaction
- The core principle underlying our system is that, in a vision-based setting, users should be able to provide high-level intermediate supervision that circumvents challenges in teleoperation or kinesthetic teaching which allow a robot to not only learn a task efficiently but also to autonomously practice

# Paper Content

## I. INTRODUCTION
- Reinforcement learning (RL) offers an appealing choice for such settings, as it in principle enables a robot to learn to adeptly apply contact forces and manipulate objects without strong modeling assumptions, directly from real-world experience.
- However, running RL on real-world robotic platforms raises a number of practical issues that lie outside the standard RL formulation, such as difficulties with reward specification, state estimation and the practicalities of autonomous training.
- Addressing such issues typically requires significant manual engineering or human *Both authors contributed equally https://sites.google.com/view/dexterous-avail/ intervention.
- This has led researchers to study alternative solutions, such as transfer from simulation [1]- [3], imitation learning [4]- [6], or use of cumbersome instrumentation, such as motion capture [2], [7].
- Even when these issues can be overcome, effective real-world reinforcement learning typically requires considerable reward engineering [8], complex reset mechanisms or scripts [9], and other manually designed components.
- Each of these solutions erodes the original benefits of autonomy and ease of use that RL should in principle provide.
- Solving even the simplest tasks with RL requires considerable domain and robotics expertise to program reward functions and reset mechanisms for autonomous operation.
- Thus, in order to allow for learningbased dexterous manipulation systems to reach their full potential in terms of practicality, accessibility, and scalability, it is critical to limit the assumptions on manual engineering while still providing enough supervision for reinforcement learning to be tractable.
- In this work, we propose a robotic learning system that can learn to control high-dimensional multi-fingered robotic hands from raw visual observations, without the need for extensive engineering for every new task.
- In the absence of simulation, manual reward shaping, and hand-designed state estimation instrumentation, we aim to enable RL to be as autonomous as possible.
- The robot should be able to practice the task for a long period of time without human intervention, and the task itself should be specified in a way that does not require per-task programming or humanin-the-loop supervision.
- To this end, we propose a system that autonomously practices a sequence of sub-skills based on high-level milestone specifications provided by the user that break up a complex task into more manageable subproblems.
- The milestone specifications consist of snapshots of critical states illustrated by posing the robot and objects in the scene.
- For multi-fingered hands, such examples are significantly easier to provide than full demonstrations, and our system can use them to learn reward functions that provide sufficient shaping for RL in the real-world without pertask engineering or specific reward design.
- The system uses multi-camera visual observations to localize and manipulate arXiv:2212.09902v1 [cs.LG] 19 Dec 2022 objects, with policies learned end-to-end from pixels and no motion capture.
- By sequencing the sub-skills appropriately and introducing very simple physical instrumentation (in our experiments, tethering the object to prevent it from falling out of reach), the robot can learn dexterous behaviors by practicing for up to 48 hours fully autonomously.
- The milestone decomposition makes both reward inference and autonomous practicing significantly easier, enabling realworld learning of complex tasks.
- Our experiments (see Fig. 1 for an example) show that this approach can learn skills that involve basic grasping, in-hand reorientation, and object manipulation, through significant amounts of practicing, entirely from images and without task-specific reward engineering.

## II. RELATED WORK
- Prior work has studied control of complex hands using trajectory optimization, policy search, simulation to real-world transfer, and realworld reinforcement learning.
- In contrast to our work, the majority of this prior work has assumed access to compact state representations or accurate simulators and object models.
- Closer to the system we describe in this paper is prior work on learning visuomotor policies for dexterous manipulation, but with the exception of some work we discuss below, prior systems on RL for dexterous manipulation typically require assumptions on manually designed rewards, or ground truth object state observations.
- An important consideration in our system is the ability to specify a task without manual reward engineering, by using intermediate milestone examples.
- Previously studied methods for task specification include having humans provide demonstrations for imitation learning, using inverse RL, active settings where users can provide corrections, or ranking-based preferences.
- While some prior work also uses subgoals, these are firstly restricted to reaching only particular goal states rather than more abstract milestones and are only applied in much simpler simulated problems with perfect state estimation.
- Motivated by the goal of broader applicability, we do not assume access to expert demonstrations (e.g., via teleoperation or kinesthetic teaching), which can themselves be difficult to provide for high-dimensional systems.
- For example, providing kinesthetic demonstrations for a full hand-arm robotic system requires very challenging coordination and several simultaneous demonstrators, and is incompatible with vision-based policy learning (as the demonstrator is in the scene, and often occludes the robot or objects).
- In contrast, we utilize sparse images of intermediate outcomes that can be obtained simply by positioning the robot and object in particular states and build on the VICE framework for reward inference.

## Method No Hand Engineered Reward
- Multi-Task Vision is a computer vision algorithm that can handle multiple tasks simultaneously.
- High-DoFs means that the algorithm can track objects with a high degree of accuracy.

## Ours
- Our system is based on prior work, but the combination of the building blocks and the capabilities they enable are novel
- Our assumptions about lightweight instrumentation and vision-based autonomous learning are similar to those of Zhu et al. (2020) [37], but our work tackles a more challenging setting
- Our focus is on providing a framework that can enable vision-based learning of object manipulation skills with high-dimensional hands via a lightweight milestonebased task specification mechanism.

## III. ROBOTIC PLATFORM AND PROBLEM OVERVIEW
- Overview of robot platform
- Problem setting
- Robotic hand and arm assembly
- Policy and control
- Task setup
- Image supervision
- Reinforcement learning

## IV. PROBLEM FORMALISM AND USER ASSUMPTIONS
- The Markov decision process is a model for describing the behavior of a system that is in a state and can choose between different actions.
- The environment in an episodic RL system can be described by the tuple (S, A, p dyn , ρ, γ, R), where S is the state space, A is the action space, p dyn is the environment dynamics, ρ is the initial state distribution, and γ is the discount factor.
- The typical objective of episodic RL is to optimize the discounted return.
- Standard RL assumes a reward function that in practice must often be hand engineered and tuned per-task by a user.
- We propose a method where a user supplies the robot with a set of sub-problems to practice. These sub-problems are defined by "milestone" examples, which constitute a graph structure: Definition IV.1.
- We assume the user provides a set of outcome images that can be summarized by a graph G = (V, E) of cardinality |K| indexed by z, where each vertex v ∈ V is composed of a set of M outcome images {s z i } M i=1 .
- Each set of outcome images characterizes a semantically meaningful sub-task to be solved.
- In addition, upon accomplishing a sub-task, a directed edge (v, v ) ∈ E, or equivalently a binary label, is provided which indicates which sub-task is to be practiced next.
- Consistent with the goal of having the agent continuously practice (i.e., not get stuck), we assume there are no sink nodes in the provided graph.
- Then, instead of optimizing a single-task objective J(π), we instead optimize all of the sub-tasks in the milestone graph simultaneously, resulting in a multi-task RL problem.
- Concretely, we learn a set of K policies π z indexed by a categorical variable z (one for each milestone), optimizing a set of MDPs, M ≡ (S, A, p dyn (s t+1 |a t , s t ), {R z } K−1 z=0 , p task (z|s)) , where we have introduced a per milestone reward R z and task predictor p task .
- This leads to the following objective: Fig. 4: An overview of our approach. The user provides a set of visual milestones (bottom left, K = 4 here) and transitions. We use this to formulate a multi-task learning problem where we leverage this supervision to learn all the components necessary for intervention-free learning.
- Prior to training, we learn a task selection model (blue module), which is used to choose amongst a set of K policies for data collection (Sec. V-B). This data is used to learn a success classifier (top right, Sec. V-A), which is used to automatically assign rewards.
- We show that our system is capable of autonomously learning complex manipulation behaviors on a real world anthropomorphic hand with modest instrumentation beyond the robot's own joint encoders and camera.

## V. THE AVAIL SYSTEM: AUTONOMY VIA
- AVAIL is a system for autonomously learning with minimal user input
- The system reframes the RL training process as a multitask problem that can learn directly from sparse milestone examples provided by users
- This makes the process of solving complex tasks with "programming-free" reinforcement learning significantly more approachable

## A. Visual Multi-Task Policy and Reward Learning from User Milestones
- The ability for the robot to assign rewards to its own experience relieves the burden of manual reward engineering
- To resolve this challenge in our setting where we similarly require compound behavior, we leverage the sparse milestone supervision to learn a set of success classifiers {p o z (•|s)} K−1 z=0 that decomposes the entire long horizon task
- Importantly, the provision of a number of significant milestones takes the burden off a single learned classifier to provide accurate reward shaping
- For each individual classifier p o z (•|s), we build on the VICE algorithm [36], extending it to a multi-task setting
- We learn a binary classifier p o z (•|s) over the set of user-provided examples {D 1 , D 2 , . . . , D K } as the positive class, and the agent's own experience sampled on-policy as the negative class
- Once trained, the classifier probability p o z (o|s), or a monotonic transformation of it (e.g., log p o z (•|s)), can be used as R z from Section III.

## B. Multi-Task Learning without Oracles
- The agent must decide which task to attempt next, which depends on the current situation
- In order to infer which task the agent should execute, we allow the user to provide next milestone labels (labels of what discrete milestone z k should be attempted at a particular state s k ) to train a task dynamics model by performing supervised learning over the milestones and labels provided at the beginning of training
- Rather than re-sample this task indicator at every step, we sample it every T steps and keep it fixed during data collection
- This scheme explicitly separates the task inference, and task learning allows each "sub-problem" to be treated effectively as a separate MDP

## C. Algorithm Summary and Implementation Details
- AVAIL uses supervised learning to learn the next task transitions
- AVAIL uses an observed state to collect experience
- AVAIL parameterizes each policy as a deep neural network and trains each policy using the soft actor-critic algorithm
- AVAIL does not resample the task every step

## VI. EXPERIMENTAL EVALUATION
- The paper evaluates AVAIL's ability to learn complex manipulation skills in the real world
- The paper evaluates AVAIL's ability to learn complex manipulation skills in the real world using visually indicated milestones
- The paper evaluates AVAIL's ability to learn complex manipulation skills in the real world using a real world manipulation task that requires successfully sequencing a set of skills and performing complex coordinated finger motions to manipulate objects
- The paper provides results from a real world evaluation and a simulation evaluation

## A. Real-World Task Descriptions
- The goal of the first task is to scrape a plate with a two sided cleaning brush
- The goal of the second task is to attach a cylindrical hose connector to a peg connector
- The goal of the third task is to attach a hook to a handle
- The task requires grasping, reorienting, and performing the insertion as the task milestones
- The task requires visual perception to carefully insert the connector onto the peg connector

## B. Real-world evaluation
- Save policies at regular intervals
- Evaluate their performance after training
- For all tasks, the evaluation metric for each milestone is a binary success measurement based on the distance of the hand and object to the desired pose
- Provide more details on our evaluation setup on our project website
- Real-world skill learning: We plot the performance of the evaluation runs as a function of the training step at which the policy was recorded in Fig. 6, providing learning curves for real-world training
- Observe that AVAIL automatically provides a degree of scaffolding by successfully learning skills early on in training (blue curve in left plot, blue and orange curve in center right plot) that correspond to being able to regrasp and reorient the object
- This allows the robot to continuously retry later milestones
- By the end of training, we find that the robot is able to successfully perform all milestones with a > 80% success rate
- We note that for all three tasks, no additional instrumentation is required beyond changing the object and fixture
- Upon specifying the visual milestones, the robot is capable of completely unattended learning for approximately 48 hours of robot time

## C. Simulated Comparison
- Comparison of AVAIL to prior autonomous RL methods on the (DHandValvePickup-v0) simulation domain
- SAC is compared to a standard state-of-the-art RL algorithm, soft actor-critic
- R3L is compared to our approach with two milestones
- Forward backward controller is the only prior method that succeeds in making progress

## VII. DISCUSSION AND FUTURE WORK
- Our method, AVAIL, constructs a task graph from a modest number of user-provided milestone examples
- Compared to methods with fewer degree of supervision, K = 0, 1, 2 milestones (blue, orange, red), our results illustrate the benefits of milestone supervision
- Our experiments show that this approach effectively produces a learning process where the agent first practices the easier tasks, and then builds up the more complex tasks on top of them, all the while learning autonomously without resetting

## VIII. ACKNOWLEDGEMENT
- The research project was partially supported by the Office of Naval Research.
- The research project used computing resources donated by Microsoft Azure.
