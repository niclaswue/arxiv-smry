---
title: "Biomedical image analysis competitions: The state of current participation practice"
date: 2022-12-16T16:44:46.000Z
author: "Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Patrick Godau, Veronika Cheplygina, Michal Kozubek, Sharib Ali, Anubha Gupta, Jan Kybic, Alison Noble, Carlos Ortiz de Solórzano, Samiksha Pachade, Caroline Petitjean, Daniel Sage, Donglai Wei, Elizabeth Wilden, Deepak Alapatt, Vincent Andrearczyk, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Vivek Singh Bawa, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Jinwook Choi, Olivier Commowick, Marie Daum, Adrien Depeursinge, Reuben Dorent, Jan Egger, Hannah Eichhorn, Sandy Engelhardt, Melanie Ganz, Gabriel Girard, Lasse Hansen, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Hyunjeong Kim, Bennett Landman, Hongwei Bran Li, Jianning Li, Jun Ma, Anne Martel, Carlos Martín-Isla, Bjoern Menze, Chinedu Innocent Nwoye, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Carole Sudre, Kimberlin van Wijnen, Armine Vardazaryan, Tom Vercauteren, Martin Wagner, Chuanbo Wang, Moi Hoon Yap, Zeyun Yu, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Rina Bao, Chanyeol Choi, Andrew Cohen, Oleh Dzyubachyk, Adrian Galdran, Tianyuan Gan, Tianqi Guo, Pradyumna Gupta, Mahmood Haithami, Edward Ho, Ikbeom Jang, Zhili Li, Zhengbo Luo, Filip Lux, Sokratis Makrogiannis, Dominik Müller, Young-tack Oh, Subeen Pang, Constantin Pape, Gorkem Polat, Charlotte Rosalie Reed, Kanghyun Ryu, Tim Scherr, Vajira Thambawita, Haoyu Wang, Xinliang Wang, Kele Xu, Hung Yeh, Doyeob Yeo, Yixuan Yuan, Yan Zeng, Xin Zhao, Julian Abbing, Jannes Adam, Nagesh Adluru, Niklas Agethen, Salman Ahmed, Yasmina Al Khalil, Mireia Alenyà, Esa Alhoniemi, Chengyang An, Talha Anwar, Tewodros Weldebirhan Arega, Netanell Avisdris, Dogu Baran Aydogan, Yingbin Bai, Maria Baldeon Calisto, Berke Doga Basaran, Marcel Beetz, Cheng Bian, Hao Bian, Kevin Blansit, Louise Bloch, Robert Bohnsack, Sara Bosticardo, Jack Breen, Mikael Brudfors, Raphael Brüngel, Mariano Cabezas, Alberto Cacciola, Zhiwei Chen, Yucong Chen, Daniel Tianming Chen, Minjeong Cho, Min-Kook Choi, Chuantao Xie Chuantao Xie, Dana Cobzas, Julien Cohen-Adad, Jorge Corral Acero, Sujit Kumar Das, Marcela de Oliveira, Hanqiu Deng, Guiming Dong, Lars Doorenbos, Cory Efird, Di Fan, Mehdi Fatan Serj, Alexandre Fenneteau, Lucas Fidon, Patryk Filipiak, René Finzel, Nuno R. Freitas, Christoph M. Friedrich, Mitchell Fulton, Finn Gaida, Francesco Galati, Christoforos Galazis, Chang Hee Gan, Zheyao Gao, Shengbo Gao, Matej Gazda, Beerend Gerats, Neil Getty, Adam Gibicar, Ryan Gifford, Sajan Gohil, Maria Grammatikopoulou, Daniel Grzech, Orhun Güley, Timo Günnemann, Chunxu Guo, Sylvain Guy, Heonjin Ha, Luyi Han, Il Song Han, Ali Hatamizadeh, Tian He, Jimin Heo, Sebastian Hitziger, SeulGi Hong, SeungBum Hong, Rian Huang, Ziyan Huang, Markus Huellebrand, Stephan Huschauer, Mustaffa Hussain, Tomoo Inubushi, Ece Isik Polat, Mojtaba Jafaritadi, SeongHun Jeong, Bailiang Jian, Yuanhong Jiang, Zhifan Jiang, Yueming Jin, Smriti Joshi, Abdolrahim Kadkhodamohammadi, Reda Abdellah Kamraoui, Inha Kang, Junghwa Kang, Davood Karimi, April Khademi, Muhammad Irfan Khan, Suleiman A. Khan, Rishab Khantwal, Kwang-Ju Kim, Timothy Kline, Satoshi Kondo, Elina Kontio, Adrian Krenzer, Artem Kroviakov, Hugo Kuijf, Satyadwyoom Kumar, Francesco La Rosa, Abhi Lad, Doohee Lee, Minho Lee, Chiara Lena, Hao Li, Ling Li, Xingyu Li, Fuyuan Liao, KuanLun Liao, Arlindo Limede Oliveira, Chaonan Lin, Shan Lin, Akis Linardos, Marius George Linguraru, Han Liu, Tao Liu, Di Liu, Yanling Liu, João Lourenço-Silva, Jingpei Lu, Jiangshan Lu, Imanol Luengo, Christina B. Lund, Huan Minh Luu, Yi Lv, Yi Lv, Uzay Macar, Leon Maechler, Sina Mansour L., Kenji Marshall, Moona Mazher, Richard McKinley, Alfonso Medela, Felix Meissen, Mingyuan Meng, Dylan Miller, Seyed Hossein Mirjahanmardi, Arnab Mishra, Samir Mitha, Hassan Mohy-ud-Din, Tony Chi Wing Mok, Gowtham Krishnan Murugesan, Enamundram Naga Karthik, Sahil Nalawade, Jakub Nalepa, Mohamed Naser, Ramin Nateghi, Hammad Naveed, Quang-Minh Nguyen, Cuong Nguyen Quoc, Brennan Nichyporuk, Bruno Oliveira, David Owen, Jimut Bahan Pal, Junwen Pan, Wentao Pan, Winnie Pang, Bogyu Park, Vivek Pawar, Kamlesh Pawar, Michael Peven, Lena Philipp, Tomasz Pieciak, Szymon Plotka, Marcel Plutat, Fattaneh Pourakpour, Domen Preložnik, Kumaradevan Punithakumar, Abdul Qayyum, Sandro Queirós, Arman Rahmim, Salar Razavi, Jintao Ren, Mina Rezaei, Jonathan Adam Rico, ZunHyan Rieu, Markus Rink, Johannes Roth, Yusely Ruiz-Gonzalez, Numan Saeed, Anindo Saha, Mostafa Salem, Ricardo Sanchez-Matilla, Kurt Schilling, Wei Shao, Zhiqiang Shen, Ruize Shi, Pengcheng Shi, Daniel Sobotka, Théodore Soulier, Bella Specktor Fadida, Danail Stoyanov, Timothy Sum Hon Mun, Xiaowu Sun, Rong Tao, Franz Thaler, Antoine Théberge, Felix Thielke, Helena Torres, Kareem A. Wahid, Jiacheng Wang, YiFei Wang, Wei Wang, Xiong Wang, Jianhui Wen, Ning Wen, Marek Wodzinski, Ye Wu, Fangfang Xia, Tianqi Xiang, Chen Xiaofei, Lizhan Xu, Tingting Xue, Yuxuan Yang, Lin Yang, Kai Yao, Huifeng Yao, Amirsaeed Yazdani, Michael Yip, Hwanseung Yoo, Fereshteh Yousefirizi, Shunkai Yu, Lei Yu, Jonathan Zamora, Ramy Ashraf Zeineldin, Dewen Zeng, Jianpeng Zhang, Bokai Zhang, Jiapeng Zhang, Fan Zhang, Huahong Zhang, Zhongchen Zhao, Zixuan Zhao, Jiachen Zhao, Can Zhao, Qingshuo Zheng, Yuheng Zhi, Ziqi Zhou, Baosheng Zou, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08568v1.webp" # image path/url
    alt: "Biomedical image analysis competitions: The state of current participation practice" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08568)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08568).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/biomedical-image-analysis-competitions-the).

# Abstract
- Number of international benchmarking competitions in ML increasing
- Survey conducted to understand development of algorithms in biomedical imaging
- 70% of participants motivated by knowledge exchange, 16% by prize money
- 80 working hours spent on method development, 32% didn't have enough time
- 25% perceived infrastructure to be a bottleneck
- 94% of solutions deep learning-based, 84% based on standard architectures
- 43% of data samples too large to process at once, addressed by patch-based training, downsampling, and solving 3D analysis tasks as a series of 2D tasks
- K-fold cross-validation on training set performed by 37%, 50% performed ensembling
- 48% applied postprocessing steps

# Paper Content

## Purpose
- Validation of biomedical image analysis algorithms is conducted through challenges.
- Challenges compare algorithm performance on identical datasets.
- ML models used to solve tasks have increased in complexity.
- Challenges have increased in scientific impact.
- Results are often published in prestigious journals.
- Survey was issued to participants of challenges in 2021.

## Methods
- BIAS guideline defines a biomedical image analysis challenge as an open competition on a scientific problem
- Survey was developed by Helmholtz Imaging and the SIG for Challenges of the MICCAI society
- Survey was structured in 5 parts and covered general information, expertise and environment, strategy, algorithm characteristics, and miscellaneous information
- Survey was sent to organizers of IEEE ISBI 2021 and MICCAI 2021 challenges
- Survey was conducted in closed-access or open-access mode

## Results
- 80 competitions included in the study
- 11% of problems addressed were considered solved
- 292 survey forms completed, 249 met inclusion criteria
- 86% of respondents affiliated with academic institutions, 12% with industry, 4% with no institution

## Expertise and team composition
- Almost all respondents had an academic degree
- 45% had a master's degree, 27% had a doctoral degree, and 24% had a bachelor's degree
- Backgrounds were computer science (48%), electrical engineering (17%), or biomedical engineering (15%)
- 34% were doctoral students, 19% master's students, 10% postdoctoral researchers, and 9% professors
- 10% were developers/engineers and 4% were team leads or managers
- Median of 3 team members contributed to the challenge submission
- 22% of lead developers worked alone, 16% participated entirely alone
- 43% had regular meetings with supervisors, 47% with colleagues/method experts
- 12% of teams had multiple members working on/implementing a single approach
- 27% of teams had multiple members exploring/implementing diverse approaches
- 22% of teams had a domain expert involved
- 54% of respondents had no experience in machine learning competitions
- Most experienced member had a median of 2 challenge experiences
- 49% rated their experience with similar tasks as moderately/extremely familiar
- 65% felt moderately/extremely familiar with similar methods
- 64%, 64%, and 54% felt moderately/extremely familiar with similar datasets
- 48% of team members rated other team members as moderately/extremely familiar with similar tasks
- 50% rated other team members as moderately/extremely familiar with similar methods
- 61%, 64%, and 58% rated other team members as moderately/extremely familiar with similar datasets
- 25% thought infrastructure was a bottleneck
- 92% used GPU resources
- Total training time of all models was a median of 267 GPU hours
- Total training time of final model was a median of 24 GPU hours
- Python was the main programming language (96%)
- Top low-level, core, and high-level libraries were PyTorch (76%), NumPy (74%), NiBabel (34%), SimpleITK (33%), and torchvision (29%)

## Strategy for the challenge
- Knowledge exchange was the most important incentive for participation
- Possibility to compare own method to others was second most important incentive
- Awards/prize money was important to only 16% of respondents
- Median time of 2.5 weeks prior to submission deadline
- 60 working hours (median) spent prior to decision to submit results
- Most work dedicated to method development, running baseline method, analyzing data/annotations, hyperparameter tuning, literature research, failure case analysis, challenge design
- 42% of respondents based approach on existing work, 15% reimplemented closest reference method
- Half of respondents reimplemented a method based on a publication
- 57% used code base of baseline method
- 94% used deep learning-based approach
- Most time spent on selecting/configuring architecture, data augmentation, exploring loss functions, ensembling
- 17% explored additional data
- 38% expected substantial performance boost with more time

## Algorithm characteristics
- 9% of deep learning-based approaches used additional data
- Types of data used included biomedical data from public and private datasets, and non-biomedical data from public datasets
- Additional data was used for pre-training and co-training

## Network topology.
- 84% of networks based on common computer vision architecture
- 33% of networks pre-trained on another image dataset
- Median of 7.8 million trainable parameters
- Median of 10 hyperparameter combinations explored
- 13% used architecture search to find final network
- 80% used matching architecture type, 5% used non-standard approach
- 57% modified architecture to improve performance
- 71% took challenge metrics into account while searching for hyperparameters
- 80%, 66%, 44%, 43% used data augmentation, batch normalization, dropout, weight decay to avoid overfitting
- 85% used data augmentation
- 43% reported data samples too large to process at once
- 69% used patch-based training, 37% used downsampling, 18% used 3D analysis as series of 2D analysis tasks, 5% used time-lapse analysis as series of single-frame analysis tasks
- 39% used cross-entropy loss, 32% used combined CE and Dice loss, 26% used Dice loss, 9% used custom-designed loss, 5% used MSE loss
- 29% used early stopping, 12% used warmup
- 52% used single train:val(:test) split, 37% used K-fold crossvalidation

## Ensemble methods.
- Half of the respondents used a single model trained on all available data
- 6% proposed an ensemble of multiple identical models, each trained on the full training set
- 21% proposed an ensemble of multiple identical models, each trained on a randomly drawn subset of the training set
- 9% ensembled multiple different models, each trained on the whole training set
- 8% ensembled multiple different models, each trained on a randomly drawn subset of the training set
- Median of 5 models used in final solution, max of 21 models
- 48% of respondents applied postprocessing steps

## Outlook
- Conducted international survey on biomedical challenges
- Linking strategies of teams to challenge ranking to determine why winner is best
