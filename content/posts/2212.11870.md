---
title: "Impossibility Theorems for Feature Attribution"
date: 2022-12-22T17:03:57.000Z
author: "Blair Bilodeau, Natasha Jaques, Pang Wei Koh, Been Kim"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11870v1.webp" # image path/url
    alt: "Impossibility Theorems for Feature Attribution" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11870)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11870).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/impossibility-theorems-for-feature).

# Abstract
- Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods
- In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way
- We show that for even moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear--for example, Integrated Gradients and SHAP--can provably fail to improve on random guessing for inferring model behaviour
- Our results apply to common end-tasks such as identifying local model behaviour, spurious feature identification, and algorithmic recourse

# Paper Content

## Introduction
- Feature attribution methods are used to answer local counterfactual questions about machine learning models
- Feature attribution methods fall into two categories: first-order approximations of f (x) in a sufficiently small neighbourhood around x (e.g., taking the gradient of f near x (SVZ13), including more sophisticated approaches such as SmoothGrad (STK+17) and LIME (RSG16)), and methods that incorporate how the model behaves on a baseline distribution µ over examples that might be far from the example of interest (e.g., SHAP (LL17) and Integrated Gradients (STY17))
- The failure modes of local, first-order methods like taking the gradient of f are well understood
- For any feature attribution method that satisfies the completeness and linearity axioms, users cannot generally do better than random guessing for end-tasks such as algorithmic resource and spurious feature identification
- Conversely, for every pair of distinct feature attribution values, there are uncountably many pairs of models that match these feature attributions and yet have identical counterfactual model behaviour
- Furthermore, unlike simple local methods such as taking the gradient, complete and linear methods remain unreliable even if we restrict our attention to infinitesimally small neighbourhoods around the example of interest x, because they still remain sensitive to how the model behaves on the baseline µ that might be far from x

## Problem Framework
- Counterfactual model behaviour is a simple notion that is closely related to common end-tasks such as recourse and spurious feature detection.
- For a model f , the user wants to use the output of a feature attribution method to infer the model's dependence on feature j near an example x (that is, within a δ-neighbourhood).
- Counterfactual model behaviour describes f (x ) for x ∈ X such that x j ∈ (x j −δ, x j +δ) for a given radius δ > 0 and feature j ∈ [p].
- Given some candidate model behaviour g (0) , g (1) : (x j − δ, x j + δ) → Y, the primary task we consider in this work is inferring which model behaviour is more likely.
- Where do g (0) and g (1) come from? In general, the counterfactual model behaviour to be inferred is not precise enough to be described by a single g.
- However, if the user can reliably infer whether the model is increasing, there must exist some pair g (0) , g (1) (the former increasing, the latter decreasing) that the user can reliably distinguish between.

### Framing the Problem: Hypothesis Testing
- The goal of the user is to infer counterfactual model behaviour
- The user's goal is to determine whether the model has certain behaviour: the null hypothesis
- The null hypothesis and alternate hypothesis should encode necessary (but potentially not sufficient) questions that must be answered to succeed at the task of inferring counterfactual model behaviour
- For recourse, the user must be able to infer if the model is increasing or decreasing, while for spurious features the user must be able to infer if the model is sensitive or insensitive to perturbations of a feature
- After collecting information about the model using a feature attribution method, the user will then conduct a hypothesis test and either reject or fail to reject the null hypothesis
- Formally, the null and alternate hypotheses define subsets F (0) ⊆ F and F (1) ⊆ F, with While it must be the case that these sets have no overlap (F (0) ∩F (1) = ∅), it is often the case that they do not exhaustively contain all models (F (0) ∪ F (1) = F)
- A feature-attribution hypothesis test is any way for the user to draw their conclusion for the hypothesis test solely on the output of a feature attribution method at an example
- For a more complete discussion of precise definitions and properties (including the definition of SHAP), see Appendix A
- The goal of our work is to see if certain feature attribution methods can reliably be used to conduct the hypothesis tests described above
- Classically, the quality of a hypothesis test is determined by its specificity, which is the probability that the user fails to reject the null hypothesis whenever the model satisfies the null hypothesis (a true negative, or 1− probability of a type-II error), and its sensitivity, which is the probability that the user rejects the null hypothesis whenever the model satisfies the alternate hypothesis (a true positive, or 1− probability of a type-I error)
- In particular, for a fixed feature attribution method Φ, baseline µ ∈ P(X ), example x ∈ X , and null and alternate hypotheses F (0) , F (1) ⊆ F, For every hypothesis test, it is trivial to construct some model with accurate inference; our measure of performance instead asks if the hypothesis test does well for all models of interest (mathematically, this is the role of the inf1 ).
- The goal of the user is to simultaneously maximize specificity and sensitivity (both take values in [0, 1]).

### Notation
- We define a standard basis vector e i for any integer i.
- For any set A, A c denotes its complement.
- For any example x ∈ X, x j denotes the jth element/feature.
- Often, we want to evaluate f(z) where z ∈ R p is defined such that for some examples x, x ∈ X and feature j ∈ [p].
- That is, replacing the jth feature of x with x j.
- For clarity, we overload the notation of f and write f(x[p]\{j} , x j ) = f(z).
- We use shorthand vector notation, letting
- For any matrix M, let M j denote the jth row of M.
- We use capital letters to denote random variables, and for any µ ∈ P(X) and f : with similar notation for variances, covariances, etc.
- For any j ∈ [p], let µ j denote the jth marginal distribution of µ.

## Impossibility Theorems
- It is impossible to conclude that the user does better than random guessing at inferring counterfactual model behaviour using common feature attribution methods without strong additional assumptions on the learning algorithm or data distribution.
- We also apply this result to two common end-tasks, showing that under these conditions, it is impossible to conclude that the user does better than random guessing for algorithmic recourse and spurious feature identification using these feature attribution methods.
- Even for very simple models (so our richness condition does not hold), these feature attribution methods provably still incorrectly infer counterfactual model behaviour with significant probability.

### Main Result
- Theorem 3.3 states that without imposing additional assumptions on the underlying data or learning algorithm to significantly reduce the model complexity, the user cannot conclude that they have learned any information about the model.
- In Section 4, we demonstrate that this holds empirically for real data and real models.
- For simplicity, we state our results for counterfactual model behaviour with respect to how the model depends on a single feature at a time, and defer the extension to arbitrary groups of features as well as all proofs to Appendix B.
- The generality of Theorem 3.3 means that it applies to inferring any form of counterfactual model behaviour.
- Returning to the setting of Liu et al. (LRW+21), the user may wish to infer if the model output changes as a function of a certain feature (e.g., is the hazard ratio sensitive to the exclusion criteria).
- In order to answer this, they must be able to distinguish between g (0) ≡ 0 and some g (1) that changes with feature j.
- However, Theorem 3.3 implies that for every such g (1) , the user cannot conclude that they do better than random guessing at this inference task, and hence they cannot conclude that they do better than random guessing at identifying whether the model depends on the feature.
- Thus, if this method is used for purposes such as determining membership in a clinical trial, the conclusions are not guaranteed to be any more reliable than random guessing.

## Experiments
- The theoretical guarantees of the previous section demonstrate that common feature attribution methods such as SHAP and IG do not provide sufficient information to reliably infer counterfactual model behaviour.
- While our assumptions are satisfied by moderately rich model classes (including neural networks), and we have proved analogous results for even simpler classes in Section 3.5, our theory does not rule out the possibility that additional structure extracted from the training data or learning algorithm is further aiding feature attribution methods in practice.
- In particular, it remains to answer: Will the theoretical results apply in practice once we restrict consideration to neural networks trained with stochastic gradient descent on real datasets? The experiments in this section answer this question in the affirmative.

### Methods
- To estimate the specificity and sensitivity of a feature attribution method, we conduct independent repetitions of the experiment conditional on the data.
- For each experiment, we sample new training and test data, train a neural network with fixed architecture, compute multiple feature attribution methods for this trained model, and finally convert the feature attribution method to a hypothesis test for the two task of interest.
- While there are many ways to convert the output of a feature attribution method (an element of R p ) into a hypothesis test (an element of [0, 1]), we select one for concreteness, which we describe in the next subsection.
- Finally, to evaluate the ability of the feature attribution method to infer model behaviour, we compare the output of the hypothesis test to the ground-truth that is computed by brute-force model queries (see Section 5 for more details).

### Results
- SHAP and IG are unreliable for solving end-tasks like algorithmic recourse and spurious feature identification
- Gradient can be accurate for inferring sufficiently local model behaviour, but this can also clearly fail even for simple tasks that demand understanding a moderately local region
- For a given end-task, what should a practitioner do?
- Brute-force solving end-tasks via repeated model evaluations is guaranteed to work

## Related Literature
- Two impossibility results have recently appeared in the literature
- Fokkema, de Heide, and van Erven (Fdv22) show that continuous feature attribution methods (including SHAP and IG) can fail for algorithmic recourse at examples near the decision boundary of a model (i.e., where the true recourse direction should switch)
- In contrast, we prove that for sufficiently rich model classes, recourse-like counterfactual model behaviour cannot be reliably inferred at any example
- Han, Srinivas, and Lakkaraju (HSL22) show that for perturbation-based feature attribution methods (including SHAP and IG), there will always exist a neighbourhood where the feature attribution does not match the model (in the sense of local function approximation)
- Our results imply that this holds for exactly the neighbourhood where the feature attribution method is centered; that is, not only do feature attribution methods fail somewhere to capture model behaviour globally, they fail to do so in the exact local region of interest as well
- Beyond such impossibility results, others have proposed various ways to formalize the task of feature attribution
- Watson et al. (WGT+21) use the logic concepts of necessity and sufficiency to discuss whether feature attributions can be tied to model behaviour
- Watson and Floridi (WF21) propose studying interpretability as a decision theory problem, where two players iterate to find a "good" explanation, which they formalize as learning a local approximation of the model
- Afchar, Hennequin, and Guigue (AHG21) formalize ground truth for feature attribution methods using a local form of functional feature dependence
- Zhou and Shah (ZS22) formalize feature attribution methods as loss minimizers, which they use to unify certain methods
- While all of these formalizations provide a useful lens through which to study feature attribution methods, none have been used to prove results about the performance of existing or proposed feature attribution methods
- Our hypothesis testing framework can be viewed as a formalization of (some of) these ideas that enables concrete mathematical reasoning about the performance of feature attribution methods
- Finally, there exist many individual counterexamples for common feature attribution methods in the literature

## Conclusion
- SHAP is complete
- Integrated Gradients is complete
- SHAP is linear
- Integrated Gradients is linear
