---
title: "Economic impacts of AI-augmented R&D"
date: 2022-12-15T23:48:18.000Z
author: "Tamay Besiroglu, Nicholas Emery-Xu, Neil Thompson"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08198v2.webp" # image path/url
    alt: "Economic impacts of AI-augmented R&D" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08198)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08198).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/economic-impacts-of-ai-augmented-r-d).

# Abstract
- Since its emergence around 2010, deep learning has rapidly become the most important technique in Artificial Intelligence (AI).
- In two computer vision tasks that are considered key test-beds for deep learning, we find that AI idea production is notably more capital-intensive than traditional R&D.
- This increased capital-intensity of R&D accelerates the investments that make scientists and engineers more productive, which has the potential to speed up technological change and economic growth.

# Paper Content

## Introduction
- AI has the potential to impact idea production and, subsequently, productivity and economic growth.
- Deep learning has led to a rapid increase in the rate of progress in Artificial Intelligence (LeCun et al. 2015;Goodfellow et al. 2016, Ch. 1;Russell and Norvig 2020, Ch. 1.3).
- AI has the potential to improve knowledge production by effectively searching through and recombining a wider range of ideas than is possible by human scientists and that this could result in accelerated economic growth.
- Mixed results exist about the impacts of AI on wages, factor shares, and economic growth.
- The relationship between AI and data is important in understanding the impacts of AI on the innovation process.
- The impacts of AI on the innovation process deserve special attention.

## Capital-intensive R&D and deep learning

### The role of capital in idea production
- The role of capital does not usually receive center-stage in the analysis of R&D-based growth, but it has been shown to generate permanent growth effects by increasing the marginal product of labour in R&D and thus increasing investment in the R&D sector
- A very similar point is made by Aghion et al. 2019 in their study of the growth effect of AI, which shows that in the classic endogenous growth case, a one-time increase in R&D automation will raise the long-run growth rate
- Empirical work has validated the importance of physical capital investment in idea production. For example, Helmers and Overman 2017 showed that the creation of the UK's Diamond Light Source synchrotron increased the research capital available to local scientists, which in turn increased their research publication output relative to UK scientists located elsewhere
- Since the overall capital-intensity of academic science is only 4%, there is enormous room for more capital-intensive approaches
- Computing theory also suggests reasons deep learning is capital-intensive and why it is likely to become more so. In classical statistical learning theory, there generally is a trade-off between bias and variance
- Once a model grows beyond a certain complexity threshold, it tends to "overfit" the data, worsening test performance2 as the variance term dominates. Deep neural networks seem capable of evading this trade-off by vastly expanding the size of the network ("overparameterization"), that is by deploying more computational capital
- Surprisingly, empirical analyses have shown that the performance gains that accrue to these networks with millions or billions of parameters are highly predictable. Generally, these analyses of "neural scaling laws" find that test error falls according to a power law in the scale of such models, and therefore in the amount of compute used
- Researchers and practitioners are harnessing these dependencies to get better performance. For example, Thompson et al. 2020 shows that progress is highly dependent on computational resources across a wide range of machine learning tasks.

### R&D capital intensity in a semi-endogenous growth model
- Consider a simple semi-endogenous growth model along the lines of Jones 1995.
- There are two sectors, a goodsproducing sector where output is produced and an R&D sector where additions to the stock of knowledge are made.
- A fraction α l of the labour force is used in the R&D sector and fraction 1 − α l in the goods-producing sector. Similarly, fraction α k of the capital stock is used in R&D and the rest in goods production.
- We make similar simplifying assumptions as Romer 1990 by supposing that α l and are α k exogenous and constant for expositional clarity.
- Ideas are non-rivalrous and the full stock is used equally in both sectors of the economy.
- For further simplicity, we assume constant returns to scale in the production of final goods.
- The production of new ideas depends on the quantities of capital and labour engaged in research and on the level of technology.
- We assume there are diminishing returns in the production of new ideas in inputs (β + θ < 1). This assumption ensures a unique steady-state growth rate, and prevents the growth rate from exploding as the R&D inputs grow without bound.
- The idea stock grows as follows: where B is a positive shift parameter.
- We further make the simplifying assumptions, not uncommon in the literature, that there is a constant saving rate s, and that capital depreciates at a constant rate δ. Moreover, our model is a semi-endogenous one. Hence, we suppose that population grows at exogenous rate n.
- Thus capital and labour accumulation are described as follows: , and L(t) = nL(t), where s, δ ∈ (0, 1) and n > 0.
- Using equations (1-3), we solve for the steady-state rates of growth in ideas and capital (denoted as g * a and g * k respectively.4 )
- These are:
- Proposition 1. Consider a shift in the technology of R&D that creates a positive shock to the R&D elasticity to capital and, at worst, a proportional negative shock to the R&D elasticity to scientists. That is, consider a shift from an R&D setting described by (2) to a setting described as follows: Such a shift in the technology of R&D has the following implications: (a) the rate of idea accumulation is strictly and permanently increased, and (b) the rate of economic growth is strictly and permanently increased.
- Proof. Define ∆ X ≡ X − X as the difference between pre-and post-values of parameter X. By assumption, ∆ β ≥ −∆ γ . The steady-state rate of growth in ideas is strictly increased if: which follows from the fact that ∆ β ≥ −∆ γ . This proves part (a) of the proposition. The proof of part (b) may be found in Appendix A2.

## Data
- γ estimates for image classification are 0.246 (model A1) and 0.350 (model A2), both significant at the 0.1% significance level.
- γ estimates for object detection tasks are 0.352 (model B1) and 0.319 (model B2), both statistically significant at the 5% significance level.
- We find that the implied capital cost share for object detection is higher than for image classification by a margin of roughly ten percentage points. However, this difference is not statistically significant.

### Data on computer vision experiments
- Our dataset on the compute costs and performance covers 151 models published between 2012 and 2021
- The compute estimates are derived from the underlying papers following the procedure described by Sevilla et al. 2022b
- The inclusion and exclusion criteria used to generate our datasets is described in Thompson et al. 2020
- Deep learning models in this dataset span two well-known benchmarks: image classification on the ImageNet dataset and object detection on the Microsoft COCO dataset, usually known as the MS COCO dataset
- ImageNet is perhaps the most well-known and widely used computer vision dataset
- For inputs, many deep learning papers fail to report even basic details of their computational usage
- For outputs, some areas of deep learning struggle to define objective measures of performance

### Data on authors and publications
- Our dataset on machine learning publications comes from arXiv, a pre-print server commonly used in various STEM fields, including computer science
- Our dataset includes all papers on arXiv that were posted between 1993 and 2021 that are from the subfields typically associated with machine learning: Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), and Learning (cs.LG)
- We match the authors of these papers to their corresponding entries in Scopus using a variety of string distance-based matching approaches
- This technique allows us to match 90.1% of authors, and spot testing on 300 random matches shows that 96% were correct
- With the connection between papers to their authors' publication histories, we construct a timeseries for each author that shows their number of publications, h-index, and citations (excluding self-citations)
- We supplement this data with similar timeseries of grant funding for each author's institution and department over time from the Dimensions grant database, institutional rankings over time from csmetrics.org, and measures of the scientific influence of computer science journals and conferences using from SCImago

## Empirical strategy
- The assumption is that idea production depends on three factors: labour (scientists' human capital), specialised capital goods (computational capital), and total factor productivity (the extant level of 'ideas' or technology upon which researchers build).
- To estimate this using data, we replace Ȧ(t) with a measure of the performance of deep learning models, C(t) with data on computational inputs, and S(t) with estimates of scientific human capital inputs.

### Empirical specification
- Technology grows exponentially on the balanced growth path in the way standardly assumed in growth theory models
- In these cases, the level of performance-usually measured as a type of predictive accuracy-falls on the unit interval
- We assume that performance relates to technology according to the logistic function, reflecting that the most challenging parts of innovation are being able to make some initial headway with a problem and then perfecting it
- This is a similar assumption used to model how effort relates to various outcomes when the outcomes are bounded, such as in contests (Vojnović 2015;Baik 1998), conflict interactions (Hirshleifer 1989;Jia, Skaperdas, et al. 2012), and persuasion (Skaperdas and Vaidya 2012)

### Operationalizing innovations
- The goal of the study is to estimate proportional performance improvements in terms of observables.
- We measure this using our baseline data, where authors of each paper have indicated the touchstone models in the literature whose ideas they are building on.
- That is, for any particular task, we define relative performance gains as follows: That is, i's innovation is defined as the proportional improvement over the performance of a model that is considered, by the contemporaneous literature, to be a relevant baseline model.
- This operationalization is chosen for two reasons. Firstly, it is common practice to report these values in the machine learning literature, as the extent of innovations are often illustrated through comparisons to existing baseline levels of performance (Armstrong et al. 2009;Melis et al. 2017;Pressel et al. 2018). Secondly, this notion of an improvement over a model lines up well with the usual notion of the change in stock of knowledge in R&D-based growth models, such as those from Romer 1990; Grossman and Helpman 1994 and others; it represents the extent of the innovation of a new design relative to the existing stock of ideas.

## Modelling human capital
- The final remaining piece needed to estimate the deep learning production functions for these computer vision tasks is to construct a measure of the scientific human capital used for each of these models.
- This measure should be predictive of outcomes that are strongly influenced by human capital and must be inferable from available data about the scientists' track records.
- In addition to overall predictiveness, it is particularly important that estimates are good for junior researchers, who contribute importantly to this young field.
- Prior work has used various measures of the quality or status of scientists and engineers, including impactbased metrics, such as citation counts (Azoulay et al. 2019;Jones et al. 2014;Zucker et al. 2002), the number of high-impact citations (Azoulay et al. 2014) and bibliometric indices such as the h-index (Teplitskiy et al. 2019;Fisman et al. 2018;Breschi et al. 2014).
- These approaches have substantial limitations as approaches to measuring the human capital of teams of scientists and engineers.
- As we shall see, almost all of these measures are only weakly predictive of key outcomes where we would expect scientific and technical human capital to be important, such as the number of citations the work will receive in the future, or the quality of the journal or conference the publication is to be published in.
- Moreover, impact-based metrics, such as citation counts or the h-index generally assign low scores to junior researchers, as citations often take a long time to accrue following the publication of scholarly work.
- Our strategy for modelling researcher human capital is as follows.
- We construct a deep neural network (DNN) and train it to develop a single-dimensional representation of the total quality-adjusted research input ("human capital") that is highly predictive of key bibliometric and publication-related outcomes.
- Our approach implements an encoder that maps many features about the publication's authors input to a single-dimensional representation, and a decoder model (built explicitly to have the function as a linear regression) that maps this representation onto citation-and publication-related outcomes.
- Our approach exploits the ability of DNNs for nonlinear data compression of high-dimensional input features (see e.g. Hinton and Salakhutdinov 2006;Kramer 1991).
- Our approach finds human capital representations that are highly predictive of key bibliometric and publication-related outcomes, and that substantially outperform the typical approaches used in the literature.

### Our machine learning approach to estimating human capital
- We use our dataset of 49,251 machine learning publications to train a neural network to predict bibliometric and publication-related outcomes.
- The predicted outcomes include the citation trajectories for each publication and its SJR-values, a measure of the quality of the journal or conference where the work ends up being published.
- Figure 2a provides a diagrammatic overview of our data pipeline and the training set-up used to produce our model.
- Further details of the training procedure may be found in Appendix F.
- Figure 2b shows our neural network architecture.
- Highlighted is the human capital unit, whose activations are strongly related to the quality of the research team.
- The numbers on each layer represent the number of units in that layer (for the human capital unit, this is just 1).
- Our architecture is constructed as follows. We first stack of 15 sets hidden layers, each consisting of a 4096 or 2048 node layer, followed by a batch-norm layer (Ioffe and Szegedy 2015).
- These feed into a single unit -the "human capital" unit.
- This layer forces the neural network to reduce the dimensionality of its representations and distil the relevant features into a single scalar.
- The human capital result is then concatenated with the publication date, and fed into a series of independent sub-branches, one for each output being predicted.
- The final layer effectively implements separate linear regressions of the sort y i = α + xβ, meaning that the learned human-capital representations can only be linearly re-scaled and offset in order to make predictions about citations or journal quality.
- In other words, our approach implements an encoder that maps the input to the representation space, and a set of decoder regression models that map the representation onto citation-and publication-related outcomes.

### Validating our estimates
- Our measure has better predictive performance relative to other commonlyused human capital predictors
- In all cases, our estimates predict more than 55% of the variation in these measures, roughly 4-5 times as much as other proxies that are commonly used
- When restricting the dataset to just publications with junior researchers (defined as publications with 2 or fewer prior publications), we find that our human capital estimates are still highly predictive of each of the bibliometric and publication-related outcomes

## Empirical Analysis
- Having validated our human capital measures, we combine them with the compute data to estimate production functions for two important AI tasks
- In particular, we estimate individual regression models and a pooled model described by equation 13
- We estimate using OLS, except where a Breusch-Pagan test finds the presence of heteroskedascitity, in which case we estimate a GLS model by Maximum Likelihood
- Our results for models A1-B2 are displayed in Table 4
- We also estimate a pooled model with distinct time-fixed effects, which combines data across both computer vision tasks
- A likelihood ratio test indicates that the pooled model fits the data better than separately estimated models
- The estimates of models C1-C2 are displayed in Table 5

## R&D with deep learning
- Having estimated the capital intensity of R&D that is augmented with AI, we analyze the potential productivity effects that the widespread adoption of deep learning would have on economic productivity and growth.
- To do so, we suppose that the widespread adoption of deep learning would act as a one-time shock, raising the capital intensity of knowledge production in the economy to the levels we estimated in computer vision.
- Along the balanced growth path, the steady-state growth rate in the stock of knowledge is described by equation 4.
- Using this, we can substitute in the parameter values implied by our empirical estimates from the prior section into our semi-endogenous growth model and compute the predicted change in R&D productivity growth conditional on the widespread of adoption of deep learning.
- As shown in Figure 5, depending on the model specification, the results from image classification would imply a productivity growth rate between 1.6% and 1.8%, whereas those from object detection would imply a rate between 3.1% and 3.9%.
- Our preferred estimate, both because more data inform it and the estimates are more precise, is the pooled estimate for computer vision.
- With the widespread adoption of deep learning raising ideas production in the economy to this level of capital intensity, we would expect the productivity growth rate to rise to between 2.1% and 2.4%.
- To put that in context, this would amount to increase of between 1.7-and 2-fold relative to the 1.2% average U.S. productivity growth from 1948 to 2021, and a 2.6-to 3-fold increase against the post-2000 0.8% growth (Fed 2022).

## Robustness and external validity
- The results in Section 7 are robust to outliers and to different choices of how granular or coarse-grained time periods are specified
- The assumptions about the substitutability of labour and capital are consistent with the data
- The results from computer vision can be generalized to other R&D tasks

### Sensitivity to outliers
- It is known that certain empirical results from high-profile studies can be reversed by removing less than 1% of the sample even when standard errors are small.
- In this section, we assess the sensitivity of our results to outliers by showing that the removal of a small fraction of the data is not determinative of our findings.
- To test the robustness of our results to the removing of samples, we re-run our analysis between around 10 4 and 10 6 times (depending on which model is re-estimated) on random sub-samples of our datasets that excludes a fraction of our observations.
- The point estimates are plotted in Figure 6.
- For our dataset on object detection, we estimated models on all possible sub-samples that exclude 3 observations (of which there are 40! For our dataset on image classification and for our pooled dataset, we estimated models on 10 6 random sub-samples that exclude 5 observations.
- The sub-samples considered covers around 1.6% of all possible sub-samples for the image classification dataset and 0.3% of all sub-samples for the pooled dataset and is, therefore, a non-trivial fraction of all possible permutations.

### Alternative model specifications
- Cross-sectional variation is exploited by estimating the variation in performance due to changes in inputs amongst contemporaries
- This variation is assumed to be due to inputs, not changes in the stock of knowledge
- However, this variation is estimated with fewer data points, making it noisier and more prone to over-fitting
- The estimates are robust to different choices of how granular or coarse-grained time periods are specified.

### Sensitivity to model assumptions about the substitutability of human scientists
- The assumption that the elasticity of substitution between human scientists and compute is 1 is important because it implies that the stock of specialized capital goods, such as computers, will grow faster than the stock of scientists.
- If human scientists were more easily substitutable than we supposed, the capital intensity of R&D would be lower than our estimates imply.
- To investigate whether σ = 1 is a reasonable level of substitutability to assume, we estimate equation 15.
- Our estimates of σ are very tightly clustered around unity, which reinforces the assumption that σ = 1 is reasonable.
- For the tasks for which we have the most data, we observe that our estimates σ are slightly above 1.

### Limits to external validity stemming from our choice of domain
- Deep learning is a subfield of computer science that uses artificial neural networks to learn patterns in data
- The key insights gained from studying a wide range of architectures for computer vision can be applied to a wider range of R&D problems
- There are pronounced architectural divides across domains, but these differences are also represented in the data

### How data availability influences our estimates
- The returns to compute might be higher than they would be in lower-data-abundance regimes
- This suggests that the returns to compute in high-data regimes can be unusually high
- For economically important R&D tasks, we might expect complementary investments in generating the necessary datasets to train machine learning models to be made

## Discussion

### Summary and implications
- Deep learning is more capital-intensive than other forms of R&D
- If deep learning is widely adopted in R&D and this increases the returns to computational capital, then technological change will permanently accelerate
- Consequentially, according to semi-endogenous growth theory, we will also get an acceleration of economic growth.
- We make a methodological contribution by introducing a novel machine learning-based method for inferring human capital from scientific publications
- In our approach, this encoder is tasked with learning representations of human capital that are most predictive of publication and citation-related outcomes
- Our work identifies three areas of future work that we expect to be fruitful
- First, it would be valuable to better understand the diffusion of deep learning techniques within the economy
- Second, to look at whether improvements in AI techniques tend to make R&D more capital-intensive
- Third, to investigate how AI-augmented R&D makes deeper changes to the knowledge production

### Selection issues and measurement error
- The data used in our empirical work may suffer from selection bias issues
- The dataset contains just models for which authors reported enough to infer how the model was trained and how much computation was needed
- If it did, it would likely positively bias our estimates of the returns to physical capital due to two effects
- First, we suspect that publications that report these details are more likely to use large amounts of computation
- Second, because the cost of deploying hardware for machine learning training is expensive (see e.g. Sharir et al. 2020), large compute deployments are likely associated with more effort to optimise training runs that ensures that these resources are used efficiently. Such efficiencies could lead to those deployments being better able to leverage computation to achieve particular results
- As a consequence, our data would disproportionately be composed of those using computation more effectively
- There are reasons to expect that such bias would be small
- First, leading machine learning conferences require or at least encourage researchers to report details about hardware usage
- Second, the implementation of training runs are likely to be fairly similar across papers, as researchers typically use one of only a few types of open-source software that have distributed training settings and profilers that might be broadly comparable in achieving utilization
- Nevertheless, we address this selection issue by using two different methods for inferring the amount of computation that was used in training (so that we could capture a wider range of models than would otherwise be possible)
- Thus, although selection bias might be introduced by the machine learning experiments included in our dataset, it is unclear whether this is a significant issue
- Even if our estimate for the effectiveness of compute is overestimated, it is unclear whether this would imply that our estimate of capital share is over-estimated
- This is because the expertise needed to optimize training runs will likely be captured in our human capital estimates so that these would also be overestimated, and it is unclear which effect would be more significant
- There are also potential measurement-error issues with our estimates of the amounts of training computation to train machine learning models
- Firstly, we estimate only the amount of computation used in training the model in the final training run, not all training runs
- If this were a constant fraction, this would not bias our estimates of the relevant elasticities
- Nevertheless, our use of approximate techniques in estimations could have attenuated our estimates, which would mean that our results are underestimates

## Conclusion
- Much has been written on the potential mechanisms through which AI impacts productivity and output growth.
- We provide empirical evidence for one of these mechanisms: the ability for scientists to efficiently harness more computational capital in R&D.
- We argue that this mechanism is consequential because standard endogenous growth models imply that greater capital intensity in R&D produces permanent increases in productivity growth and economic growth.
- We present evidence that deep learning-based computer vision research is significantly more capital-intensive than virtually all other R&D sectors in the U.S.
- If, as we argue, deep learning has similar impacts in other areas of R&D, then the widespread adoption of deep learning-based R&D could double U.S. productivity growth rates.
