---
title: "Scalable Adaptive Computation for Iterative Generation"
date: 2022-12-22T18:55:45.000Z
author: "Allan Jabri, David Fleet, Ting Chen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11972v1.webp" # image path/url
    alt: "Scalable Adaptive Computation for Iterative Generation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11972)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11972).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/scalable-adaptive-computation-for-iterative).

# Abstract
- The Recurrent Interface Network is a neural net architecture that allocates computation adaptively to the input according to the distribution of information.
- Hidden units of RINs are partitioned into the interface, which is locally connected to inputs, and latents, which are decoupled from inputs and can exchange information globally.
- The RIN block selectively reads from the interface into latents for high-capacity processing, with incremental updates written back to the interface.
- Stacking multiple blocks enables effective routing across local and global levels.
- While routing adds overhead, the cost can be amortized in recurrent computation settings where inputs change gradually while more global context persists, such as iterative generation using diffusion models.
- To this end, we propose a latent self-conditioning technique that "warm-starts" the latents at each iteration of the generation process.

# Paper Content

## Introduction
- Neural networks are a type of machine learning algorithm that are used to learn patterns in data.
- Convolutional neural networks are a type of neural network that are used for image recognition.
- Modern accelerator hardware is more efficient than older hardware, which has influenced the design of neural networks.
- The predominant architectures for deep learning, such as convolutional neural networks and Transformers, allocate computation uniformly over the input data.
- In highdimensional generative modeling tasks, such as image and video generation, it is important to allocate computation to regions with complex objects and textures rather than regions with little or no structure.
- To address this challenge, we propose a new architecture, called Recurrent Interface Networks, which allocates computation more effectively than uniform models.
- RINs allocate computation more effectively than uniform models while consuming 8x fewer FLOPs per inference step.

## Method

### Overview
- The interface is locally connected to the input space and initialized via a form of tokenization (e.g., patch embeddings).
- The latents are decoupled from data and initialized as learnable embeddings.
- The basic RIN block allocates computation by routing information between the interface and the latents.
- By stacking multiple blocks, we can update the interface and latents incrementally, such that bottom-up and top-down context can inform routing in the next block.
- Finally, a readout function (e.g. a linear projection) produces the network's output from the final interface representation.
- Since the interface is tied to data, it grows linearly with input size and may be large (e.g., thousands of vectors), while the number of latent units can be much smaller (e.g., hundreds of vectors).
- The computation operating directly on the interface (e.g. tokenization, read, write) is uniform across the input space, but is designed to be relatively lightweight, to limit the amount of uniform computation.
- The high-capacity processing is reserved for the latents, formed by reading information from the interface selectively, such that the bulk of the computation can be adapted to the structure and content of the input.
- When there is redundancy in the input, the latents can further compress the input for more efficient processing.

### Iterative Generation with Diffusion Models
- Diffusion models learn a series of state transitions to map noise from a known prior distribution to x 0 from the data distribution.
- To learn this (reverse) transition from noise to data, a forward transition from x 0 to x t is first defined: where ∼ N (0, I), t ∼ U(0, 1), and γ(t) is a monotonically decreasing function from 1 to 0.
- Instead of directly learning a neural net to model the transition from x t to x t−∆ , one can learn a neural net f (x t , t) to predict from x t , and then estimate x t−∆ from the estimated ˜ and x t .
- The objective for f (x t , t) is thus the 2 regression loss: To generate samples from a learned model, we follow a series of (reverse) state transition This is done by iteratively applying the denoising function f on each state x t to estimate , and hence x t−∆ , using transition rules as in DDPM (Ho et al., 2020) or DDIM (Song et al., 2020).
- As we will see, the gradual refinement of x through repeated application of the denoising function is a natural fit for RINs.
- The network takes as input a noisy image x t , a time step t, and an optional conditioning variable e.g. a class label y, and then outputs the estimated noise ˜ .
- We next describe the major components of Recurrent Interface Networks, illustrated in Figure 3.
- Interface Initialization. The interface is initialized from an input x, such as an image x image ∈ R h×w×3 , or video x video ∈ R h×w×l×3 by tokenizing x into a set of n vectors X ∈ R n×d .
- For example, we use a linear patch embedding similar to (Dosovitskiy et al., 2020) to convert an image into a set of patch tokens; for video, we use 3-D patches.
- To indicate their location, patch embeddings are summed with (learnable) positional encodings.
- Beyond tokenization, the model is domain-agnostic, as X is simply a set of vectors.
- Latent Initialization. The latents Z ∈ R m×d are (for now) initialized as learned embeddings, independent of the input.
- Conditioning variables, such as class labels and time step t of diffusion models, are mapped to embeddings; in our experiments, we simply concatenate them to the set of latents, since they only account for two tokens.
- While n is linear in size of x, m is decoupled from the input size and can remain small even for large inputs, since routing adaptively selects information from the interface for processing.
- Core Computation Block. The RIN block routes information by reading from X into Z, processing Z, and writing updates back to X. We implement these operations with key components of Transformers: MLP denotes a multi-layer perceptron, and MHA(Q, K) denotes multi-head attention with queries Q, and K being the keys and values.
- 1 The number of processing layers K allows 1 See (Vaswani et al., 2017) for details about multi-head attention, which extends single-head attention defined as Attention(Z, X) = softmax(ZWQW K X )XWV . MLP(Z) = σ(ZW1 + b1)W2 + b2 where σ is the GELU activation function (Hendrycks & Gimpel, 2016)....

### Latent Self-Conditioning
- RINs rely on routing information to dynamically allocate compute to parts of the input
- Effective routing relies on latents that are specific to the input, and input-specific latents are built by reading interface information
- This iterative process can incur additional cost that may overshadow the benefits of adaptive computation, especially if the network begins without context
- Intuitively, as humans, we face a similar "cold-start" problem under changes in the environment, requiring gradual familiarization of new state to enhance our ability to infer relevant information
- If contexts switch rapidly without sufficient time for "warm-up", we repeatedly face costly adaptation
- The "warm-up" cost in RINs can be similarly amortized in sequential computation settings where inputs gradually change while global context persists
- We posit that in such settings, there exists useful context in the latents accumulated in each forward pass.

## Experiments
- RINs improve state-of-the-art performance on benchmarks for image generation and video prediction
- RINs are more efficient than other methods

### Implementation Details
- Uses a continuous-time noise schedule function γ(t)
- Uses a cosine schedule by default
- Finds that the cosine is sometimes unstable for higher resolution images
- Investigates other schedules based the sigmoid function with different temperature
- Uses a default temperature of 0.9
- Ablates in experiments

### Experimental Setup
- For image generation, we mainly use the ImageNet dataset (Russakovsky et al., 2015).
- We only use center crops and random left-right flipping.
- We also use CIFAR-10 (Krizhevsky et al.) to show the model can be trained with small datasets.
- For evaluation, we follow common practice, using FID (Heusel et al., 2017) and Inception Score (Salimans et al., 2016) as metrics computed on 50K samples, generated by 1000 steps of DDPM.
- For video prediction, we use Kinetics-600 dataset (Carreira et al., 2018) at 16×64×64 resolution.
- For evaluation, we follow common practice (Ho et al., 2022b) and use FVD (Unterthiner et al., 2018) and Inception Scores computed on 50K samples, generated by 400 or 1000 steps of DDPM.

### Comparison to SOTA
- Image generation model works well with small datasets such as CIFAR-10
- Compared to state-of-the-art FID of 1.79 EDM, we obtain 1.81 FID without using their improved sampling procedure
- While our model has 31M parameters and trains in 3 hours on 8 TPUv3 chips, their model has 67M parameters and trains in 2 days on 8 A100 GPUs
- Video generation model without using guidance can attain strong performance
- Samples can be found in Appendix Fig. D.3
- For efficiency, we ablate using smaller architectures (latent dimension of 768 instead of 1024) on the ImageNet 64×64 and 128×128 tasks with higher learning rate (2×10−3) and fewer updates (150k and 220k, respectively)
- While this performs worse than our best models, it is sufficient for demonstrating the effect of different design choices
- Importance of Latent Self-conditioning is demonstrated by correlating self-conditioning rate and sample quality
- Tokenization can handle a wide range of patch sizes with RINs handling a large number of tokens (4096 for 1×1)
- Effect of noise schedule is studied for training and sampling and the sigmoid schedule with an appropriate temperature is better during training
- For sampling, the noise schedule has less impact and the default cosine schedule can suffice
