---
title: "PACO: Parts and Attributes of Common Objects"
date: 2023-01-04T19:28:03.000Z
author: "Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, Dhruv Mahajan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-01795v1.webp" # image path/url
    alt: "PACO: Parts and Attributes of Common Objects" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01795)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01795).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/paco-parts-and-attributes-of-common-objects).

# Abstract
- Object models are gradually progressing from predicting just category labels to providing detailed descriptions of object instances.
- This motivates the need for large datasets which go beyond traditional object masks and provide richer annotations such as part masks and attributes.
- Hence, we introduce PACO: Parts and Attributes of Common Objects.
- It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets.
- We provide 641K part masks annotated across 260K object boxes, with roughly half of them exhaustively annotated with attributes as well.
- We design evaluation metrics and provide benchmark results for three tasks on the dataset: part mask segmentation, object and part attribute prediction and zero-shot instance detection.

# Paper Content

## Introduction
- Representing objects through category labels is no longer sufficient.
- A complete object description requires more fine-grained properties like object parts and their attributes.
- There are no large benchmark datasets for common objects with joint annotation of part masks, object attributes and part attributes.
- With video object description becoming more widely studied as well [21], we construct both an image dataset (sourced from LVIS [15]) and a video dataset (sourced from Ego4D [13]) as part of PACO.
- Overall, PACO has 641K part masks annotated in 77K images for 260K object instances across 75 object classes and 456 object-specific part classes.
- It has an order of magnitude more objects with parts, compared to recently introduced PartImageNet dataset [17].
- Along with the dataset, we provide three associated benchmark tasks to help the community evaluate its progress over time. These tasks include: part segmentation, attribute detection for objects and object-parts, and zero-shot instance detection with part/attribute queries.

### Related work
- Datasets that provide annotations for objects besides category labels have played a crucial role in the acceleration of object understanding
- The task of detecting and segmenting object instances is well studied with popular benchmark datasets such as COCO, LVIS, Object365, Open Images and Pascal
- There are also domain-specific datasets for fashion, medical images and OCR
- Our dataset has ten times larger number of object boxes annotated with part masks compared to PartImageNet
- ADE20K is a 28K image dataset for scene parsing which includes part masks
- While it provides an instance segmentation benchmark for 100 object categories, part segmentation is benchmarked only for 8 object categories due to limited annotations
- Fashionpedia [20] is a popular dataset for fashion providing both part and attribute annotations in an image
- PACO aims to generalize this to common object categories
- Attributes have been long used for zero-shot object recognition

## Dataset construction

### Image sources
- Paco is a computer vision system that is constructed from LVIS and Ego4D.
- LVIS has a large object vocabulary and is easy to use for federated data construction.
- Ego4D has temporally aligned narrations, making it easy to source frames corresponding to specific objects.

### Object vocabulary selection
- We mined all object categories mentioned in the narrations accompanying Ego4D and took the intersection with common and frequent categories in LVIS.
- We chose categories with at-least 20 instances in Ego4D, resulting in 75 categories commonly found in both LVIS and Ego4D.

### Parts vocabulary selection
- We mined part names from web-images obtained through queries like "parts of a car".
- We manually curate such mined part names for an object category to only retain parts that are visible in majority of the object instances and clearly distinguishable.
- This resulted in a total of 200 part classes shared across all 75 objects.
- When expanded to object-specific parts this results in 456 object-part classes.

### Attribute vocabulary selection
- Attributes are used to distinguish different instances of the same object type
- In-depth user study was conducted to identify the sufficient set of attributes
- This led to the final vocabulary of 29 colors, 10 patterns and markings, 13 materials and 3 levels of reflectance

### Annotation pipeline
- Object bounding box and mask annotation (only for Ego4D)
- Part mask annotation
- Object and part attributes annotation
- Instance IDs annotation (only for Ego4D)

## Dataset statistics
- Fig. 2a: The number of part masks annotated for each object-part category in PACO-LVIS and PACO-EGO4D.
- Fig. 2b: The distribution of number of large, medium and small parts in PACO-LVIS.
- Fig. 2c: The number of annotations per attribute and attribute type in PACO-LVIS.
- Fig. 2d: The distribution of number of large, medium and small parts in PACO-EGO4D.
- Tab. 1: Overview of different parts and/or attributes datasets.
- PACO provides 641K part mask annotations in the joint dataset, which is 3× bigger than other datasets like ADE20K.
- While ADE20K has sizeable number of part masks overall, it doesn't provide a well defined instance-level benchmark for parts due to limited test annotations.
- PACO has 10× more object instances with parts (260K) compared to the next closest parts benchmark dataset for common objects: PartsImageNet (25K).
- In terms of attributes, the joint dataset has 124K object and 297K part masks with attribute annotations.
- While VAW has 261K object masks with attributes, the combined set of attribute annotations for part and object masks (421K) in PACO is still larger.
- VAW has a larger vocabulary of attributes 620 vs 55. However, in PACO, every object/part mask annotated with attributes is exhaustively annotated with all attributes in the vocabulary unlike VAW. This makes the density of attributes per image 23.4 much larger than VAW 3.6.

## Tasks and evaluation benchmark
- We introduce three evaluation tasks.
- Our first two tasks directly evaluate the quality of parts segmentation and attributes prediction.
- The other task aims to leverage parts and attributes for zero-shot object instance detection.

### Dataset splits
- The test set of PACO-LVIS is a strict subset of the LVIS-v1 val split and contains 9443 images.
- The train and val splits of PACO-LVIS are obtained by randomly splitting LVIS-v1 train subset for 75 classes, and contain 45790 and 2410 images respectively.
- Ego4D is split into 15667 train, 825 val and 9892 test images. The set of object instance IDs in Ego4D train and test sets are disjoint.

### Federated dataset for object categories
- Federated dataset from LVIS [15]
- Uses only three types of images - exhaustive positive, non-exhaustive positive, and negative images
- Evaluates AP for category using only these images

### Part segmentation
- True positives (TP), false positives (FP) and ignored masks (IM))
- AP is averaged over different thresholds of intersection over union (IoU)
- We use all positive or negative images of object o to compute the AP for (c, a)
- We compute AP at different IoU thresholds and report the average

### Instance-level attributes prediction
- We train a simple extensions of mask R-CNN and ViTdet models with an additional attribute head on the shared backbone.
- The attribute head uses the same ROI-pooled features as the detection head to predict object and objectpart attributes.
- We use a separate cross-entropy loss for each attribute type.
- The model is shown in more detail in the appendix.
- We report box AP values for models trained on PACO-LVIS in Tab. 3.
- We also provide results for the joint dataset in the appendix.
- During inference, we rank the detected boxes for a specific object-attribute combination by the product of the corresponding object and attribute scores.
- For parts, we rank boxes by product of corresponding object-part score and attribute score.
- Attribute prediction is a much harder task than object detection, as witnessed by the lower AP values for both objectattributes and object-part-attributes, compared to object and part AP in Tab. 2.
- We observe larger models fairing better for this task as well.
- Since we measures multiple factors together, we analyze the sensitivity of AP obj attr only to attribute prediction in Tab. 4.
- To do so, we keep detections from the trained models fixed and get (a) lower bounds by ignoring attribute scores and (b) upper bounds by assuming perfect attribute scores (details in appendix).
- We observe a huge gap between lower and upper bounds, with our original models only partially bridging it. This shows scope for future improvements in the attribute prediction ability of the models.

### Zero-shot instance detection
- We generate benchmark numbers for this task by directly leveraging the models trained in Sec.
- For a given query, we use the scores corresponding to the object, part, object attributes, and part attributes mentioned in the query to rank object bounding boxes returned by the different joint models.
- We use a simple scoring function that combines these different scores using geometric mean to get one final score for each box (explained in the appendix).
- The results for FPN models trained and evaluated on PACO-LVIS are shown in Tab. 5 (see appendix for cascade model results).
- We notice an interesting trend. For all models, L1 > L3 > L2 . This is due to the trade-off between two opposing factors: (a) more complex queries provide more information about the object instance, making L3 task easier than L2, but (b) complex queries also cause errors from multiple attribute predictions to be compounded making L1 better than L3.
- We include ablation studies in appendix measuring importance of different object and part attributes.
- We train two mask R-CNN and two ViT-det [30] models with 531 classes comprising both 75 object categories and 456 object-part categories.
- We use the standard 100-epoch schedule recommended for LVIS with federated loss [56] and LSJ [11] augmentation.
- For all experiments on part segmentation and attribute detection, we train on train, search for hyper-parameters on val and report results on test splits.
- We trained with Cascade [1] as well as Feature Pyramid Network (FPN) [31].
- The results for models trained and evaluated on PACO-LVIS are summarized in Tab. 2.
- We also provide results for models trained on joint image + video PACO dataset in the appendix.
- We observed that object-parts in general have a smaller AP compared to objects. This is due to the typically smaller size of parts compared to objects (Fig. 2b).
- Nevertheless larger and better backbones like ViT-L are seen to improve performance for the part segmentation task.
- To get a sense of the gap between open vocabulary detectors and our task-specific models, we evaluate the publicly available models from Detic [55] and MDETR [22] without further fine-tuning on PACO-LVIS and report results in Tab. 6 (details in the appendix).
- In theory, such models can handle arbitrary natural language queries describing object instances.
- We show results only for L1 queries and two additional subsets: L1 queries with only object attributes (L1 obj ) and only part attributes (L1 part ).
- We observe limited performance for the evaluated models. This is not surprising and can be attributed to the following factors. Even in the open vocabulary setting, Detic was trained specif- ically for nouns with little support for attributes. While MDETR was trained for referring expression tasks with attributes, its ability to handle negative images is limited. This highlights the opportunity for future research in open world detectors to handle more descriptive object queries besides category labels.
- PACO-EGO4D has multiple frames corresponding to the same object instance. Hence, it can serve as a useful dataset for few-shot instance detection as well.
- Few-shot instance detection is the task where an algorithm is given as input k positive frames with bounding boxes for an object instance and is expected to retrieve another bounding box of the same instance from an unseen set of images. This is similar to our zero-shot task, but the model receives sample object boxes instead of a part/attribute query.
- We compute and compare zero-shot and few-shot numbers on a subset of 1992 queries in PACO-EGO4D that have 6 or more boxes for the object instance corresponding to the query.
- We benchmark a naive 2-stage model: a pre-trained R50 FPN detector followed by ROI-pooling features from a pretrained R50 FPN backbone for nearest neighbor ranking (explained in appendix).
- We evaluate this model for k ranging from 1-5 and compare it to our zero-shot instance detection models trained on the joint PACO dataset in Fig. 3.
- We notice a 20+ point gap even between our best zero-shot model (R101 FPN) and one-shot model (k = 1). As k increases, the gap widens even further. This shows scope for future improvements to zero-shot object instance detection.

## Conclusion
- PACO is a dataset designed to enable research towards joint detection of objects, parts and attributes of common objects.
- The dataset provides part masks and attributes for 75 common object categories spanning both image and video datasets.
- We introduce three benchmark tasks which showcase unique challenges in the dataset.
- Unlike object detection, these tasks require algorithms to cope better with smaller masks belonging to parts and have features that are not invariant to instance-level attributes.
- For all tasks, we provide results from extensions of existing detection models to help calibrate future research on the dataset.
- For the object "trash can", we manually defined all the parts with illustrative reference for the annotators as shown.
- parts for few objects when the web images aren't illustrative enough. In such cases, we came up with reasonable names for different regions of an object along with reference images to guide the annotators. Such manually defined parts with sample reference images are shown in Fig. 5 as well.
- Tab. 7 contains the final taxonomy of parts for the 75 object classes.
- For zero-shot instance recognition tasks, both object and part level attributes are important.
- In order to identify the set of attributes that we should annotate that are sufficient for the tasks, we conducted an in-depth user study.
- We consider the following 5 attributes types: color, shape, reflectance, materials and patterns & marking with the aim of finding a small subset that is sufficient to discriminate between the instances.
- We show each user two different instances A and B of the same object (green mug and red mug for example), segmentation mask of common object parts between this pair.
- We use PACO-Ego4D data for this purpose.
- For object level attributes, we ask annotators to provide at most one difference (if any) for each attribute type.
- For part level attributes, annotators are asked to provide A is red and B is blue), annotators will need to write down the attribute names. Otherwise, a freeform explanation is required to articulate this difference, particularly for unnameable attributes (e.g. a unique pattern, an irregular shape, etc.);
- For each object category we sampled 106 pairs each which are annotated by 3 annotators.
- Un-nameable shape attributes. Annotators noted that > 50% shape differences contain unnameable attributes. Annotators reported these differences as very difficult to describe with words. Hence, we removed "shape" from the final list of attribute. Nevertheless, even in the absence of shape we note that the combination of the remaining attributes are seen to be sufficiently discriminative to differentiate the object instances.
- Attributes Coverage. We try to identify the discriminative power of different subsets of attributes and identify the best subset to construct our attributes taxonomy.
- We adopted a greedy algorithm to study attribute sets. We start with one attribute and gradually add one best attribute at a time to incrementally construct an attribute set at each step. More specifically at a give step, for each attribute, we check how many new pairs can be distinguished if we introduce that attribute to the existing set of attributes. The attribute that distinguishes highest number of pairs is selected first, followed by the next best attribute in a greedy fashion.
- We observe that coverage plateaus at 40 attributes. 98% of object instance pairs could be distinguished only using the 55 attributes included in our final version of PACO.
- Both object and part attributes were marked as important for differentiating instance pairs.
- Color is the biggest discriminative attribute type for instance recognition, differentiating at least 75% instance pairs with both object and part level color differences.
