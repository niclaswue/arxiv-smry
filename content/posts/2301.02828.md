---
title: "Why do Nearest Neighbor Language Models Work?"
date: 2023-01-07T11:12:36.000Z
author: "Frank F. Xu, Uri Alon, Graham Neubig"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02828v1.webp" # image path/url
    alt: "Why do Nearest Neighbor Language Models Work?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02828)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02828).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/why-do-nearest-neighbor-language-models-work).

# Abstract
- Language models (LMs) compute the probability of a text by sequentially
- Currently, most LMs calculate these representations through a neural network consuming the immediate previous context.
- However recently, retrieval-augmented LMs have shown to improve over
- In this paper, we set out to understand why retrieval-augmented language models, and
- To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one.
- Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution.
- Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component.

# Paper Content

## Introduction

### Replacing the Datastore with Trainable Embeddings
- The choice of input representation (h ds ) has a large impact on the performance of kNN-LM
- From only the experiments above, it is not possible to disentangle the effect of the choice of h ds and that of other design choices and factors in Equation 5
- To test the effect of h ds in a more controlled setting, we remove the non-parametric datastore entirely, and initialize W ds in Equation 5 with a randomly initialized word embedding matrix with the same size (N ds = V ) as the LM's output embedding W sm
- The loss function for training is the cross-entropy loss of softmax(W ds • h ds ) with respect to the ground-truth tokens, identically to how the base LM is trained
- We compare how using h ds = att or h ds = ffn affects the interpolated performance. The results are shown in Table 2
- Overall, by using a separate word embedding matrix with size V × D as an alternative to kNN, we can recover about 55% of the performance gain achieved by kNN-LM, with only a limited number of parameters and without the necessity for slow kNN retrieval every time a token is predicted. This suggests that the majority of the gain afforded by kNN-LM could be achieved by other more efficient means as well.

### Increasing the Softmax Capacity
- The large datastore is the key reason for the model working well
- Naturally, as a first step, we need to check whether such a big datastore is warranted and whether the high rank of Wds leads to better performance
- We test the effect of the datastore size for kNN retrieval on kNN-LM interpolated perplexity
- If a bigger datastore (a high rank Wds ) is better in kNN-LM than a smaller datastore, then the hypothesis of softmax capacity is more probable
- We randomly subsample the full datastore in varying percentages and the results are shown in Figure 2
- The full datastore contains more than 150M entries and storing them takes 293GB when using half-precision floating points (fp16)
- We can see that whether or not approximate kNN is used, the final perplexity decreases almost linearly with more percentage of the original datastore
- Even with just 5% of the datastore size (15G), kNN-LM still provides a benefit over just using the base LM
- However, even when the subsampling percentage reaches 90%, having more entries in the datastore still provides benefits without having significant diminishing returns, suggesting that a large datastore is beneficial
- One possible reason why a larger datastore is helpful is that words can be difficult to predict
- There are several reasons: (1) They are rare, or (2) they are frequent, but they have multiple meanings and appear in different contexts
- The softmax bottleneck (Yang et al., 2017)
- Figure 2: The effect of the size of the datastore used for kNN retrieval on final interpolated perplexity.
- We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM
- The first and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary
- To test this, we replace W sm with a much smaller matrix of size nV × D, where we allocate n embedding vectors for each word type
- When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the final probability
- mask-to-k(•) is no longer needed, as this formulation is small enough to fit the entire matrix in the GPU
- We then finetune the new Wds on the training data until convergence
- Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output ("att") or feedforward layer output ("ffn") as h ds .
- We plot the number of embeddings for each word type (nV total embeddings in Wds ) versus the interpolated perplexity, with full details found in Appendix B
- In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds ) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM
- To give a perspective, the original datastore size is about 5000V .
- Surprisingly, we find that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM
- We can see that when h ds = ffn, over-parameterization provides very limited improvements, while for h ds = att it does not bring consistent improvements at all
- Comparing the trend of increasing the embeddings in...

### Adding Softmax Temperature to kNN Distribution
- Because the number of retrieved nearest neighbors, k is usually much smaller than the vocabulary size V , intuitively, the kNN distribution P kN N used for interpolation tends to be more peaky than the standard LM output distribution.
- When k = 1024 and V = 33000, as in our experiments, P kN N will only have a few vocabulary items with a non-zero probability. Furthermore, many of the retrieved neighbors share the same target token and thus make the kNN distribution even peakier.
- One way to control the entropy, or peakiness of the distribution is to add temperature to the logits that go into the softmax function (Holtzman et al., 2019). We calculate the probability of non-parametric component P kN N with the following equation where t is the softmax temperature:
- In general, the higher the temperature, the less "peaky" the distribution would become. We experiment with both the 5% as well as the full datastore using different temperatures ranging from 0 to 3 at 0.1 intervals. The results are shown in Figure 5a and Figure 5b respectively.
- We can see that the default temperature t = 1 does not always result in the best-interpolated perplexity and tuning softmax temperature is desirable for all sizes of datastore.
- The relatively small gap between using "real score" and "FAISS score" in both datastore settings shows that the main contributor to the improvements is using approximate nearest neighbors ("FAISS mask") rather than using approximate distance values ("FAISS score").
- We hypothesize that this is related to regularization for preventing overfitting, and approximate search provides fuzziness that functions as a regularizer.
- We can think of the non-parametric part in kNN-LM, the kNN component as a model, where the datastore size is its model capacity, and the datastore is its training data. Considering that the kNN component uses the exact same training data as the base parametric LM, having ground truth, accurate kNN search may cause the kNN component to overfit the training data.
- Comparing the small datastore with only 5% with the original datastore, we see that a small datastore means a small training set for the kNN "model" and it thus it benefits more from this regularization, both both through using the FAISS mask and FAISS score (at optimal temperature settings).
- From these experiments, we can see that, surprisingly, one of the important ingredients in kNN-LM seems to be approximate kNN search, which likely prevents overfitting to the datastore created from the same training set.

### Sparsification
- The mask-to-k(•) used by kNN retrieval induces sparsity in the distribution over the vocabulary, due to a small k (typically 1024) compared to the size of the vocabulary V (33K in our experiments and 260K in the original settings of Khandelwal et al. (2020b)).
- We hypothesized that kNN-LM increases the probability of the top-k entries while taking "probability mass" from the long tail of unlikely word types. However, we could not gain any benefits solely from sparsifying the output probability of a standard LM and interpolating it with the original LM.

### Stolen Probabilities
- The stolen probabilities effect refers to the situation where the output embeddings of an LM are learned such that some words are geometrically placed inside the convex hull that is formed by other word embeddings and can thus never be "selected" as the argmax word.
- We hypothesized that kNN-LM solves the stolen probabilities problem by allowing to assign the highest probability to any word, given a test context that is close enough to that word's datastore key.
- However, we found that none of the vectors in our embedding matrix and in the original embedding matrix of Khandelwal et al. (2020b) is located in the convex hull of the others, which is consistent with the findings of Grivas et al. (2022).
- More details can be found in Appendix E.4.
- We hypothesized that the kNN component simply provides memorization of the training set.
- However, we could not improve a standard LM by interpolating its probability with another standard LM that was further trained to overfit the training set.
- More details can be found in Appendix E.6.1.
- We hypothesized that kNN-LM's improvement lies in reducing the "over-correction" error when training with 1-hot labels, as hypothesized by Yang et al. (2022), and that retrieving neighbors is not important.
- If only "soft labels" are the key, we could hypothetically improve the performance of another fresh LM with the same model architecture but trained with the soft labels from the base LM, instead of from kNN-LM.
- This separates the effect of "soft labeling" from the additional guidance provided by kNN.
- However, this does not help with the interpolated perplexity at all.

## Conclusion
- The feedforward layer output and the attention layer output -can recover 55% of the performance, even without retrieval.
- One of the most unexpected discoveries in the paper is that using approximate nearest neighbor search allows kNN-LMs to generalize better than exact nearest neighbor search, possibly due to a regularization effect.
- Tuning the softmax temperature for the kNN distribution is crucial to adjust the standard LM output distribution with the distribution created by the retrieved neighbors' distances.
- We performed extensive experiments which ruled out other hypotheses as to why kNN-LMs work, such as over-parameterization, datastore clustering, sparsification, overfitting, ensembling of distance metrics, and alternative training methods.
- We believe that this work unlocks a variety of exciting research directions for efficient kNN alternatives.
