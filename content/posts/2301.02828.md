---
title: "Why do Nearest Neighbor Language Models Work?"
date: 2023-01-07T11:12:36.000Z
author: "Frank F. Xu, Uri Alon, Graham Neubig"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02828v1.webp" # image path/url
    alt: "Why do Nearest Neighbor Language Models Work?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02828)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02828).


# Abstract
- Language models (LMs) compute the probability of a text by sequentially
- Currently, most LMs calculate these representations through a neural network consuming the immediate previous context.
- However recently, retrieval-augmented LMs have shown to improve over
- In this paper, we set out to understand why retrieval-augmented language models, and
- To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one.
- Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for
- Further, we incorporate these insights into the model architecture or the training procedure of the standard

# Paper Content

## Introduction
- Language modeling is the task of predicting the probability of a text, with broad-spanning applications across natural language processing
- Recently, retrieval-augmented LMs have shown a series of impressive results
- Retrieval-augmented LMs compute next token distributions based not only on the immediately preceding context c t and the model parameters, but also on an external datastore, from which examples are retrieved and incorporated into the base LM's prediction
- One retrieval-augmented model that is notable for both its simplicity and efficacy is the k-nearest neighbor language model (kNN-LM)
- It extends a trained base LM by linearly interpolating the output word distribution with a kNN model
- The nearest neighbors are retrieved according to the distances between the current context embedding of the base LM and all the context embeddings in the datastore
- The datastore is created by encoding all contexts from any text collection, including the original LM training data
- One of the most surprising results from Khandelwal et al. (2020b) is that kNN-LM reduces the perplexity of the base LM even when the kNN component is retrieving examples from the same training set that the LM was originally trained on, indicating that the kNN-LM improves the ability to model the training data and is not simply benefiting from access to more data
- Intrigued by this, we ask questions like, could kNN-LM be improving because of capacity issues in the parametric base LM?
- In this paper, we set out to understand why kNN-LMs work even in this setting
- In the following sections, we first elucidate connections between the added kNN component and the standard LM component. Specifically, we note that word distributions from the two components are both calculated using a softmax function, based on the similarity of the current context embedding with a set of embeddings that corresponds to different next words
- With this intuition, we formalize and generalize the non-parametric distribution calculation with the softmax layer and word embedding layer used in parametric LMs
- We then show that this generalized form exposes a variety of design choices, e.g., the number of context embeddings in the datastore, the input representation used in softmax layer, different similarity functions, as well as the approximation and sparsification implementations in the kNN search
- This provides a general framework for analyzing kNN-LM and similar models and allows us to perform ablation studies that test the importance of various design decisions
- Based on these hypotheses, we perform ablation experiments and analyze the nuances between different implementations of the generalized version of P kN N
- As the answer to our question, "why kNN-LMs work", we eventually show that the most probable reasons are threefold: 1. Ensembling the output of softmax using two representations from different layers of the transformer is important; in our experiments, this accounts for 55% of the performance gain of kNN-LM, or 6.5% relative perplexity improvement compared to the base LM. 2. kNN-LM uses approximate nearest neighbor search to handle the large number of candidates, and the lack of this preciseness in this algorithm actually helps kNN-LM to generalize better than using exact nearest neighbor search and distance calculation, possibly due to a regularization effect. The relative perplexity improvement from this factor is about 2.6%. 3. Depending on the design decisions that are chosen for modeling, adding a temperature term to the kNN non-parametric component can become crucial to the success of modeling (although coincidentally, in the original settings of Khandelwal et al. (2020b), a temperature of 1.0 is close to optimal, which hid the importance of this term). In some settings, the relative perplexity gap between the default and optimal temperature can be as high as 3.7%.
- One significant drawback to the current kNN-LM is the inefficiency of kNN search performed at each step

### Replacing the Datastore with Trainable Embeddings
- The choice of input representation (h ds ) has a large impact on the performance of kNN-LM
- From only the experiments above, it is not possible to disentangle the effect of the choice of h ds and that of other design choices and factors in Equation 5
- To test the effect of h ds in a more controlled setting, we remove the non-parametric datastore entirely, and initialize W ds in Equation 5 with a randomly initialized word embedding matrix with the same size (N ds = V ) as the LM's output embedding W sm
- The loss function for training is the cross-entropy loss of softmax(W ds • h ds ) with respect to the ground-truth tokens, identically to how the base LM is trained
- We compare how using h ds = att or h ds = ffn affects the interpolated performance. The results are shown in Table 2
- From these experiments we can find several interesting conclusions:
- In the case of "Learned W ds w/ FFN", we are essentially re-learning the weights feeding into the softmax function separately from the underlying LM encoder. Despite this fact, we can see the model achieves a PPL of 20.920, which is 0.83 points better than the base model. This suggests that there is some benefit in learning the parameters of W ds after freezing the body of the transformer encoder.
- In both cases for W ds , the interpolated perplexity is significantly better than that of using a single predictor. This is particularly the case when using the "att" representation for h ds , suggesting that the utility of ensembling predictions from two views of the data is not only useful when using kNN-LM, but also in standard parametric models as well.
- Parametric ensembles as an alternative to kNN-LM?: Overall, by using a separate word embedding matrix with size V × D as an alternative to kNN, we can recover about 55% of the performance gain achieved by kNN-LM, with only a limited number of parameters and without the necessity for slow kNN retrieval every time a token is predicted. This suggests that the majority of the gain afforded by kNN-LM could be achieved by other more efficient means as well.

### Increasing the Softmax Capacity
- The large datastore is the key reason for the model working well
- We test the effect of the datastore size for kNN retrieval on kNN-LM interpolated perplexity
- If a bigger datastore (a high rank W ds ) is better in kNN-LM than a smaller datastore, then the hypothesis of softmax capacity is more probable
- We randomly subsample the full datastore in varying percentages and the results are shown in Figure 2
- The full datastore contains more than 150M entries and storing them takes 293GB when using half-precision floating points (fp16)
- We can see that whether or not approximate kNN is used, the final perplexity decreases almost linearly with more percentage of the original datastore
- Even with just 5% of the datastore size (15G), kNN-LM still provides a benefit over just using the base LM
- However, even when the subsampling percentage reaches 90%, having more entries in the datastore still provides benefits without having significant diminishing returns, suggesting that a large datastore is beneficial
- One possible reason why a larger datastore is helpful is that words can be difficult to predict
- There are several reasons: (1) They are rare, or (2) they are frequent, but they have multiple meanings and appear in different contexts
- The softmax bottleneck (Yang et al., 2017)
- Figure 2: The effect of the size of the datastore used for kNN retrieval on final interpolated perplexity.
- We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM
- The first and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary
- When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the final probability. mask-to-k(•) is no longer needed, as this formulation is small enough to fit the entire matrix in the GPU
- We then finetune the new W ds on the training data until convergence
- Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output ("att") or feedforward layer output ("ffn") as h ds .
- We plot the number of embeddings for each word type (nV total embeddings in W ds ) versus the interpolated perplexity, with full details found in Appendix B.
- In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from N ds to nV ds ) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM
- To give a perspective, the original datastore size is about 5000V .
- Surprisingly, we find that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM
- We can see that when h ds = ffn, over-parameterization provides very limited improvements, while for h ds = att it does not bring consistent improvements at all
- Comparing the trend of increasing the embeddings in W ds , with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (W ds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still significant. This suggests that simply over-parameterizing W ds is not an effective method of achieving accuracy gains similar to kNN-LM.

### Adding Softmax Temperature to kNN Distribution
- Because the number of retrieved nearest neighbors, k is usually much smaller than the vocabulary size V, intuitively, the kNN distribution PkNNN used for interpolation tends to be more peaky than the standard LM output distribution.
- When k = 1024 and V = 33000, as in our experiments, PkNNN will only have a few vocabulary items with a non-zero probability.
- Furthermore, many of the retrieved neighbors share the same target token and thus make the kNN distribution even peakier.
- One way to control the entropy, or peakiness of the distribution is to add temperature to the logits that go into the softmax function (Holtzman et al., 2019).
- We calculate the probability of non-parametric component PkNNN with the following equation where t is the softmax temperature: PkNNN = M softmax(mask-to-k(Wds ⊗ hds )/t)
- In general, the higher the temperature, the less "peaky" the distribution would become.
- We experiment with both the 5% as well as the full datastore using different temperatures ranging from 0 to 3 at 0.1 intervals.
- The results are shown in Figure 5a and Figure 5b respectively.
- We can see that the default temperature t = 1 does not always result in the best-interpolated perplexity and tuning softmax temperature is desirable for all sizes of datastore.
- The relatively small gap between using "real score" and "FAISS score" in both datastore settings shows that the main contributor to the improvements is using approximate nearest neighbors ("FAISS mask") rather than using approximate distance values ("FAISS score").
- We hypothesize that this is related to regularization for preventing overfitting, and approximate search provides fuzziness that functions as a regularizer.
- We can think of the non-parametric part in kNN-LM, the kNN component as a model, where the datastore size is its model capacity, and the datastore is its training data.
- Considering that the kNN component uses the exact same training data as the base parametric LM, having ground truth, accurate kNN search may cause the kNN component to overfit the training data.
- Comparing the small datastore with only 5% with the original datastore, we see that a small datastore means a small training set for the kNN "model" and it thus it benefits more from this regularization, both through using the FAISS mask and FAISS score (at optimal temperature settings).
- From these experiments, we can see that, surprisingly, one of the important ingredients in kNN-LM seems to be approximate kNN search, which likely prevents overfitting to the datastore created from the same training set.
- We further analyze this unexpected result in Appendix D, where we find that longer words and words that appear in many different contexts have slightly better results with approximate nearest neighbors.
- Notably, similar effects, where an approximation component lead to better generalization, have been reported in other NLP tasks as well, and are sometimes referred to as "beneficial search bias", when modeling errors cause the highest-scoring solution to not be the correct one.
- Stahlberg and Byrne (2019) also conclude that "vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modeling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation".

### Sparsification
- The mask-to-k(•) used by kNN retrieval induces sparsity in the distribution over the vocabulary
- We hypothesized that kNN-LM increases the probability of the top-k entries while taking "probability mass" from the long tail of unlikely word types
- However, we could not gain any benefits solely from sparsifying the output probability of a standard LM and interpolating it with the original LM.

### Stolen Probabilities
- The stolen probabilities effect refers to the situation where the output embeddings of an LM are learned such that some words are geometrically placed inside the convex hull that is formed by other word embeddings and can thus never be "selected" as the argmax word.
- We hypothesized that kNN-LM solves the stolen probabilities problem by allowing to assign the highest probability to any word, given a test context that is close enough to that word's datastore key.
- However, we found that none of the vectors in our embedding matrix and in the original embedding matrix of Khandelwal et al. (2020b) is located in the convex hull of the others, which is consistent with the findings of Grivas et al. (2022).
- More details can be found in Appendix E.4.
- We hypothesized that the kNN component simply provides memorization of the training set.
- However, we could not improve a standard LM by interpolating its probability with another standard LM that was further trained to overfit the training set.
- More details can be found in Appendix E.6.1.
- We hypothesized that kNN-LM's improvement lies in reducing the "over-correction" error when training with 1-hot labels, as hypothesized by Yang et al. (2022), and that retrieving neighbors is not important.
- If only "soft labels" are the key, we could hypothetically improve the performance of another fresh LM with the same model architecture but trained with the soft labels from the base LM, instead of from kNN-LM.
- This separates the effect of "soft labeling" from the additional guidance provided by kNN.
- However, this does not help with the interpolated perplexity at all.

## Conclusion
- kNN-LM improves perplexity, even when retrieving examples from the same training data that the base LM was trained on
- kNN-LM is generalizable to other LMs
- Tuning the softmax temperature for the kNN distribution is crucial to adjust the standard LM output distribution with the distribution created by the retrieved neighbors' distances

## B Detailed Results for Increasing the Softmax Capacity
- We hypothesize that different word types have different difficulties for the language model to predict
- For those words that appear very frequently, they may appear in many different contexts
- As a result, instead of adding equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word
- Based on the intuition of Zipf's law (Clauset et al., 2009), we assign 1 + log b f v for each word type v ∈ V , based on either the frequency or the total training loss of the word, f v
- The b is a hyperparameter that could be tuned
- To ensure fair comparison, we tune b so that for each experiment the total number of embeddings matches: The results are shown in Table 6
- We can see that although nice in paper, given the same number of total embeddings, adaptively increasing the number of embeddings assigned for each word type does not make a significant difference in the final perplexity, when compared with the models that use equal number of embeddings for each word type

### C.3 Clustering Datastore
- Opposite to training the word embeddings of an increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation.
- The intuition is that the datastore contains redundant context vectors, and thus compression could make the datastore smaller without sacrificing too much performance gain.
- He et al. (2021) shows that we can safely compress the datastore by clustering to 50% of the original size without losing performance.
- We test this idea further by clustering the entire datastore into a size that could fit in GPU memory (e.g. 2V , 3V ) and thus could be easily fine-tuned further and use the resulting centroids to replace W ds .
- Within each cluster, there will be a distribution of different words with contexts, and we use the frequency of words within each cluster to calculate the aggregation matrix M in Equation 5.

## D Which Words Benefit from Approximation?
- The longer the token, the better it is with approximate nearest neighbors.
- Words with high entropy (many different contexts) are better off with approximate kNN.
- The key to kNN-LM's performance gain is the ensemble of two distance metrics: the standard dot product distance (which the LM uses) with the L2 distance (which the kNN component uses as ⊗).
- None of these attempts to replace the kNN component with a different distance metric help.

### E.3 Location within Context Window
- The base LM performs worse in one of the two context windows (beginning vs. end)
- There is no significant correlation between the performance of the base LM and the location
- The beginning of every Wikipedia article is more "predictable"
- There is no correlation between the location of the word within the document it appears in

### E.4 Stolen Probabilities
- The stolen probabilities effect refers to the situation where the output embeddings of an LM are learned such that some words are geometrically placed inside the convex hull that is formed by other word embeddings.
- Since language models generate a score for every output word by computing the dot product of a hidden state with all word embeddings, Demeter et al. (2020) prove that in such a case, it is impossible for words inside the convex hull to be predicted as the LM's most probable word (the "argmax").
- We hypothesized that kNN-LM solves the stolen probabilities problem by allowing to assign the highest probability to any word, given a test hidden state that is close enough to that word's datastore key.
- Nevertheless, as shown by Grivas et al. (2022), although this problem might happen in small RNN-based language models, in modern transformers it rarely happens in practice.
- Using the code of Grivas et al. (2022), we checked the embeddings matrix of our model and of the checkpoint provided by Khandelwal et al. (2020b). Indeed, we found that in both models -no word is un-argmaxable.
- E.5 Are kNN-LM Just Ensembling? Our hypothesis is that kNN component only provides another model for ensembling. The interpolation process is basically an ensemble model. Technically it is unsurprising that kNN-LM will have the benefit from ensembling, but we perform experiments to see how it compares to other ensembling.
- We trained another language model with the same architecture as the base LM we used throughout the experiments, with some variants having more than one embedding vector for each word (similar to Section 4.2).
- We interpolate the models with the original base LM, and the results are shown in Table 8. We can see that even just ensembling the base LM with another identical model, but trained with a different random seed, provides a huge performance boost, both on interpreted perplexity and on oracle perplexity.
- However, just because ensembling two LMs of the same architecture provides better performance than interpolating the base LM with kNN does not necessarily suggest that kNN's performance improvement can be fully replaced by model ensembling.
- In other words, we are interested in whether the kNN performance improvements are orthogonal to that of model ensembling.
- To test this, we compare the performance of the ensemble of K multiple LMs versus the ensemble of K − 1 multiple LMs plus the kNN component. The comparison is fair because we have the same number of models in the ensemble, and the only difference is whether the kNN component is included.
- The results are shown in Figure 8. For the "LM" series, each point is K LMs ensemble, and for the "kNN" series, each point is K − 1 LMs plus kNN. We can see that even at 4-ensemble, the ensemble that contain kNN as a component still have a considerable edge over the 4-ensemble that contain just LMs.
- Since kNN-LM improves perplexity even with the same training dataset as datastore, we are curious if kNN-LM works by only "memorizing" the training data.
- The hypothesis is that the datastore and the kNN suggesting the need for the extra W ds to help with these hard cases.
- However, for either "att" for "ffn" for h ds , either V or 3V for the number of embeddings in W ds , we are unable to achieve a better perplexity than just the base LM. This suggests that, while nice on paper, the interpolated loss optimization process is not trivial.
