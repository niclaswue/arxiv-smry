---
title: "Attention Is All You Need"
date: 2017-06-12T17:57:34.000Z
author: "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1706-03762v5.webp" # image path/url
    alt: "Attention Is All You Need" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1706.03762)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1706.03762).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/attention-is-all-you-need).

# Abstract
- The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration
- The best performing models also connect the encoder and decoder through an attention mechanism
- We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely
- Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train

# Paper Content

## Introduction
- Recurrent neural networks are a state of the art approach in sequence modeling and transduction problems such as machine translation
- Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures
- The fundamental constraint of sequential computation, however, remains
- Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks
- In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.

## Background
- The goal of reducing sequential computation is the foundation of the Extended Neural GPU, ByteNet, ConvS2S, and Transformer.
- In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.
- This makes it more difficult to learn dependencies between distant positions.
- In the Transformer, self-attention is used to reduce the number of operations required to compute a representation of the input and output.

## Model Architecture
- model architecture. Most competitive neural sequence transduction models have an encoder-decoder structure [5,2,35].

### Encoder and Decoder Stacks
- The encoder is composed of a stack of N = 6 identical layers.
- Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.
- We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1].
- That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.
- To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512.
- The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.
- Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.
- We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.

### Attention
- An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.
- The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.
- We call our particular attention "Scaled Dot-Product Attention" (Figure 2).
- In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
- The keys and values are also packed together into matrices K and V .
- We compute the matrix of outputs as:
- Dot-product attention is identical to our algorithm, except for the scaling factor of 1 .
- Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.
- While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.
- While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [3].
- We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4 .
- To counteract this effect, we scale the dot products by 1 .

### Position-wise Feed-Forward Networks
- Each layer in the encoder and decoder has a fully connected feed-forward network
- The encoder and decoder have the same linear transformations, but use different parameters from layer to layer
- The encoder and decoder have the same dimensionality, d model = 512

### Embeddings and Softmax
- We use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .
- We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.
- In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30].

### Positional Encoding
- The model contains no recurrence or convolution, so it needs to be given information about the relative or absolute position of tokens in a sequence.
- To this end, the model adds "positional encodings" to the input embeddings at the bottom of the encoder and decoder stacks.
- The positional encodings have the same dimension as the embeddings, so that they can be summed.
- The model uses sinusoidal encodings of different frequencies to allow the model to extrapolate to sequence lengths longer than the ones encountered during training.

## Why Self-Attention
- The self-attention layer is faster than a recurrent layer when the sequence length is smaller than the representation dimensionality
- The maximum path length between any two input and output positions in networks composed of the different layer types is O(n/r) for self-attention and O(n) for recurrent layers
- Separable convolutions decrease the complexity considerably, to O(k .

## Training
- The models are trained on a large data set
- The models are able to generalize well
- The models are able to predict future events
- The training regime is designed to improve the models' performance
- The training regime is designed to be efficient
- The training regime is designed to be reproducible
- The training regime is designed to be scalable

### Training Data and Batching
- The paper used a dataset of 4.5 million sentence pairs to train a machine learning model.
- The model was trained on a dataset of 36 million sentence pairs, split into a 32000 word-piece vocabulary.
- The paper used a sentence-pair encoding scheme called byte-pair encoding.

### Hardware and Schedule
- We trained our models on one machine with 8 NVIDIA P100 GPUs
- For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds
- We trained the base models for a total of 100,000 steps or 12 hours
- For our big models,(described on the bottom line of table 3), step time was 1.0 seconds
- The big models were trained for 300,000 steps (3.5 days)

### Optimizer
- We used the Adam optimizer
- We varied the learning rate over the course of training
- This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number

### Regularization
- We employ three types of regularization during training: residual dropout, label smoothing and dropout on the output of each sub-layer
- For the base model, we use a rate of P drop = 0.1
- Hurt perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score

### Machine Translation
- The big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.
- The configuration of this model is listed in the bottom line of Table 3.
- Training took 3.5 days on 8 P100 GPUs.
- Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.
- On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model.
- The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3.

### Model Variations
- To evaluate the importance of different components of the Transformer, we varied our base model in different ways
- We used beam search as described in the previous section, but no checkpoint averaging
- We present these results in Table 3
- In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2
- While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads
- In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality
- This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial
- We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting
- In row (E) we replace our sinusoidal positional encoding with learned positional embeddings and observe nearly identical results to the base model.

### English Constituency Parsing
- Transformer can generalize to other tasks
- Transformer performs better than previously reported models
- Transformer can be trained with only 40,000 sentences of data

## Conclusion
- Transformer is the first sequence transduction model based entirely on attention
- Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers
- On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art
- In the former task our best model outperforms even all previously reported ensembles
- We are excited about the future of attention-based models and plan to apply them to other tasks
- We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video
