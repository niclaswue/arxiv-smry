---
title: "Training trajectories, mini-batch losses and the curious role of the learning rate"
date: 2023-01-05T21:58:46.000Z
author: "Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, Nolan Miller"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02312v1.webp" # image path/url
    alt: "Training trajectories, mini-batch losses and the curious role of the learning rate" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02312)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02312).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/training-trajectories-mini-batch-losses-and).

# Abstract
- Stochastic gradient descent plays a fundamental role in nearly all applications of deep learning
- However, its efficiency and remarkable ability to converge to global minimum remains shrouded in mystery.
- The loss function defined on a large network with large amount of data is known to be non-convex.
- However, relatively little has been explored about the behavior of loss function on individual batches.
- Remarkably, we show that for ResNet the loss for any fixed mini-batch when measured along side SGD trajectory appears to be accurately modeled by a quadratic function.
- In particular, a very low loss value can be reached in just one step of gradient descent with large enough learning rate.
- We propose a simple model and a geometric interpretation that allows to analyze the relationship between the gradients of stochastic mini-batches and the full batch and how the learning rate affects the relationship between improvement on individual and full batch. Our analysis allows us to discover the equivalency between iterate aggregates and specific learning rate schedules. In particular, for Exponential Moving Average (EMA) and Stochastic Weight Averaging we show that our proposed model matches the observed training trajectories on ImageNet. Our theoretical model predicts that an even simpler averaging technique, averaging just two points a few steps apart, also significantly improves accuracy compared to the baseline. We validated our findings on ImageNet and other datasets using ResNet Architecture.

# Paper Content

## Introduction
- SGD played a fundamental role in the rise and success of deep learning
- The learning rate, the multiplier controlling the magnitude of the weight update, is perhaps the most crucial hyper-parameter that controls the training trajectory
- Despite the advances in stochastic optimization methods, novel learning rate schedules such as cosine (Loshchilov & Hutter, 2016) and cyclical learning rate schedule (Smith, 2015) continue to be the subject of active research
- The impact of stochasticity of the optimization process caused by changing input mini-batches also known as "batch noise" has been relatively poorly understood
- In this paper we show that the loss as a function of learning rate of the training batch is dramatically different than that on the held-out batch, when measured on SGD trajectory
- Our simple model is suitable for studying training trajectories with a spectrum of time scales ranging from fast to slow and corresponding to long trenches in the loss landscape
- We show how model weights quickly converge towards a quasi-stationary distribution in some degrees of freedom, while slowly drifting towards the global minimum in others and derive the expression for the autocorrelation of the averaged trajectory as a function of the averaging kernel
- Using this model we develop a simple geometric explanation of why averaging improves the accuracy in the first place
- We show that even on large datasets such as ImageNet and for deep architectures using the fixed learning rate the trajectory traverses an ellipsoid centered around the target minimum and the averaging moves the trajectory to the inside of that ellipsoid. Further, we show that for two independently trained trajectories that share a common burn-in period the distance in parameter space along one trajectory to the final solution of another monotonically decreases as training progresses and asymptotically stabilizes at a fixed value. This suggests that most of the probability mass of the training outcomes is concentrated on a sphere-like volume.
- Our final insight is the dramatic difference in the behavior of the loss function for the batch that it is being optimized v.s. a new (training) batch.

## Related work
- Averaging intermediate SGD iterates in the weight space can be traced back to Ruppert (1988).
- It was first directly analyzed in Polyak & Juditsky (1992) and has been studied in the literature since.
- For example in Mandt et al. (2017) the connection between SGD and Bayesian inference was established.
- In particular they demonstrated that SGD training that initialized within common zone of attraction can be seen as Markov chain with Gaussian stationary distribution for constant learning rate, which is related to our observation about trajectory stabilizing at a fixed distance from the global minimum.
- However in present work we mainly analyze the behavior of a single trajectory.
- This has immediate advantage that it allows us to reason about finite trajectory lengths, as well as the connection between learning rate schedules and averaging.
- Another related line of inquiry is Stochastic Weight Averaging (SWA) by Izmailov et al. (2018), where they use a single trajectory with cyclic learning rates and averaging every c steps along the points on the trajectory as a way to improve accuracy.
- In theory it is slightly different from Ruppert-Polyak averaging, since not every point on the trajectory is averaged, however, many of the experiments presented in that paper still used c = 1.
- In that work it was also observed that averaging between trajectory allows to move inside the surface, but no further discussion has followed.
- Additionally, the connection between SWA and generalization was further explored in He et al. (2019), there it was showed that averaging introduces bias towards flatter part of the basin, which in turn results in better generalization.
- In contrast here our goal to illuminate the mechanisms by which SWA changes the behavior of empirical loss because of using stochastic gradient descent.
- Connecting current work and the generalization performance is an interesting open direction.
- The quadratic theoretical model we consider is reminiscent of that in Schaul et al. (2013), but our analysis is both simpler than former and more general than latter.
- We note that analysis only applies to stochastic gradient descent. In case of full gradient descent there has been several recent works showing that quadratic approximation model might be too simplistic.

## Empirical observations
- The loss for a single step along the training batch direction can bring the loss down to values considerably lower than the best average loss of a fully trained model.
- The loss for a fixed minibatch as we branch off different parts of the training trajectory leads to the same basin.

## Analytical model
- The high dimensional weight vector θ is interpolation between two trajectories with fixed-batch starting at t and t + 10000
- We use two points along the ImageNet training trajectory with t = 75 000, and t = 85 000.
- From each point we perform 9 steps of gradient descent on a fixed batch and measure the loss on interpolation between corresponding pairs of points of each trajectory.
- The left graph shows the loss on the training batch, and note how within just 3 steps it reaches nearly 0.
- The middle graph shows the loss on held out. At step 0 the training and held out batch loss profiles are very similar.
- Rightmost graph shows the visualization of the process. it moves along the SGD trajectory, in such a way that it can explaining the phenomena that we described in Section 3: the loss for the individual batches behaves like a low-degree polynomial and reaches remarkably low value in a single step.
- Thus we can use quadratic function that models the loss function for individual samples: where A x is a matrix and c x is a vector describing the loss function for a random sample x ∼ D.
- This model is similar to the one proposed in Schaul et al. (2013), however it analyzed only the special case of a constant and diagonal A x , while we consider a general scenario.
- Following the mini-batch gradient where individual batches are sampled from some distribution we then will be moving in a direction whose expectation is ∂θ .
- Without loss of generality we will assume that ∂E x∼D [L(0,x)] ∂θ = 0.
- The global loss over the sample distribution D is where we take expectation over all possible samples.
- The last transition holds since we assumed that the loss function achieves minimum at 0, and thus A x and c x satisfy E A T x c x = 0.
- The stochastic gradient and the full gradient can then be written respectively as:
- Now we estimate how close we can get to 0 for a fixed learning rate.
- Note, even though the individual steps gradients can be assumed to be unbiased
- Figure 4: Stochastic gradient descent.
- Figure 4a shows how stochastic gradient direction decomposes into a drift and a drag component.
- The magnitude of the drag component changes as √ λ, while the magnitude of the drift component is proportional to λ.
- Further, the drift component has a quadratic bias to increase loss, thus enabling us to pick learning rate that allows to reduce the loss.
- Figure 4b illustrates how stationary trajectory traverses the sphere of a fixed loss, while taking a midpoint enables to get inside the sphere.
- Best viewed in color.
- estimators of global loss gradient, there is generally no guarantee that trajectory will converge to a minimum.
- Instead we can show that the trajectory will be traversing an ellipsoid around the minimum.

## Experiments
- In ImageNet, using the same setup as in Section 3, using ResNet-34 and SGD with momentum 0.9, results show that learning rate schedules described in Table 1 match stochastic averaging
- On Figure 5, validation accuracy is shown for the ImageNet dataset, and the results also hold for the training splits as well
- On Figure 6, it is shown that the solution for two-point averaging diverges from the equivalent learning rate schedule, even though the average point is significantly apart
- Gradient alignment and divergence of trajectories are both maintained despite the loss change

### Synthetic model experiments
- The effect of weight averaging can be simulated approximately by using a properly chosen learning rate schedule.
- However, in practice, it is often advantageous to use fixed learning rate throughout training, especially if the new data is constantly arriving and the model needs to be trained continuously.
- In this case, the weight averaging can improve model performance, while still allowing us to train the model with a large learning rate guaranteeing fast convergence.
- The beneficial effect of weight averaging is particularly pronounced in situations where training dynamics is characterized by a wide spectrum of time scales, which is typical in practical scenarios.
- This rich spectrum of timescales manifests itself as the existence of elongated "trenches" in the loss landscape and long tails characteristic for the evolution of the loss during training.
- Since weight averaging is generally sensitive to the time scale of the underlying weight evolution, one can expect it to have different effects on "slow" and "fast" degrees of freedom.
- As discussed in Appendix A in more detail, weight averaging will have little effect on the slow dynamics, but is expected to reduce the size of the stationary distribution for fast degrees of freedom and reduce the corresponding contribution to the loss as if the learning rate was in fact smaller.
- Notice that using smaller learning rate without weight averaging would have a disadvantage of slowing down the convergence in slow coordinates and thus using large learning rate with weight averaging is expected to help us improve model performance without sacrificing the convergence speed.
- We illustrate this intuition by solving equation 5 for b x ∼ N (0, 1) and a diagonal A T x A x with values 1 and 0.015 for the fast and slow degrees of freedom correspondingly.
- The evolution of the loss function in this system is shown in Figure 9 for two different learning rates: (a) λ 0 = 5•10 −2 and (b) λ 1 = 2•10 −2 .
- Experiments with λ = λ 0 are conducted both with and without the exponential moving weight averaging (with a decay rate of 450 steps).
- Figure 9 illustrates that there are indeed two time scales in L(t), fast and slow, and that weight averaging can dramatically reduce the stationary loss L without sacrificing the speed of convergence, unlike when training with the smaller learning rate λ = λ 1 .
- Also, as shown in Figure 11b, in the appendix, weight averaging has a much larger effect on the fast degrees of freedom, significantly reducing corresponding contributions to the overall loss, while having only a mild effect on slow coordinates.

## Open question and conclusions
- The learning rate affects the speed of the algorithm
- The learning rate schedule can be observed in simple theoretical models and in large-scale training on multiple datasets
- There are other phenomena that can be studied when the learning rate is changed

## A Weight Averaging
- equation 14 can be reduced to equation 14a by using a coordinate transformation that whitens the batch noise
- equation 15 can be simplified by introducing a moving average of the training trajectory
- equation 17 can be solved by analogy with how equation 16 was solved for equation 15
- equation 18 can be approximated as follows: where S λ is the covariance of the stationary distribution in the weight space
- equation 19 can be applied to an arbitrary averaging procedure
