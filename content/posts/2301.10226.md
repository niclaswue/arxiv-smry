---
title: "A Watermark for Large Language Models"
date: 2023-01-24T18:52:59.000Z
author: "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers and 1 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-10226v1.webp" # image path/url
    alt: "A Watermark for Large Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.10226)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.10226).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-watermark-for-large-language-models).

# Abstract
- Watermarking model output can help mitigate potential harms of large language models
- Watermark can be embedded with minimal impact on text quality
- Watermark can be detected using an open-source algorithm without access to the language model API or parameters
- Watermark works by selecting a randomized set of whitelist tokens and promoting use of whitelist tokens during sampling
- Statistical test for detecting the watermark with interpretable p-values
- Information-theoretic framework for analyzing the sensitivity of the watermark

# Paper Content

## Introduction
- Large language models (LLMs) can write documents, create executable code, and answer questions.
- There is a risk that LLMs may be used for malicious purposes.
- Synthetic data on the web complicates future dataset creation efforts.
- Detecting and auditing the usage of machine-generated text is a key principle of harm reduction.
- Watermarking of language model output can be used to detect synthetic text from short spans of tokens.
- Watermark detection algorithm can be made public or kept private.

### Notation & language model basics
- Language models have a "vocabulary" containing words or word fragments
- Vocabularies typically contain 50,000 tokens or more
- AI system generates a sequence of tokens in response to a prompt
- Language model is a function that takes a sequence of known tokens as input and outputs a vector of logits
- Logits are passed through a softmax operator to convert them into a discrete probability distribution
- Next token is sampled from the distribution using multinomial sampling or greedy decoding
- Beam search can be used to consider multiple possible sequences before selecting the one with the highest score

### A caveat: the difficulty of watermarking low-entropy sequences
- Low entropy text makes it difficult to distinguish between human and machine-generated text
- Low entropy text makes it difficult to watermark, as any changes to the choice of tokens may lead to unexpected tokens

## A simple proof of concept
- Algorithm 1 describes a simple "hard" blacklist watermark
- It is easy to analyze, detect and hard to remove but has poor generation quality on low entropy sequences

## A more sophisticated watermark
- Soft watermark promotes use of whitelist for high entropy tokens.
- Softmax operator converts logits into probability vector.
- Algorithm 2 adds constant to logits of whitelist tokens.
- Soft blacklist rule enforces watermark with little impact on quality.
- High entropy case has many comparably large logits, δ rule biases output towards whitelist.

### Detecting the soft watermark
- Process for detecting soft watermark is same as hard watermark
- Null hypothesis is assumed and z-statistic is computed using equation
- Watermark is detected if z is greater than a threshold
- Algorithm 2 Text Generation with Soft Blacklist applies language model to prior tokens

## Analysis of the soft watermark
- Expected number of whitelist tokens used by a watermarked language model depends on entropy of generated text fragment
- Blacklist is sampled uniformly at random, but pseudo-random sampling is explored in Section 5
- Text is generated by multinomial random sampling, greedy decoding, and beam search
- Spike entropy is used to quantify how spread out a distribution is
- Theorem 4.2 predicts the number of whitelist tokens in a sequence with the watermark
- Variance of number of whitelist tokens is bounded

### Sensitivity of the watermark test
- Sensitivity of soft watermark can be computed using type-II error analysis
- Estimate type-II error rate with γ = .5 and δ = 2
- 200 tokens generated using OPT-1.3B and C4 dataset's RealNewsLike subset
- Detection threshold of z = 4 with type-I error rate of 3 x 10-5
- Average spike entropy per sample of S = 0.807
- Expected number of whitelist tokens per generation is at least 142.2
- 98.6% sensitivity with Gaussian approximation for whitelist count
- 98.4% of generations detected at z = 4 threshold with multinomial sampling
- 99.6% empirical sensitivity with 4-way beam search
- 100% of generations detected at z = 4 threshold with entropy above 25th percentile

### Impact on quality of generated text
- Soft watermark has little impact on perplexity of tokens with high or low entropy.
- Watermark rule does impact perplexity of tokens with moderate entropy.

## Private watermarking
- Watermark algorithms can be operated in private mode, using a secret key
- Private watermarking uses a pseudorandom function (PRF)
- To create a private watermark, a random key is chosen
- If the attacker has no knowledge of the key, it is more difficult to remove the watermark
- Testing for the presence of the watermark requires using the same secure API
- Brute-force attack to discover the watermarking rules requires |V| 1+h tokens to be submitted to the detection API
- When h = 1, the blacklists produced by many tokens can be discovered
- When h > 1, a unique blacklist for each ordered combination of words is created
- To limit attack amplification, a small window (h = 2 or 3) should be used
- Algorithm 3 has several nice security properties and is free of attack amplification

## Experiments
- OPT-1.3B model used to measure watermark strength
- Pytorch backend of Huggingface library used to implement watermark
- Random selection of texts from C4 dataset used to simulate language modeling scenarios
- 500 ± 10 sequences of length T = 200 ± 5 tokens used for each parameter choice
- Beam search used to achieve strong watermark with little perplexity cost
- Watermark strength decreases as sequence length increases
- Table 2 provides error rate for various watermarking parameters
- Figure 4 shows ROC charts for various thresholds

## Attacking the watermark
- Care must be taken when implementing a watermark and watermark detector to maintain security.
- Normalizing text before hashes are computed can help avoid simple attacks.
- Three types of attacks are possible: insertion, deletion, and substitution.
- Paraphrasing attacks require extensive human intervention.
- Discreet alterations such as adding whitespaces or misspelling words can be used to impact the computation of the hash.
- Tokenization attacks modify text so that sub-word tokenization of a subsequent word changes.
- Homoglyph and zero-width attacks use unicode characters to break tokenization.
- Generative attacks prompt the model to change its output in a predictable and easily reversible way.
- Negative examples of malicious prompts can be included during finetuning to defend against generative attacks.

### Degradation under attack: span replacement using a lm
- Attacker does not have access to watermark algorithm
- Attacker attempts to modify text by replacing tokens at random indices
- Attacker has a budget constraint to maintain semantic similarity
- Attacker uses T5-Large language model to replace tokens
- Attack is effective at creating blacklist tokens but only decreases watermark strength by 0.01 AUC

## Related work
- Watermarking is a form of steganography, which is the task of embedding hidden information into data
- Early approaches to watermarking digital text were limited by their rule-based understanding of natural text
- Modern neural language models have allowed for improved watermarking/steganography
- Post-hoc detection methods require the language model to be significantly biased away from human text
- Detection schemes are susceptible to false-positives, which could lead to academic problems
- The watermarking scheme proposed is designed to reduce false positives

## Conclusion
- Watermark is computationally simple to verify
- False positive detections are statistically improbable
- Watermark degrades gracefully under attack
- Can be retro-fitted to any existing model
- Depends only on whitelist size parameter and hash function
- Can deploy with context-specific choices
- Can turn on only in certain contexts
- Open questions regarding watermarking
- Threat model defined
- Multiple watermarks possible
- Selective watermarks in response to malicious activity
- Attacker needs to distinguish modification of whitelisted logits from naturally occurring biases
- Probability of sampling a token from the whitelist is at least γα
