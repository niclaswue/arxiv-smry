---
title: "Reprogramming Pretrained Language Models for Protein Sequence Representation Learning"
date: 2023-01-05T15:55:18.000Z
author: "Ria Vinod, Pin-Yu Chen, Payel Das"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02120v1.webp" # image path/url
    alt: "Reprogramming Pretrained Language Models for Protein Sequence Representation Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02120)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02120).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/reprogramming-pretrained-language-models-for).

# Abstract
- Machine learning-guided solutions for protein learning tasks have made significant headway in recent years
- However, success in scientific discovery tasks is limited by the accessibility of well-defined and labeled in-domain data
- To tackle the low-data constraint, recent adaptions of deep learning models pretrained on millions of protein sequences have shown promise; however, the construction of such domain-specific large-scale model is computationally expensive
- Here, we propose Representation Learning via Dictionary Learning (R2DL), an end-to-end representation learning framework in which we reprogram deep models for alternate-domain tasks that can perform well on protein property prediction with significantly fewer training samples. R2DL reprograms a pretrained English language model to learn the embeddings of protein sequences, by learning a sparse linear mapping between English and protein sequence vocabulary embeddings. Our model can attain better accuracy and significantly improve the data efficiency by up to $10^5$ times over the baseline set by pretrained and standard supervised methods.

# Paper Content

## Introduction
- Recent advances in artificial intelligence (AI), particularly in deep learning, have led to major innovations and advances in many scientific domains, including biology.
- Deep learning models aim to learn a highly accurate and compressed representation of the biological system, which then can be employed for a range of tasks.
- There has been notable success across a range of tasks, from high-quality protein structure prediction from protein sequences [1; 2], accurate prediction of protein properties, to enabling novel and functional peptide discoveries [3; 4].
- Many of these advances rely on developing deep learning models [1; 5; 6] which are trained from scratch on massive amounts (on the order of billions of tokens) of data.
- However, labeled data in biology is scarce and sparse, which is also the case for many other real-world scenarios in the scientific domain.
- In the biological domain, label annotation can involve biological assays, high resolution imaging and spectroscopy, which are all costly and time consuming processes.
- The technique of pretraining deep learning models was proposed to address this issue.
- Pretraining methods leverage large amounts of sequence data and can learn to encode features that can explain the variance seen in sequences across biological task-specific training samples.
- In the context of protein sequences, pretraining has enabled meaningful density modelling across protein functions, structures, and families [7].
- In this work, we reference two types of pretraining methods: (i) unsupervised pretraining, where all data is unlabeled, and (ii) self-supervised pretraining, where a model learns to assign labels to its unlabeled data.
- Large models then pretrain on massive amounts of unlabeled data, specifically biological sequences, which are available at scale.
- Once pretrained, these foundation models (FMs) [8] are finetuned on smaller amounts of labeled data, which correspond to a specific downstream task.
- Interestingly, for the large-scale models pretrained on protein sequences, biological structure and function seem to emerge in the learned protein representation, even though such information was not included in model training [5].
- Though highly powerful, the training of those domain-specific foundation models from scratch is highly resourceintensive [9].
- For example, one training run of BERT (the language model considered in this work) learns 110 million parameters, costs up to $13,000 USD and takes 64 days (without parallelized computing) and results in 0.7 tons of carbon emissions [10].
- A single training run of another popular language model, the T5 transformer, learns 11 billion parameters, costs up to $1.3 million USD, takes 20 days, and results in 47 tons of carbon emissions [11; 12].
- Such pretrained language models and size variants are abundantly available with the advent of models libraries (e.g., Hugging Face [13]) which host pretrained models and datasets.
- The scale of data, compute, and financial resources required to train these models is not only available to a limited number of researchers, but is also infeasible for applications with limited labeled data.
- However, in the scientific domain, we still aim to train models with similar
- properties to those trained on standard supervised data sets.
- One known fact is that biological sequences are similar to natural language, as they also contain long-range dependencies and follow Zipf's law [16].
- Such similarity has motivated the use of deep learning architectures and mechanisms that are widely used in natural language processing (NLP) to build protein sequence models from scratch.
- In this work, we explore an alternative warm-start paradigm, i.e. how to effectively and efficiently reprogram an existing, fully-trained large English language model to learn a meaningful (i.e., biomedically relevant) representation of protein sequences.
- The goal is to create a more...

## Results
- Figure 2 illustrates the proposed Representation Reprogramming via Dictionary Learning (R2DL) framework
- The framework learns to embed a protein sequence dataset of interest by training on the representations of a transformer that is pretrained on an English text corpus
- A one-to-one label mapping function is assigned for each downstream protein prediction task for cross-domain machine learning, and a class label or a regression value is predicted using R2DL for each protein sequence during testing

### R2DL Framework Formulation
- The R2DL objective is to reprogram a source model (pretrained language model) to be able to correctly classify, or predict the regression values of, protein sequences (for a target prediction task).
- We use pretrained instances of BERT, a bidirectional transformer (termed the source model), which has been finetuned separately for different language tasks (e.g., sentiment classification, named entity recognition) [10; 22].
- For a protein sequence classification task, we use the source model trained on a language task for which there are n sentence output classes (e.g., positive and negative for senitiment classification), and n protein sequence classes (e.g., toxic, non-toxic).
- The output-label mapping h is then a simple 1-1 correspondence between the source task labels and the target task labels (e.g., positive → toxic and negative → non-toxic).
- For a regression task, R2DL uses a mapping between the regression values in protein sequence feature space and the classification probability values in the source model embedding space.
- It does so by learning optimal thresholds of regression values that map to the source model class labels.
- The input data of the source English language model is tokenized at the word level. These tokens form the atoms for our dictionary representation of V S , a matrix with its rows corresponding to embedding vectors of source tokens.
- The input data to the target task, protein sequences, are tokenized on a character level with only 20 distinct tokens (corresponding to the set of 20 discrete natural amino acid characters).
- R2DL obtains V S from the learned embeddings of the source model and learns to represent V T , the matrix of the target token embedding, as a weighted combination of the English token embeddings.
- We propose token reprogramming by approximating a linear mapping between V S and V T . That is, we aim to find a transformation of the latent representation of the protein sequences, such that it can be embedded in the pretrained language model's latent space and enable R2DL to leverage these re-embedded tokens for learning.
- Specifically, we learn the linear map Θ by approximating a dictionary using a k-SVD solver [23]. That is, we want to approximate V T = ΘV S .
- The k-SVD solver guarantees a task-specific level of sparsity in the coefficients when linearly combining English token embeddings to represent a protein sequence token embedding. In other words, it helps select k English tokens and use their linearly combined embeddings as the embedding of a target token.
- Additionally, with a one-to-one label mapping function of the protein sequence label to the English text label, we are able to use the pretrained language model for inference on the embedded protein dataset, V T .

### R2DL Training and Optimization Procedure
- We are given a pretrained classifier, C (which has been pretrained on a source-task dataset with source tokens denoted by {v Si } |V S | i=1 ) and a target-task dataset with target tokes denoted by {V T j } |V T | j=1 .
- The embedding matrices are V S and V T respectively.
- We can encode an output label mapping function translating between source and target labels.
- In Figure 2, we show how R2DL aims to find a linear mapping function Θ that learns the optimal coefficients for our atoms in V T to be represented as a sparse encoding of the dictionary V S such that V T = ΘV S .
- The map Θ is used to reprogram C to be able to correctly classify the protein sequences through the transformation h(C(θ t , t)) where t is a protein sequence from a protein task and θ t is the linear weights associated with the protein sequence t in Θ.
- We note that for each of the downstream protein property prediction task, R2DL only trains a corresponding token mapping function Θ while keeping the pretrained classifier C intact.
- Therefore, the number of trainable parameters in R2DL is simply the size of the matrix Θ, which is usually much smaller compared to the number of parameters in the pretrained deep neural network classifier C.
- To approximate the dictionary, we use a k-SVD solver to optimize over the cross entropy loss for updates to Θ.
- We then apply the assigned label mapping h for protein classification tasks, or thresholding for regression tasks, and train the mapping function Θ using gradient-based optimization evaluated on the task-specific cross-entropy loss.

### Benchmark Tasks and Evaluation
- We consider four physicochemical structure and property prediction tasks from a well-established protein benchmark from [6]
- Secondary structure prediction involves predicting secondary structure y ∈ {Helix, Strand, Other} for each amino acid x in a given protein sequence.
- Solubility prediction considers mapping an input protein sequence x to a label of y ∈ {Membrane-Bound, Water Soluble}.
- Homology detection is a sequence classification task, where each input protein x is mapped to a label y ∈ {1, ..., 1195}, representing different possible protein folds.
- Stability prediction is a regression task.
- We further consider three biomedically relevant function prediction tasks, which are sequence classification tasks (represented in Figure 1).
- Using R2DL, we predict for a given sequence x, its binary class label y ∈ {AMP, non-AMP} for antimicrobial-nature prediction [3] or y ∈ {Toxic, non-Toxic} for toxicity prediction [3].
- Finally, we predict antigen and non-specific binding of antibody variant sequences from [14]: given a sequence x, the task is to predict y ∈ {on-target, off-target}.

### Model Baselines and Data
- The baseline models we consider in this work are of two types: supervised and unsupervised.
- For supervised models, the baseline model we compare to has been pretrained on a subset of the UniProt database where sequences are limited to being 50 residues long.
- For unsupervised models, the baseline model we consider is pretrained on the Pfam corpus.
- Pretraining poses the advantage of modeling the density over a range of protein families and structures.
- R2DL eliminates this requirement by repurposing existing pretrained English language models.

### Data Efficiency and Accuracy of Reprogramming
- R2DL outperforms task-specific baselines on all prediction tasks
- R2DL is more data efficient than pretrained models
- R2DL is more accurate than baseline models in class-imbalanced datasets

### R2DL Performance vs. Pretraining Performance in Low Data Settings
- R2DL is a task-specific predictive model that is more efficient than a baseline model when trained on restricted data
- R2DL outperforms pretraining in all but one task
- R2DL is more efficient than a random guess in all but one task

### Correlation Between Learned Embeddings and Evolutionary Distances
- The R2DL dictionary learning framework shows interpretable correspondences between the learned embeddings in the latent space and the specific protein property
- Clear separation between different protein classes is evident
- The euclidean distance between the latent representation at the last layer for each amino acid embedding, and compares it to the pairwise evolutionary distance with the BioPython module. In Figure 5(d), the euclidean distances between the latent embeddings learned in the R2DL model and the pairwise evolutionary distances between protein sequences, as estimated using BLOSUM62 matrix implemented in the pairwise function of BioPython module, shows a correlation of close to 1.0 along the diagonal showing a perfect correspondence between the learned representation and the empirical observations of amino acid relatedness.

## Discussion
- We propose a new framework, R2DL, to reprogram large language models for various protein tasks.
- R2DL demonstrates powerful predictive performance across tasks that involve evolutionary understanding, structure prediction, property prediction and protein engineering.
- We thus provide a strong alternative to pretraining large language models on upto 10 6 protein sequences.
- With only a pretrained natural language model (which are abundantly available at the time of writing), a small-sized labeled protein data set of interest, and a small amount of cross-domain finetuning, we can achieve better performance for each protein prediction task with interpretable correspondences between features.
- Beyond improvements in predictive performance, we show that the ratio of performance improvements to pretraining and training samples involved in the R2DL framework make R2DL up to 105 times more data-efficient than any current methods.
- This work opens many doors to biological prediction tasks that can acquire very few labeled, high quality data samples.
- We emphasize the results of the data-efficiency of R2DL, when applied to biomedically relevant protein predictions, which are critical to advancing scientific understanding and discovery, but have been unsuccessful until now.
- While R2DL does make gradient updates in the framework, the data and resource requirements of the R2DL method is much lower than any unsupervised or self-supervised pretraining approach for protein sequence modeling.
- Though R2DL has the same data and resource requirements as any standard supervised training approach, R2DL demonstrates much higher task accuracy across a broad and diverse range of property prediction tasks.
- We claim that R2DL is able to do this because it can leverage the deep representational capacity induced by reprogramming, which standard supervised models cannot achieve without an unjustifiably large number of parameters.
- R2DL is thus more efficient than existing baseline models in the following aspects: (i) R2DL only requires a pretrained transformer (trained on English language data) and a small-sized, labeled protein sequence data set of interest. We do not make any updates to the pretrained model itself, unlike traditional transfer learning methods. Rather we make updates to the R2DL model during a supervised training process that optimizes over class-mapped labels.
- (iii) Further, R2DL does not require any large-scale supervised pretraining, which has been found beneficial in protein-specific tasks [6] as well as in computer vision [27]. Labeling protein sequences at scale, particularly for biomedical function, is almost infeasible for the size of dataset that is required for supervised pretraining.
- With these three considerations in mind, we pose R2DL as a data-efficient alternative to pretraining methods for protein prediction tasks of biological and biomedical relevance.

## Method

### Representation of Tokens
- The R2DL framework uses 2 input datasets, an English language text dataset (source dataset) and a protein sequence dataset (target dataset).
- The vocabulary size of a protein sequence dataset at a unigram level is 20, as proteins are composed of 20 different natural amino acids.
- We obtain a latent representation of the English text vocabulary, V S , by extracting the learned embeddings of the data from a pretrained language model (source model).
- The protein sequence data is embedded in the same latent space, and is termed the target vocabulary, V T .
- For each task, the token embedding matrix is of dimensions (n, m) where n is the number of tokens and m is the length of the embedding vectors.

### Procedure Description of the R2DL Framework for a Protein Task
- Pretrained English sentence classifier C, target model training data X for task , class mapping label function, h (if classification) where ∈ {Secondary Structure, Fluorescence, Homology, Solubility, Antimicrobial, Toxicity, Antibody}.
- Maximum number of iterations T 1 for updates to Θ, number of iterations T 2 for k-SVD, step size {α t } T1 t=1
- Random initialization of Θ, obtain the source token embedding matrix V S
- Define Objective Function: Objective function for k-SVD:
- Calculate the Loss and Perform Gradient Descent: and return to the previous K-SVD step
- Output Protein Sequence Labels for Protein Sequence x of Task : h (C(Θ, x))
- We are given a pretrained English classifier, C, and a protein sequence target-task dataset X .
- We denote the task with , such that ∈ {Secondary Structure, Fluorescence, Homology, Solubility, Antimicrobial, Toxicity, Antibody}.
- We also encode an output label mapping function h specifying the one-to-one correspondence between source and target labels.
- As shown in Figure 2, the source vocabulary embedding, V S , is extracted from the pretrained model, C.
- The next objective is to learn Θ that approximates the embedding of tokens in X (denoted by V T ) in the representation space of the source model.
- We aim to learn Θ ∈ R a×b that finds the optimal coefficients {θ t } for each of the target tokens t ∈ {1, ..., a} in V T ∈ R a×m to be represented as a sparse encoding of the dictionary, V S ∈ R b×m , such that V T = ΘV S .
- For a given target protein sequence x from the -th task, Θ is used to perform the target task through the transformation h (C(Θ, x)).
- While we do not make any modification to the parameters or architecture of C, we assume access to the gradient ∇ Θ loss(•) for loss evaluation and parameter updates during training.
- A target token embedding v t ∈ R m can be represented as a sparse linear combination of the source token embeddings (rows) in V S , v t = θ t V s . v t is the representation of the protein token in the dictionary space and satisfies ||v t − θ t V s || p ≤ , where • p is an L p norm and θ t is made to be sparse by satisfying ||θ t || 0 ≤ k for all t.
- An exact solution v t = θ t V S is computationally expensive to find, and is subject to various convergence traps, so for the purpose of our efficient fine-tuning approach we approximate v t ≈ θ t V S using k-SVD.
- We first fix the dictionary V S , as extracted from C, and then find the optimal Θ according to the optimization problem, by minimizing the alternative objective a t=1 ||θ t || 0 subject to V T − ΘV S 2 F ≤ as explored in [23].
- While algorithms exist to choose an optimal dictionary (an exact solution to k-SVD) that can be continually updated [23], we penalize computational expense over performance for the purpose of maintaining an efficient solution (at the cost of statistically insignificant improvements in accuracy) by using a predetermined number of iterations for k-SVD convergence, which is then used to evaluate the cross entropy loss on h (C(Θ, x)) and update the mapping function Θ.

### Data
- We provide five biologically relevant downstream physicochemical property prediction tasks, adapted from [6] to serve as benchmarks.
- The sizes of the individual datasets vary between 4,000 and 50,000 (see supplementary for details on data sizes and train-test splits).
- Secondary Structure Prediction (Structure Task): Secondary structure (SS) is critical to understanding the function and stability of a protein, and SS prediction is an important intermediate step in designing designing protein complexes.
- This dataset, obtained from [28] has 8,678 data samples.
- It is derived from the CB513 dataset, and each amino acid, x in a protein sequence is mapped to y ∈ {Helix, Strand, Other}.
- The benchmark for this task is a transformer that reports a best performance of 80% accuracy.
- Solubility: This task takes an input protein x and maps it to a label of y ∈ {Membrane-Bound, Water Soluble}.
- Determining the solubility of proteins is useful when designing proteins or evaluating their function for particular cellular tasks.
- This dataset, obtained from [29] has 16,253 data samples.
- The benchmark is a pretrained transformer, that achieves a best performance of 91% on a binary classification task.
- Antigen Affinity (Protein Engineering): Therapeutic antibody development requires the selection and engineering of molecules with high affinity and other drug-like biophysical properties.
- This dataset, obtained from [14] has 4,000 data samples.
- The task is to map an input protein x to a label y ∈ {on-target, off-target}.
- The task corresponds to predicting antigen and non-specific binding.
- The benchmark for this task is a Linear Discriminant Analysis model with Spearman's ρ values for antigen binding (0.87) and for non-specific binding (0.67).
- Antimicrobial Prediction (AMP) (Property Task): Determining the antimicrobial nature of a peptide is a critical step in developing antimicrobials to fight against resistant pathogens.
- This dataset, obtained from [3] consists of 6,489 labeled protein sequences x, is mapped to a label y ∈ {AMP, non-AMP}.
- The original model trained on this data provides a de novo approach for discovering new, broad-spectrum and low-toxic antimicrobials.
- The benchmark for this task is a transformer that reports a best performance of 88% accuracy with a pretrained classifier.
- Toxicity (Property Task): Improving the functional profile of molecules, especially in the context of drug discovery, requires optimizing for toxicity and other physicochemical properties.
- This dataset, obtained from [3] consists of 8,153 antimicrobial peptide sequences which are either toxic (positive class), or non-toxic (negative class).
- The benchmark for this task is a transformer that reports a best performance of 93.78% accuracy with a pretrained classifier.
- Stability (Protein Engineering Task): This regression task where each protein, x i is mapped to y i ∈ R based on maintaining its fold beyond a threshold of concentration.
- Stability is an important protein engineering task, as we can use this fold concentration to test protein inputs such that design candidates are stable in the settings of different tasks.
- The benchmark for this task is a transformer that reports a best performance of 0.73 Spearman's ρ.
- Homology (Evolutionary Understanding Task): This is a sequence classification task where each input protein, x is mapped to a protein fold represented by y ∈ {1, ..., 1195}.
- This dataset, obtained from [31] has 12,312 data samples.
- Detecting homologs is particularly important in a biomedical context as they inform structural similarity across a set of sequences, and can indicate emerging resistance of antibiotic genes [cite].
- The original model removes entire homologous groups during model training, thereby enforcing that models generalize well when large evolutionary gaps are introduced.
- The benchmark for this task is a LSTM that reports a best performance of 26% Top-1 Accuracy.

### R2DL Settings and Hyperparameter Details AMP
- The AMP dataset is 8112 records long.
- We use a training set size of 6489 records and a test set size of 812 records.
- We use the L 0 norm in our objective function.
- We perform 10,000 k-SVD iterations.
- The value of the objective function is 0.045.

### Toxicity
- The Toxicity dataset size is 10,192.
- We use a training set size of 8153 and a test set size of 1020.
- We use the L 0 norm in our objective function.

### Secondary Structure
- The Toxicity dataset has a size of 9270.
- We use a training set size of 7416 and a test set size of 1854.
- We use the L 0 norm in our objective function.

### Stability
- The full Stability dataset size is 56,126
- We use a training set size of 44,900 and a test set size of 11,226
- We use the L 0 norm in our objective function, 6,000 k-SVD iterations and = 0.29

### Homology
- The Homology dataset has a size of 13,048
- We use a training set size of 10,438 and a test set size of 2,610
- We use the L 0 norm in our objective function

### Solubility
- The Solubility dataset has a size of 43,876.
- We use a training set size of 35,100 and a test set size of 8,775.
- We use the L 0 norm in our objective function.
- We use 9,000 k-SVD iterations and = 0.42.

### Data and Code Availability
- Downstream supervised protein task dataset sizes and test accuracy of the 3 comparable methods introduced in Figure 1.
- Data efficiency of R2DL vs. pretrained methods as illustrated in Figure 1.
- Confusion matrix of the baseline model trained in [14] for the antibody affinity prediction task.
- Confusion matrix of the R2DL model for the antibody affinity prediction task.
