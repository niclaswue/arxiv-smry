---
title: "Reprogramming Pretrained Language Models for Protein Sequence Representation Learning"
date: 2023-01-05T15:55:18.000Z
author: "Ria Vinod, Pin-Yu Chen, Payel Das"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02120v1.webp" # image path/url
    alt: "Reprogramming Pretrained Language Models for Protein Sequence Representation Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02120)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02120).


# Abstract
- Machine learning-guided solutions for protein learning tasks have made significant headway in recent years
- However, success in scientific discovery tasks is limited by the accessibility of well-defined and labeled in-domain data
- To tackle the low-data constraint, recent adaptions of deep learning models pretrained on millions of protein sequences have shown promise; however, the construction of such domain-specific large-scale model is computationally expensive
- Here, we propose Representation Learning via Dictionary Learning (R2DL), an end-to-end representation learning framework in which we reprogram deep models for alternate-domain tasks that can perform well on protein property prediction with significantly fewer training samples
- R2DL reprograms a pretrained English language model to learn the embeddings of protein sequences, by learning a sparse linear mapping between English and protein sequence vocabulary embeddings
- Our model can attain better accuracy and significantly improve the data efficiency by up to $10^5$ times over the baseline set by pretrained and standard supervised methods

# Paper Content

## Introduction
- Deep learning models are trained from scratch on massive amounts of data to achieve high accuracy
- Pretraining methods leverage large amounts of sequence data and can learn to encode features that can explain the variance seen in sequences across biological task-specific training samples
- In the context of protein sequences, pretraining has enabled meaningful density modelling across protein functions, structures, and families
- Large models pretrain on massive amounts of unlabeled data, specifically biological sequences, which are available at scale
- Once pretrained, these foundation models (FMs) are finetuned on smaller amounts of labeled data, which correspond to a specific downstream task
- Interestingly, for the large-scale models pretrained on protein sequences, biological structure and function seem to emerge in the learned protein representation, even though such information was not included in model training

## Results
- The Representation Reprogramming via Dictionary Learning (R2DL) framework learns to embed a protein sequence dataset of interest by training on the representations of a transformer that is pretrained on an English text corpus.
- A one-to-one label mapping function is assigned for each downstream protein prediction task for cross-domain machine learning, and a class label or a regression value is predicted using R2DL for each protein sequence during testing.

### R2DL Framework Formulation
- The R2DL objective is to reprogram a source model (pretrained language model) to be able to correctly classify, or predict the regression values of, protein sequences
- We use pretrained instances of BERT, a bidirectional transformer (termed the source model), which has been finetuned separately for different language tasks (e.g., sentiment classification, named entity recognition) [10; 22]
- For a protein sequence classification task, we use the source model trained on a language task for which there are n sentence output classes (e.g., positive and negative for senitiment classification), and n protein sequence classes (e.g., toxic, non-toxic)
- The output-label mapping h is then a simple 1-1 correspondence between the source task labels and the target task labels (e.g., positive → toxic and negative → non-toxic)
- For a regression task, R2DL uses a mapping between the regression values in protein sequence feature space and the classification probability values in the source model embedding space. It does so by learning optimal thresholds of regression values that map to the source model class labels.
- The input data of the source English language model is tokenized at the word level. These tokens form the atoms for our dictionary representation of V S , a matrix with its rows corresponding to embedding vectors of source tokens.
- The input data to the target task, protein sequences, are tokenized on a character level with only 20 distinct tokens (corresponding to the set of 20 discrete natural amino acid characters). R2DL obtains V S from the learned embeddings of the source model and learns to represent V T , the matrix of the target token embedding, as a weighted combination of the English token embeddings.
- We propose token reprogramming by approximating a linear mapping between V S and V T . That is, we aim to find a transformation of the latent representation of the protein sequences, such that it can be embedded in the pretrained language model's latent space and enable R2DL to leverage these re-embedded tokens for learning. Specifically, we learn the linear map Θ by approximating a dictionary using a k-SVD solver [23]
- The k-SVD solver guarantees a task-specific level of sparsity in the coefficients when linearly combining English token embeddings to represent a protein sequence token embedding. In other words, it helps select k English tokens and use their linearly combined embeddings as the embedding of a target token. Additionally, with a one-to-one label mapping function of the protein sequence label to the English text label, we are able to use the pretrained language model for inference on the embedded protein dataset, V T .

### R2DL Training and Optimization Procedure
- We are given a pretrained classifier, C (which has been pretrained on a source-task dataset with source tokens denoted by {v Si } |V S | i=1 ) and a target-task dataset with target tokes denoted by {V T j } |V T | j=1 .
- The embedding matrices are V S and V T respectively.
- We can encode an output label mapping function translating between source and target labels.
- In Figure 2, we show how R2DL aims to find a linear mapping function Θ that learns the optimal coefficients for our atoms in V T to be represented as a sparse encoding of the dictionary V S such that V T = ΘV S .
- The map Θ is used to reprogram C to be able to correctly classify the protein sequences through the transformation h(C(θ t , t)) where t is a protein sequence from a protein task and θ t is the linear weights associated with the protein sequence t in Θ.
- We note that for each of the downstream protein property prediction task, R2DL only trains a corresponding token mapping function Θ while keeping the pretrained classifier C intact. Therefore, the number of trainable parameters in R2DL is simply the size of the matrix Θ, which is usually much smaller compared to the number of parameters in the pretrained deep neural network classifier C.
- To approximate the dictionary, we use a k-SVD solver to optimize over the cross entropy loss for updates to Θ.
- We then apply the assigned label mapping h for protein classification tasks, or thresholding for regression tasks, and train the mapping function Θ using gradient-based optimization evaluated on the task-specific cross-entropy loss.

### Benchmark Tasks and Evaluation
- We consider four physicochemical structure and property prediction tasks from a well-established protein benchmark from [6]
- Secondary structure prediction involves predicting secondary structure y ∈ {Helix, Strand, Other} for each amino acid x in a given protein sequence.
- Solubility prediction considers mapping an input protein sequence x to a label of y ∈ {Membrane-Bound, Water Soluble}.
- Homology detection is a sequence classification task, where each input protein x is mapped to a label y ∈ {1, ..., 1195}, representing different possible protein folds.
- Stability prediction is a regression task.
- We further consider three biomedically relevant function prediction tasks, which are sequence classification tasks (represented in Figure 1).
- Using R2DL, we predict for a given sequence x, its binary class label y ∈ {AMP, non-AMP} for antimicrobial-nature prediction [3] or y ∈ {Toxic, non-Toxic} for toxicity prediction [3].
- Finally, we predict antigen and non-specific binding of antibody variant sequences from [14]: given a sequence x, the task is to predict y ∈ {on-target, off-target}.

### Model Baselines and Data
- The baseline models we consider in this work are of two types: models trained in a supervised manner, by training standard sequence LSTM models from scratch, and models that are pretrained in an unsupervised manner on protein sequence data and are finetuned for a particular downstream task.
- Pretraining methods that do not use labeled data pose an advantage, as those models can then learn from a significantly larger number of data samples.
- In the cases of the toxicity and antimicrobial prediction tasks, the baseline model we compare to has been pretrained on a subset of the UniProt database where sequences are limited to being 50 residues long.
- The pretraining corpus size is then 1.7 million peptide sequences.
- Using unlabeled data for pretraining is thus much more advantage than pretraining in a supervised scheme.
- Of these 1.7 million sequences, only 9,000 are labeled (0.005% of sequences).
- The model is a Wasserstein Autoencoder, which is a generative model that undergoes unsupervised pretraining on the subset of UniProt data.
- The WAE embeddings of the labeled sequences are then used to train a logistic regressor model on the labeled dataset to obtain a binary classifier for Antimicrobial/non-Antimicrobial (6489 labeled samples) or for toxic/non-toxic (8153 labeled samples) label prediction.
- For the physicochemical property prediction tasks, the baseline model we consider is pretrained on the Pfam corpus.
- This corpus consists of 31 million protein domains and is widely used in bioinformatics pipelines.
- Sequences are grouped by protein families which are categorized by evolutionarily-related sequences.
- In contrast, the downstream physicochemical tasks of structure, homology, stability and solubility prediction have labeled datasets that range from 5,000 to 50,000 samples which the model can be finetuned on.
- Pretraining thus poses the advantage of modeling the density over a range of protein families and structures, but stipulates that there must be sequence datasets that contain structural and functional information about the downstream task datasets, and typically be of a size on the order of millions of sequences.

### Data Efficiency and Accuracy of Reprogramming
- R2DL outperforms task-specific baselines on all prediction tasks
- R2DL is more data efficient than pretrained models

### R2DL Performance vs. Pretraining Performance in Low Data Settings
- R2DL is a task-specific predictive model that is more efficient than baseline models
- R2DL is more efficient than a random guess when predicting downstream tasks
- R2DL outperforms pretraining until the cutoff point that is the intersection of the random guess curve with the accuracy curves

### Correlation Between Learned Embeddings and Evolutionary Distances
- The R2DL dictionary learning framework shows interpretable correspondences between the learned embeddings in the latent space and the specific protein property
- Clear separation between different protein classes is evident
- The euclidean distance between the latent representation at the last layer for each amino acid embedding, and compares it to the pairwise evolutionary distance with the BioPython module. In Figure 5(d), the euclidean distances between the latent embeddings learned in the R2DL model and the pairwise evolutionary distances between protein sequences, as estimated using BLOSUM62 matrix implemented in the pairwise function of BioPython module, shows a correlation of close to 1.0 along the diagonal showing a perfect correspondence between the learned representation and the empirical observations of amino acid relatedness.

## Discussion
- R2DL is a new framework that can reprogram large language models for various protein tasks
- R2DL demonstrates powerful predictive performance across tasks that involve evolutionary understanding, structure prediction, property prediction and protein engineering
- R2DL has the same data and resource requirements as any standard supervised training approach, but R2DL demonstrates much higher task accuracy across a broad and diverse range of property prediction tasks

## Method

### Representation of Tokens
- The R2DL framework uses two input datasets, an English language text dataset and a protein sequence dataset.
- The vocabulary size of a protein sequence dataset at a unigram level is 20, as proteins are composed of 20 different natural amino acids.
- We obtain a latent representation of the English text vocabulary, V S , by extracting the learned embeddings of the data from a pretrained language model.
- The protein sequence data is embedded in the same latent space, and is termed the target vocabulary, V T .
- For each task, the token embedding matrix is of dimensions (n, m) where n is the number of tokens and m is the length of the embedding vectors.

### Procedure Description of the R2DL Framework for a Protein Task
- pretrained english sentence classifier
- target model training data
- task
- class mapping label function
- h (if classification)
- source vocabulary embedding matrix
- objective function
- loss
- gradient descent
- protein sequence labels

### Data
- The dataset has 8,678 data samples
- The dataset is derived from the CB513 dataset
- The benchmark for this task is a transformer that reports a best performance of 80% accuracy
- This dataset has 16,253 data samples
- The benchmark is a pretrained transformer, that achieves a best performance of 91% on a binary classification task
- This dataset has 4,000 data samples
- The task is to map an input protein to a label y ∈ {on-target, off-target}
- The task corresponds to predicting antigen and non-specific binding
- The benchmark for this task is a transformer that reports a best performance of 93.78% accuracy with a pretrained classifier
- This task takes an input protein x and maps it to a label of y ∈ {Membrane-Bound, Water Soluble}
- Determining the solubility of proteins is useful when designing proteins or evaluating their function for particular cellular tasks
- This dataset has 16,253 data samples
- The benchmark is a pretrained transformer, that achieves a best performance of 91% on a binary classification task
- This dataset has 4,000 data samples
- The task is to map an input protein to a label y ∈ {AMP, non-AMP}
- The task corresponds to predicting antigen and non-specific binding
- The benchmark for this task is a transformer that reports a best performance of 93.78% accuracy with a pretrained classifier
- This dataset has 21,446 data samples
- Stability is an important protein engineering task, as we can use this fold concentration to test protein inputs such as design candidates
- The benchmark for this task is a transformer that reports a best performance of 0.73 Spearman's ρ

### R2DL Settings and Hyperparameter Details AMP
- The AMP dataset is 8112 records long.
- We use a training set size of 6489 records and a test set size of 812 records.
- We use the L 0 norm in our objective function.
- We perform 10,000 k-SVD iterations.
- The value of the objective function is 0.045.

### Toxicity
- The Toxicity dataset size is 10,192
- We use a training set size of 8153 and a test set size of 1020
- We use the L 0 norm in our objective function
- 10,000 k-SVD iterations and = 0.045

### Secondary Structure
- The Toxicity dataset has a size of 9270
- The training set size is 7416 and the test set size is 1854
- The objective function is used which is the L 0 norm
- The number of iterations is 9,000 and the value of the parameter is 0.38

### Stability
- The full Stability dataset size is 56,126
- We use a training set size of 44,900 and a test set size of 11,226
- We use the L 0 norm in our objective function, 6,000 k-SVD iterations and = 0.29

### Homology
- The Homology dataset has a size of 13,048
- We use a training set size of 10,438 and a test set size of 2,610
- We use the L 0 norm in our objective function
- The objective function is 4,000 k-SVD iterations and = 0.73

### Solubility
- The Solubility dataset has a size of 43,876.
- We use a training set size of 35,100 and a test set size of 8,775.
- We use the L 0 norm in our objective function.
- We use 9,000 k-SVD iterations and = 0.42.

### Data and Code Availability
- Downstream supervised protein task dataset sizes and test accuracy of the 3 comparable methods introduced in Figure 1.
- Data efficiency of R2DL vs. pretrained methods as illustrated in Figure 1.
- Confusion matrix of the baseline model trained in [14] for the antibody affinity prediction task.
- Confusion matrix of the R2DL model for the antibody affinity prediction task.
