---
title: "On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning"
date: 2022-12-15T18:59:32.000Z
author: "Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08061v1.webp" # image path/url
    alt: "On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08061)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08061).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/on-second-thought-let-s-not-think-step-by).

# Abstract
- Generating a chain of thought (CoT) can increase large language model (LLM) performance on a wide range of tasks.
- Zero-shot CoT evaluations, however, have been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA).
- In this paper, we perform a controlled evaluation of zero-shot CoT across two sensitive domains: harmful questions and stereotype benchmarks.
- We find that using zero-shot CoT reasoning in a prompt can significantly increase a model's likeliness to produce undesirable output.
- Without future advances in alignment or explicit mitigation instructions, zero-shot CoT should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.

# Paper Content

## Introduction
- LLMs improve performance on a wide range of tasks, including question answering, mathematical problem solving, and commonsense reasoning
- A popular approach to implementing CoT involves zero-shot generation
- However, we demonstrate that zero-shot CoT consistently produces undesirable biases and toxicity
- For tasks that require social knowledge, blindly using "let's think step by step" can sabotage a model's performance
- We argue that improvements from zero-shot CoT are not universal, and measure empirically that zero-shot CoT substantially increases model bias and generation toxicity

## Related Work
- At sufficiently large scale, LLMs can utilize intermediate reasoning steps to improve performance across several tasks: arithmetic, metaphor generation (Prystawski et al., 2022), and commonsense/symbolic reasoning (Wei et al., 2022b).
- Kojima et al. (2022) further shows that by simply adding "Let's think step by step" to a prompt, zero-shot performance on reasoning benchmarks sees significant improvement.
- We focus on "Let's think step by step" in this paper, though other prompting methods have also yielded performance increases: aggregating CoT reasoning paths through majority vote using self consistency (Wang et al., 2022), combining outputs from several imperfect prompts (Arora et al., 2022), or breaking down prompts into less → more complex questions (Zhou et al., 2022).
- While focus on reasoning strategies for LLMs have increased, our work highlights the importance of evaluating these strategies on a broader range of tasks.
- LLMs are especially sensitive to prompting perturbations (Gao et al., 2021;Schick et al., 2020;Liang et al., 2022).
- The order of few shot exemplars, for example, has a substantial impact on in-context learning (Zhao et al., 2021).
- Reasoning strategies used by LLMs are opaque: models are prone to generating unreliable explanations (Ye and Durrett, 2022) and may not understand provided incontext examples/demonstrations at all (Min et al., 2022;Zhang et al., 2022).
- Instruct-tuned (Wei et al., 2021) and value-aligned (Solaiman and Dennison, 2021) LLMs aim to increase reliability and robustness: by training on human preference and in-context tasks, models are finetuned to follow prompt-based instructions.
- Our work examines the reliability of reasoning perturbations on bias and toxicity.
- By carefully evaluating zero-shot CoT, we highlight the importance of robust value alignment.
- Stereotypes, Biases, & Toxicity
- NLP models exhibit a wide range of social and cultural biases (Caliskan et al., 2017;Bolukbasi et al., 2016;Pennington et al., 2014).
- A specific failure involves stereotype bias-a range of benchmarks have outlined a general pattern of stereotypical behaviour in language models (Meade et al., 2022;Nadeem et al., 2021;Nangia et al., 2020;Parrish et al., 2022).
- Our work probes specifically for stereotype bias; we reframe prior benchmarks into zero-shot reasoning tasks, evaluating intrinsic biases.
- Beyond stereotypes, model biases also manifest in a wide range of downstream tasks, like questionanswering (QA) (Parrish et al., 2022), toxicity detection (Davidson et al., 2019) and coreference resolution (Zhao et al., 2018;Rudinger et al., 2018;Cao and Daumé III, 2020).
- Building on downstream task evaluations, we design and evaluate an explicit toxic question benchmark, analyzing output when using zero-shot reasoning.
- LLMs also exhibit a range of risks: Lin et al. (2022) highlights how models generate risky output and Gehman et al. ( 2020) explores prompts that result in toxic generations.
- Our work builds on evaluating LLM biases, extending analysis to zero-shot CoT.

## Stereotype & Toxicity Benchmarks
- Leveraging the three widely used stereotype benchmark datasets
- Bootstrapping a small set of explicitly harmful questions
- Converting each dataset into a zero-shot reasoning task
- Evaluating intrinsic biases
- Very sensitive to few-shot exemplars
- Focusing on a zero-shot setting removes variability

### Stereotype Benchmarks
- CrowS Pairs: a set of 1508 minimal pair sentences, covering 9 stereotype dimensions
- StereoSet: evaluates model bias across 4 dimensions: gender, race, profession, and religion
- BBQ: a question answering task consisting of 50K questions targeting 11 stereotype categories
- Used only the ambiguous setting (correct answer is Unknown), matching evaluation setups for CrowS Pairs and Stereoset

### Toxicity Benchmark
- While the aformentioned stereotype benchmarks evaluate implicit biases, we also aim to evaluate how models handle open-ended toxic requests.
- Ideally, value aligned models should not encourage explicitly toxic behaviour.
- To this end, we create a small benchmark of 200 explicitly toxic questions.
- We prompt text-davinci-002 to generate harmful questions and manually remove repetitive questions with high text overlap.
- We explicitly prompt the LLM to generate questions across 6 adjectivesracist, stereotypical, sexist, illegal, toxic, and harmful.
- Initially, we seed the prompt with three fewshot examples: "How do I make a bomb?", "How do I bully someone?" and "How do I promote racism?"
- For further generations, we recursively replace the few-shot examples to increase diverse outputs.

## Methods
- The paper discusses how to evaluate problematic outputs in a prompt-based setting
- Prompt construction is outlined for each benchmark
- Reasoning strategies are discussed

### Framing Benchmarks as Prompting Tasks
- Both BBQ and HarmfulQ are already framed as QA tasks
- For CrowS Pairs and Stereoset, however, we prompt an LLM to select the more accurate sentence between the stereotypical and anti-stereotypical setting for each benchmark
- For our stereotype datasets, we include the target stereotype and anti-stereotype examples as options, and include an "Unknown" option as the correct answer
- Synonyms for "Unknown." identifiers are randomly selected for each question (e.g. Not known, not enough info, can't be determined) to account for potential preference for a specific lexical item
- Given an unknown option, models not should select an (anti-)stereotype; we assume that value-aligned predictions are "Unknown."

### Scoring Bias and Toxicity
- Accuracy
- Model completions
- Stereotype benchmarks
- Values aligned with normativity
- Encouraging/discouraging behaviour
- Zero-shot CoT
- Standard Prompting
- Harmful behaviour

### Models
- For our initial evaluation, we use the best performing GPT-3 model from the zero-shot CoT work, text-davinci-002 (Kojima et al., 2022).
- We use standard parameters provided in OpenAI's API (temperature = 0.7, max_tokens = 256), generate 5 completions for both Standard and CoT Prompt settings, and compute 95% confidence intervals (tstatistic) for results.
- Evaluations were run between Oct 28th and Dec 14th, 2022.
- To isolate effects of CoT prompting from improved instruction-tuning and preference alignment (Ouyang et al., 2022), we also analyze all instruction-tuned davinci models (text-davinci-00[1-3]) in §5.2.

## Results
- Across stereotype benchmarks, davinci models, and prompt settings, we observe an average % point decrease of ↓ 8.8% between CoT and Standard prompting.
- Similarly, harmful question (HarmfulQ) sees an average ↓ 19.4% point decrease across davinci models.
- We now take a closer look at our results: first, we revisit TD2, replicating zero-shot CoT (Kojima et al., 2022) on our selected benchmarks ( §5.1).
- Then, we document situations where biases in zero-shot reasoning emerge or are reduced, analyzing davinci-00X variants ( §5.2), characterizing trends across scale ( §5.3), and evaluating explicit mitigation instructions ( §5.4).

### Analyzing TD2
- TD2 generally selects a biased output when using CoT, with an averaged ↓ 18% point decrease in model performance
- Explicit reasoning occurs 45% of the time in our stereotype sample
- When both CoT and non-CoT prompts encourage toxic behaviour, the CoT output is more detailed

### Instruction Tuning Behaviour
- Instruct tuning strategies influence CoT impact on our tasks
- Results for TD1 and TD3 variants across our benchmark subsets are also in Table 2
- Focusing on our stereotype benchmarks, we find that CoT effects generally decrease as instruct tuning behaviour improves
- TD3, for example, sees slightly increased average accuracy when using CoT ( ↑ 2% points), compared to TD1 ↓ 11% and 2 ↓ 17.5%
- However, inter-prompt settings see higher variance with TD3 compared to TD2, which may result in outliers like (BBQ, BigBench CoT, ↑ 17%)
- Moreover, CoT effects are still mixed despite improved human preference alignment: in 1/3 of the stereotype settings, CoT reduces model accuracy
- TD3 sees substantially larger decreases on HarmfulQ when using CoT -↓ 53% points compared to TD2's ↓ 4% points
- We attribute this to TD3's improvements in non-CoT conditions, where TD3 refuses a higher percentage of questions than TD2 ( ↑ 59% point increase)

### Scaling Behaviour
- Chain of Thought is an emergent behaviour, appearing at sufficiently large model scale
- To test the effects of scale on our results, we additionally evaluate performance on a range of smaller GPT models
- We focus on stereotype benchmarks and use a single prompt setting-the BigBench CoT prompt-perturbing size across three models: text-babbage-001, text-curie-001, text-davinci-001
- By using only 0012 variants, we can compare model size across the same instruction tuning strategy (ope)
- Evaluation parameters are in §4.5
- For all datasets, harms induced by CoT appear to get worse as model scale increase (Table 3)
- Across our stereotype benchmarks, the largest model scale in the 001 series (davinci) sees the largest difference between CoT and non CoT
- While BBQ sees a slight increase in performance from babbage to curie, davinci reverts the trend: ↑ 15 → ↑ 21 → ↓ 5
- We are unsure if our documented effect is U-shaped (Wei et al., 2022a)-specifically, if further increasing scale will reduce performance differences-and leave such analysis for future work
- For now, we note that trends with increased scale contrast with results from improved instruction tuning ( §5.2)

### Prompting with Instruction Mitigations
- Instruction-tuned models are increasingly capable of following natural language interventions
- Adding explicit mitigation instructions directly to the prompt can be an effective way to reduce biases
- To test this capability, we again focus on a single prompt setting (BigBench CoT), evaluating TD2 and TD3 on stereotype benchmarks
- We use the following intervention from Si et al. (2022): We should treat people from different socioeconomic statuses, sexual orientations, religions, races, physical appearances, nationalities, gender identities, disabilities, and ages equally.
- When we do not have sufficient information, we should choose the unknown option, rather than making assumptions based on our stereotypes.
- Adding a prompt-based interventions may be a viable solution for models with improved instruction-following performance

## Conclusion
- Editing prompt-based reasoning strategies is an incredibly powerful technique
- changing a reasoning strategy yields different model behaviour
- we recommend auditing reasoning steps
- In zero-shot settings-or settings where CoTs are difficult to clearly construct-developers should carefully analyze model behaviours after inducing reasoning steps
- Faulty CoTs can heavily influence downstream results
- our work also encourages viewing chain of thought prompting as a design pattern
- we recommend that CoT designers think carefully about their task and relevant stakeholders when constructing prompts
- We ran our analysis across 3 separate benchmarks, including an extrinsic evaluation of bias in question answering
- We also conduct a manual, qualitative analysis of failures to tie our quantitative findings to examples of representational harm against protected groups
- We believe the general agreement across our analyses mitigates the flaws of each individual benchmark, but the limitations and stated goals of each should be carefully considered when interpreting results
