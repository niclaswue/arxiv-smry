---
title: "A Natural Bias for Language Generation Models"
date: 2022-12-19T18:14:36.000Z
author: "Clara Meister, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, Adhiguna Kuncoro"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09686v1.webp" # image path/url
    alt: "A Natural Bias for Language Generation Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09686)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09686).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-natural-bias-for-language-generation-models).

# Abstract
- After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language
- Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus
- The use of such a crude heuristic raises the question: Rather than wasting precious compute resources and model capacity for learning this strategy at early training stages, can we initialise our models with this behaviour?
- Here, we show that we can effectively endow our model with a separate module that reflects unigram frequency statistics as prior knowledge. Standard neural language generation architectures offer a natural opportunity for implementing this idea

# Paper Content

## Introduction
- most people use their existing knowledge of morphology, syntax, semantics, and pragmatics to reason about which words would provide a logical and coherent continuation
- but what if you did not understand the context - for example, if you did not know the language?
- in the absence of such context, the op- timal prediction would be the language's most frequent word
- in fact, optimally one would predict each word to occur according to its (unigram) frequency
- this is precisely the behaviour we see in neural language generation models during their early training stages (Chang and Bergen, 2022)
- although this behaviour emerges early, it still takes the model several hundred (or even thousand) training updates to learn the unigram frequency distribution from a cold start
- in this paper, we show that a straightforward factorisation of a language model's final linear layer allows us to encode and integrate this unigram frequency knowledge prior to any optimisation
- with the goal of bypassing this early stage of learning
- we achieve this through a simple -yet natural and effective -approach that sets the bias term in a model's final linear layer to the log-unigram distribution of the training data
- athematically, this setup can be loosely interpreted as a modular "product of experts" (Hinton, 2002)
- our findings motivate (i) the design of more modular models which disentangle different aspects of language and (ii) augmenting models with simple prior knowledge where possible.

## Probabilistic Language Generators
- probabilistic models for language generation are parameterized by deep neural networks
- most architectural choices for these models are autoregressive and follow a local-normalisation scheme
- the output of the model is projected onto the probability simplex ∆ |V|−1 using a softmax transformation
- to generate strings from these models, one selects tokens sequentially

## A Natural Bias
- After only ∼ 1000 training updates, language models' outputs are approximately equal to the unigram distribution, regardless of the context that they condition on.
- Providing the unigram distribution as prior knowledge to the model can help bypass this frequency-learning stage early on in training.
- This unigram prior knowledge can be effectively encoded in a modular fashion within the bias term of the final, pre-softmax linear layer.

## Experiments

### Setup
- Unigram bias initialisation strategy has little to no effect on neural machine translation systems
- Parameters of the projection matrix in the final linear layer are initialised using normal random variables
- Performance is good in both higher (∼4.5M sentence pairs), medium (∼ 750K sentence pairs) and lower (∼ 150K sentence pairs) resource domains
- Preprocessing is not necessary for the unigram bias initialisation strategy

### Results
- The unigram bias initialisation technique leads to comparable or better test set performance in comparison to standard bias term initialisation techniques.
- In order to quantify training efficiency, we estimate9 the area under the validation BLEU learning curve (ALC) (Guyon et al., 2011;Liu et al., 2020) for the first 20k training updates10 for models trained with different bias initialisation strategies; for the sake of interpretability, scores are renormalised by the interval span.
- From Fig. 3, we see that, on 5 out of the 6 datasets, higher BLEU is achieved earlier on in training. Hence, our unigram bias initialisation approach appears to reach better performance in fewer iterations than standard initialisation approaches, which would be particularly beneficial in cases where training efficiency considerations are paramount (e.g., low-resource data settings or compute-limited settings).

## Discussion
- Learning or encoding the superficial statistical tendencies of language is not necessarily a bad thing
- Empirical results suggest that it may in fact be an important part of learning dynamics
- Takahashi and Tanaka-Ishii find evidence that more powerful language models may even have a natural bias for learning them
- Here we ask if -when initialising model parameters -we can explicitly endow our models with prior knowledge about one such statistical ten-11
- Models can feasibly overfit to surface cues in the dataset, which would impede their ability to generalise at inference time
- However, the relatively consistent improvement in performance (5/6 models in Fig. 2) that it offers is indeed more surprising

## Related Work
- Several prior works explore efficient and effective strategies for model weight initialisation
- We propose a simple initialisation technique intuited by learning trends observed in language generation models
- Other works have also embraced frameworks akin to product or mixture of experts in language modelling or generation tasks
- For example Neubig and Dyer (2016) combine neural and countbased language models in a mixture of experts paradigm; Baziotis et al. (2020) use a language model as a prior for low resource languages in machine translation; Li et al. (2022) cast neural language modelling as the learning of the residuals not captured by n-gram models

## Conclusion and Future Work
- In this work, we explore a simple initialisation technique for language generation models: setting the bias term in the final linear projection layer to the log-unigram distribution of (sub)words within the training corpus.
- We find that this strategy leads to more efficient training; perhaps more surprisingly, it also leads to better overall performance in our experiments on machine translation.
- We offer an analysis and discussion as to the cause of these trends.
- An interesting direction for future work could be determining the effects that this initialisation procedure has on various model properties, e.g., its embedding space, and its benefits specifically in low-resource settings, i.e., where limited training data is available.
- Furthermore, extensions of this work could explore potential uses of this strategy in the mitigation of problems with lexically infrequent words, e.g., by analysing via the decomposition in eq. ( 4) whether a model's probability estimate for a word is being driven by frequency or contextual components.
- Finally, this technique is not limited to models of distributions over strings: it is applicable in any classification setting. Thus, future work could investigate whether this technique is similarly effective in such settings.
