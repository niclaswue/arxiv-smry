---
title: "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models"
date: 2022-12-16T19:58:52.000Z
author: "Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, Shiyu Chang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08698v1.webp" # image path/url
    alt: "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08698)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08698).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/uncovering-the-disentanglement-capability-in).

# Abstract
- Generative models have been widely studied in computer vision
- Recently, diffusion models have drawn substantial attention due to the high quality of their generated images
- A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content
- Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network
- In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., "a photo of person") to one with style (e.g., "a photo of person with smile") while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.

# Paper Content

## Introduction
- Image generation has been a widely-studied research problem in computer vision, with many competitive generative models proposed over the last decade, such as generative adversarial networks (GANs) [5,10,18,30,32] and variational autoencoders (VAE) [39,57,59,60].
- Recently, diffusion models [23,[71][72][73], with their ability to gener- ate high-quality and high-resolution images in different domains, have soon attracted wide research attention.
- One important research direction regarding image generative models is the ability to disentangle different aspects of the generated images, such as semantic contents and styles, which is crucial for image editing and style transfer.
- A generative model with a good disentanglement capability should satisfy the following two desirable properties. First, it should permit separate modification of one aspect without changing other aspects. As an example shown in Fig. 2, in text-to-image generation, when the text input changes from "a photo of person" to "a photo of person with smile", the generative model should have the ability to modify just the expression of the person (i.e., from the top image to middle image in Fig. 2) without changing the person's identity (the bottom image in Fig. 2). Second, the parameters learned from modifying one image should transfer well to other similar images.
- Previous studies have discovered that GANs are inherently endowed with a strong disentanglement capability. Specifically, it is found that there exist certain directions in the latent space separately controlling different attributes. Therefore, by identifying these directions, e.g., via principal component analysis [19], GAN can achieve effective disentanglement without any re-training or fine-tuning.
- On the other hand, such an inherent disentanglement capability has yet to be found in diffusion models. Hence come our research questions: Do diffusion models also possess a disentanglement capability with the aforementioned nice properties? If so, how can we uncover it?
- In this paper, we seek to answer these research questions. Our finding is that for stable diffusion model [61], one of the diffusion models that can generate images based on an input text description, disentangled image modifications can be achieved by partial modifications in the text embedding space. In particular, if we fix the standard Gaussian noises introduced in the denoising process, and partially change the input text embedding from a neutral description (e.g., "a photo of person") to one with style (e.g., "a photo of person with smile"), the generated image will also shift towards the target style without changing the semantic content.
- Based on this finding, we further propose a simple, light-weight algorithm, where we optimize the mixing weights of the two text embeddings under two objectives, a perceptual loss for content preservation and a CLIP-based style matching loss. The entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model. Our experiments show that the inherent disentanglement capability in stable diffusion model can already disentangle a wide range of concepts and attributes, ranging from global styles such as painting styles to local styles like facial expressions, as shown in Table 1.
- As shown in Fig. 1, by learning the optimal mixing weights of the two descriptions, stable diffusion models can generate convincing image pairs that only modify the target attribute, and the optimal weights can generalize well to different images. The experiment results also show that our proposed image editing algorithm, without fine-tuning the diffusion model, can match or outperform the more sophisticated diffusion-model-based image-editing baselines that require fine-tuning.

## Related Works
- The ability to disentangle different attributes is a key desired property of generative models.
- Previous work that studies disentanglement mainly aims to learn parameters that allow modifications on a target aspect without changing other aspects, and the learned parameters should generalize to different images.
- For pre-trained GANs, it has been shown that the disentanglement can be achieved by moving towards particular directions in its latent space.
- Multiple methods have been proposed to discover these latent directions, which leverage auxiliary classifiers, principal component analysis, contrastive learning, and information maximization.
- Besides GANs, disentanglement has also been studied in VAE and flow-based models.
- Recently, two works study disentanglement in diffusion models. The first work disentangles attributes by learning a shift in the embedding space of an intermediate layer of U-Net, such that applying the shift satisfies the disentanglement criteria.
- The second work [54] trains an encoder to generate an image-specific representation, which is later used as input to diffusion models to reconstruct the original image.
- Disentanglement is done by finding corresponding directions in this representation space similar to methods in GANs.
- However, their method requires re-training a diffusion model from scratch, whereas we fix the pre-trained diffusion model.
- Diffusion Models: Diffusion models are a family of generative models that have achieved state-of-the-art performance in image synthesis and have advanced research in super-resolution, inpainting, density estimation, video synthesis, and areas beyond computer vision.
- Building on top of diffusion models, various methods have been proposed to control the generation process through external models or additional inputs.
- One type of conditional generation models is the text-to-image diffusion models, which take text descriptions as inputs and generate images that match the text descriptions.
- Due to the expressiveness of text and superior generation quality of diffusion models, these models allow unprecedented control over generated images and have inspired many novel applications.
- Image Editing: Image editing is a widely-studied task. Many GAN-based editing works have demonstrated strong controllability.
- Recently, diffusion models have been broadly adapted to image editing task.
- With the CLIP encoder [55] that bridges text and image, generation process can be guided by arbitrary text descriptions.
- To preserve the contents in a local region, [8] relies on an auxiliary mask, such that contents in the unmasked region are largely kept unchanged during generation.
- Moreover, some works propose to invert the input image to find text embeddings that can synthesize the same object but in different scenes and views.
- Although these works have demonstrated successful edits, there are two limitations. First, most of the methods require fine-tuning diffusion models. For each editing task, they have to fine-tune and store the whole diffusion model, making them unscalable to a large amount of edits. Second, many methods rely on auxiliary inputs such as image masks [4,7,8] or multiple examples of the edited object [16,63], which are not always available.
- Besides, only editing masked region may cause incoherence between masked and unmasked regions.

## Attribute Disentanglement in Stable Diffusion Models
- Disentanglement properties inherent in diffusion models can be used to disentangle image modifications
- Proposed approach for disentangled image modification and editing utilizing these properties.

### Preliminaries on Diffusion Models
- The denoising diffusion implicit model (DDIM) is a model used in the stable diffusion model which is conditioned on text descriptions.
- The goal of the text-conditioned DDIM is to generate an image that conforms to the text description.
- The denoising process of DDIM tries to denoise from XT all the way back to X0.
- In most common settings, generating one image only requires one text description.

### The Disentanglement Properties
- The stable diffusion model is inherently capable of disentangling styles from semantic content
- This capability is triggered by partially replacing the text embeddings

### Optimizing for Disentanglement
- In Sec. 3.2, we have seen one way of realizing a disentangled generation of image using c (0) and c (1) .
- However, this is not necessarily the optimal way of combining the two embeddings in terms of disentanglement.
- In this subsection, we will propose a principled and tractable optimization scheme to combine a given pair of c (0) and c (1) to achieve the best disentanglement.
- The key relaxation for the optimization framework is the soft combination of the text embeddings.
- Specifically, instead of feeding either c (0) or c (1) at each denoising step t, we feed a soft combination of the two, namely ct = λtc (1) where λt is a learnable combination weight.
- The soft combination offers a much richer representation power, with both cases discussed in Sec. 3.2 being its special cases (by setting λt to either 1 or 0).
- Given a random noise XT and the text description pair c (0) and c (1) , the optimization procedure for λ1:T is as follows.
- First, two images are generated, a style-neutral one generated with c (0) and the other generated with c (λ) 1:T :
- Our goal is to find an optimal λ1:T such that X (λ) 0 maintains the same semantic content as X (0) 0 but conforms to the style described in c (1) , which is achieved by solving the following optimization problem similar to [41,53]: β is the hyperparameter that balances the two loss terms.
- Lclip is the directional CLIP loss [17] that encourages the image change from X (0) 0 to X (λ) 0 matches the text change from c (0) to c (1) in the CLIP embedding space.
- 1 Lperc is the perceptual loss [29] that prevents drastic changes in semantic content: where h(•) denotes a perceptual network that encodes a given image.

### Extension to Image Editing
- The disentangled image modification algorithm
- The disentangled image editing setting
- The neutral image is now externally given
- If we could find a value for initial random variable XT such that the stable diffusion model can generate exactly the same image as I when conditioned on c (0) , we can then use the same approach in Sec. 3.3 for the image editing task
- To this end, we adapt the image inversion approach proposed in [20,35]
- The re-diffusion approach in [50] is adopted to further enhance the quality of the edited image.

## Experiments
- We will perform experiments to explore the inherent disentanglement capability in the stable diffusion model
- For all experiments, we use the diffusion model stable-diffusion-v1-4 [61], which is pre-trained on laion dataset [67]
- The pretrained model is frozen throughout all experiments, and we keep the default hyperparameters of the model
- All images generated by our method are in size 512 × 512
- We use a variant of the DDIM sampler [44] to synthesize images with 50 total backward diffusion steps
- To optimize λ1:T , we use Adam [37] optimizer with learning rate 0.03
- To balance two loss terms, we set β to 0.05 for all human face experiments and 0.03 for all experiments on scenes and buildings
- When editing real images, the number of re-diffusion steps is 20

### Exploring the Disentanglement Capability
- The stable diffusion model can inherently disentangle well
- The method is strong at disentangling global styles that cover largely the whole image
- The method can also disentangle many local attributes like facial expressions

### Evaluation on Disentangled Image Editing
- Our proposed method outperforms the baseline in almost all three aspects
- Our method is less competitive in human-related editing, underperforming the baseline in two attributes
- Compared with other baselines, our method arguably changes slightly more semantic content

### Ablation Study
- The optimal λ1:T depends on specific values of c (0) , c (1) , and X T .
- To check the robustness against c (0) and c (1) in the image editing task, we generate different edited images from the same source image by varying the text descriptions.
- We study three types of variations. First, we study how the way that c (1) appends the target attribute description can affect the results. Specifically, we fix c (0) and the target attribute, and generate three c (1) 's with different ways to concatenate the target attribute: direct concatenation, concatenation separated by a comma, and concatenation separated by "with", as illustrated in the top row of Fig. 7. As can be seen, the resulting edit images are hardly impacted by such variations.
- Second, we study whether varying the complexity of the target attribute description in c (1) would affect the results. To this end, we again fix c (0) and gradually add more modifiers describing the same target attribute to c (1) , as shown in the second row of Fig. 7. As can be seen, while having more modifiers can amplify the editing effects, having one  modifier is sufficient to generate an effective edit.
- Finally, we study whether variations in c (0) would affect the results. Specifically, we fix the target attribute description and generate three versions of c (0) that describe the same source object, a short description, a longer description by adding non-informative words, and a description generated by an image captioning model [75]. As shown in the bottom row of Fig. 7, the image editing is largely consistent in almost all cases, but fails with the c (0) generated by the image captioning model. One possible cause is that the generated caption is usually lengthy and contains more details, which may overwhelm the target attribute description.
- More image examples are shown in Appendix G, and the overall conclusion is that the edited results are highly robust against most variations in the text descriptions.
- We also perform some experiments that show strong robustness against variations in the images on which λ1:T are learned, which is presented in Appendix H.

## Conclusion
- The stable diffusion model inherently has the disentanglement capability
- This capability can be triggered by partially replacing the text embedding from a style-neutral description to one with desired style
- We propose a simple and light-weight disentanglement algorithm where the combination weights of the two text embeddings are optimized for style matching and content preservation
- With only 50 parameters being optimized, our method demonstrates generalizable disentanglement ability and outperforms sophisticated baselines that require fine-tuning on image editing task
- To help reproduce our results, we report the detailed hyperparameter settings and model architectures used in this work in Table 2
- In terms of optimization, we use different loss balancing weight β and different initialization of combination weights λt for attributes on person and scenes, since edits on person require more strict content preservation than edits on scenes
- Specifically, when editing person, we adopt a larger β on perceptual loss and initialize λt such that c1:T is more similar to the style-neutral description c (0) . Both these settings encourage content preservation when disentangling attributes on person
- For the directional CLIP loss, we use the pre-trained ViT-B/32 [58] . For the model used to compute perceptual loss, we adopt pretrained VGG-16 [29] . The stable diffusion model we use is the pre-trained stable-diffusion-v1-4 [4] .
- We use all pre-trained models without changing any parameters
- In this section, we provide the exact text descriptions we use to disentangle target attributes and perform image editing in this paper
- For each target attribute or edit, the text description consists of a style-neutral description and a description with explicit styles, whose embeddings are denoted as c (0) and c (1) respectively
- For brevity, we use these notations to represent their corresponding text descriptions and list them in Table 3
- Each attribute in the table also provides the corresponding figure that demonstrates the visual effects
- We emphasize that our method is generally robust to the choice of text descriptions and is not restricted to the text listed here
- We compare the performance of our method with DIF-FUSIONCLIP [35] on the image editing task
- Specifically, we consider two existing datasets used by DIFFUSION-CLIP: Celeb-A [71] that focuses on person and LSUNchurch [76] that focuses on churches
- For each dataset, we select the first 20 images as source images 2 and evaluate 4 types of edits used in [35], i.e., tanned, male, sketch, and pixar for human faces, and golden, wooden, snowy, and red brick for churches
- We compare our editing results with the results generated by the official checkpoints of DIFFUSION-CLIP that are fine-tuned for each target edit
