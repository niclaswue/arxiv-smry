---
title: "What Estimators Are Unbiased For Linear Models?"
date: 2022-12-29T06:19:44.000Z
author: "Lihua Lei, Jeffrey Wooldridge"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14185v1.webp" # image path/url
    alt: "What Estimators Are Unbiased For Linear Models?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14185)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14185).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/what-estimators-are-unbiased-for-linear).

# Abstract
- The recent thought-provoking paper by Hansen [2022, Econometrica] proved that the Gauss-Markov theorem continues to hold without the requirement that competing estimators are linear in the vector of outcomes.
- Despite the elegant proof, it was shown by the authors and other researchers that the main result in the earlier version of Hansen's paper does not extend the classic Gauss-Markov theorem because no nonlinear unbiased estimator exists under his conditions.
- To address the issue, Hansen [2022] added statements in the latest version with new conditions under which nonlinear unbiased estimators exist.

# Paper Content

## Introduction
- The Gauss-Markov theorem is a central theorem in econometrics theory
- The Gauss-Markov theorem states that the OLS estimator is the best linear unbiased estimator (BLUE) when the variance-covariance has a scalar form
- The result was later generalized by Aitken [1935], who proved that the generalized least squares (GLS) estimator is BLUE under a linear model with the covariance matrix known up to a multiplicative constant
- Both efficiency results are proved within the class of linear estimators, raising the possibility that there might exist unbiased estimators nonlinear in the response variable that outperform the OLS/GLS estimators
- To the best of our knowledge, the problem of finding unbiased, nonlinear estimators with smaller variances was first studied by Theodore W. Anderson in the early 1960s [Anderson, 1962]
- Our results (Theorem 3.3 and Theorem 3.4) substantially generalize Pötscher and Preinerstorfer [2022], Portnoy [2022], andKoopmann [1982] by allowing the coefficients and covariance matrix to take only a finite number of values, the higher moments of the estimator and the dependent variable to exist, and the error distribution to be discrete, absolutely continuous, or dominated by another probability measure
- They are based on (a slightly stronger version of) a generic result in Rüschendorf [1987], which unifies and generalizes a line of work on unbiasedness and completeness [e.g. Halmos, 1946, Fraser, 1954, Hoeffding, 1977, Fisher, 1982, Koopmann, 1982]
- Roughly speaking, it states that the class of unbiased estimators of zero under a class of distributions defined by a set of moment conditions are linear combinations of these moments

### Families of linear models
- Linear models: We formally define several families of linear models that have been investigated in the literature and will be studied in this paper.
- The standard (fixed-design) linear model can be characterized as often with r = 2 and sometimes with r = 1 [e.g. Jensen, 1979].
- In the econometrics literature, the covariance matrix is often assumed to be known or estimable.
- We further define the stratum F r (X; β, Λ) of F r (X; β) by fixing the covariance matrix Λ, i.e., Note that F r (X; β, Λ) = F 2 (X; β, Λ) for any r ∈ [1, 2] and, for any r ≥ 2, where 0 k×k denotes the k × k zero matrix and A B (resp. ≻) iff A − B is positive semidefinite (resp. positive definite).
- Using this notation, the linear model considered in Aitken's theorem can be expressed as In particular, the sets F 0 2 and F 2 defined in Hansen [2022] are equivalent to F In 2 (X), where I n denotes the n×n identity matrix, and Σ 0 k×k F Σ 2 (X) = F 2 (X) in our notation; see Remark 2.1 of Pötscher and Preinerstorfer [2022] for a similar clarification.
- All above classes only restrict the moments of Y . Sometimes one is willing to impose further independence assumptions on the error terms Y − Xβ. Specifically, let Similarly, we can further restrict the errors to be identically distributed: Analogous to ( 2) and ( 4), we can define ) gives all linear models with homoskedastic and independent errors (resp. with i.i.d. errors). All have been previously studied in the literature [e.g., Anderson, 1962, Gnot et al., 1992, Hansen, 2022] and will be examined in later sections.
- Sometimes we further restrict the above model classes by intersecting them with a dominating class: or a subset of P(µ) that only includes distributions with bounded Radon-Nikodym derivatives: Note that the set of absolutely continuous distributions in R n , denoted by P cont , is given by P(µ) with µ chosen as the Lebesgue measure 2 . Furthermore, as with Pötscher and Preinerstorfer [2022] and Portnoy [2022], we also consider the class of discrete distributions 3 :

### Unbiased estimators for fixed-design linear models
- Unbiased estimators exist only if β is identifiable.
- A sufficient and necessary condition, which will be assumed throughout, is rank(X) = k ≤ n, 2 P(µ) would include mixed distributions if µ is mixed.
- All results in the paper continue to hold if we redefine P disc to include discrete distributions with a countable support.
- In which case the coefficient β can be identified as where ′ denotes the transpose.
- For any r ≥ 1 and family of linear models F(X) with a full-rank X, denote by U r (F(X)) as the class of estimators (i.e., measurable functions of Y ) that are unbiased for β with finite r-th moments under every where L r (F ) denotes the set of measurable functions that have finite r-th moments under F .
- The recent work [Hansen, 2022, Pötscher and Preinerstorfer, 2022, Portnoy, 2022] studied the special case r = 1.
- Clearly, U r (•) satisfies the following monotonicity property, which will be repeatly invoked.
- Proposition 2.1. In the following, we will consider the class of linear estimators and the class of linear unbiased estimators (LUE):
- It is well-known that all LUE estimators are unbiased.
- Proposition 2.2. For any r ≥ 1, let F r (X) be defined in (2). Then,
- Proof. For any F ∈ F(X) and By Rosenthal's inequality [Rosenthal, 1970],
- Most recent discussions about Hansen [2022] are essentially about when U 1 (F 1 (X)) ⊂ H n,k lin .
- The following result shows that whenever it is true, the LUE estimators represent all unbiased estimators.
- Proposition 2.3. For any r ≥ 1 and F(X) ⊂ F r (X) such that span{β : By Rosenthal's inequality [Rosenthal, 1970], A ′ Y ∈ L r (F ) for any A. Since {β : F ∈ F(X)} spans R k , we must have I k − A ′ X = 0 k×k and hence u ∈ H LUE (X).
- The proof is then completed by Proposition 2.2.
- The main result of Pötscher and Preinerstorfer [2022] can be paraphrased as follows. Theorem 2.1 (Pötscher and Preinerstorfer [2022], Theorem 3.4; see also Portnoy [2022]). Together with Proposition 2.3, Theorem 2.1 implies that only LUE estimators can be unbiased under linear models without further constraints on the covariance matrix, even if the model only involves discrete distributions with finite second moments, i.e., see also the footnote 7 of Pötscher and Preinerstorfer [2022].
- While only linear estimators can be unbiased under F 2 (X), the conclusion fails for F I k 2,iid (X).
- Proposition 2.4 (Hansen [2022], remark above Theorem 5). Hansen [2022] provides a simple example that adds a mean-zero term, Y i multiplied by the leave-i-thobservation-out residual, onto the OLS estimator.
- Theorem 2.1 also fails if further assumptions are imposed on the covariance matrix.
- Consider the class of LPQ estimators: where R n×n sym is the set of all symmetric matrices in R n×n .
- This class was first studied by Anderson [1962] who showed that, in the model class F I k 2,iid , an LPQ estimator with appropriately chosen B j 's can have a smaller variance for estimating a specific linear contrast of β than the best LUE...

### Revisiting B(L)UE for fixed-design linear models

### A master theorem
- For any class of fixed-design linear models F(X) ⊂ F r (X), let
- Since βOLS is linear, by Rosenthal's inequality [Rosenthal, 1970],
- Thus, where ⊗k denotes the k-th order tensor product, and thus,
- As a consequence, to find all unbiased estimators of β, it suffices to characterize all unbiased estimators of 0. It is not hard to see that both F r (X; β) and F r (X; β, Λ) can be formulated as where and x ′ i denotes the i-th row of X.
- Intuitively, if we treat F as an element in the "dual space" of real-valued measurable functions, though this is incorrect in the rigorous sense, F(X) is the orthogonal complement of G in the "dual space" with respect to the bilinear form (u, ) can be regarded as the orthogonal complement of F(X).
- Then we should expect, in an unrigorous sense, that U 0 r (F(X)) = (G ⊥ ) ⊥ , which is the closure of the span of G in an appropriately defined normed vector space.
- This intuitive argument, together with (20), suggests that all unbiased estimators under F r (X; β) are linear and those under F r (X; β, Λ) are LPQ, hence implying the results by Pötscher and Preinerstorfer [2022], Portnoy [2022], and Koopmann [1982].
- We formalize the above heuristics in the following theorem.
- Below, for a given class of distributions F, we write L r (F) for F ∈F L r (F ). Moreover, for any subsets of functions G and H, we say G ⊂ H almost surely under F if for each g ∈ G there exists h ∈ H such that g − h = 0 almost everywhere under F , and say G = H if G ⊂ H and H ⊂ G almost surely under F .
- Theorem 3.1 (slight generalization of Theorem 1 of Rüschendorf [1987]).
- When G only contains a finite number of functions, as in (20), it is well-known that span(G) is closed (see Proposition A.2).
- Then Theorem 3.1 implies that
- On the other hand, for any

### Unbiased estimators without second moment constraints
- Lemma 3.1: For any r ≥ 1, all unbiased estimators must be linear.
- Theorem 3.2: For any r ≥ 1, all unbiased estimators must be linear if the domain of β has only two distinct values with the same direction and the error distribution is restricted into a dominating class.
- Proposition 2.1: For any r ≥ 1, all unbiased estimators must be linear if F is a probability measure in F(X).
- Proposition 2.2: For any r ≥ 1, for any subset P of F(X), all unbiased estimators must be linear.

### Unbiased estimators with second moment constraints
- Next, we will consider the case where the covariance matrix is constrained.
- Surprisingly, even if Cov(Y ) can only take two distinct values that differ by a multiplicative constant and β can only take three distinct values pointing to the same direction, any unbiased estimator must be LPQ.
- Further, let F be any probability measure in F(X) such that LPQ almost surely under F.
- Proof. Throughout this proof, we will write = and ⊂ for equality and subsets almost surely under F .
- By Proposition 2.1, Without loss of generality, assume that F ∈ F r (X; c 1 β * , σ 2 1 Σ).
- Taking any u ∈ U 0 r (F(X)), we must have u ∈ U 0 r (F r (X; c 1 β * , σ 2 1 Σ)).
- By (20) and Theorem 3.2, there exists a i , B iℓ ∈ R such that We can enforce B iℓ = B ℓi by replacing both with ( B jℓ Σ jℓ = 0. Thus, u can be simplified into Again, since u ∈ U 0 r (F(X)), for any j = 2, 3, u ∈ U 0 r (F r (X; c j β * , σ 2 1 Σ)∩P(F )).
- Choose F j ∈ F r (X; c j β * , σ 2 1 Σ)∩ P(F ), which is non-empty. Then Then u(y) can be further simplified into Therefore, We can then generalize Koopmann's representation theorem (Theorem 2.2).
- Throughout the rest of the paper, we will define the vectorization of any symmetric matrix Ω ∈ R k×k sym as vec * (Ω) = (Ω 11 / √ 2, Ω 12 , . . . , Ω 2k , . . .
- By definition, for any For any r ≥ 1, Σ ≻ 0 k×k , and (a) For any probability measure (b) Let µ be the Lebesgue measure on R k . Then U r (F(X) ∩ P cont ) = H Σ Km (X) almost surely under µ.
- Let P disc be defined in (10). Then U r (F(X) ∩ P disc ) = H Σ Km (X).

### A new nontrivial case where OLS/GLS is BUE
- The powerful representation theorem for U r (F Σ 2r (X) ∩ P) discussed in Theorem 3.4 allows us to show that the OLS/GLS estimator is BUE in a sufficiently rich class that does not rule out quadratic estimators with respect to a subset of F Σ 2r (X).
- Theorem 3.6 states that, given any r ≥ 1 and Σ ≻ 0 k×k , let V DV ′ be the eigen-decomposition of Σ and where and βΣ.
- Since C I k includes all diagonal matrices, we must have B j,ii = 0, i = 1, . . . , n, j = 1, . . . , k; that is, all diagonal elements of B j must be zero.
- Next, we prove that βOLS (Y ) is BUE in U r (F(X)) with respect to F I k 2r (X)∩G I k 3 (X).
- For any u ∈ U r (F(X)), it suffices to show that, By ( 28), we can write u(y) as where A ′ X = I k , X ′ B j X = 0 k×k , and B j,ii = 0 for all i = 1, . . . , n.
- Let ǫ = Y − Xβ. Recall that we assume that Σ = I k without loss of generality. By definition of F I k 2r (X) in (4), for any F ∈ F(X), Cov F [ǫ] = σ 2 I k for some σ 2 > 0, and, analogous to (31), Then for any j = 1, . . . , k, Consider any B with zero diagonal elements.
- For any i = 1, . . . , n, let z ′ i be the i-th row of (X ′ X) −1 X ′ . Then Since F ∈ G I k 3 (X), for any m, a, b with a = b, Putting ( 33) -( 35) together, we prove our goal (32) and thus βOLS (Y ) is BUE.

## Families of linear models
- Linear models: We formally define several families of linear models that have been investigated in the literature and will be studied in this paper.
- The standard (fixed-design) linear model can be characterized as often with r = 2 and sometimes with r = 1 [e.g. Jensen, 1979].
- In the econometrics literature, the covariance matrix is often assumed to be known or estimable.
- We further define the stratum F r (X; β, Λ) of F r (X; β) by fixing the covariance matrix Λ, i.e., Note that F r (X; β, Λ) = F 2 (X; β, Λ) for any r ∈ [1, 2] and, for any r ≥ 2, where 0 k×k denotes the k × k zero matrix and A B (resp. ≻) iff A − B is positive semidefinite (resp. positive definite).
- Using this notation, the linear model considered in Aitken's theorem can be expressed as In particular, the sets F 0 2 and F 2 defined in Hansen [2022] are equivalent to F In 2 (X), where I n denotes the n×n identity matrix, and Σ 0 k×k F Σ 2 (X) = F 2 (X) in our notation; see Remark 2.1 of Pötscher and Preinerstorfer [2022] for a similar clarification.
- All above classes only restrict the moments of Y . Sometimes one is willing to impose further independence assumptions on the error terms Y − Xβ. Specifically, let Similarly, we can further restrict the errors to be identically distributed: Analogous to ( 2) and ( 4), we can define ) gives all linear models with homoskedastic and independent errors (resp. with i.i.d. errors). All have been previously studied in the literature [e.g., Anderson, 1962, Gnot et al., 1992, Hansen, 2022] and will be examined in later sections.
- Sometimes we further restrict the above model classes by intersecting them with a dominating class: or a subset of P(µ) that only includes distributions with bounded Radon-Nikodym derivatives: Note that the set of absolutely continuous distributions in R n , denoted by P cont , is given by P(µ) with µ chosen as the Lebesgue measure 2 . Furthermore, as with Pötscher and Preinerstorfer [2022] and Portnoy [2022], we also consider the class of discrete distributions 3 :

## Unbiased estimators for fixed-design linear models
- Unbiased estimators exist only if β is identifiable.
- A sufficient and necessary condition, which will be assumed throughout, is rank(X) = k ≤ n, 2 P(µ) would include mixed distributions if µ is mixed.
- All results in the paper continue to hold if we redefine P disc to include discrete distributions with a countable support.
- In which case the coefficient β can be identified as where ′ denotes the transpose.
- For any r ≥ 1 and family of linear models F(X) with a full-rank X, denote by U r (F(X)) as the class of estimators (i.e., measurable functions of Y ) that are unbiased for β with finite r-th moments under every where L r (F ) denotes the set of measurable functions that have finite r-th moments under F .
- The recent work [Hansen, 2022, Pötscher and Preinerstorfer, 2022, Portnoy, 2022] studied the special case r = 1.
- Clearly, U r (•) satisfies the following monotonicity property, which will be repeatly invoked.
- Proposition 2.1. In the following, we will consider the class of linear estimators and the class of linear unbiased estimators (LUE): It is well-known that all LUE estimators are unbiased.
- Proposition 2.2. For any r ≥ 1, let F r (X) be defined in (2). Then,
- Proof. For any F ∈ F(X) and By Rosenthal's inequality [Rosenthal, 1970], Most recent discussions about Hansen [2022] are essentially about when U 1 (F 1 (X)) ⊂ H n,k lin .
- The following result shows that whenever it is true, the LUE estimators represent all unbiased estimators.
- Proposition 2.3. For any r ≥ 1 and F(X) ⊂ F r (X) such that span{β : By Rosenthal's inequality [Rosenthal, 1970], A ′ Y ∈ L r (F ) for any A. Since {β : F ∈ F(X)} spans R k , we must have I k − A ′ X = 0 k×k and hence u ∈ H LUE (X).
- The proof is then completed by Proposition 2.2.
- The main result of Pötscher and Preinerstorfer [2022] can be paraphrased as follows. Theorem 2.1 (Pötscher and Preinerstorfer [2022], Theorem 3.4; see also Portnoy [2022]). Together with Proposition 2.3, Theorem 2.1 implies that only LUE estimators can be unbiased under linear models without further constraints on the covariance matrix, even if the model only involves discrete distributions with finite second moments, i.e., see also the footnote 7 of Pötscher and Preinerstorfer [2022].
- While only linear estimators can be unbiased under F 2 (X), the conclusion fails for F I k 2,iid (X).
- Proposition 2.4 (Hansen [2022], remark above Theorem 5). Hansen [2022] provides a simple example that adds a mean-zero term, Y i multiplied by the leave-i-thobservation-out residual, onto the OLS estimator.
- Theorem 2.2 also fails if further assumptions are imposed on the covariance matrix.
- Consider the class of LPQ estimators: where R n×n sym is the set of all symmetric matrices in R n×n .
- This class was first studied by Anderson [1962] who showed that, in the model class F I k 2,iid , an LPQ estimator with appropriately chosen B j 's can have a smaller variance for estimating a specific linear contrast of β than the best LUE estimator, unless n =...

## Revisiting B(L)UE for fixed-design linear models

## A master theorem
- For any class of fixed-design linear models F(X) ⊂ F r (X), let
- Since βOLS is linear, by Rosenthal's inequality [Rosenthal, 1970],
- Thus, where ⊗k denotes the k-th order tensor product, and thus,
- As a consequence, to find all unbiased estimators of β, it suffices to characterize all unbiased estimators of 0. It is not hard to see that both F r (X; β) and F r (X; β, Λ) can be formulated as where and x ′ i denotes the i-th row of X.
- Intuitively, if we treat F as an element in the "dual space" of real-valued measurable functions, though this is incorrect in the rigorous sense, F(X) is the orthogonal complement of G in the "dual space" with respect to the bilinear form (u, ) can be regarded as the orthogonal complement of F(X).
- Then we should expect, in an unrigorous sense, that U 0 r (F(X)) = (G ⊥ ) ⊥ , which is the closure of the span of G in an appropriately defined normed vector space.
- This intuitive argument, together with (20), suggests that all unbiased estimators under F r (X; β) are linear and those under F r (X; β, Λ) are LPQ, hence implying the results by Pötscher and Preinerstorfer [2022], Portnoy [2022], and Koopmann [1982].
- We formalize the above heuristics in the following theorem.
- Below, for a given class of distributions F, we write L r (F) for F ∈F L r (F ). Moreover, for any subsets of functions G and H, we say G ⊂ H almost surely under F if for each g ∈ G there exists h ∈ H such that g − h = 0 almost everywhere under F , and say G = H if G ⊂ H and H ⊂ G almost surely under F .
- Theorem 3.1 (slight generalization of Theorem 1 of Rüschendorf [1987]).
- When G only contains a finite number of functions, as in (20), it is well-known that span(G) is closed (see Proposition A.2).
- Then Theorem 3.1 implies that
- On the other hand, for any

## Unbiased estimators without second moment constraints
- Lemma 3.1: For any r ≥ 1, all unbiased estimators must be linear.
- Theorem 3.2: For any r ≥ 1, all unbiased estimators must be linear if the domain of β has only two distinct values with the same direction and the error distribution is restricted into a dominating class.
- Proposition 2.1: For any r ≥ 1, all unbiased estimators must be linear if F is a probability measure in F(X).
- Proposition 2.2: For any r ≥ 1, for any subset P of F(X), all unbiased estimators must be linear.

## Unbiased estimators with second moment constraints
- Next, we will consider the case where the covariance matrix is constrained.
- Even if Cov(Y ) can only take two distinct values that differ by a multiplicative constant and β can only take three distinct values pointing to the same direction, any unbiased estimator must be LPQ.
- Further, let F be any probability measure in F(X) such that LPQ almost surely under F.
- Proof. Throughout this proof, we will write = and ⊂ for equality and subsets almost surely under F .
- By Proposition 2.1, Without loss of generality, assume that F ∈ F r (X; c 1 β * , σ 2 1 Σ).
- Taking any u ∈ U 0 r (F(X)), we must have u ∈ U 0 r (F r (X; c 1 β * , σ 2 1 Σ)).
- By (20) and Theorem 3.2, there exists a i , B iℓ ∈ R such that We can enforce B iℓ = B ℓi by replacing both with ( B jℓ Σ jℓ = 0. Thus, u can be simplified into Again, since u ∈ U 0 r (F(X)), for any j = 2, 3, u ∈ U 0 r (F r (X; c j β * , σ 2 1 Σ)∩P(F )).
- Choose F j ∈ F r (X; c j β * , σ 2 1 Σ)∩ P(F ), which is non-empty. Then Then u(y) can be further simplified into Therefore, We can then generalize Koopmann's representation theorem (Theorem 2.2).
- Throughout the rest of the paper, we will define the vectorization of any symmetric matrix Ω ∈ R k×k sym as vec * (Ω) = (Ω 11 / √ 2, Ω 12 , . . . , Ω 2k , . . .
- By definition, for any For any r ≥ 1, Σ ≻ 0 k×k , and (a) For any probability measure (b) Let µ be the Lebesgue measure on R k . Then U r (F(X) ∩ P cont ) = H Σ Km (X) almost surely under µ.
- Let P disc be defined in (10). Then U r (F(X) ∩ P disc ) = H Σ Km (X).

## A new nontrivial case where OLS/GLS is BUE
- The powerful representation theorem for U r (F Σ 2r (X) ∩ P) discussed in Theorem 3.4 allows us to show that the OLS/GLS estimator is BUE in a sufficiently rich class that does not rule out quadratic estimators with respect to a subset of F Σ 2r (X).
- Theorem 3.6 states that βOLS (Y ) is BUE in U r (F(X)) with respect to F I k 2r (X)∩G I k 3 (X) when Σ is diagonal.
- Neither result implies the other.

## A Proofs
- Proof of Theorem 3.1
- F r,G ∩ P B (F ) = F ∩ P B (F )
- It suffices to prove
- For any probability measure F , L ∞ (F ), the space of all almost surely bounded functions under F is the dual space of L 1 (F )
- Denote by 1 the constant function that maps any element of R n to 1, which is clearly a function in L 1 (F )
- Define the orthogonal complement of G ∪ {1} in L 1 (F ) (in the sense of Proposition A.1) as
- For any h ∈ G ⊥ (F ), h is almost surely bounded, hence there exists c > 0 such that 1 + ch ≥ 0 almost surely under F
- Let F be a probability measure with Radon-Nikodym derivative d F /dF = 1 + ch. Then 1d F = (1 + ch)dF = 1 and thus F ∈ P(F ). Moreover, for any g ∈ G, The first term is zero since F ∈ F r,G and the second term is zero since h ∈ G ⊥ (F ). This implies E F [g(Y )] = 0 for any g ∈ G. Moreover, suppose |1 + ch| ≤ B for some constant B. Then, for any g ∈ G, This entails that F ∈ F r,G . Combing two pieces together, we have proved that As a result, F r,G is a probability space.

## Since this holds for any
- The result is proved
- Let e 1 , . . . , e n be the canonical basis of R n
- For any β ∈ B, let where δ x denotes the Dirac-measure (point mass) at x
- For any y ∈ R n and β * = c 11 β * 1, let As a result, F y ∈ F(X) ∩ P disc and Fβ ∈ P(F β ) ∈ P(F y ) for any β ∈ B
- By Proposition 2.1 and the part (a), U r (F(X) ∩ P disc ) ⊂ U r (F(X) ∩ P(F y )) = H GLS (X) almost surely under F y
- Next we will prove Theorem 3.4
- For each β ∈ B ∪ −B, where−B = {−β : β ∈ B}, let F 0,β = 1 2|Y 0 | z∈Y0 (δ z + δ 2Xβ−z ).
- For any Λ ∈ {σ 2 1 Σ, σ 2 2 Σ}, since Λ ≻ 0, there exists c β,Λ ∈ (0, 1) such thatΛ ≻ c β,Λ Cov F 0,β [Y ]. Applying eigen-decomposition on Λ − c β,Λ Cov F 0,β [Y ], there exists w 1,β,Λ , . . . , w n,β,Λ ∈ R n such that Λ = c β,Λ Cov F 0,β [Y ] + 1 − c β,Λ n n i=1 w i,β,Λ w ′ i,β,Λ . Thus, F β,Λ ∈ F 2r (X; β, Λ) ∩ P disc . Fix any y ∈ R n , β * = c 11 β * 1 and Λ * = σ 2 4Xβ * − y) = Xβ * .
- Using the same argument as above, there exists c y > 0 and w 1 , . . . , w n,β,Λ ∈ (0, 1) such that c y = w 1,β,Λ + . . . + w n,β,Λ for any y ∈ R n
