---
title: "What Estimators Are Unbiased For Linear Models?"
date: 2022-12-29T06:19:44.000Z
author: "Lihua Lei, Jeffrey Wooldridge"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14185v1.webp" # image path/url
    alt: "What Estimators Are Unbiased For Linear Models?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14185)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14185).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/what-estimators-are-unbiased-for-linear).

# Abstract
- Hansen [2022] proved that the Gauss-Markov theorem holds without the requirement that competing estimators are linear in the vector of outcomes.
- Hansen [2022] added statements in the latest version with new conditions under which nonlinear unbiased estimators exist.
- Study a fundamental problem: what estimators are unbiased for a given class of linear models?
- Establish new representation theorems for unbiased estimators under different restrictions.

# Paper Content

## Introduction
- Gauss-Markov theorem states OLS/GLS estimator is BLUE
- Aitken [1935] generalized theorem to linear model with known covariance matrix
- Anderson [1962] studied problem of finding unbiased, nonlinear estimators with smaller variances
- Theorem 3.3 and 3.4 generalize Pötscher and Preinerstorfer [2022], Portnoy [2022], and Koopmann [1982]
- Rüschendorf [1987] unifies and generalizes a line of work on unbiasedness and completeness
- OLS/GLS estimator is BUE in new case analogous to Theorem 5 of Hansen [2022]

### Families of linear models
- Formally defined several families of linear models
- X is a design matrix, Y is a vector of dependent variable
- F r (X; β) is a class of distributions of Y with mean Xβ and finite r-th moment
- Standard linear model is characterized with r = 2 or r = 1
- Covariance matrix is assumed to be known or estimable
- F r (X; β, Λ) is a stratum of F r (X; β) with fixed covariance matrix
- Further restrictions on errors to be independent or identically distributed
- Dominating class or subset of P(µ) with bounded Radon-Nikodym derivatives
- Class of discrete distributions

### Unbiased estimators for fixed-design linear models
- Unbiased estimators exist only if β is identifiable.
- Rank of X must be k ≤ n.
- P(µ) includes mixed distributions if µ is mixed.
- Discrete distributions with countable support can be included.
- β is a functional of the distribution of Y.
- U r (F(X)) is the class of estimators that are unbiased for β with finite r-th moments.
- LUE estimators are unbiased.

### Revisiting b(l)ue for fixed-design linear models
- An estimator u*(Y) is BUE in a class of unbiased estimators H(X) with respect to a class of distributions F(X).
- OLS estimator βOLS is BUE if one of four conditions hold.
- Choice of H(X) is crucial - OLS/GLS can be BUE in a smaller class of estimators, but may not be optimal in a larger class.
- Aitken [1935] showed that U1(F2(X)) is not part of Hn,klin.

### A master theorem
- Fixed-design linear models can be formulated as a bilinear form
- Unbiased estimators of β can be found by characterizing all unbiased estimators of 0
- All unbiased estimators under F r (X; β) are linear
- All unbiased estimators under F r (X; β, Λ) are LPQ
- Theorem 3.1 implies that all unbiased estimators of β are linear or LPQ
- Theorem 3.2 generalizes Theorem 1A and 2A of Hoeffding [1977] and Theorem 3.1 of Koopmann [1982]

### Unbiased estimators without second moment constraints
- Hansen [2022] discusses F 2 (X) which restricts the first moment of Y
- Generalize Pötscher and Preinerstorfer [2022] and Portnoy [2022] to show U r (F(X)) = H GLS (X) even when r > 1 and F(X) is much smaller than F r (X)
- All unbiased estimators must be linear even when domain of β has two distinct values with same direction and error distribution is restricted
- Lemma 3.1 states that for any r ≥ 1, U r (F(X) ∩ P) = H GLS (X) almost surely under F
- Proposition 2.1 and Proposition 2.2 show that for any r ≥ 1, U r (F(X) ∩ P) ⊂ H GLS (X)
- Let F be multivariate normal distribution with mean X(c 11 β * 1 ) and covariance matrix I k
- U r (F(X) ∩ P cont ) = H GLS (X) almost surely under µ
- U r (F(X) ∩ P disc ) = H GLS (X)

### Unbiased estimators with second moment constraints
- Covariance matrix is constrained
- Unbiased estimator must be LPQ
- Probability measure F(X) exists
- U0r(F(X)) is defined
- Vectorization of symmetric matrix is defined
- U r (F(X) ∩ P) is defined
- Constraint on covariance matrix matters
- Quadratic estimators excluded even if domain of β and Σ are finite

### A new nontrivial case where ols/gls is bue
- The OLS/GLS estimator is BUE in a sufficiently rich class that does not rule out quadratic estimators with respect to a subset of F Σ 2r (X).
- Theorem 3.6 proves that the OLS/GLS estimator is BUE in U r (F(X)) with respect to F I k 2r (X)∩G I k 3 (X).
- Cov F [ǫ] = σ 2 I k for some σ 2 > 0.
- For any B with zero diagonal elements, βOLS (Y ) is BUE.
- Theorem 3.6 is analogous to Theorem 5 of Hansen [2022].
- Neither result implies the other.

## Families of linear models
- Formally defined several families of linear models
- X is a design matrix, Y is a vector of dependent variable
- F r (X; β) is a class of distributions of Y with mean Xβ and finite r-th moment
- Standard linear model is characterized with r = 2 or r = 1
- Covariance matrix is assumed to be known or estimable
- F r (X; β, Λ) is a stratum of F r (X; β) with fixed covariance matrix
- Further restrictions on errors to be independent or identically distributed
- Dominating class or subset of P(µ) with bounded Radon-Nikodym derivatives
- Class of discrete distributions

## Unbiased estimators for fixed-design linear models
- Unbiased estimators exist only if β is identifiable.
- Rank of X must be k ≤ n.
- P(µ) includes mixed distributions if µ is mixed.
- Discrete distributions with countable support can be included.
- β is a functional of the distribution of Y.
- U r (F(X)) is the class of estimators that are unbiased for β with finite r-th moments.
- LUE estimators are unbiased.

## Revisiting b(l)ue for fixed-design linear models
- An estimator u*(Y) is BUE in a class of unbiased estimators H(X) with respect to a class of distributions F(X).
- OLS estimator βOLS is BUE if one of four conditions hold.
- Choice of H(X) is crucial - OLS/GLS can be BUE in a smaller class of estimators, but may not be optimal in a larger class.
- Aitken [1935] showed that U1(F2(X)) is not part of Hn,klin.

## A master theorem
- Fixed-design linear models can be formulated as a bilinear form
- Unbiased estimators of β can be found by characterizing all unbiased estimators of 0
- All unbiased estimators under F r (X; β) are linear
- All unbiased estimators under F r (X; β, Λ) are LPQ
- Theorem 3.1 implies that all unbiased estimators of β are linear or LPQ
- Theorem 3.2 generalizes Theorem 1A and 2A of Hoeffding [1977] and Theorem 3.1 of Koopmann [1982]

## Unbiased estimators without second moment constraints
- Hansen [2022] discusses F 2 (X) which restricts the first moment of Y
- Generalize Pötscher and Preinerstorfer [2022] and Portnoy [2022] to show U r (F(X)) = H GLS (X) even when r > 1 and F(X) is much smaller than F r (X)
- All unbiased estimators must be linear even when domain of β has two distinct values with same direction and error distribution is restricted
- Lemma 3.1 states that for any r ≥ 1, U r (F(X) ∩ P) = H GLS (X) almost surely under F
- Proposition 2.1 and Proposition 2.2 show that for any r ≥ 1, U r (F(X) ∩ P) ⊂ H GLS (X)
- Let F be multivariate normal distribution with mean X(c 11 β * 1 ) and covariance matrix I k
- U r (F(X) ∩ P cont ) = H GLS (X) almost surely under µ
- U r (F(X) ∩ P disc ) = H GLS (X)

## Unbiased estimators with second moment constraints
- Covariance matrix is constrained
- Unbiased estimator must be LPQ
- Probability measure F(X) exists
- U0r(F(X)) is defined
- Vectorization of symmetric matrix is defined
- U r (F(X) ∩ P) is defined
- Constraint on covariance matrix matters
- Quadratic estimators excluded even if domain of β and Σ are finite

## A new nontrivial case where ols/gls is bue
- The OLS/GLS estimator is BUE in a sufficiently rich class that does not rule out quadratic estimators with respect to a subset of F Σ 2r (X).
- Theorem 3.6 proves that the OLS/GLS estimator is BUE in U r (F(X)) with respect to F I k 2r (X)∩G I k 3 (X).
- Cov F [ǫ] = σ 2 I k for some σ 2 > 0.
- For any B with zero diagonal elements, βOLS (Y ) is BUE.
- Theorem 3.6 is analogous to Theorem 5 of Hansen [2022].
- Neither result implies the other.

## A proofs
- Theorem 3.1 states that U 0 1 (F) is a subset of cl F (span(G)).
- It is sufficient to prove that when F ∈ F ∩ P B (F ), d F /dF is bounded almost surely.
- For any h ∈ G ⊥ (F ), h is almost surely bounded and there exists c > 0 such that 1 + ch ≥ 0 almost surely under F.
- For any g ∈ G, E F [g(Y )] = 0 for any g ∈ G and |1 + ch| ≤ B for some constant B.

## Since this holds for any
- There exists a finite subset Y0 of Rn such that span(zvec*(zz')) for z in Y0 = R(n+1)n/2
- For any β in B, there exists δx which denotes the Dirac-measure (point mass) at x
- For any y in Rn and β* = c11β*1, Fy in F(X) ∩ Pdisc and Fβ in P(Fβ) ∈ P(Fy) for any β in B
- U r (F(X) ∩ P disc ) ⊂ U r (F(X) ∩ P(F y )) = H GLS (X) almost surely under F y
- For any u in U r (F(X) ∩ P disc ), there exists u0,y in H GLS (X) which does not depend on y
- For any positive integer n, there exists a finite subset Y0 of Rn such that span z vec * (zz ′ ) : z ∈ Y0 = R (n+1)n/2
- For any β in B ∪ -B, there exists F0,β = 1 2|Y 0 | z∈Y0 (δ z + δ 2Xβ−z )
- For any Λ in {σ 2 1 Σ, σ 2 2 Σ}, there exists c β,Λ ∈ (0, 1) such thatΛ ≻ c β,Λ Cov F 0,β [Y ]
- For any y in Rn, β* = c11β*1 and Λ* = σ2 4Xβ*−y, there exists c y > 0 and w1, ...
