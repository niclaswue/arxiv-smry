---
title: "Point Transformer V2: Grouped Vector Attention and Partition-based Pooling"
date: 2022-10-11T17:58:03.000Z
author: "Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, Hengshuang Zhao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-05666v2.webp" # image path/url
    alt: "Point Transformer V2: Grouped Vector Attention and Partition-based Pooling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.05666)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.05666).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/point-transformer-v2-grouped-vector-attention).

# Abstract
- Point Transformer achieves impressive results on multiple highly competitive benchmarks
- In this work, we propose group vector attention, which is more effective than the previous version of vector attention
- Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer
- We also strengthen the position information for attention by an additional position encoding multiplier
- Our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks

# Paper Content

## Introduction
- PTv1 introduces the self-attention networks to 3D point cloud understanding
- PTv1 achieves remarkable performance in several 3D point cloud recognition tasks, including shape classification, object part segmentation, and semantic scene segmentation
- In this work, we analyze the limitations of Point Transformer (PTv1) [1] and propose a new elegant and powerful backbone named Point Transformer V2 (PTv2)
- Our PTv2 improves upon PTv1 with several novel designs, including the advanced grouped vector attention with improved position encoding, and the efficient partition-based pooling scheme
- The vector attention layers in PTv1 utilize MLPs as the weight encoding to map the subtraction relation of query and key into an attention weight vector that can modulate the individual channels of the value vector. However, as the model goes deeper and the number of channels increases, the number of weight encoding parameters also increases drastically, leading to severe overfitting and limiting the model depth. To address this problem, we present grouped vector attention with a more parameter-efficient formulation, where the vector attention is divided into groups with shared vector attention weights.
- Meanwhile, we show that the well-known multi-head attention [3] and the vector attention [2,1] are degenerate cases of our proposed grouped vector attention. Our proposed grouped vector attention inherits the merits of both vector attention and multi-head attention while being more powerful and efficient.
- Furthermore, point positions provide important geometric information for 3D semantic understanding. Hence, the positional relationship among 3D points is more critical than 2D pixels. However, previous 3D position encoding schemes mostly follow the 2D ones and do not fully exploit the geometric knowledge in 3D coordinates. To this end, we strengthen the position encoding mechanism by applying an additional position encoding multiplier to the relation vector. Such a design strengthens the positional relationship information in the model, and we validate its effectiveness in our experiments.
- Moreover, it is worth noting that the irregular, non-uniform spatial distributions of points are significant challenges to the pooling modules for point cloud processing. Previous point cloud pooling approaches rely on a combination of sampling methods (e.g. farthest point sampling [4] or grid sampling [5]) and neighbor query methods (e.g. kNN or radius query), which is time-consuming and not spatially well-aligned. To overcome this problem, we go beyond the pooling paradigm of combining sampling and query, and divide the point cloud into non-overlapping partitions to directly fuse points within the same partition. We use uniform grids as partition divider and achieve significant improvement.
- In conclusion, we propose Point Transformer V2, which improves Point Transformer [1] from several perspectives:
- We propose an effective grouped vector attention (GVA) with a novel weight encoding layer that enables efficient information exchange within and among attention groups.
- We introduce an improved position encoding scheme to utilize point cloud coordinates better and further enhance the spatial reasoning ability of the model.
- We design the partition-based pooling strategy to enable more efficient and spatially betteraligned information aggregation compared to previous methods. We conducted extensive analysis and controlled experiments to validate our designs. Our results indicate that PTv2 outperforms predecessor works and sets the new state-of-the-art on various 3D understanding tasks.

## Related Works
- With the great success of ViT, the absolute dominance of convolution in vision tasks is shaken by Vision Transformer, which becomes a trend in 2D image understanding.
- ViT introduces the far-reaching scaled dot-product self-attention and multi-head self-attention theory [3] in NLP into vision by considering image patches as tokens. However, operating global attention on the entire image consumes excessive memory.
- To solve the memory consumption problem, Swin Transformer [7] introduces the grid-based local attention mechanism to operate the transformer block in a sequence of shifted windows.
- Learning-based methods for processing 3D point clouds can be classified into the following types: projection-based, voxel-based, and point-based networks.
- An intuitive way to process irregular inputs like point clouds is to transform irregular representations into regular ones.
- Projection-based methods project 3D point clouds into various image planes and utilize 2D CNN-based backbones to extract feature representations [11,12,13,14].
- An alternative approach operates convolutions in 3D by transforming irregular point clouds into regular voxel representations [15,16].
- Those voxel-based methods suffer from inefficiency because of the sparsity of point clouds until the introduction and implementation of sparse convolution [17,18].
- Point-based methods extract features directly from the point cloud rather than projecting or quantizing irregular point clouds onto regular grids in 2D or 3D [19,4,20,5].
- The recently proposed transformer-based point cloud understanding approaches, introduced in the next paragraph, are also categorized into point-based methods.
- Point Transformer [1] proposed by Zhao et al. directly performs local attention between each point and its adjacent points, which alleviated the memory problem mentioned above.
- Point Transformer achieves remarkable results in multiple point cloud understanding tasks and state-of-art results for several competitive challenges.
- In this work, we analyze the limitations of the Point Transformer [1], and propose several novel architecture designs for the attention and pooling module, to improve the effectiveness and efficiency of the Point Transformer. Our proposed model, Point Transformer V2, performs better than the Point Transformer across a variety of 3D scene understating tasks.

## Point Transformer V2

### Problem Formulation and Background
- Problem formulation:
- Let M = (P, F) be a 3D point cloud scene containing a set of points x i = (p i , f i ) ∈ M, where p i ∈ R 3 represents the point position, and f i ∈ R c represents the point features.
- Point cloud semantic segmentation aims to predict a class label for each point x i , and the goal of scene classification is to predict a class label for each scene M.
- M(p) denotes a mapping function that maps the point at position p to a subset of M denoted as "reference set".
- Next, we revisit the self-attention mechanism used in PTv1 [1].
- Local attention. Conducting the global attention [6,21] over all points in a scene is computationally heavy and infeasible for large-scale 3D scenes. Therefore, we apply local attention where the attention for each point x i works within a subset of points, i.e., reference point set, M(p i ).
- Shifted-grid attention [7], where attention is alternatively applied over two sets of non-overlapping image grids, has become is a common practice [22,23,24,25] for image transformers.
- Similarly, the 3D space can be split into uniform non-overlapping grid cells, and the reference set is defined as the points within the same grid, i.e., M(p i ) = {(p j , f j ) | p j in the same grid cell as p i }.
- However, such attention relies on a cumbersome shift grid operation to achieve a global receptive field, and it does not work well on point clouds where the point densities within different grids are not consistent.
- PTv1 adopts neighborhood attention, where the reference point set is a local neighborhood of the given point, i.e., M(p i ) = {(p j , f j ) | p j ∈ Neighborhood(p i )}.
- Specifically, the neighborhood point set M(p i ) is defined as the k nearest neighboring (kNN) points of p i in PTv1.
- Our experiments (Sec. 4.3) show that neighborhood attention is more effective than shifted-grid attention, so our approach adopts the neighborhood attention.
- Scalar attention and vector attention.
- Given a point x i = (p i , f i ) ∈ M, we apply linear projections or MLPs to project the point features f i to the feature vectors of query q i , key k i , and value v i , each with c h channels.
- The standard scalar attention (SA) operated on the point x i and its reference point set M(p i ) can be represented as follows,
- The attention weights in the above formulation are scalars computed from the scaled dot-product [3] between the query and key vectors.
- Multi-head scalar attention (MSA) [3] is an extension of SA which runs several scalar attentions in parallel.
- MSA is widely applied in transformers, and we will show in Sec. 3.2 that MSA is a degenerate case of our proposed grouped vector attention.
- Instead of the scalar attention weights, PTv1 applies vector attention, where the attention weights are vectors that can modulate the individual feature channels.
- In SA, the scalar attention is computed by the scaled dot-product between the query and key vectors.
- In vector attention, a weight encoding function encodes the relation between query and key to a vector.
- The vector attention [2] is formulated as follows, where is the Hadamard product. γ is a relation function (e.g., subtraction). ω : R c → R c is a learnable weight encoding (e.g., MLP) that computes the attention vectors to re-weight v j by channels before aggregation.
- Fig. 2 (a) shows a method using vector attention with linear weight encoding.

### Grouped Vector Attention
- In vector attention, as the network goes deeper and there are more feature encoding channels, the number of parameters for the weight encoding layer increases drastically.
- The large parameter size restricts the efficiency and generalization ability of the model.
- In order to overcome the limitations of vector attention, we introduce the grouped vector attention, as illustrated in Fig. 1 (left).
- Attention groups. We divide channels of the value vector v ∈ R c evenly into g groups (1 ≤ g ≤ c).
- The weight encoding layer outputs a grouped attention vector with g channels instead of c channels.
- Channels of v within the same attention group share the same scalar attention weight from the grouped attention vector.
- Mathematically, where γ is the relation function and ω : R c → R g is the learnable grouped weight encoding defined in the next paragraph.
- The second equation in Eq. 3 is the grouped vector aggregation.
- Fig. 2 (a) presents a vanilla GVA implemented by a fully connected weight encoding, the number of the grouped weight encoding function parameters reduced compared with the vector attention (Fig. 2 (b)), leading to a more powerful and efficient model.
- GVA is a generalized formulation of VA and MSA.
- Our GVA degenerates to vector attention (VA) when g = c, and it degenerates to multi-head self-attention (MSA) if ω in Eq. 3 is defined as follows, where c g = c/g and r ∈ R 1×c .
- Grouped linear. Inspired by the weight encoding function of MSA, we design the grouped linear layer ζ(r) : R c → R g where different groups of the input vector are projected with different parameters independently.
- Grouped linear further reduce the number of parameters in the weight encoding function.
- Our final adopted grouped weight encoding function is composed of the grouped linear layer, normalization layer, activation layer, and a fully connected layer to allow inter-group information exchange.

### Position Encoding Multipler
- Different from the discrete, regular-grid pixels in 2D images, points in the 3D point cloud are unevenly distributed in a continuous Euclidean Metric space, making the spatial relationship in 3D point cloud much more complicated than 2D images.
- In transformers and attention modules, the spatial information is obtained with the position encoding δ bias (p i − p j ) added to the relation vector γ(q i , k j ) as a bias.
- Due to the generalization limitation of vector attention in PTv1 mentioned in Sec. 3.2, adding more position encoding capacity to vector attention will not help to improve the performance.
- In PTv2, the grouped vector attention has an effect of reducing overfitting and enhancing generalization.
- With grouped vector attention restricting the capacity of the attention mechanism, we strengthen the position encoding with an additional multiplier δ mul (p i − p j ) to the relation vector, which focuses on learning complex point cloud positional relations.
- As shown in Fig. 1 (left), our improved position encoding is as follows, where is the Hadamard product. δ mul , δ bias : R d → R d are two MLP position encoding functions, which take relative positions as input.

### Partition-based Pooling
- Traditional sampling-based pooling procedures adopt a combination of sampling and query methods.
- In the sampling stage, farthest point sampling or grid sampling is used to sample points reserved for the following encoding stage.
- For each sampled point, a neighbor query is performed to aggregate information from the neighboring points.
- In these sampling-based pooling procedures, the query sets of points are not spatially-aligned since the information density and overlap among each query set are not controllable.
- To address the problem, we propose a more efficient and effective partition-based pooling approach, as shown in Fig. 1.
- Pooling. Given a point set M = (P, F), we partition M into subsets [M 1 , M 2 , ..., M n ] by separating the space into non-overlapping partitions.
- We fusion each subset of points M i = (P i , F i ) from a single partition as follows, where (p i , f i ) is the position and features of pooling point aggregated form subset M i , and U ∈ R c×c is the linear projection.
- Collecting the pooling points from n subsets gives us the point set for the next stage of encoding.
- In our implementation, we use uniform grids to partition the point cloud space, and thus our partition-based pooling is also called grid pooling.
- Unpooling. The common practice of unpooling by interpolation is also applicable to partition-based pooling. Here we introduce a more straightforward and efficient unpooling method.
- To unpool the fused point set M back to M, the point locations in M are record from the pooling process, and we only need to obtain the features for each point in M.
- With the help of the grid-based partitioning [M 1 , M 2 , ..., M n ] during the pooling stage, we can map point feature to all points from the same subset, The initial feature dimension is 48, and we first embed the input channels to this number with a basic block with attention groups of 6.
- Then, we double this feature dimension and attention groups each time entering the next encoding stage.
- For the four encoding stages, the feature dimensions are [96,192,384,384], and the corresponding attention groups are [12,24,48,48].
- Output head. For point cloud semantic segmentation, we apply an MLP to map point features produced by the backbone to the final logits for each point in the input point set. For point cloud classification, we apply global average pooling over the point features produced by the encoding stages to obtain a global feature vector, followed by an MLP classifier for prediction.

## Experiments
- To validate the effectiveness of the proposed method, we conduct experimental evaluations on ScanNet v2 [44] and S3DIS [45] for semantic segmentation, and ModelNet40 [46] for shape classification.
- Implementation details are available in the appendix.
- [31] point 63.5
- 66.6 SegGCN [32] point -58.9
- RandLA-Net [33] point -64.5
- KPConv [5] point 69.2
- 68.6 JSENet [34] point -69.9
- FusionNet [35] point -68.8
- SparseConvNet [17] voxel 69.3
- 72.5 MinkUNet [18] voxel 72.2
- 73.6 PTv1

### Ablation Study
- We conduct ablation studies to examine the effectiveness of each module in our design
- The ablation study results are reported on ScanNet v2 validation set
- We study the effects of different weight encoding functions ω in Table 6
- The weight encoding functions are introduced in Sec. 3.1 and Sec. 3.2, and different attention mechanisms adopt different weight encoding functions
- We use the vanilla position encoding in PTv1 [1] and our proposed grid pooling scheme in all of the experiments in Table 6
- The results in Table 5 illustrate that our partition-based pooling achieves higher mIoU than the sampling-based method

### Model Complexity and Latency
- We conduct model complexity and latency studies to examine the superior efficiency of several design in our work.
- We record the amortized forward time for each scan in the ScanNet v2 validation set with batch size 4 on a single TITAN RTX.
- Pooling methods. Table 8 shows the forward time and mIoU of PTv2 with different pooling methods and pooling ratios.
- We compare our pooling method with two classical sampling-based pooling methods: FPS-kNN and Grid-kNN.
- FPS-kNN pooling [4,1] uses farthest point sampling (FPS) to sample a specified number of points and then queries k nearest neighbor points for pooling. We call the pooling method in Strided KPConv [5] Grid-kNN pooling, as it uses a uniform grid to sample points and then applies the kNN method to index neighbors. This leads to uncontrollable overlaps of the pooling receptive fields.
- As shown in the table, our grid pooling method is not only faster but also achieves higher mIoUs.
- Table 9 summarizes comparisons of model complexity, time consumption, and evaluation performances on ScanNet v2 validation set.
- Meanwhile, we drop the first batch of forwarding time for GPU preparation. In Table 9, GVA refers to grouped vector attention. L refers to grouped weight encoding implemented by a single Linear. GL refers to grouped weight encoding implemented by a Grouped Linear. GL-N-A-L refers to the grouped linear layer, followed by batch normalization, activation, and a linear layer as grouped weight encoding function. GP refers to partition-based pooling implemented by grid. PEM refers to the position encoding multiplier.
- To ensure fair comparison, PTv1 is set to be the same depth and feature dimensions as our model architecture of PTv2.
- By comparing experiment 1 and 2 , we can study the effect of GVA. The same spirit goes on for experiment 3 , 4 , and 5 , where each experiments adds one additional module, so that we can study the effect of the added module respectively.
- Comparing experiments 1 , 2 , 3 , and 4 , the introduction of grouped vector attention (GVA) with grouped weight encoding dramatically improves the model performance and slightly reduces execution time. The comparison between 4 and 5 indicates that the grid pooling strategy can significantly speed up the network and further enhance the generalization ability of our model.
- Position encoding multiplier is the only design that increases the number of model parameters, but experiment 6 demonstrates its effectiveness in improving performance. Meanwhile, our model is still lightweight compared to voxel-based backbones, such as MinkUNet42 [18] with 37.9M parameters.

## Conclusion
- We propose Point Transformer V2 (PTv2), a powerful and efficient transformer-based backbone for 3D point cloud understanding.
- Our work makes several non-trivial improvements upon Point Transformer V1 [1], including the grouped vector attention, improved position encoding, and partitionbased pooling.
- PTv2 model achieves state-of-the-art performance on point cloud classification and semantic segmentation benchmarks.
- We show ablation experiments on the depths of each decoder block with different unpooling methods in Table 12. Applying at least one attention block in each decoding stage can significantly improve the model performance, and this phenomenon is more evident while utilizing our mapping pooling.
- But deeper decoders (from a depth of 1 to 2 for each decoder block) do not improve performance. These phenomena are reasonable since the naive unpooling methods such as interpolation and mapping require a learnable block to optimize the sampled features.
- In 2D, deconvolution with stride is widely used to unpooling and optimize features simultaneously, and such a process usually does not require a deep network.
- Table 13 shows an additional ablation study on PE Multiplier. PE Multiplier does not work well with PTv1 since PTv1 already overfits to the training set. Adding more capacity to the PTv1 will not help improve the performance.
- On PTv2, the group vector attention (GVA) has an effect of reducing overfitting and enhancing generalization. With GVA restricting the capacity of the attention mechanism, the addition of PE Multiplier can focus on learning complex point cloud positional relations.
- PE Multiplier compliments group vector attention to achieve a good balance of network capacity.
- We compare three different feature-level pooling methods: FPS-kNN, Grid-kNN, and our partitionbased pooling implemented by grid.
- FPS-kNN pooling [4,1] uses the farthest point sampling (FPS) to sample a specified number of points and then queries k nearest neighbor points for pooling. Strided KPConv [5] uses a uniform grid to sample points and then applies the kNN method to index neighbors, leading to an uncontrollable overlapping of the pooling receptive field.
- We name this method Grid-kNN Pooling. Our method values non-overlapping receptive fields and fusion points within each non-overlapping grid cell.
- To distinguish it from the previous sampling-based method, we name it Grid Pooling.
- Benchmark with synthetic data.
- Table 14 provides benchmark results for the different pooling methods with synthetic data.
- We generate n points uniformly at random in the unit cube space. The sampling ratio r the sample size divided by the population size.
- For comparison, we keep the sampling ratio r the same for different pooling methods.
- For Grid-kNN and Grid Pooling, the grid size is computed as (n × r) − 1 3 since the point cloud is sampled uniformly at random.
- We show the combined time of pooling and uppooling for these three methods.
