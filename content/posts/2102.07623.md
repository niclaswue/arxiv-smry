---
title: "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization"
date: 2021-02-15T16:04:10.000Z
author: "Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, Qi Dou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2102-07623v2.webp" # image path/url
    alt: "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2102.07623)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2102.07623).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/fedbn-federated-learning-on-non-iid-features-1).

# Abstract
- The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data.
- In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups.
- Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence.
- Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city).
- In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments.
- These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg.

# Paper Content

## INTRODUCTION
- Federated learning (FL) has gained popularity for various applications involving learning from distributed data.
- In FL, a cloud server (the "server") can communicate with distributed data sources (the "clients"), while the clients hold data separately.
- A major challenge in FL is the training data statistical heterogeneity among the clients (Kairouz et al., 2019;Li et al., 2020b).
- It has been shown that standard federated methods such as FedAvg (McMahan et al., 2017) which are not designed particularly taking care of non-iid data significantly suffer from performance degradation or even diverge if deployed over non-iid samples (Karimireddy et al., 2019;Li et al., 2018;2020a).
- Instead, we focus on the shift in the feature space, which has not yet been explored in the literature. Specifically, we consider that local data deviates in terms of the distribution in feature space, and identify this scenario as feature shift.
- This type of non-iid data is a critical problem in many real-world scenarios, typically in cases where the local devices are responisble for a heterogeneity in the feature distributions.
- For example in cancer diagnosis tasks, medical radiology images collected in different hospitals have uniformly distributed labels (i.e., the cancer types treated are quite similar across the hospitals). However, the image appearance can vary a lot due to different imaging machines and protocols used in hospitals, e.g., different intensity and contrast.
- In this example, each hospital is a client and hospitals aim to collaboratively train a cancer detection model without sharing privacy-sensitive data.
- Tackling non-iid data with feature shift has been explored in classical centralized training in the context of domain adaptation. Here, an effective approach in practice is utilizing Batch Normalization (BN) (Ioffe & Szegedy, 2015): recent work has proposed BN as a tool to mitigate domain shifts in domain adaptation tasks with promising results achieved (Li et al., 2016;Liu et al., 2020;Chang et al., 2019).
- Inspired by this, this paper proposes to apply BN for feature shift FL.
- To illustrate the idea, we present a toy example that illustrates how BN may help harmonizing local feature distributions.
- Observation of BN in a FL Toy Example: We consider a simple non-convex learning problem: we generate data x, y ∈ R with y = cos(w true x) + , where x ∈ R is drawn iid from Gaussian distribution and is zero-mean Gaussian noise and consider models of the form f w (x) = cos(wx) with model parameter w ∈ R.
- Local data deviates in the variance of x.
- First, we illustrate that local batch normalization harmonizes local data distributions. We consider a simplified form of BN that normalizes the input by scaling it with γ, i.e., the local empirical standard deviation, and a setting with 2 clients.
- As Fig. 1 shows, the local squared loss is very different between the two clients. Thus, averaging the model does not lead to a good model.
- However when applying local BN, the local training error surfaces become similar and averaging the models can be beneficial.
- To further illustrate the impact of BN, we plot the error surface for one client with respect to both model parameter w ∈ R and BN parameter γ ∈ R in Fig. 2. The figure shows that for an optimal weight w * 1 , changing γ deteriorates the model quality. Similarly, for a given optimal BN parameter γ * 1 , changing w deteriorates the quality.
- In particular, the average model w = (w * 1 + w * 2 )/2 and average BN parameters γ = (γ * 1 + γ * 2 )/2 has a high generalization error.
- At the same time, the average model w with local BN parameter γ * 1 performs very well.
- Motivated by the above insight and observation, this paper proposes a novel federated learning method, called FedBN, for addressing non-iid training data which keeps the client BN layers updated locally, without communicating, and aggregating them at the server.
- In practice, we can simply update the non-BN layers using FedAvg, without modifying any optimization or aggregation scheme. This approach has zero parameters to tune, requires minimal additional computational resources, and can be...

## RELATED WORK
- The widely known aggregation strategy in FL, FedAvg (McMahan et al., 2017), often suffers when data is heterogeneous over local client.
- Empirical work addressing non-iid issues, mainly focus on label distribution skew, where a non-iid dataset is formed by partitioning a "flat" existing dataset based on the labels.
- FedProx (Li et al., 2020b), a recent framework tackled the heterogeneity by allowing partial information aggregation and adding a proximal term to FedAvg.
- Zhao et al. (2018) assumed a subset of the data is globally shared between all the clients, hence generalizes to the problem at hand.
- FedMA (Wang et al., 2020) proposed an aggregation strategy for non-iid data partition that shares global model in a layer-wise manner.
- However, so far there are only limited attempts considering non-iid induced from feature shift, which is common in medical data collecting from different equipment and natural image collected in various noisy environment.
- Very recently, FedRobust (Reisizadeh et al. , 2020) assumes data follows an affine distribution shift and tackles this problem by learning the affine transformation.
- This hampers the generalization when we cannot estimate the explicit affine transformation.
- Concurrently to our work, SiloBN Andreux et al. (2020) empirically shows that local clients keeping some untrainable BN parameters could improve robustness to data heterogeneity, but provides no theoretical analysis of the approach.
- FedBN instead keeps all BN parameters strictly local.
- Recently, an orthogonal approach to the non-iid problem has been proposed that focuses on improving the optimization mechanism (Reddi et al., 2020;Zhang et al., 2020).

## PRELIMINARY
- Non-IID data is a category of data that is not related to the labels of data.
- We introduce the concept of feature shift in federated learning, which is a new category of non-iid data distribution.
- Feature shift covers cases where the marginal distributions of data varies across clients, even if the conditional distribution of data is the same.
- FedAvg is a federated learning algorithm that uses the average of local client's updates to update the global model.
- Although FedAvg has shown successes in classical federated learning tasks, it suffers from slow convergence and low accuracy in most non-iid contents.

## FEDERATED AVERAGING WITH LOCAL BATCH NORMALIZATION

## PROPOSED METHOD -FEDBN
- FedBN is an efficient and effective learning strategy
- FedBN improves the convergence rate under feature shift

## PROBLEM SETUP

## CONVERGENCE ANALYSIS
- The trajectory of networks FedAvg (f ) and FedBN (f * )'s prediction through the neural tangent kernel (NTK) introduced by Jacot et al. (2018) is studied
- Recent machine learning theory studies (Arora et al., 2019;Du et al., 2018;Allen-Zhu et al., 2019;van den Brand et al., 2020;Dukler et al., 2020) have shown that for finite-width over-parameterized networks, the convergence rate is controlled by the least eigenvalue of the induced kernel in the training evolution
- To simplify tracing the optimization dynamics, we consider the case that the number of local updates E is 1
- Decomposition of the NTK into magnitude component G(t) and direction component V(t)/α 2 is given in Appendix B.1
- According to NTK, the convergence rate is controlled by λ min (Λ(t))
- For α > 1, convergence is dominated by G(t)
- The convergence analysis is based on the auxiliary version of the Gram matrices
- The key results in Dukler et al. (2020) are used to show that G ∞ is positive definite and G * ∞ is positive definite
- The distance between G(t) and its auxiliary version is small in over-parameterized neural network, such that G(t) remains positive definite
- Based on our formulation, the convergence rate of FedAvg (Theorem 4.4) can be derived from Dukler et al. (2020) by considering non-identical covariance matrices

## Proof sketch
- The key to minimizing the error is to find λ min (G ∞ )
- This is done by comparing equation (4) and (5)
- It is found that λ min (G ∞ ) ≤ λ min (G * ∞ ).

## EXPERIMENTS
- Using local BN parameters is beneficial in the presence of feature shift across clients with heterogeneity data
- FedBN achieves more robust and faster convergence for feature shift non-iid datasets and obtains better model performance compared to alternative methods

## BENCHMARK EXPERIMENTS
- We perform an extensive empirical analysis using a benchmark digits classification task containing different data sources with feature shift where each dataset is from a different domain.
- Data of different domains have heterogeneous appearance but share the same labels and label distribution.
- Specifically, we use the following five datasets: SVHN Netzer et al. (2011), USPS Hull (1994), SynthDigits Ganin & Lempitsky (2015), MNIST-M Ganin & Lempitsky (2015) and MNIST LeCun et al. (1998).
- To match the setup in Section 4, we truncate the sample size of the five datasets to their smallest number with random sampling, resulting in 7438 training samples in each dataset.
- Testing samples are held out and kept the same for all the experiments on this benchmark dataset.
- Our classification model is a convolutional neural network where BN layers are added following each feature extraction layer (i.e., both convolutional and fully-connected).
- The architecture is detailed in Appendix D.2.
- For model training, we use the cross-entropy loss and SGD optimizer with a learning rate of 10 −2 .
- If not specified, our default setting for local update epochs is E = 1, and the default setting for the amount of data at each client is 10% of the dataset original size.
- For the default non-iid setting, the FL system contains five clients. Each client exclusively owns data sampled from one of the five datasets.
- More details are listed in Appendix D.2.

## Analysis of Local Dataset Size:
- We vary the data amount for each client from 100% to 1% of its original dataset size, in order to observe FedBN behaviour over different data capacities at each client.
- The results in Fig. 4 (b) present the accuracy of FedBN and SingleSet4 .
- Testing accuracy starts to significantly drop when each of the local client is only attributed 20% percentage of data from its original data amount.
- The improvement margin gained from FedBN increases as local dataset sizes decrease.

## Effects of Statistical Heterogeneity:
- Simulates a federated settings with varying heterogeneity
- Parses each dataset into 10 subsets, one for each clients, with equal number of data samples and the same label distribution
- Treats the clients generated from the same dataset as iid clients, while the clients generated from different datasets as non-iid clients
- Adds one client from each datasets while keep the existing clients n times, for n ∈ {1, . . .
- Trains models from scratch
- Shows the testing accuracy under different level of heterogeneity
- Includes a comparison with FedAvg, which is designed for iid FL
- Performs 5-trial repeating experiment with different random seeds

## EXPERIMENTS ON REAL-WORLD DATASETS
- The proposed algorithm is effective in feature-shift noniid
- The proposed algorithm is effective on three real-world datasets
- The proposed algorithm is effective on a medical application

## Results and Analysis:
- The experimental results are shown in Table 1 in the form of mean (std).
- On Office-Caltech10, FedBN significantly outperforms the state-of-the-art method of FedProx, and improves at least 6% on mean accuracy compared with all the alternative methods.
- On DomainNet, FedBN achieved supreme accuracy over most of the datasets.
- Interestingly, we find the alternative FL methods achieves comparable results with SingleSet except Quickdraw, and FedBN outperforms them over 10%.
- Surprisingly, for the above two tasks, the alternative FL strategies are ineffective in the feature shift non-iid datasets, even worse than using single client data for training for most of the clients.
- In ABIDE I, FedBN excell by a non-negligible margin on three clients regarding the mean testing accuracy.

## CONCLUSION AND DISCUSSION
- FedBN keeps the local Batch Normalization parameters not synchronized with the global model, such that it mitigates feature shifts in non-IID data.
- We provide convergence guarantees for FedBN in realistic federated settings under the overparameterized neural networks regime, while also accounting for practical issues.
- In our experiments, our evaluation across a suite of federated datasets has demonstrated that FedBN can significantly improve the convergence behavior and model performance of non-IID datasets.
- We also demonstrate the effectiveness of FedBN in scenarios that where a new client with an unknown domain joins the FL system (see Appendix G).
- FedBN is independent of the communication and aggregation strategy and thus can in practice be readily combined with different optimization algorithms, communication schemes, and aggregation techniques.

## B.3 PROOF OF COROLLARY 4.6
- The exponential factor in the convergence rates is (1 − ηµ 0 /2) and (1 − ηµ * 0 /2) for FedAvg and FedBN, respectively.
- It reduces to comparing µ 0 = λ min (G ∞ ) and µ * 0 = λ min (G * ∞ ).
- Comparing equation ( 7) and ( 9), G * ∞ takes the M × M block matrices on the diagonal of G ∞ : where Thus, (1 − ηµ 0 /2) ≥ (1 − ηµ * 0 /2) and we can conclude that the convergence rate of FedBN is faster than the convergence of FedAvg.

## C FEDBN ALGORITHM
- The algorithm is described in Algorithm 1
- The algorithm is for a six-layer Convolutional Neural Network
- The data sample number are kept into the same size according to the smallest dataset, i.e. Office-Caltech10 uses 62 training samples and DomainNet uses 105 training samples on each dataset
- For simplicity, we choose top-10 class based on data amount from DomainNet containing images over 345 categories

## D.4 ABIDE DATASET AND TRAINING DETAILS
- The medical datasets come from real-world medical records.
- The preprocessing and training details include feature engineering, machine learning, and data pre-processing.
- The results show that the proposed model is able to accurately predict the outcomes of a patient.

## Dataset:
- The study was carried out using resting-state fMRI (rs-fMRI) data from the Autism Brain Imaging Data Exchange dataset (ABIDE I preprocessed, (Di Martino et al., 2014)).
- ABIDE is a consortium that provides preciously collected rs-fMRI ASD and matched controls data for the purpose of data sharing in the scientific community.
- We downloaded Regions of Interests (ROIs) fMRI series of the top four largest sites (UM, NYU, USM, UCLA viewed as clients) from the preprocessed ABIDE dataset with Configurable Pipeline for the Analysis of Connectomes (CPAC) and parcellated by Harvard-Oxford (HO) atlas.
- Skipping subjects lacking filename, resulting in 88, 167, 52, 63 subjects for UM, NYU, USM, UCLA separately.
- Due to a lack of sufficient data, we used sliding windows (with window size 32 and stride 1) to truncate raw time sequences of fMRI.
- The compositions of four sites were shown in Table 10.
- The number of overlapping truncate is the dataset size in a client.
- In Figure 5, we compare the performance with respect to accuracy of FedBN and alternative methods.
- We show the detailed accuracy in the following:
- The accuracy of FedBN was 88.5%
- The accuracy of alternative methods was 87.5%
- The accuracy of FedBN was better than alternative methods

## NYU UM1 USM UCLA1
