---
title: "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars"
date: 2023-01-06T19:58:47.000Z
author: "Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, Sergey Tulyakov"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-02700v1.webp" # image path/url
    alt: "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02700)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02700).


# Abstract
- Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure.
- Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible.
- Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets.
- We then distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling -- as a byproduct -- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions -- for the first time -- allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.

# Paper Content

## Introduction
- 3D-GANs can generate flat geometry and become 2D-GANs essentially
- A natural question arises whether a 3D-GAN can synthesize consistent novel views of images belonging to artistically stylized domains
- In this work, we propose a domain-adaption framework that allows us to answer the question positively
- We fine-tune a pre-trained 3D-GAN using a 2D-GAN trained on a target domain
- Despite being well explored for 2D-GANs [26,65], existing domain adaptation techniques are not directly applicable to 3D-GANs
- The geometry and texture of stylized 2D datasets can be arbitrarily exaggerated depending on the context, artist, and production requirements
- Due to this, no reliable way to estimate camera parameters for each image exists, whether using an off-the-shelf pose detector [72] or a manual labeling effort
- To enable the training of 3D-GANs on such challenging datasets, we propose three contributions
- 1 An optimization-based method to align distributions of camera parameters between domains
- 2 Texture, depth, and geometry regularizations to avoid degenerate, flat solutions and ensure high visual quality
- Furthermore, we redesign the discriminator training to make it compatible with our task
- We then propose 3 a Thin Plate Spline (TPS) 3D deformation module operating on a tri-plane representation to allow for certain large and sometimes extreme geometric deformations, which are so typical in artistic domains
- The proposed adaptation framework enables the training of 3D-GANs on complex and challenging artistic data

## Related Work
- GANs are a popular type of generative model
- StyleGAN is the current state-of-the-art GAN
- CLIP based image editing is enabled by StyleGAN
- GAN inversion is a prerequisite for GAN-based image editing
- Data augmentation and freezing lower layers of the discriminator are useful tools when fine-tuning a 2D-GAN
- Some works are trained on 3D data or require heavy labeling/engineering
- In this work, we employ 2D to 3D domain adaptation and distillation and use synthetic 2D data from StyleCariGAN and DualStyleGAN

## Domain Adaptation for 3D-GANs
- The goal of domain adaptation for 3D-GANs is to adapt (both texture and geometry) to a particular style defined by a 2D dataset.
- In contrast to 2D-StyleGAN-based fine-tuning methods that are conceptually simpler, fine-tuning a 3D-GAN on 2D data introduces challenges in addition to domain differences, especially on maintaining the texture quality while preserving the geometry.
- Moreover, there is no explicit shape and camera information for these datasets. We define the domain adaptation task as follows: Given a prior 3D-GAN (G s ) of source domain (T s ), we aim to produce a 3D Avatar GAN (G t ) of the target domain (T t ) while maintaining the semantic, style, and geometric properties of G s , and at the same time preserving the identity of the subject between the domains (T s ↔ T t ).

### How to align the cameras?
- Selecting appropriate ranges for camera parameters is of paramount importance
- Typically, such parameters are empirically estimated, directly computed from the dataset using an off-the-shelf pose detector, or learned during training
- In domains we aim to bridge, such as caricatures for which a 3D model may not even exist, directly estimating the camera distribution is problematic and, hence, is not assumed by our method
- Instead, we find it essential to ensure that the camera parameter distribution is consistent across the source and target domains
- For the target domain, we use StyleGAN2 trained on FFHQ and fine-tuned on artistic datasets as G 2D
- Assuming that the intrinsic parameters of all the cameras are the same, we aim to match the distribution of extrinsic camera parameters of G s and G 2D and train our final G t using it
- To this end, we define an optimization-based method to match the sought distributions

### What loss functions and regularizers to use?
- Next, the generator G t is designed to converge to an easier degenerate solution with a flat geometry.
- To benefit from the geometric prior of G s , another important step is to design the loss functions and regularizers for a selected set of parameters to update in G t .
- Next, we discuss these design choices: Loss Functions, Texture Regularization, Geometry Regularization, and Depth Regularization.
- Finally, regularization is defined as: where D t is the depth map of the image I t sampled from G t and J is the matrix of ones having the same spatial dimensions as D t .

### What discriminator to use?
- Given that the data in T s and T t is not paired, the choice of the discriminator (D) used for this task is also a critical design choice.
- We use the unconditional version of the dual discriminator proposed in EG3D; hence, we do not condition the discriminator on the camera information.
- As a result, during the training, G t generates arbitrary images with pose using M(θ , φ , c , r ), and the discriminator discriminates these images using arbitrary images from T t .
- We train the discriminator from scratch, and in order to adapt T s → T t , we use the StyleGAN-ADA training scheme and use R1 regularization.

### How to incorporate larger geometric deformations between domains?
- The regularizers are used to limit the geometric changes when adapting from T s to T t
- Modeling large geometric deformations, e.g., in the caricature dataset, is another challenge
- One choice to edit the geometry is to use the properties of tri-plane features learned by EG3D
- We start out by analyzing these three planes in G s
- We observe that the frontal plane encodes most of the information required to render the final image
- To quantify this, we sample images and depth maps from G s and swap the front and the other planes from two random images
- Then we compare the difference in RGB values of the images and the Chamfer distance of the depth maps
- While swapping the frontal tri-planes, the final images are completely swapped, and the Chamfer distance changes by 80 ∼ 90% matching the swapped image depth map
- In the case of the other two planes, the RGB image is not much affected, and the Chamfer distance of the depth maps is reduced by only 20 ∼ 30% in most cases
- Given the analysis, we focus on manipulating the 2D front plane features to learn additional deformation or exaggerations
- We learn a TPS (Thin Plate Spline) [62] network on top of the front plane
- Our TPS network is conditioned both on the front plane features as well as the W space to enable multiple transformations
- The architecture of the module is similar to the standard StyleGAN2 layer with an MLP appended at the end to predict the control points that transform the features
- Hence, as a byproduct, we also enable 3D-geometry editing guided by the learned latent space
- We train this module separately after G t has been trained
- We find that joint training is unstable due to exploding gradients arising from the large domain gap between T s and T t in the initial stages

## Personalized Avatar Generation and Editing
- The task is to project a real image into the latent space of G s , transfer the latent to G t , and further optimize it to construct a 3D avatar.
- First, we use an optimization-based method to find the w code that minimizes the similarity between the generated and the real image in G s .
- To achieve this, the first step is to align the cameras.
- We follow the steps mentioned in Sec. 3.1 for this step.
- Next, we use pixel-wise MSE loss and LPIPS loss to project the image into G s [1].
- Additionally, as some datasets [25] provide the coupled attribute information of real images and caricatures, we use attribute classifiers to encourage the consistency of attributes to preserve the identity of the subject.
- We use such attribute classifier [25,26] in a post-hoc manner as we notice that such networks can affect the texture in the target domain and could degenerate to narrow style outputs if applied during training.
- To avoid overfitting into G s and encourage an easier transfer of the optimized latent code to G t , we use W space optimization for this step.
- Finally, we initialize this w code for G t and use additional attribute classifier loss [26] for T t domain along with Depth regularization R(D) (Eq. 4).
- Since our 3D domain adaptation is designed to preserve the properties of W and S spaces, we can perform semantic edits via InterFace-GAN [53], GANSpace [23], StyleSpace [63] etc., and geometric edits using TPS (Sec. 3.4) and ∆s interpolation (Sec. 3.2).
- To perform video editing, we design an encoder for EG3D based on e4e [59] to encode videos and transfer the edits from G s to G t based on the w codes [4,6,60].
- We leave a more fine-grained approach for video processing as future work.

## Results

### Quantitative Results
- Texture quality: G base and G t have similar FID scores
- Geometric quality: G base has a higher M d and S d scores than G t
- Identity preservation: G base preserves the identity better across the domains

### Qualitative Results
- For qualitative results, we show the results of the domain adaptation, as well as the personalized edits (geometric and semantic) performed on the resultant 3D avatars.
- First, in order to show the quality of domain adaptation, identity preservation, and geometric consistency, in Fig. 3, we show results from G s and corresponding results from 3D avatar generator G t trained on Caricatures, Pixar toons, Cartoons, and Comic domains.
- Next, in order to show that the method generalizes to real images, we use the method described in Sec. 4 to project and transfer the latent code from G s to G t to produce the 3D avatars.
- In Fig. 4, we show our real to 3D avatar transfer results. Notice the quality both in terms of texture as well as geometry for both these results achieved by our method.
- Next, we show geometric and semantic edits possible to produce personalized 3D avatars: Geometry Edits. We show two type of geometric edits i.e. ∆s interpolation (Sec. 3.2) and deformation using TPS (Sec. 3.4).
- First, in Fig. 5, we show the geometry interpolation by interpolating between original s activations of G s and learned ∆s parameters. In Fig. 6, we show some additional exaggerations in caricatures using the learned 3D deformation latent space of TPS module.
- Semantic Edits and Animation. Since in our method, we encourage the latent regularization to preserve the properties of the latent space learned by the G s generator, in Fig. 7, we show S space edits performed on the 3D avatars. Notice the quality of edits in terms of locality and adaptability. Additionally, we can edit semantics like hair as opposed to 3D morphable model-based methods.
- In Fig. 8, thanks to the latent space semantics preservation ensured by our method, we can perform some video edits to create a coherent ani-mation based on the difference of w codes of video encoded in G s (Sec. 4) and applied to layers 7 − 10 in G t . Notice the quality of expressions, identity preservation, and 3D consistency across each identity in each row.

## Limitations
- StyleGAN2 pretraining is limited in quality
- Edits are largely limited to semantic edits of EG3D and global space deformations by TPS
- Face specific specialization is justified by the importance of human models
- Domain adaptation of general 3D-GANs will be an interesting avenue of future work

## Conclusion
- We proposed the first domain adaptation method for 3D-GANs
- This part yields two linked EG3D generators, one in the photorealistic source domain of faces and another EG3D generator in an artistic target domain
- As possible target domains, we show results for cartoons, caricatures, and Comics
- In the second part, we built on domain adaptation to create 3D avatars in an artistic domain that can be edited and animated
- Our framework consists of multiple technical components introduced in this paper
- First, we propose a technique for camera space estimation for artistic domains
- Second, we introduce a set of regularizers and loss functions that can regularize the fine-tuning of EG3D in such a way that enough of the 3D structure and geometry of the original model is kept while the distinguishing attributes of the artistic domain, such as textures and colors and local geometric deformations can still be learned
- Third, we introduce a geometric deformation module that can reintroduce more significant geometric deformations in a controlled manner
- These larger geometric deformations can interact and cooperate with EG3D so that semantic edits are still possible
- Finally, we propose an embedding algorithm especially suitable for two linked EG3D generator networks

## Training Details
- We train our models on 4 V100 GPUs with a batch size of 8
- Similar to EG3D, we start training from the neural rendering resolution of 64 2 which is increased during the training.
- Then we do fine-tuning on 128 2 resolution to produce the final 512 2 outputs.
- We sample 100k samples from each dataset.
- We train Caricatures on ∼ 880 kimgs, Pixar, and Cartoons on ∼ 500 kimgs.
- We fine-tune these models on 128 2 neural rendering resolution for an additional 80 − 160 kimgs.
- Other hyperparameters of learning rates are the same as the EG3D.
- We train the TPS module on ∼2000 kimgs.
- We set the weight for the regularization term R(∆s) as 0.001, and R(D) as 0.005.
- For the TPS training we use the weights for α, β, σ and R(T 2 ) as 150, 1, 3 and 1 respectively.
- For inversion, we perform 200 steps for the source domain inversion and 400 steps for the target domain to generate the final avatar.

## Ablation Study
- Adding the T P S module improves the corresponding scores in FID, M d , S d , and ID.
- The FID score is worse with more perturbation in the background.

## Importance of Front Tri-plane Features
- The front triplane of the EG3D architecture encodes most of the texture and depth information in the output.
- In Fig. 9, we show two images with their front and other side plane swapped. Then we show the corresponding effect on the output image. Notice that the results are consistent with the analysis in Sec. 3.4 where the front tri-plane dominates the information for output texture and depth.

## Stylization
- To validate that our chosen layers in Sec.3.2 are responsible for geometrical and texture changes, we resort to a stylization technique.
- For stylization given an arbitrary reference image e.g. painting, we use the Style Loss [19] to update the layers of G s or G t .
- Essentially, we use the same parameters used in Sec. 3.1.
- A critical technique to achieve multi-view consistency and circumvent the ghosting face artifact due to single image overfitting is to rotate the camera to cover the θ and φ ranges in Sec.3.1 in the main paper uniformly during the optimization.
- In Fig. 10, we show some results using only the layers used in Texture Regularization (Sec.3.2).
- Note the high-quality texture change in the images.
- In Fig. 11, we show results by adding layers of Geometry Regularization (Sec. 3.2).
- Note that the geometry is changed in the examples when we use this module.
- Note that in this example, the geometry is not expected to match as there is no such loss in the optimization. This example results in some arbitrary geometry change that is not flat.
- This validates our choice of geometry and texture layers used in this paper.

## Failure cases
- The method has some failure cases that stem from the samples of the 2D-GANs
- In the caricature domain, the random samples generated in the case of G t trained on StyleCari-
- Here the style (second row) is changed using the texture layers and the geometry (third row) is changed using the geometry layers.
- Note that the geometry is not correct as we apply Style Loss using a single image and is only used to demonstrate the usage of different layers.
- Failure cases in caricature generation that stems from the artifacts in the 2D-GAN from which the dataset is generated.
- Here the artifacts are the result of the generated results of StyleCariGAN [26] GAN samples can have some severe artifacts (See Fig. 12).
- Although these do not appear often, such a sample can be improved by attribute classifier and depth regularization losses used on a single image as discussed in Sec. 4 of the main paper.

## Depth Map visualization
- In Fig. 13, the authors show some samples from G base and G t .
- In Fig. 14, they show some samples from their method with depth maps on the Caricature, Pixar toon, Cartoon, and Comic datasets.
- In Fig. 15, they show some grid samples of their method with depth maps on the Caricature, Pixar toon, Cartoon, and Comic datasets.

## Video Results
- We designed a simple UI to show that the avatars can be edited in an interactive manner.
- Figure 2 shows comparison with naive fine-tuning.
- The corresponding sub-figures show comparisons in terms of texture quality (top two rows) and geometry (bottom two rows).
- Domain adaptation results of images from source domain Ts (top row in each sub-figure) to target domain Tt.
- Rows two to five shows corresponding 3D avatar results from different viewpoints.
- Figure 3 shows domain adaptation.
- Figure 4 shows 3D avatars from real images.
- Projection of real images on the 3D avatar generators.
- s (w, θ, φ, c, r) = G s (w, M(θ, φ, c, r)) and I 2D (w) = G 2D (w) represent an arbitrary image generated by G s and G 2D , respectively, given the w code variable.
- Let k d be the face key-points detected by the detector K d [72], then (c , r ) := arg min (c,r)
- Geometric deformation using the interpolation of learned ∆s parameters.
- Deformations using TPS. Geometric edits using our proposed TPS (Thin Plate Spline) module learned on the frontal tri-plane features.
- Each sub-figure shows a 3D avatar and three examples of TPS deformations sampled from the learned 3D deformation space.
- Figure 7 shows local edits.
- Figure 8 shows 3D avatar animation.
- Figure 9 shows swapping tri-planes.
- Figure 10 shows validation of texture regularization.
- Figure 11 shows validation of geometry regularization.
- Figure 13 shows comparison with Naive method.
- For more results refer to the videos in the supplementary.
