---
title: "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"
date: 2022-05-20T01:25:57.000Z
author: "Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, Alexander I. Rudnicky"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2205-09921v2.webp" # image path/url
    alt: "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.09921)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.09921).


# Abstract
- Relative positional embeddings (RPEs) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation.
- We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences.
- We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention.
- The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at https://github.com/chijames/KERPLE.git.

# Paper Content

## Introduction
- Transformer-based models have excelled in various natural language processing tasks such as chatbot [Roller et al., 2021], code completion [Chen et al., 2021a], and paper abstract summarization [Zhang et al., 2020].
- These sequence modeling tasks often require the model to operate well on significantly longer text sequences than the fixed maximum length L used at training time.
- Training (or retraining) the model using a substantially larger value of L is often infeasible since the transformer training cost is O(L 2 ).
- Hence, one desires a transformer that continues to perform well on longer sequences than those used during training; i.e., perform length extrapolation at inference time.
- Most transformer designs do not have this property [Press et al., 2022].
- While recent work on absolute positional embeddings demonstrated the extrapolation ability [Kiyono et al., 2021, Likhomanenko et al., 2021], it is believed that relative positional embeddings are more robust to input length change [Likhomanenko et al., 2021], for example, ALiBi [Press et al., 2022] and T5 [Raffel et al., 2020].
- Hence, we are motivated to study the inner workings of relative positional embeddings.
- Relative positional embeddings (RPE) encode the idea of shift-invariance: for any shift p, (m + p) − (n + p) = m − n.
- It is often added directly to the self-attention matrix before Softmax normalization [Chen et al., 2021b].
- Inspired by shift-invariance and the ability of a kernel to define a similarity function, there have been studies on shift-invariant kernels for RPE [Wennberg and Henter, 2021] with a focus on Gaussian kernel.
- However, in our preliminary experiments, the Gaussian kernel Figure 1: The 3-Para-Log Variant of Our KERPLE Framework.
- a, b, and p are learnable parameters in each attention head shared across layers. Since # of heads is H, there are 3 • H learnable parameters.
- The learnable parameters are trained with length-3 sequences. At the inference time, the last row (in dashed squares) becomes active, and the model extrapolates to length-4 sequences.
- Note we focus on causal language modeling following ALiBi, so the matrices are triangular.
- ) demonstrates limited length extrapolation ability (see Appendix A.3).
- Hence, a distinct class of shift-invariant kernels is needed to achieve adequate length extrapolation.
- To this end, we note a set of well-established conditionally positive definite (CPD) kernels suitable for modeling distance metrics [Schölkopf, 2000].
- However, CPD kernels do not conform to an inner product. We can remedy this issue by transforming a CPD kernel into a PD kernel by adding a sufficiently large constant. This constant offset is subsequently absorbed implicitly in the Softmax normalization (see the discussion below Eq. (2)).
- For example, ALiBi implicitly admits a PD kernel of the form c − |m − n| (see the end of section 4), which is reduced to a CPD kernel −|m − n|.
- The CPD kernel and Softmax normalization combination opens the door to a sea of possible CPD kernels.
- We investigate structures from this class that exhibit a strong length extrapolation ability, like ALiBi.
- Our main result is a framework for KErnelize Relative Positional Embedding for Length Extrapolation (KERPLE).
- The framework elucidates key principles that encourage the length extrapolation property.
- We show that ALiBi is a particular instance within our framework.
- Our subsequent experiments suggest that the proposed method yields better length extrapolation on large datasets such as OpenWebText2, GitHub, and ArXiv.

### Preliminary
- Let {w m } L m=1 be the input tokens to a transformer model
- Each w m is a scalar and is used to index the embedding vector e m ∈ R d as the input to the transformer
- A transformer converts each e m into query, key, and value vectors in R d : where W q , W k , W v ∈ R d×d are learnable matrices
- Then, the self-attention module computes the scaled attention scores and generates the output vector o m at position m as:
- Since the operation is position-agnostic, it is believed that positional information helps model token interactions [Vaswani et al., 2017]

### Positional Embedding
- Absolute positional embeddings assign a positional vector to each position and adds it to the embedding vector
- The very first version of which is the predefined sinusoidal function
- Followed by the success of BERT, learnable absolute positional embeddings have been applied to the task of masked language modeling
- Autoregressive-decoding, sequence-to-sequence and recent work studied ways to extrapolate sinusoidal positional embeddings to longer sequences by randomly shifting absolute positions during training
- Relative positional embeddings model the positional difference
- T5 model that considers bucketed relative distances and log-binning has been shown to perform well on various transformer architectures

### Kernel and its Application in Transformer
- The kernel trick is a classic approach to generalize the inner product to high dimensional spaces
- In the context of transformers, there has been interest in applying kernels to the self-attention structure to enhance the performance
- Examples of such work include kernel for positional embeddings [Tsai et al., 2019, Wu et al., 2021b, Wennberg and Henter, 2021, Luo et al., 2021]
- Another line of research leverages the kernel's feature map [Rahimi and Recht, 2007] to linearize the self-attention module and reduce the computational cost

### PD and CPD Kernels
- shift invariant conditionally positive definite kernels are used to model the effect of relative positional differences
- the notion of relative positional difference is modeled by a shift invariant function
- fact 1 suggests that CPD kernels generalize distance metrics to high dimensional spaces
- to better understand the effect of CPD kernels on self-attention, relations between CPD and PD kernels need to be established

### Constructing PD Kernels From CPD Kernels via Constant Shifts
- The CPD shift lemma implies that all CPD kernels can be turned into PD kernels.
- The CPD shift lemma also implies that the CPD kernels in Corollary 1 can be made PD if a large enough constant is added.
- The CPD shift lemma does not have an explicit construction of c, but it can be left as an under-determined constant in our positional embedding design (Eq. (1) in section 4).
- Given a set of test points {x i } N i=1 , one can do a geometric sequence search to search for a c such that the 0.

## Kernelized Relative Positional Embedding
- Let {q m } L m=1 and {k n } L n=1 be the input queries and keys.
- Let (r 1 , ..., r ) be learnable parameters.
- We propose a kernelized relative positional embedding as follows.
- where kr1,...,r (m, n) is any shift-invariant CPD kernel with parameters.
- Due to Lemma 1, Eq. ( 1) can be reformulated into its kernel form as follows.
- a m,n (2) (*) is due to the shift-invariant property of the Softmax normalization: for any c ∈ R.
- The second equality defines a bias kernel which is positive definite using Lemma 1: The last equality introduces a composite kernel k comp : R d+1 × R d+1 → R as Interpretation.
- The proposed method can be interpreted as applying a composite kernel to selfattention.
- The composite kernel combines the information from query q m , key k n , and positions (m, n) in a way that augments the original self-attention structure by multiplicative and additive position embeddings.
- The augmentation allows k comp to not only retain the original q m k n but also include positional information from the bias kernel k r1,...,r .
- Practical Choice.
- In section 5.2, we fix = 2 and experiment on two variants of the composite kernel, Eq. ( 4), where we call these the power variant and the logarithmic variant of our proposed KERPLE framework, Eq. ( 2).
- These are from a combination of Corollary 1 and Eq. ( 3).
- (power) We note that these are not the only variants of the composite kernel.
- In section 5.3, we experiment with two more complicated variants, but only find lower training speeds and marginal improvement in perplexities (e.g., logarithmic variant vs. 3-para-log).
- Thus, based on our study, the choices above hold advantages in both performance and speed.
- Connection to Prior Work.
- When the bias kernel, Eq. ( 3), is a triangle kernel: c − |m − n|, our model reduces to ALiBi [Press et al., 2022].
- Wennberg and Henter [2021] discuss the situation where the bias kernel is a Gaussian kernel.
- Tsai et al. [2019] is the case where there is no bias kernel and the attention product q m k n is multiplied by an exponentiated inner product kernel, exp(x y).
- Since ALiBi is the state-of-the-art and has great input length extrapolation, we will focus on comparison with ALiBi in our experiments.
- The logarithmic variant has an implicit connection to T5 positional bias [Raffel et al., 2020].
- According to the official GitHub repository https://github.com/google-research/ text-to-text-transfer-transformer and the HuggingFace Transformer [Wolf et al., 2020], T5 bias is implemented with a log-binning strategy.
- For each head of the transformer, they maintain a bucket of 32 learnable parameters and assign the relative positional bias b m−n to these parameters as where • is the floor function.
- Note that the log factor is approximately 7.7 log m−n 16 .
- Therefore, T5 is using a logarithmic bucket assignment, which turns out to extrapolate to different input lengths.
- Compared with T5, our logarithmic variant uses less parameters (2x12 vs. 32x12) but cannot learn non-monotonic relations (the log function is monotonic).
- We will conduct more comparisons with T5 bias in our experiments.

## Experiments

### Dataset and Implementation Description

### Experiments on Complicated Kernels
- In section 4, there are three different types of composite kernels: bias+weight, bias+r1, and bias+r2.
- In section 5.2, the two-parameter logarithmic kernel is better than the three-parameter logarithmic kernel.
- However, in the context of RPE, the three-parameter logarithmic kernel is better than the two-parameter logarithmic kernel.

### Plots of Kernel Functions
- ALiBi is a kernel function that can learn different types of kernels
- ALiBi quickly reaches a very negative value
- The log variant successfully discovers several flat kernels
- This corroborates our previous observation that KERPLE-log can utilize more distant token information

### Position-wise Perplexity Evaluation
- Plot position-wise perplexity with evaluation length=4096 in Figure 5
- KERPLE-log lies below KERPLElog-windowed@512, indicating its usage of more distant information than window attention
- The PPL of KERPLE-log continues to decrease till the end of 4096 positions (Not plateauing)
- T5 lies below KERPLE-log-windowed@512 most of the time and fluctuates around KERPLE-logwindowed@512 after length=3000
- ALiBi lies above KERPLE-log-windowed@512 for almost all the positions, indicating that window attention might be a better choice than ALiBi
- Although window attention is a strong baseline, our KERPLE-log is almost like a free lunch compared to window attention

## Conclusion and Future Work
- A general framework, KERPLE, is proposed to kernelize relative positional embeddings for length extrapolation
- At the core of this framework is the application of CPD kernels and the derivation of practical variants
- We show that these CPD kernels can be implicitly converted to PD kernels, which keep the inner product interpretation of self-attention
- We also demonstrate that the logarithmic variant achieves exceptional extrapolation performance on three large language modeling datasets
- We believe our work paves the way for some interesting future directions that resolve our limitations.

## Broader Impact
- There exists a large enough c such that c + k(x, y) is a positive definite kernel.
- K is positive semidefinite and f c (v) ≥ 0 for c ≥ 0.
- It is sufficient to consider K without zero eigenvalues.

### A.2 Shift-invariant Kernels with Bounded and Unbounded Ranges

### A.3 Experiments on Gaussian-like Kernels
- The Gaussian-like kernels have limited extrapolation ability.
- The power and logarithmic variants derived from KERPLE achieve superior performance on length extrapolation across various datasets.
- The effective length of a kernel is the minimum distance between the kernel and the data points.
- The Gaussian-like kernels have a smaller effective length than the power and logarithmic variants derived from KERPLE.
