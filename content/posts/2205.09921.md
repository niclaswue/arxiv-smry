---
title: "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"
date: 2022-05-20T01:25:57.000Z
author: "Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, Alexander I. Rudnicky"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2205-09921v2.webp" # image path/url
    alt: "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.09921)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.09921).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/kerple-kernelized-relative-positional).

# Abstract
- Relative positional embeddings (RPEs) have been receiving considerable attention since they effectively model the relative distance among tokens and enable length extrapolation.
- We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences.
- We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics.
- To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention.
- The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way.
- Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at https://github.com/chijames/KERPLE.git.

# Paper Content

## Introduction
- Transformers have been successful in various natural language processing tasks
- RPE encode the idea of shift-invariance, and a shift-invariant kernel is needed to achieve adequate length extrapolation
- We note a set of well-established conditionally positive definite (CPD) kernels suitable for modeling distance metrics, but they do not conform to an inner product
- We transform a CPD kernel into a PD kernel by adding a sufficiently large constant, which is subsequently absorbed implicitly in the Softmax normalization

### Preliminary
- Let {w m } L m=1 be the input tokens to a transformer model
- Each w m is a scalar and is used to index the embedding vector e m ∈ R d as the input to the transformer
- A transformer converts each e m into query, key, and value vectors in R d : where W q , W k , W v ∈ R d×d are learnable matrices
- Then, the self-attention module computes the scaled attention scores and generates the output vector o m at position m as:
- Since the operation is position-agnostic, it is believed that positional information helps model token interactions [Vaswani et al., 2017]

### Positional Embedding
- Absolute positional embeddings assign a positional vector to each position and adds it to the embedding vector.
- The very first version of which is the predefined sinusoidal function.
- Followed by the success of BERT, learnable absolute positional embeddings have been applied to the task of masked language modeling.
- Autoregressive-decoding, sequence-to-sequence and recent work studied ways to extrapolate sinusoidal positional embeddings to longer sequences by randomly shifting absolute positions during training.
- Relative positional embeddings model the positional difference.
- In particular, the T5 model that considers bucketed relative distances and log-binning has been shown to perform well on various transformer architectures.

### Kernel and its Application in Transformer
- Generalizes the inner product to high dimensional spaces
- Used in transformers to improve performance
- Applied to self-attention structures to reduce computational cost

### PD and CPD Kernels
- shift-invariant conditionally positive definite kernels are used to model the effect of relative positional differences
- the notion of relative is modeled by a shift-invariant function: a bivariate function k over two positions (m, n) such that k(m, n) = f (m − n) for some univariate f
- fact 1 suggests that CPD kernels generalize distance metrics to high dimensional spaces
- to better understand the effect of CPD kernels on self-attention, relations between CPD and PD kernels need to be established

### Constructing PD Kernels From CPD Kernels via Constant Shifts
- The CPD shift lemma implies that all CPD kernels can be turned into PD kernels by adding a constant.
- The CPD shift lemma also implies that the CPD kernels in Corollary 1 can be made PD if a large enough constant is added.
- The CPD shift lemma does not have an explicit construction of c, but it can be left as an under-determined constant in our positional embedding design (Eq. (1) in section 4).
- Given a set of test points {x i } N i=1 , one can do a geometric sequence search to search for a c such that the 0.

## Kernelized Relative Positional Embedding
- Let {q m } L m=1 and {k n } L n=1 be the input queries and keys.
- Let (r 1 , ..., r ) be learnable parameters.
- We propose a kernelized relative positional embedding as follows.
- where kr1,...,r (m, n) is any shift-invariant CPD kernel with parameters.
- Due to Lemma 1, Eq. ( 1) can be reformulated into its kernel form as follows.
- a m,n (2) (*) is due to the shift-invariant property of the Softmax normalization: for any c ∈ R.
- The second equality defines a bias kernel which is positive definite using Lemma 1: The last equality introduces a composite kernel k comp : R d+1 × R d+1 → R as Interpretation.
- The proposed method can be interpreted as applying a composite kernel to selfattention.
- The composite kernel combines the information from query q m , key k n , and positions (m, n) in a way that augments the original self-attention structure by multiplicative and additive position embeddings.
- The augmentation allows k comp to not only retain the original q m k n but also include positional information from the bias kernel k r1,...,r .
- Practical Choice.
- In section 5.2, we fix = 2 and experiment on two variants of the composite kernel, Eq. ( 4), where we call these the power variant and the logarithmic variant of our proposed KERPLE framework, Eq. ( 2).
- These are from a combination of Corollary 1 and Eq. ( 3).
- (power) We note that these are not the only variants of the composite kernel.
- In section 5.3, we experiment with two more complicated variants, but only find lower training speeds and marginal improvement in perplexities (e.g., logarithmic variant vs. 3-para-log).
- Thus, based on our study, the choices above hold advantages in both performance and speed.
- Connection to Prior Work.
- When the bias kernel, Eq. ( 3), is a triangle kernel: c − |m − n|, our model reduces to ALiBi [Press et al., 2022].
- Wennberg and Henter [2021] discuss the situation where the bias kernel is a Gaussian kernel.
- Tsai et al. [2019] is the case where there is no bias kernel and the attention product q m k n is multiplied by an exponentiated inner product kernel, exp(x y).
- Since ALiBi is the state-of-the-art and has great input length extrapolation, we will focus on comparison with ALiBi in our experiments.
- The logarithmic variant has an implicit connection to T5 positional bias [Raffel et al., 2020].
- According to the official GitHub repository https://github.com/google-research/ text-to-text-transfer-transformer and the HuggingFace Transformer [Wolf et al., 2020], T5 bias is implemented with a log-binning strategy.
- For each head of the transformer, they maintain a bucket of 32 learnable parameters and assign the relative positional bias b m−n to these parameters as where • is the floor function.
- Note that the log factor is approximately 7.7 log m−n 16 .
- Therefore, T5 is using a logarithmic bucket assignment, which turns out to extrapolate to different input lengths.
- Compared with T5, our logarithmic variant uses less parameters (2x12 vs. 32x12) but cannot learn non-monotonic relations (the log function is monotonic).
- We will conduct more comparisons with T5 bias in our experiments.

## Experiments

### Dataset and Implementation Description
- Dataset: We conduct experiments on OpenWebText2, GitHub, and ArXiv datasets gathered in Gao et al. [2020].
- GitHub includes open-source repositories written in primary coding languages such as Java, C/C++, Python, and Go.
- ArXiv includes papers written in LaTex in Math, Computer Science, Physics, and some related fields.
- These tasks are motivated by the downstream applications such as online chatting [Roller et al., 2021], code completion [Chen et al., 2021a], and academic paper summarization [Zhang et al., 2020].
- Our model is trained on a machine with one NVIDIA A100 GPU with 40 GB of memory.
- We adopt almost all configurations of small GPT-NeoX2 , except that we change the train-micro-batch-size to 32, seq-length to 512, and max-position-embeddings to 512.
- Table 2 summarizes the important configurations fixed throughout our experiments.
- In particular, the floating-point encoding is set as To validate the effectiveness of our method, we compare KERPLE with Sinusoidal [Vaswani et al., 2017], Rotary [Su et al., 2021], T5 [Raffel et al., 2020], and ALiBi [Press et al., 2022].
- Table 3 reports the perplexities at different extrapolation lengths.
- We perform non-overlapping evaluation: Suppose text is segmented in a different manner for 512 and 1024 tokens, we have N sentences and N/2 correspondingly to evaluate.
- We also perform a paired two-sided t-test to validate the statistical significance (significance level=0.05).
- We compare each candidate RPE with our proposed logarithmic variant and mark the candidate with a † if the log variant is statistically significantly better.
- Table 4 reports the training speeds. These tables yield three conclusions. First, within the KERPLE framework, the logarithmic variant is better than the power variant. Secondly, the logarithmic variant is 9.7% faster than T5. In terms of extrapolation, the logarithmic variant generally does better than T5 but could be slightly worse than T5 at shorter lengths. Third, the logarithmic variant is slightly slower than some prior work (ALiBi, Rotary, and Sinusoidal) but consistently outperform these methods at all extrapolation lengths.

### Experiments on Complicated Kernels
- In section 4, there are 3 practical variants and 2 complicated variants of the composite kernel
- The (3-para-log) variant is better than the (two-parameter) logarithmic variant in extrapolation performance
- Enlarging the complexity of kernels does not necessarily give better performance in the context of RPE

### Plots of Kernel Functions
- ALiBi is a fast algorithm for learning kernel functions
- ALiBi quickly reaches a very negative value
- The log variant successfully discovers several flat kernels

### Position-wise Perplexity Evaluation
- Plot position-wise perplexity with evaluation length=4096 in Figure 5
- KERPLE-log lies below KERPLElog-windowed@512, indicating its usage of more distant information than window attention
- T5 lies below KERPLE-log-windowed@512 most of the time and fluctuates around KERPLE-logwindowed@512 after length=3000
- ALiBi lies above KERPLE-log-windowed@512 for almost all the positions, indicating that window attention might be a better choice than ALiBi
- Although window attention is a strong baseline, our KERPLE-log is almost like a free lunch compared to window attention

## Conclusion and Future Work
- A general framework, KERPLE, is proposed to kernelize relative positional embeddings for length extrapolation
- At the core of this framework is the application of CPD kernels and the derivation of practical variants
- We show that these CPD kernels can be implicitly converted to PD kernels, which keep the inner product interpretation of self-attention
- We also demonstrate that the logarithmic variant achieves exceptional extrapolation performance on three large language modeling datasets
- We believe our work paves the way for some interesting future directions that resolve our limitations. For instance, we can consider general kernel families and model non-monotonic effects due to positional differences. In addition, the use of learnable parameters in KERPLE might enable better generalization to inputs higher than one-dimensional.
- Last but not least, there is always room for improving memory efficiency by adjusting the model architecture and training procedure.
