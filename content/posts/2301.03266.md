---
title: "Doc2Query--: When Less is More"
date: 2023-01-09T11:02:49.000Z
author: "Mitko Gospodinov, Sean MacAvaney, Craig Macdonald"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03266v2.webp" # image path/url
    alt: "Doc2Query--: When Less is More" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03266)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03266).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/doc2query-when-less-is-more).

# Abstract
- Doc2Query is prone to hallucination, which harms retrieval effectiveness and inflates the index size.
- Filtering out these harmful queries prior to indexing can improve the retrieval effectiveness of Doc2Query by up to 16%.

# Paper Content

## Introduction
- Neural network models, particularly those based on contextualised language models, have been shown to improve search effectiveness
- In this work, we focus on one of these first-stage approaches: Doc2Query
- This approach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries that may be relevant to a particular text.
- Then, when indexing, this model is used to expand the document by generating a collection of queries and appending them to the document.
- Though computationally expensive at index time [34], this approach has been shown to be remarkably effective even when retrieving using simple lexical models like BM25 [28]
- Numerous works have shown that the approach can produce a high-quality pool of results that are effective for subsequent stages in the ranking pipeline
- However, sequence-to-sequence models are well-known to be prone to generate content that does not reflect the input text -a defect known in literature as "hallucination"
- We find that existing Doc2Query models are no exception.
- Figure 1 provides example generated queries from the state-of-the-art T5 Doc2Query model [28]
- In this example, we see that many of the generated queries cannot actually be answered by the source passage (score â‰¤ 1).
- Original Passage: Barley (Hordeum vulgare L.), a member of the grass family, is a major cereal grain. It was one of the first cultivated grains and is now grown widely. Barley grain is a staple in Tibetan cuisine and was eaten widely by peasants in Medieval Europe. Barley has also been used as animal fodder, as a source of fermentable material for beer and certain distilled beverages, and as a component of various health foods.
- Based on this observation, we hypothesise that retrieval performance of Doc2Query would improve if hallucinated queries were removed.
- In this paper, we conduct experiments where we apply a new filtering phase that aims to remove poor queries prior to indexing.
- Given that this approach removes queries, we call the approach Doc2Query--(Doc2Query-minus-minus).
- Rather than training a new model for this task, we identify that relevance models are already fit for this purpose: they estimate how relevant a passage is to a query.
- We therefore explore filtering strategies that make use of existing neural relevance models.
- Through experimentation on the MS MARCO dataset, we find that our filtering approach can improve the retrieval effectiveness of indexes built using Doc2Query--by up to 16%; less can indeed be more.
- Meanwhile, filtering naturally reduces the index size, lowering storage and query-time computational costs.
- Finally, we conduct an exploration of the index-time overheads introduced by the filtering process and conclude that the gains from filtering more than make up for the additional time spent generating more queries.
- The approach also has a positive impact on the environmental costs of applying Doc2Query; the same retrieval effectiveness can be achieved with only about a third of the computational cost when indexing.

## Related Work
- query reformulation -including stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) -and document expansion [9,30,35];
- classically, query expansion models have been popular, as they avoid the costs associated with making additional processing for each document needed for document expansion. However, query expansion may result in reduced performance [11], as queries are typically short and the necessary evidence to understand the context of the user is limited.
- latent representations of queries and documents, such as using latent semantic indexing [8] allow retrieval using to not be driven directly by lexical signals.
- more recently, transformer-based language models (such as BERT [6]) have resulted in representations of text where the contextualised meaning of words are accounted for.
- in dense retrieval, queries and documents are represented in embeddings spaces [14,37], often facilitated by Approximate Nearest Neighbour (ANN) data structures [13].
- even when using ANN, retrieval can still be inefficient or insufficiently effective [15].
- others have explored approaches for augmenting lexical representations with additional terms that may be relevant.
- in this work, we explore Doc2Query [29], which uses a sequence-to-sequence model that maps a document to queries that it might be able to answer.
- by appending these generated queries to a document's content before indexing, the document is more likely to be retrieved for user queries when using a model like BM25.
- an alternative style of document expansion, proposed by MacAvaney et al. [19] and since used by several other models (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM) mechanism.
- MLM expansion generates individual tokens to append to the document as a bag of words (rather than as a sequence).
- although MLM expansion is also prone to hallucination,2 the bag-of-words nature of MLM expansion means that individual expansion tokens may not have sufficient context to apply filtering effectively.
- we therefore focus only on sequence-style expansion and leave the exploration of MLM expansion for future work.

## Experimental Setup
- Does Doc2Query--improve the effectiveness of document expansion?
- What are the trade-offs in terms of effectiveness, efficiency, and storage when using Doc2Query--?
- We conduct tests using the MS MARCO [26] v1 passage corpus.
- We use five test collections: 3 (1) the MS MARCO Dev (small) collection, consisting of 6,980 queries (1.1 qrels/query); (2) the Dev2 collection, consisting of 4,281 (1.1 qrels/query); (3) the MS MARCO Eval set, consisting of 6,837 queries (held-out leaderboard set); (4/5) the TREC DL'19/'20 collections, consisting of 43/54 queries (215/211 qrels/query).
- We evaluate using the official task evaluation measures: Reciprocal Rank at 10 (RR@10) for Dev/Dev2/Eval, nDCG@10 for DL'19/'20.
- We tune systems4 on Dev, leaving the remaining collections as held-out test sets.
- We use the T5 Doc2Query model from Nogueira and Lin [28], making use of the inferred queries released by the authors (80 per passage).
- To the best of our knowledge, this is the highest-performing Doc2Query model available.

## Results
- RQ1: relevance filtering can improve the retrieval of Doc2Query models
- RQ2: Doc2Query--provides higher effectiveness at lower query-time costs

## Conclusions
- The paper demonstrates that there are untapped advantages in generating natural language for document expansion.
- Specifically, they presented Doc2Query--, which is a new approach for improving the effectiveness and efficiency of the Doc2Query model by filtering out the least relevant queries.
- They observed that a 16% improvement in retrieval effectiveness can be achieved, while reducing the index size by 48% and mean query execution time by 30%.
- The technique of filtering text generated from language models using relevance scoring is ripe for future work.
- For example, relevance filtering could potentially apply to approaches that generate alternative forms of queries [38], training data [2], or natural language responses to queries [5] -all of which are potentially affected by hallucinated content.
- Further, future work could explore approaches for relevance filtering over masked language modelling expansion [19], rather than sequence-to-sequence expansion.
