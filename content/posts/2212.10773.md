---
title: "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"
date: 2022-12-21T05:17:06.000Z
author: "Zhiyang Xu, Ying Shen, Lifu Huang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10773v1.webp" # image path/url
    alt: "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10773)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10773).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multiinstruct-improving-multi-modal-zero-shot).

# Abstract
- Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks.
- However, it's still not explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 47 diverse multimodal tasks covering 11 broad categories. Each task is designed at least with 5,000 instances (input-out pairs) from existing open-source datasets and 5 expert-written instructions.
- We take OFA as the base pre-trained model for multimodal instruction tuning, and to improve its performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate its strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from text-only instructions. We also design a new evaluation metric: Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that the model is less sensitive to the varying instructions after finetuning on a diverse set of tasks and instructions for each task.

# Paper Content

## Introduction
- With the advances in large-scale pre-trained language models (PLMs), recent studies have explored various efficient learning paradigms (Brown et al., 2020;Liu et al., 2021;Wei et al., 2021;Xie et al., 2021) to generalize PLMs to new tasks without task-specific tuning.
- Among these, instruction tuning (Wei et al., 2021) has achieved significant success in zero-shot learning on natural language processing tasks.
- By fine-tuning a PLM on tasks described through instructions, instruction tuning allows the model to learn to understand and follow the instructions to perform predictions on unseen tasks.
- Recent advancement in multimodal pretraining (Wang et al., 2022a;Alayrac et al., 2022;Bao et al., 2022;Wang et al., 2022c) has shown the potential of jointly interpreting text and images in a shared semantic space, which further leads us to ask: can the instruction tuning be leveraged to improve the generalizability of PLMs on vision and multi-modal tasks?
- In this work, we propose MULTIINSTRUCT, the first benchmark dataset for multimodal instruction tuning with 47 diverse tasks from 11 broad categories. MULTIINSTRUCT covers most of the multimodal tasks that require visual understanding and multimodal reasoning, such as Visual Question Answering (Goyal et al., 2017;Zhu et al., 2016;Suhr et al., 2017), Image Captioning (Lin et al., 2014), Image Generation (Changpinyo et al., 2021), Visual Relationship Understanding (Krishna et al., 2017) and so on.
- For each task, we create at least 5,000 instances (i.e., input-output pairs) and 5 instructions that are manually written by two experts in natural language processing.
- As shown in Figure 1, we formulate all the tasks into a unified sequenceto-sequence format in which the input text, images, instructions, and bounding boxes are represented in the same token space.
- We use OFA (Wang et al., 2022a), a unified model that is pre-trained on a diverse set of crossmodel and unimodal tasks in a single Transformerbased sequence-to-sequence framework, as the base pre-trained multimodal language model, and fine-tune it on MULTIINSTRUCT.
- Considering that the scale of NATURAL INSTRUCTIONS (Mishra et al., 2022), a text-only instruction tuning dataset, is much larger than MULTIINSTRUCT, we further explored several transfer learning strategies, including Mixed Instruction Tuning, Sequential Instruction Tuning, and Adapter-based Sequential Instruction Tuning.
- Experimental results demonstrate that (1) instruction tuning significantly reduces the sensitivity of OFA to the varying wording of instructions, and (2) it's also beneficial to reduce the sensitivity by adopting a diverse set of instructions and tasks for instruction tuning.

## Related Work
- Multimodal pretraining has gained increasing attention in recent years and significantly advanced the downstream vision-language tasks.
- Several recent studies (Cho et al., 2021;Wang et al., 2022a,c;Lu et al., 2022) also started to explore building a unified pre-training framework to handle a diverse set of cross-modal and unimodal tasks.
- Among them, VL-T5 (Cho et al., 2021) tackles vision-andlanguage tasks with a unified text-generation objective conditioned on multimodal inputs, while OFA (Wang et al., 2022a) further extends the textgeneration objective to image generation tasks by using a unified vocabulary for all text and visual tokens.
- Similarly, Unified-IO (Lu et al., 2022) uses a unified vocabulary to homogenize all input and output modalities but applies the architecture to a wider set of classical computer vision tasks.
- BEIT-3 (Wang et al., 2022c) uses a unified masked modality modeling approach to process both unimodal and multimodal data.
- It utilizes a novel shared Multiway Transformer network with a shared selfattention module that is able to learn how to align different modalities.
- Compared with all these studies, our work focuses on creating a large-scale multimodal instruction tuning benchmark dataset and improving the generalization and zero-shot performance on various unseen multimodal tasks.
- Efficient language model tuning strategies have been proposed recently.
- Prompt tuning aims to learn a taskspecific prompt while keeping most of the parameters of the model freezed (Liu et al., 2021;Li and Liang, 2021;Han et al., 2022;Wang et al., 2022b).
- In-context learning takes one or a few examples as the prompt to demonstrate the task.
- MetaICL (Min et al., 2021) is a new meta-training framework for few-shot learning, in which a pre-trained language model is fine-tuned to perform in-context learning on a large set of training tasks.
- Min et al. (2022) show that incontext learning does not rely on the correctness of demonstrations but instead benefits from knowing the output label space through empirical studies.
- Instruct tuning is another simple yet effective strategy to improve the generalizability of large language models.
- Wei et al. (2021) first propose FLAN to fine-tune language models on various tasks described by natural language instructions and show significant zero-shot performance on unseen tasks.
- Instruct-GPT (Ouyang et al., 2022) first finetunes GPT-3 (Brown et al., 2020) on crowdsourcing instructions with supervised learning and then continues to optimize it via human feedback.
- Natural-Instructions (Mishra et al., 2022;Wang et al., 2022d) is a meta-dataset containing 61 distinct tasks with human-authored definitions, things to avoid, and demonstrations.
- Researchers (Mishra et al., 2022;Wang et al., 2022d) have shown that NATURAL INSTRUCTIONS can greatly improve the generalizability of language models even when the size is relatively small (e.g., BART_base).

### Multimodal Task and Data Collection
- The MULTIINSTRUCT dataset is designed to cover a wide range of multimodal tasks that require reasoning among regions, image, and text.
- To build MULTIINSTRUCT, we first collect 26 tasks from the existing studies in visual and multimodal learning that require these skills, covering Visual Question Answering (Goyal et al., 2017;Krishna et al., 2017;Zhu et al., 2016;Suhr et al., 2017;Liu et al., 2022;Hudson and Manning, 2019;Singh et al., 2019;Marino et al., 2019), Image Captioning (Lin et al., 2014), Grounded Generation (Krishna et al., 2017;Yu et al., 2016;Lin et al., 2014), Image-Text Matching (Lin et al., 2014;Goyal et al., 2017), Grounded Matching (Krishna et al., 2017;Veit et al., 2016;Yu et al., 2016), Visual Relationship (Krishna et al., 2017), Image Attributes (Chiu et al., 2020), Image Generation (Changpinyo et al., 2021), Commonsense Reasoning (Zellers et al., 2019;Xie et al., 2019), Temporal Ordering tasks that are created from WikiHow 1 , and Miscellaneous (Yao et al., 2022;Kiela et al., 2020;Das et al., 2017).
- Each of the 26 tasks can be found with one or multiple open-source datasets, which are incorporated into MULTIINSTRUCT.
- Details of each task and their corresponding datasets are shown in Tables 7 to 9 in Appendix.
- For each of these tasks, we examined the possibility of decomposing the task into simpler or related tasks to further test and improve the model's capabilities.
- For example, Visual Grounding requires the model to generate a caption for a given region in the image. We derive two additional tasks that are related to this complex skill: Grounded Caption Selection, which is a simpler skill that requires the model to select the corresponding caption from multiple candidates for the given region, and Visual Grounding Selection, which requires the model to select the corresponding region from the provided candidate regions based on a given caption.
- These two tasks are meant to improve the model's capability on similar but simpler skills that are needed to complete the Visual Grounding task.
- In this way, we further derived 21 new additional tasks from the 26 existing tasks.
- We divide all 47 tasks into 11 broad categories as shown in Figure 2.
- For the existing tasks, we use their available open-source datasets to create instances (i.e., input and output pairs) while for each new task, we create its instances by extracting the necessary information from the instances of existing tasks or reformulating them.
- In this way, each new task is created with 5,000 to 5M instances.
- We split the 47 tasks into training and evaluation based on the following 1 https://www.wikihow.com. Image Quality Image Captioning Temporal Ordering  criteria: (1) we take the tasks that are similar to the pre-training tasks of OFA (Wang et al., 2022a) for training; and (2) we also take the tasks that require simple or common skills for training and complex tasks for evaluation, based on the judgment of two expert researchers in Natural Language Processing.

### Task Instruction Creation
- An instruction is defined with a template that describes how the task should be performed and contains an arbitrary number of placeholders, including <TEXT>, <REGION> and <OPTION>, for the input information from the original task.
- To produce high-quality instructions that accurately convey the intended tasks, we employ an iterative annotation process involving two expert annotators who have a thorough understanding of the task and the dataset.
- Step 1: each annotator first writes 2-3 instructions for each task. These annotators are provided with clear and detailed information about the task and the dataset, including the specific goals of this task, the format of the input data, and 10 example instances randomly sampled from the dataset. The details about the dataset are typically obtained from either the dataset's README file or the original publication that introduced the dataset.
- For tasks that are newly derived, we provide task descriptions along with 10 constructed example instances to the annotators.
- Step 2: to guarantee the quality of the instructions and make sure that they are effective for the intended tasks, we have each annotator review the instruction created by the other, checking if they can clearly understand and identify the intended task by just reading the instruction.
- If any issues are identified, the reviewing annotator provides suggestions and works with the original annotator to revise the instructions.
- Step 3: to ensure the instructions from different annotators do not conflict with or repeat each other, we have both annotators review the sets of instructions together and identify any discrepancies or inconsistencies. If any are found, the annotators work together to resolve them and create a final set of instructions that accurately and clearly describe the task.
- In this way, each task will be created with 5 high-quality instructions.
- Step 4: we repeat steps 1-3 to create 5 instructions for each of the training and evaluation tasks. Finally, both annotators review each task and its instructions and filter out the task that is not representative or overlaps with other tasks.

### Multimodal Instruction Formatting
- Represent images, text, and bounding box coordinates as tokens in a unified vocabulary
- Apply bytepair encoding (BPE) to encode text input
- Apply VQ-GAN to generate discrete image tokens through image quantization
- Represent regions or bounding boxes of an image with location tokens such as "<bin_NUM> <bin_180> <bin_736> <bin_475>"

### Problem Setup
- We follow the same instruction tuning setting as previous studies (Wei et al., 2021)
- Specifically, given a pre-trained multimodal language model, we aim to finetune it on a collection of instruction tasks
- Each task is associated with a number of training instances
- The input information from x t j will be used to fill in the placeholders in the instruction
- We use OFA (Wang et al., 2022a) as the pretrained multimodal model
- We further finetune it on our MULTIINSTRUCT dataset to demonstrate the effectiveness of instruction tuning
- Specifically, as OFA is a transformer-based seq2seq framework, we use its transformer encoder to encode the instruction with all the necessary filled information and an optional image and predict the output with the transformer decoder
- Given that the training dataset contains many tasks, we mix all the training instances from these tasks and randomly shuffle them
- For each instance, we also randomly sample an instruction template for each batch-based training
- Note that, though some of the training tasks in MULTIINSTRUCT are similar to the pre-training tasks of OFA2, we ensure that the evaluation tasks in MULTIINSTRUCT do not overlap with either the pre-training tasks in OFA or the training tasks in MULTIINSTRUCT
- We notice that the scale of NATURAL INSTRUC-TIONS (Mishra et al., 2022;Wang et al., 2022d) Tuning the whole large pre-trained language model is computationally expensive and impractical as the size of the model and training tasks/instances grows
- Considering this, we further propose a parameter-efficient instruction tuning strategy by introducing adapters
- Specifically, we insert an adapter after the feedforward layer in each transformer layer in the decoder and encoder of OFA
- The adapters are based on the multi-layer-perceptron structure which consists of a feedforward layer, followed by a non-linear activation layer and another feedforward layer
- To reduce the number of training parameters, we downsize the input features size by four times which is 256 given the 1024 feature dimension in OFA
- During training, we freeze the parameters of OFA and sequentially optimize the parameters of the adapter on NATURAL INSTRUCTIONS and MULTI-INSTRUCT
- During inference, we directly apply the adapter-based OFA to perform zero-shot prediction on various unseen tasks

### Evaluation Metrics
- The accuracy for classification tasks and ROUGE-L for all generation tasks and some classification tasks following Mishra et al. (2022)
- For each task, we evaluate the model using one of the five instructions, resulting in a total of five experiments
- We report the mean and maximum performance across all five experiments
- We also compute the aggregated performance for each model where we take the average of the model's performance score on all unseen tasks
- For most tasks, we use Rouge-L as the performance score
- For tasks that only have accuracy as a metric, we use accuracy to compute the aggregated performance
- We further design a new metric as follows: Sensitivity which refers to the model's capability of consistently producing the same results, regardless of slight variations in the wording of instructions, as long as the intended task remains the same

### Approaches for Comparison
- OFA (Wang et al., 2022a) is a pre-trained model that was not fine-tuned
- OFA-large3 (Wang et al., 2022a) is a pre-trained model that was fine-tuned on 8 tasks
- OFA MultiInstruct (Wang et al., 2022a) only fine-tunes OFA on the newly introduced MULTIINSTRUCT dataset
- OFA NaturalInstruct (Wang et al., 2022a) only fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS dataset
- To ensure a fair comparison, we evaluated this baseline on instruction templates that had removed all specific tokens
- OFA MixedInstruct (Wang et al., 2022a) fine-tunes OFA on the mix of the large-scale NATURAL INSTRUCTIONS dataset and MULTIINSTRUCT dataset
- OFA SeqInstruct (Wang et al., 2022a) sequentially fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS dataset and MULTIINSTRUCT dataset

### Training Details
- We set the maximum length of input tokens to 1024 and the maximum target length to 512.
- For image preprocessing, we strictly follow the process in the OFA.
- We train the models on 8 Nvidia A100 GPUs with a batch size 8 per GPU, a learning rate of 1e-05, and float16 enabled for 3 epochs for all the setups and datasets.
- 6 Results and Discussion
- We evaluate the zero-shot performance of various approaches on all the unseen evaluation tasks, as shown in Table 3, 4 and 5.
- Our results indicate that OFA MultiInstruct significantly improves the model's zero-short performance over the original pre-trained OFA model across all unseen tasks and metrics, demonstrating the effectiveness of multimodal instruction tuning on MULTIIN-STRUCT.
- As seen in Table 3, OFA achieves extremely low (nearly zero) zero-shot performance on the Grounded VQA task, which requires the model to generate region-specific tokens in order to answer the question.
- By examining the generated results, we find that OFA, without instruction tuning, failed to follow the instruction and produce results that contain region tokens.
- However, by fine-tuning OFA on MULTIINSTRUCT with instruction tuning, the model is able to better interpret and follow the instruction to properly generate the expected output.
- One key question in multimodal instruction tuning is how to effectively leverage the large-scale text-only NATURAL INSTRUCTIONS dataset to enhance the instruction guidance and zero-shot performance on multimodal tasks.
- We observe that simply fine-tuning OFA on NATURAL INSTRUC-TIONS actually degrades the model's zero-shot performance, as shown by comparing OFA NaturalInstruct and OFA.
- There are several potential reasons for this decline in performance.
- One potential cause is the misalignment in the embedding space between the vision and language modules, as the language module is updated more frequently than the vision module.
- Another possibility is that fine-tuning the model on a text-only instruction dataset may make it pay less attention to image and region inputs.
- Another observation is that simply training OFA on the mixture of MULTIINSTRUCT and NATU-RAL INSTRUCTIONS does not lead to the same level of performance improvement compared with OFA only trained on MULTIINSTRUCT, which is possibly due to the imbalance of the training tasks from these two datasets.
- On the other hand, OFA SeqInstruct achieves similar if not better performance compared with OFA MultiInstruct , demonstrating the potential benefit of the much larger text-only instruction datasets to multimodal instruction tuning.
- Compared with OFA SeqInstruct which fine-tunes the whole parameter set of OFA, roughly 473M parameters, OFA AdapterInstruct only tunes 12.6M parameters (2.7% of the parame-ters tuned in OFA SeqInstruct ) but achieves comparable or slightly worse performance on all the unseen multiple tasks. This suggests the potential of OFA AdapterInstruct as a more parameter-efficient method for instruction tuning, especially when the size of the pre-trained multimodal language model or training tasks and datasets is very large.

### Number of Multimodal Instruction Tasks
- The number of multimodal instruction tasks has a positive impact on aggregated performance and intra-task sensitivity
- The proposed MULTIINSTRUCT dataset is effective

### Effect of Diverse Instructions on Instruction Tuning
- We hypothesize that using a diverse set of instructions for each task during multimodal instruction tuning can improve the model's zero-shot performance on unseen tasks and reduce its intra-task sensitivity to variation in the instructions.
- To test this hypothesis, we train an OFA model on MULTI-INSTRUCT but with a single fixed instruction template per task, and compare its performance with the one that is tuned on 5 different instructions.
- As shown in Table 6, OFA which is finetuned on 5 instructions significantly improves the overall zeroshot performance on all evaluation tasks and shows lower sensitivity. These results demonstrate the effectiveness of increasing the diversity of instructions and suggest that future work could explore crowd-sourcing or automatic generation strategies to create even more diverse instructions for multimodal instruction tuning.

## Conclusion and Future Work
- MULTIINSTRUCT is a new large-scale multimodal instruction tuning benchmark dataset
- OFA (Wang et al., 2022a) is a recently state-of-the-art multimodal pre-trained language model
- By finetuning OFA on MULTIINSTRUCT, its zero-shot performance on various unseen multimodal tasks is significantly improved
- Several transferring learning techniques are explored to leverage the much larger text-only NAT-URAL INSTRUCTIONS dataset and demonstrate their benefit
- Task Name Dataset Description Exist VQA Open-Domain VQAv2 (Goyal et al., 2017), Visual Genome (Krishna et al., 2017) Answer the question <QUESTION> based on the content of the given image. Visual7w (Zhu et al., 2016) Answer a visual question <QUESTION> by selecting an answer from given options. <OPTION> Given the region <REGION> in the image, generate a caption for that region. Visual Genome (Krishna et al., 2017) Given a caption <TEXT> for some region in the image, identify the region and generate its bounding box. Object Identification MSCOCO (Lin et al., 2014) Identify the type of an object in <REGION>. Object Grounding MSCOCO (Lin et al., 2014) What are the regions containing the object [TEXT]? × RefCOCO (Yu et al., 2016) Locate a region in an image based on the referring expression [TEXT]. RefCOCO (Yu et al., 2016) Generate the referring expression for an object in region <REGION>. Text Localization COCO-Text (Veit et al., 2016) Select a region from options that contain the text <TEXT> in the image. <OPTION> Image Captioning MSCOCO (Lin et al., 2014) Generate a sentence to describe the content of the image. Image-Text Matching MSCOCO (Lin et al., 2014) Decide if the text matches the image. × VQAv2 (Goyal et al., 2017) Decide if the image contains an answer to the question <QUESTION>. × Image-Text Selection MSCOCO (Lin et al., 2014) Select the text that best matches the image. <OPTION> × Visual Genome (Krishna et al., 2017) Decide if the caption matches the given region <REGION> in the image. × Visual Genome (Krishna et al., 2017) Given a region <REGION> in the image, select a caption from given options for that region. <OPTION> Missing Object Selection MSCOCO (Lin et al., 2014) Select an object from options that does not appear in any of the given regions <REGION>. <OPTION> Visual Genome (Krishna et al., 2017) Given a caption <TEXT> for some region in the image, select the region from the options. <OPTION> RefCOCO (Yu et al., 2016) Select a region from options based on the referring expression <TEXT>. <OPTION> × Object-Region Matching MSCOCO (Lin et al., 2014) Does region <REGION> contain the object <TEXT>? × MSCOCO (Lin et al., 2014) Select the region containing the given object <TEXT>. <OPTION> × Object Matching MSCOCO (Lin et al., 2014) Do objects in region <REGION1> and region <REGION2> have the same type?
