---
title: "Muse: Text-To-Image Generation via Masked Generative Transformers"
date: 2023-01-02T14:43:38.000Z
author: "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-00704v1_VwQx9Mz2F.jpg" # image path/url
    alt: "Muse: Text-To-Image Generation via Masked Generative Transformers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00704)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00704).


# Abstract
- Muse is a text-to-image transformer that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models.
- Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM).
- Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding.
- The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc.
- Muse 3B parameter model achieves an FID score of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32.

# Paper Content

## Introduction
- Generative image models conditioned on text prompts have taken an enormous leap in quality and flexibility in the last few years
- This was enabled by a combination of deep learning architecture innovations (Van Den Oord et al., 2017;Vaswani et al., 2017); novel training paradigms such as masked modeling for both language (Devlin et al., 2018;Raffel et al., 2020) and vision tasks (He et al., 2022;Chang et al., 2022); new families of generative models such as diffusion (Ho et al., 2020;Rombach et al., 2022;Saharia et al., 2022) and masking-based generation (Chang et al., 2022); and finally, the availability of large scale image-text paired datasets (Schuhmann et al., 2021).
- In this work, we present a new model for text-to-image synthesis using a masked image modeling approach (Chang et al., 2022).
- Our image decoder architecture is conditioned on embeddings from a pre-trained and frozen T5-XXL (Raffel et al., 2020) large language model (LLM) encoder. In agreement with Imagen (Saharia et al., 2022), we find that conditioning on a pre-trained LLM is crucial for photorealistic, high quality image generation.
- Our models (except for the VQGAN quantizer) are built on the Transformer (Vaswani et al., 2017) architecture. We have trained a sequence of Muse models, ranging in size from 632M parameters to 3B parameters (for the image decoder; the T5-XXL model has an additional 4.6B parameters). Each model consists of several sub-models (Figure 3): First, we have a pair of VQGAN "tokenizer" models (Esser et al., 2021b), which can encode an input image to a sequence of discrete tokens as well as decode a token sequence back to an image. We use two VQGANs, one for 256x256 resolution ("low-res") and another for 512x512 resolution ("high-res"). Second, we have a base masked image model, which contains the bulk of our parameters. This model takes a sequence of partially masked low-res tokens and predicts the marginal distributionfor each masked token, conditioned on the unmasked tokens and a T5XXL text embedding. Third, we have a "superres" transformer model which translates (unmasked) low-res tokens into high-res tokens, again conditioned on T5-XXL text embeddings. We explain our pipeline in detail in Section 2.
- Compared to Imagen (Saharia et al., 2022) or Dall-E2 (Ramesh et al., 2022) which are built on cascaded pixel-space diffusion models, Muse is significantly more efficient due to the use of discrete tokens; it can be thought of as a discrete diffusion process with the absorbing state ([MASK]) (Austin et al., 2021).
- Compared to Parti (Yu et al., 2022), a state-of-the-art autoregressive model, Muse is more efficient due to the use of parallel decoding. Based on comparisons on similar hardware (TPU-v4 chips), we estimate that Muse is more than 10x faster at inference time than either Imagen-3B or Parti-3B models and 3x faster than Stable Diffusion v1.4 (Rombach et al., 2022) (see Section 3.2.2).
- All these comparisons are when images of the same size: either 256 × 256 or 512 × 512. Muse is also faster than Stable Diffusion (Rombach et al., 2022), in spite of both models working in the latent space of a VQGAN. We believe that this is due to the use of a diffusion model in Stable

## Model
- The model is built on a number of components
- The model is built in a supervised learning algorithm
- The model is able to predict the next letter in a text
- The model is able to predict the next word in a text
- The model is able to predict the next sentence in a text
- The model is able to predict the next paragraph in a text
- The model is able to predict the next document in a corpus
- The model is able to predict the next sentence in a document
- The model is able to predict the next paragraph in a document
- The model is able to predict the next document in a corpus

### Pre-trained Text Encoders
- Pre-trained large language models (LLM) are beneficial to high-quality image generation
- The embeddings extracted from an LLM such as T5-XXL (Raffel et al., 2020) carry rich information about objects (nouns), actions (verbs), visual properties (adjectives), spatial relationships (prepositions), and other properties such as cardinality and composition
- Our hypothesis is that the Muse model learns to map these rich visual and semantic concepts in the LLM embeddings to the generated images; it has been shown in recent work (Merullo et al., 2022) that the conceptual representations learned by LLM's are roughly linearly mappable to those learned by models trained on vision tasks

### Semantic Tokenization using VQGAN
- Uses semantic tokens obtained from a VQGAN model
- Encoder and decoder are built entirely with convolutional layers
- Uses a cross-entropy loss at the output to predict masked tokens in the next stage

### Base Model
- Our base model is a masked transformer (Vaswani et al., 2017;Devlin et al., 2018)
- We leave all the text embeddings unmasked and randomly mask a varying fraction of image tokens (see Section 2.6)
- We then linearly map image tokens into image input embeddings of the required Transformer input/hidden size along with learned 2D positional embeddings
- Following previous transformer architecture (Vaswani et al., 2017), we use several transformer layers including self-attention block, cross-attention block and MLP block to extract features
- At the output layer, an MLP is used to convert each masked image embedding to a set of logits (corresponding to the VQGAN codebook size) and a cross-entropy loss is applied with the ground truth token label as the target
- At training, the base model is trained to predict all masked tokens at each step. However, for inference, mask prediction is performed in an iterative manner which significantly increases quality

### Super-Resolution Model
- Two VQGAN models were trained at 16 × 16 and 256 × 256 resolutions
- The lower resolution model was trained with text conditioning and cross-attention
- The higher resolution model was trained with latent map translation

### Decoder Finetuning
- Increases the capacity of the VQGAN decoder by the addition of more residual layers and channels
- Keeps the encoder capacity fixed
- Finetunes the new decoder layers while keeping the VQGAN encoder weights, codebook and transformers (i.e., base model and super resolution model) frozen
- Shows that the finetuned decoder can reconstruct more sharper details in the store front

### Variable Masking Rate
- The model is trained with a variable masking rate based on a Cosine scheduling
- The bias towards higher masking rates makes the prediction problem harder
- In contrast with autoregressive approaches, which learn conditional distributions P (x i |x <i ) for some fixed ordering of tokens, random masking with a variable masking ratio allows the models to learn P (x i |x Λ ) for arbitrary subsets of tokens Λ

### Classifier Free Guidance
- Employ classifier-free guidance (CFG) to improve generation quality and text-image alignment
- Remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention)
- Compute a conditional logit c and an unconditional logit u for each masked token
- Form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: Intuitively, CFG trades off diversity for fidelity.
- Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure.
- This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens.
- We also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a "negative prompt".

### Iterative Parallel Decoding at Inference
- Parallel decoding is used to reduce the number of decoding steps required
- The model is effective even when the number of tokens to be predicted is large
- The model can be applied to text-to-image generation

## Results
- We train a number of base Transformer models at different parameter sizes
- Each of these models is fed in the output embeddings from a T5-XXL model, which is pre-trained and frozen and consists of 4.6B parameters
- Our largest base model of 3B parameters consists of 48 Transformer layers with cross-attention from text to image and self-attention among image tokens
- All base models share the same image tokenizer
- We use a CNN model with 19 ResNet blocks and a quantized codebook of size 8192 for the tokenization
- Larger codebook sizes did not result in performance improvements
- The super-resolution model consists of 32 multi-axis Transformer layers (Zhao et al., 2021) with cross-attention from concatenated text and image embedding to high resolution image and self-attention among high resolution image tokens
- This model converts a sequence of tokens from one latent space to another: the first latent space being that of the base model tokenizer, a latent space of 16 × 16 tokens, to that of a higher resolution tokenizer with 64 × 64 tokens
- After token conversion, the decoder for the higher resolution tokenizer is used to convert to the higher resolution image space
- Further details of configurations are provided in the appendix
- We train on the Imagen dataset consisting of 460M text-image pairs (Saharia et al., 2022)
- Training is performed for 1M steps, with a batch size of 512 on 512-core TPU-v4 chips (Jouppi et al., 2020)
- This takes about 1 week of training time
- We use the Adafactor optimizer (Shazeer & Stern, 2018) to save on memory consumption which allowed us to fit a 3B parameter model without model parallelization
- We also avoid performing exponential moving averaging (EMA) of model weights during training, again to save on TPU memory
- In order to reap the benefits of EMA, we checkpoint every 5000 steps, then perform EMA offline on the checkpointed weights with a decay factor of 0.7

### Qualitative Performance
- Figure 6 qualitatively demonstrates the capabilities of Muse for text prompts with different properties.
- The top left of Figure 6 shows examples that demonstrate a basic understanding of cardinality. For objects with non-unity cardinality, instead of generating the same object pixels multiple times, Muse instead adds contextual variations to make the overall image more realistic, e.g., elephant size and orientation, wine bottle wrapper color, and tennis ball rotation.
- The top right of Fig, 6 demonstrates understanding of multi-object composition and relativeness. Instead of placing objects at random locations, Muse generates images that preserve prepositional object relations in the text, e.g., on vs under, left vs right, etc.
- The middle left of Figure 6 demonstrates its ability to generate images spanning many styles, both specific to a renowned artist (e.g., Rembrandt) as well as general to a style as a whole (e.g., pop art and Chinese ink and wash).
- The middle right of Figure 6 demonstrates the ability of Muse to render words and phrases. Text generation is fundamentally different than generating most other objects. Instead of the model learning a mapping between an object name and its characteristics (e.g., that "elephant" maps to "large", "gray", and "peanut eating"), the virtual continuum of possible words and phrases demands that the model learn differently. It must instead learn a hierarchical understanding between phrases, words, and letters.
- The bottom left of Figure 6 demonstrates that Muse uses the entirety of a text prompt when rendering instead of focusing Three elephants standing on top of each other.
- A tiny football in front of three yellow tennis balls. Three small yellow boxes on a large blue box. A large present with a red ribbon to the left of a Christmas tree. Two baseballs to the left of three tennis balls.
- Portrait of a well-dressed raccoon, oil painting in the style of Rembrandt. A portrait of a man wearing sunglasses and a business suit, painting in pop art style. Portrait of a tiger wearing a train conductor's hat and holding a skateboard that has a yin-yang symbol on it. Chinese ink and wash painting. A t-shirt with Carpe Diem written on it. High-contrast image of the word "WOMBAT" written with thick colored graffiti letters on a white wall with dramatic splashes of paint.
- The saying "BE EXCEL-LENT TO EACH OTHER" written in a stained glass window.
- An art gallery displaying Monet paintings. The art gallery is flooded. Robots are going around the art gallery using paddle boards.
- A photograph of the inside of a subway train. There are raccoons sitting on the seats. One of them is reading a newspaper. The window shows the city in the background. Two cups of coffee, one with latte art of yin yang symbol. The other has latter art of a heart.
- A cartoon of a dog saying "I see what you did there".
- A basketball game between a team of four cats and a team of three dogs.
- exclusively on only a few salient words.
- Finally, Figure 7 shows comparisons between Muse, Dall-E 2 (Ramesh et al., 2022), and Imagen (Saharia et al., 2022) for some select prompts, showing that Muse is at par with Imagen and qualitatively better than Dall-E2 for many prompts. However, as demonstrated in the bottom right of Figure 6, Muse is limited in its ability to generate images well aligned with certain types of prompts.
- For prompts which indicate that long, multi-word phrases should be directly rendered, Muse has a tendency to render those phrases incorrectly, often resulting in (unwanted) duplicated rendered words or rendering of only a portion of the phrase. Additionally, prompts indicating high object cardinality tend to result in generated images which do not correctly reflect that desired cardinality (e.g., rendering only 7 wine bottles when the prompt specified 10).
- In general, the ability of Muse to render the correct cardinalities of objects decreases as the cardinality increases.

### Quantitative Performance
- Both Muse models were trained on CC3M
- The COCO results are zero-shot, using a model trained on the same dataset as Imagen
- Our 632M model achieves SOTA results on CC3M, significantly improving upon the state of the art in FID score, and also achieving state of the art CLIP score.
- Our 3B model achieves an FID score of 7.88 which is slightly better than the score of 8.1 achieved by the Parti-3B model which has a similar number of parameters.
- Our CLIP score of 0.32 is higher than the CLIP score of 0.29 achieved by Imagen (which is achieved when the FID is significantly higher 20).
- For the FID of 7.27, Imagen achieves a CLIP score of around 0.27 (see Figure 4 in (Saharia et al., 2022)).
- Our sampling algorithm (Section 2.8) has a number of hyperparameters, such as guidance scale, sampling temperature, whether or not to linearly increase guidance during sampling, etc.
- We perform evaluation sweeps over these parameters.
- We find subsets of sampling parameters that are Pareto efficient, in the sense that we cannot improve FID without hurting CLIP.
- This allows us to study the tradeoff between diversity and image/text alignment, which we show in Figure 8.
- Similar to previous works (Yu et al., 2022;Saharia et al., 2022), we perform side-by-side evaluations in which human raters are presented with a text prompt and two images, each generated by a different text-to-image model using that prompt.
- The raters are asked to assess prompt-image alignment via the question, "Which image matches with the caption better?"
- Each image pair is anonymized and randomly ordered (left vs right).
- Raters have the option of choosing either image or that they are indifferent1 .
- Each (prompt, image pair) triplet is assessed by five independent raters; the raters were provided through the Google internal crowd computing team and were completely anonymous to the Muse team.
- For the set of prompts presented to raters, we used PartiPrompts (Yu et al., 2022), a collection of 1650 text prompts curated to measure model capabilities across a variety of categories.
- For the two text-to-image models, we compared Muse (3B parameters) to that of Stable Diffusion v1.4 (Rombach et al., 2022), the text-to-image model most comparable to Muse in terms of inference speed.
- For each prompt, 16 image instances were generated, and the one with the highest CLIP score (Radford et al., 2021) was used.
- The stable diffusion images were generated via the CompVis Stable Diffusion v1.4 notebook (CompVis, 2022).
- We required at least a 3 rater consensus for results to be counted in favor of a particular model.
- From this analysis, we found that Muse was chosen as better aligned than Stable Diffusion for 70.6% of the prompts, Stable Diffusion was chosen as better aligned than Muse for 25.4%, and no rater consensus was chosen for 4%.
- These results are consistent with Muse having significantly better caption matching capability (∼ 2.7x).

### Image Editing
- The model can condition on arbitrary subsets of image tokens
- This has the effect of moving the tokenized image into the typical set of the conditional distribution of images given a text prompt
- We perform the editing using the low-resolution base model, then perform superres on the final output (conditioned on the editing prompt)

## Related Work

### Image Generation Models
- Variational autoencoders and Generative Adversarial Models have been shown to be excellent at generating images
- Diffusion models, based on progressive denoising principles, are now able to synthesize images and video at equal or higher fidelity
- Hybrid approaches that combine principles from multiple approaches have also shown excellent performance

### Image Tokenizers
- Image tokenizers are useful for multiple generative models
- A number of tokenization approaches have been developed, with the latter being the highest-performing
- ViT-VQGAN is used rather than ViT-VQGAN as it performs better

### Large Language Models
- Our work leverages a pre-trained large language model (LLM) that has been trained on multiple text-to-text tasks
- LLMs (including T5, BERT (Devlin et al., 2018), and GPT (Brown et al., 2020;Radford et al., 2019)) have been shown to learn powerful embeddings which enable few-shot transfer learning
- We leverage this capacity in our model by applying a transformer to predict visual tokens

### Text-Image Models
- Leveraging paired text-image data is proving to be a powerful learning paradigm for representation learning and generative models.
- CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) train models to align pairs of text and image embeddings, showing excellent transfer and few-shot capabilities.
- Imagen (Saharia et al., 2022) and Parti (Yu et al., 2022) use similar large scale text-image datasets (Schuhmann et al., 2021;2022) to learn how to predict images from text inputs, achieving excellent results on FID and human evaluations.
- A key trick is the use of classifier-free guidance (Ho & Salimans, 2022;Dhariwal & Nichol, 2021) that trades off diversity and quality.

### Image Editing with Generative Models
- GANs have been extensively studied for image editing and manipulation capabilities
- A number of techniques have been developed on diffusion models to enable editing, personalization and inversion to token space
- Dreambooth (Ruiz et al., 2022) and Imagic (Kawar et al., 2022) involve fine-tuning of the generative models
- ImagenEditor (Wang et al., 2022) frames the editing task as text-guided image inpainting, and involves user specified masks

## Discussion and Social Impact
- The Muse model confirms the findings of Saharia et al. ( 2022) that frozen large pretrained language models serve as powerful text encoders for text-to-image generation.
- We also tried in our initial experiments to learn a language model from scratch on the training data, but found that performance was significantly worse than using a pre-trained LLM, especially on long prompts and rare words.
- We also show that non-diffusion, non-autoregressive models based on the Transformer architecture can perform at par with diffusion models while being significantly more efficient at inference time.
- We achieve SOTA CLIP scores, showing an excellent alignment beteween image and text.
- We also show the flexibility of our approach with a number of image editing applications.
- We recognize that generative models have a number of applications with varied potential for impact on human society.
- However, it is well known that they can also be leveraged for misinformation, harassment and various types of social and cultural biases.
- Due to these important considerations, we opt to not release code or a public demo at this point in time.
