---
title: "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units"
date: 2022-12-15T18:58:28.000Z
author: "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08055v1.webp" # image path/url
    alt: "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08055)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08055).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/unity-two-pass-direct-speech-to-speech).

# Abstract
- Direct speech-to-speech translation is advantageous over cascaded approaches
- We present a novel two-pass direct S2ST architecture, UnitY, which first
- We enhance the model performance by subword prediction in the first-pass decoder,
- To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder

# Paper Content

## Introduction
- Automatic speech translation is an essential technology for international communications
- A traditional approach of speech-to-speech translation is to cascade automatic speech recognition, machine translation, and text-to-speech components, each of which is optimized separately on different data
- With the emergence of sequence-to-sequence models, it is becoming more prevalent to adopt a direct approach of translating input speech into another language
- The direct approach is attractive for building a low-latency system with a simplified pipeline, but direct speech-to-text translation models suffer from poor performance due to data scarcity
- In the field of speech-to-unit translation, data shortage has been addressed by leveraging pre-training, multi-task learning, and pseudo labeling
- Consequently, the translation quality of direct speech-to-text translation models is approaching that of cascaded speech-to-text translation models
- Recent works propose to model discrete acoustic units instead of a continuous speech signal, which enables usage of a standard crossentropy loss during training
- The discrete units are directly converted to the waveform with a unit-based neural vocoder bypassing spectrogram representation
- On the other hand, Trans-latotron2 decomposes the target representations into linguistic and acoustic counts
- In Jia et al. (2022b), it is defined as a model that directly predicts the target spectrogram
- In this paper, we use a more general definition that the entire architecture is optimized jointly and the translation is conducted in a more direct way
- We do not include a vocoder in the training pipeline of all direct models
- UnitY achieves 4.2, 3.7, and 2.5 ASR-BLEU improvements over the S2UT model on the Fisher Es→En (Post et al., 2013), CVSS-C (Jia et al., 2022c), and multi-domain En↔Es (Popuri et al., 2022) corpora, respectively
- The improvement holds even with high-resource data and pre-training
- In addition, our proposed design improves Trans-latotron2 as well, indicating its versatility for two-pass direct speech-to-text translation architectures regardless of the choice of the target

### Architecture
- Let N 1st , N 2nd , and N t2u be the depth of the first-pass decoder, second-pass decoder, and T2U encoder of UnitY, respectively.
- We set (N 1st , N 2nd , N t2u ) to (4, 2, 2) on Fisher and CVSS-C.
- On the multi-domain corpus, we use (12, 2, 2) when pre-training the first-pass decoder with mBART. Otherwise, we use (6, 6, 2).
- We describe the other architecture configurations in Appendix G.
- The first-pass text decoder TextDec generates a sequence of subwords Y autoregressively by attending the speech encoder output H.
- The training objective of the first pass is to minimize the direct S2TT loss L s2t as: where D text i ∈ R d model is the i-th continuous decoder state right before projecting it to the logit.
- We consider that D text contains rich acoustic information in addition to contextual information thanks to multiple multi-head cross-attention over H.
- There are five advantages of generating subwords instead of phonemes. First, the sequence length is considerably reduced, leading to better training and inference efficiencies (Cherry et al., 2018). Second, using large vocabularies improves the translation quality of the first pass (Gowda and May, 2020). Third, the text output helps the audience understand the translation content while listening to the audio. Fourth, our approach can easily scale to more target languages, as it is unnecessary to prepare separate grapheme-to-phoneme (G2P) models for each target language. Last, readable text can be generated without any complicated post-processing such as WFST (Mohri et al., 2002;Bahdanau et al., 2016).
- The T2U encoder bridges the gap in representations between text and unit decoders without changing the sequence length.
- The second-pass unit decoder UnitDec generates a sequence of discrete units U autoregressively by attending to only the T2U encoder output Z.
- The training objective of the second pass is to minimize L s2u similar to the S2UT task while being conditioned on Y as: where D unit i ∈ R d model is the i-th continuous decoder state right before projecting it to the logit.
- The unit decoder does not attend to H to synchronize the text and unit outputs, similar to the motivation in (Jia et al., 2022b).

### Training with R-Drop
- UnitY introduces an intermediate S2TT sub-task to make the optimization tractable while maintaining the end-to-end differentiability.
- However, the easier S2TT task is more likely to overfit than the primary S2UT task.
- To tackle this problem, we apply a more effective regularization based on R-Drop (Wu et al., 2021) to the first-pass decoder in addition to standard regularization such as dropout (Srivastava et al., 2014) and label smoothing (Szegedy et al., 2016).
- Theoretically, R-Drop reduces the inconsistency of model predictions between training and inference by dropout, thus improving the generalization ability.
- R-Drop duplicates the network input during training and calculates two output probability distributions with different dropout masks.
- Then, a constraint is introduced by minimizing the Kullback-Leibler (KL) divergence loss between the two probability distributions.
- We apply R-Drop to both text and unit decoders.
- The total training objective of UnitY with R-Drop, L total , is formulated as follows: where X i is a duplicated input from X, L s2u kl and L s2t kl are R-Drop losses for the unit and text decoders, w s2t is a weight for the S2TT loss, and α and β are weights for the R-Drop losses, respectively.

### Text decoder pre-training
- Similar to ASR and S2TT studies, S2ST models also benefit from self-supervised pre-training
- In addition to the speech encoder pretraining with wav2vec2.0 (Baevski et al., 2020), Popuri et al. (2022) initializes the unit decoder of the S2UT model with a unit-based mBART, which is pre-trained with discrete units converted from a large amount of unlabeled speech data. However, unlabeled text data cannot be leveraged for the single-pass decoder pre-training, although it is more accessible in many written languages. To fully leverage the unlabeled text data, we initialize the first-pass decoder with a text-based mBART, which is pre-trained with unlabeled text data in a self-supervised way. Following Li et al. (2021); Popuri et al. (2022), we freeze parameters in the feed-forward network (FFN) of the text decoder during S2ST fine-tuning.
- We initialize the T2U encoder and second-pass unit decoder randomly.

### Search algorithm
- Inference: We perform two-pass beam search decoding.
- First, we find the most probable text hypothesis Ŷ in the first-pass decoder using beam search with a beam size of B 1st .
- We then feed continuous decoder states D text corresponding to Ŷ to the T2U encoder.
- Next, we generate the most probable discrete unit sequence Û in the second-pass decoder by another beam search with a beam size of B 2nd .
- Finally, Û is taken as input to a separate unit-based vocoder to generate the waveform.
- We find it more effective to assign a larger beam size to the first pass, i.e., B 1st > B 2nd , because there is more diversity among beam candidates than the second pass.
- The computation time is also reduced since the sequence length of text is much shorter than that of discrete units.

### Deep-shallow two-pass decoders
- The number of layers in a deepshallow two-pass decoder can be increased to improve translation quality and inference efficiency simultaneously
- This is studied in Kasai et al. (2021)

## Experimental setting
- In this section, we describe experimental settings for our experiments.
- In §4, we describe our experiments.

### Data
- We use three datasets: Fisher Es→En (Post et al., 2013), CVSS-C (Jia et al., 2022c), and mutlidomain En↔Es (Popuri et al., 2022) corpora.
- The Fisher Es→En corpus contains 170-hour Spanish conversational telephone speech with the corresponding transcriptions as well as the English translations.
- The CVSS-C corpus is a public multilingual S2ST corpus based on CoVoST2 (Wang et al., 2021b).
- We use the CVSS-C part of the CVSS corpus, in which a single-speaker female TTS synthesizes the target speech.
- We combine all language directions to train a single many-to-English multilingual model.
- Popuri et al. (2022), we use all samples from multiple public S2TT corpora in each direction to improve the robustness of model training (Jia et al., 2022b;Chan et al., 2021).
- We also use all samples from validation sets in all domains for checkpoint selection.
- We further augment the S2ST training data by pseudo-labeling ASR corpora with MT and T2U/TTS models.
- We used a T2U model (Lee et al., 2022b) for direct speech-to-unit models and a TTS model for the rest.
- Both T2U and TTS models are based on Transformer.

### Pre-processing
- We use the same pre-processing as Lee et al. (2022a,b) and Popuri et al. (2022).
- We discard over-generated target speech/unit by TTS/T2U models.

### Pre-training
- We use the same En/Es wav2vec2.0 and unit En-Es mBART models as Popuri et al. (2022)
- We train a multilingual w2v-BERT (Chung et al., 2021) model trained on 51 languages
- For text decoder pre-training, we use the same En-Es and 50-language mBART models as Wang et al. (2022) and Tang et al. (2020)

### Baseline
- We build two cascaded S2ST systems and four direct S2ST systems
- All speech encoders are based on Conformer
- When pre-training the speech encoder of direct systems with wav2vec2.0/w2v-BERT, we also pre-train that of ASR and S2TT models in the cascaded systems with the same wav2vec2.0/w2v-BERT for a fair comparison
- R-Drop is applied to all the models that predict discrete symbols
- The training objective of each system is described in Appendix F.
- We combine a Conformer ASR, a Transformer MT, and a Transformer TTS model
- We set the reduction factor of TTS models to 4
- We combine a Conformer direct S2TT model and a Transformer TTS model
- For the multi-domain corpora, we pre-train the S2TT's decoder with mBART
- Translatotron
- We build a direct S2ST model that predicts spectrogram with a single Transformer decoder, similar to Lee et al. (2022a)
- We train an improved version of Translatotron2 (Jia et al., 2022b) by enhancing the architecture and training with the proposed methods for UnitY.
- First, we replace phoneme targets with subwords in the first pass.
- Second, we replace LSTM decoders with Transformer decoders.
- Third, we introduce an additional encoder between text and spectrogram decoders, which we refer to as a text-to-spectrogram (T2S) encoder.
- The second-pass decoder attends to the T2S encoder output only.
- Fourth, we use an autoregressive Transformer decoder instead of a non-attentive Tacotron (NAT) (Shen et al., 2020) for the secondpass spectrogram decoder.
- Last, we apply R-Drop to the first-pass decoder.

### Vocoder
- We use a HiFi-GAN vocoder to convert spectrograms to the waveform for TTS and direct speech-to-spectrogram models.
- We use a unit-based HiFi-GAN vocoder to convert discrete units to the waveform for direct speech-to-unit models.

### Training
- We optimize all models with the mixed precision training.
- We implement our models based on the Fairseq toolkit.
- The detailed training hyperparameters are described in Appendix H.

### Decoding
- We use a beam width of 10 for ASR, S2TT, and S2UT models.
- For UnitY, we set B 1st and B 2nd to 10 and 1, respectively.
- We use a beam width of 10 for the first-pass decoder in Translatotron2+.

### Evaluation
- Lee et al. (2022a) use a pre-trained ASR model to transcribe the generated target speech
- ASR model is fine-tuned from a wav2vec2.0 with the connectionist temporal classification (CTC) objective
- We use the sacrebleu toolkit (Post, 2018) to calculate the BLEU scores.

## Experimental results
- The three corpora studied are Fisher, a speech recognition task; Translatotron2+, a machine translation task; and a set of six machine translation tasks from the Google Translate corpus.
- The models studied are a direct system (A11, A15, A18, A20), a Conformer-based S2UT (A18), a Translatotron2+ (A15), a two-pass direct model (A15, A20), a cascaded system (A6), and a pre-trained speech encoder (A12, A16, A19, A21).
- The direct systems (A11, A15, A18, A20) outperform the corresponding cascaded system (A7) by a large margin. The two-pass direct models (A15, A20) outperform a cascaded system (A6) by a small margin.

### CVSS-C
- The results on CVSS-C are listed in Table 2.
- We observed consistent trends with the results on Fisher.
- UnitY (B21, B22) outperformed the S2UT model (B18, B19) by 1.6 and 2.9 ASR-BLEU on average with and without encoder pre-training with an S2TT model, respectively.
- The encoder pretraining with the S2TT model in the cascaded system (B3) improved ASR-BLEU scores of all the direct S2ST models (B7, B16, B19, B22), similar to (Jia et al., 2022c).
- Translatotron2+ (B16) also achieved similar translation quality to UnitY and outperformed Translatotron2 (B11) by 1.1 ASR-BLEU on average.
- Compared to the best cascaded system (B2), the two-pass models (B16, B19) showed better translation quality.
- We also pre-trained the speech encoder of all models with multilingual w2v-BERT (Chung et al., 2021) and first-pass text decoder of UnitY and Translatotron2+ with mBART.
- Among models pre-trained with w2v-BERT (B4, B8, B12, B20, B23), UnitY (B23) showed the best ASR-BLEU.
- B23 still outperformed Translatotron2 with a joint speech-text pre-training with mSLAM (Bapna et al., 2022) (B13) and TTS augmentation (B14) by a large margin.
- We present results on the multi-domain corpora (Popuri et al., 2022) in Table 3.
- C1', C2', and C5' are our improved models of C1, C2, and C5, respectively.
- We observed that UnitY with text decoder pre-training (C7) improved the S2UT model with unit decoder pre-training (C5') by 1.3 and 2.5 ASR-BLEU on average in En→Es and Es→En directions, respectively.
- This confirms the effectiveness of the two-pass modeling in the highresource scenario.
- Furthermore, UnitY without text decoder pre-training (C6) already outperformed C5' and degraded from C7 only slightly.
- Comparing UnitY and Translatotron2+, we cannot spot a clear winner. UnitY outperformed Translatotron2+ in Es→En on Europarl-ST and mTEDx, but Trans-latotron2+ performed better in En→Es.
- The proposed text decoder pre-training helped Transla-totron2+ performance too, especially in En→Es direction (C4).
- Finally, we also confirmed that UnitY approached the performance of a strong cascaded system and even outperformed it on Must-C.

### Decoding efficiency
- The encoder in all the models is pre-trained with wav2vec2.0.
- XX Dec-PT stands for pre-training of the first-pass decoder with a XXbased mBART model.
- N 1st L→ N 2nd L stands for an N 1st -layer first-pass decoder with an N 2nd -layer second-pass decoder.
- unit prediction and two-pass decoding, thanks to reduced output sequence lengths.
- Deep-shallow twopass decoders also improved the decoding speed a lot.
- We found that the translation quality of the two-pass models improved by increasing the beam width of the first-pass decoder up to 10.
- On the other hand, the quality did not degrade significantly by decreasing the beam width of the second-pass decoder down to 1, i.e. greedy decoding.
- This indicates that the first pass involves more challenges in the modeling pipeline. Therefore, we can obtain better translation quality and decoding speed by assigning more computation time to the first pass.

## Analysis
- The multidomain Es→En corpus is used to study the source of improvements in UnitY
- The multidomain Es→En corpus is used to study whether the same techniques used for UnitY are helpful for Translatotron2+
- The multidomain Es→En corpus is used with different random seeds to report average dev scores

### Ablation study
- Conducted an ablation study for two-pass direct S2ST models
- Evaluated the translation quality of outputs from both decoders
- Added an additional T2U/T2S encoder for bridging the gap in representations between the first-pass and second-pass decoders
- R-Drop was beneficial for boosting the translation quality of the first-pass decoder
- Explored parallel and sequential cross-attention, but neither showed any improvement

### Output unit for first-pass decoder
- The subword unit (E6) was the most effective for the first-pass decoder in both UnitY and Trans-latotron2+
- It gained the largest decoding speed-up.

### Pre-training first-pass decoder
- We explored a better pre-training strategy for the first-pass text decoder in UnitY.
- We investigated pre-training it with an MT model trained with bitext data from scratch (Supervised MT1, Supervised MT2).
- Supervised MT1 used CCMatrix (Schwenk et al., 2021)  model in the cascaded system10 . Moreover, we finetuned the mBART model to the MT task in an unsupervised MT way via online back translation (Liu et al., 2020) on CC100 (unsupervised MT). Furthermore, we studied initializing the speech encoder and the text decoder with a separate direct S2TT model.
- The S2TT model was fine-tuned from wav2vec2.0 and mBART models on the same corpus. After the initialization, we fine-tuned the whole parameters of UnitY (S2TT).
- The results in Table 6 showed that pre-training the text decoder with the vanilla mBART (F2) or the unsupervised MT model (F3) was the most effective.
- Pre-training with supervised MT models (F4, F5) did not improve performance, even for the first pass. This is consistent with a finding in Jia et al. (2022a) although they pre-train the first-pass phoneme decoder of Translatotron2 with a phoneme-based supervised MT model. Therefore, leveraging a separate MT system is effective for generating weak supervisions (Popuri et al., 2022) rather than parameter initialization.
- Pretraining a part of UnitY with an independent S2TT model (F7) was not helpful either. Surprisingly, the BLEU score from the text decoder in UnitY was better than that of F7. Therefore, training signals from the unit decoder never affect the text decoder.

### Capacity assignment to two-pass decoders
- The model capacity should be assigned to the two decoders in UnitY to obtain a better translation quality
- A 12-layer text decoder with a two-layer unit decoder (G8) was the best in translation quality and decoding speed when initializing the first-pass decoder randomly (G1-G6,G8 decoder with mBART (G9) brought a large ASR-BLEU gain with a slight speed degradation compared to G8.11)
- It is most effective to pre-train the deep text decoder only and keep the unit decoder shallow.

### Data scale
- UnitY consistently outperformed the Translatotron2+ and S2UT models when the data size was no less than 50 hours
- Pre-training the text decoder of UnitY was essential for obtaining decent performances in the low-resource settings

### Human evaluation
- Finally, we conducted an audio-only human evaluation to assess the translation quality while removing the necessity of ASR systems.
- We adopted crosslingual semantic textual similarity (XSTS) (Licht Figure 4: Results of human evaluation on multi-domain Es→En corpus et al., 2022), which emphasizes adequacy rather than fluency, and percent acceptable translations, the percentage of items that received an XSTS score of three or above.
- We used the mTEDx test set (989 samples) and generated the target audio from the S2ST systems.
- Moreover, we randomly sampled 495 samples and generated the target audio from the reference translation followed by TTS.
- The reference translations serve as a reference point and a ceiling against which to compare our systems.
- Three bilingual annotators evaluated each item and assigned it a score from one to five.
- The median score was taken per item.
- More details are described in Appendix J.
- The results are presented in Figure 4.

### Two-pass sequence generation
- The two-pass decoding approach maintains the end-to-end optimization capability
- The two-pass approach can incorporate an additional search process to find a better output
- The two-pass approach can rescore the intermediate hypotheses using an external module such as a language model
- The two-pass approach can inject specific information in the intermediate decoder to bias the output toward the desired domain
- The two-pass approach makes the optimization tractable

### Direct speech-to-spectrogram translation
- Direct speech-to-spectrogram translation models predict spectrogram in the target language from the source speech in an end-to-end fashion.
- Translatotron (Jia et al., 2019b) is the first direct S2ST model but suffered from poor performance even with auxiliary ASR and S2TT tasks.
- Kano et al. ( 2021) subsequently pre-trains the components with ASR and S2TT models, which is more effective for distant language pairs.
- Translatotron2 (Jia et al., 2022b) significantly improves Translatotron by incorporating two-pass decoding.
- We showed that our methods further improved Translatotron2.

### Direct speech-to-unit translation
- Direct speech-to-unit translation models predict discrete units rather than spectrogram.
- Uses vector-quantized variational autoencoder (Van Den Oord et al., 2017) while Lee et al. (2022a) used HuBERT (Hsu et al., 2021) to extract target discrete units.
- (Lee et al., 2022b) normalizes speaker identity of real target speech using a CTC-based speech-to-unit model.
- Further improves the normalization by considering rhythm, pitch, and energy.

## Conclusion
- In this work, we proposed UnitY, a novel efficient two-pass direct S2ST model that subsequently generates both text and discrete unit outputs.
- We improved the model performance by predicting subwords in the first pass, bridging decoder representations by an additional encoder, deep-shallow two-pass decoders, regularizing the training with R-Drop, and pre-training the firstpass decoder with mBART.
- Experimental evaluations on the Fisher Es→En, CVSS-C, and multidomain En↔Es corpora demonstrated that UnitY outperformed a single-pass S2UT model consistently in translation accuracy and inference speed.
- We showed that the proposed methods improve the two-pass direct speech-to-spectrogram model as well, confirming their versatility.
- Still, UnitY achieved 2.51× decoding speed-up over the case.
- Limitation Currently, the target audio quality of UnitY depends on the quality of discrete units generated by self-supervised discrete models.
- We consider that exploring a better target discrete unit representation would bridge a performance gap from Translatotron2+ on Fisher and multi-domain En→Es corpora.
- Although R-Drop greatly improved translation quality, it requires additional computation to generate the second probability distribution.
- Since two-pass models require linguistic units as the target for the first-pass decoder, they cannot be used when the target language is unwritten.
- A promising direction is finding more coarse discrete units whose sequence length is shorter than the original discrete units.
- To accelerate training and decoding efficiencies, it is promising to take filterbank features as input instead of a waveform to reduce the input sequence length.
- We discarded the duration information of each discrete unit, which imposes a heavy burden on the unit-based vocoder.
- To overcome this, we will revisit keeping the duration information.
- Since greedy decoding has shown to be enough for the second-pass unit decoder in §4.4, we consider that using a non-autoregressive decoder (Gu et al., 2018) in the second pass further reduces the computation time without quality degradation.
- Similarly to (Jia et al., 2022a;Bapna et al., 2022), joint speech-text self-supervised encoder pre-training followed by joint speech-text fine-tuning would improve the performance further.
- We believe it is complementary to our first-pass decoder pre-training with mBART.
- UnitY can also be extended to a multilingual model that generates speech in multiple target languages.
- We consider the two-pass approach more suitable for that purpose than the single-pass approach because we can obtain structured representations before generating speech, which disentangles multilingual translation and multilingual speech synthesis.
- Finally, it is also interesting to extend UnitY to the streaming S2ST task.
