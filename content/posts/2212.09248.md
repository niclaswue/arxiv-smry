---
title: "Natural Language to Code Generation in Interactive Data Science Notebooks"
date: 2022-12-19T05:06:00.000Z
author: "Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09248v1.webp" # image path/url
    alt: "Natural Language to Code Generation in Interactive Data Science Notebooks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09248)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09248).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/natural-language-to-code-generation-in).

# Abstract
- Computational notebooks, such as Jupyter notebooks, are interactive computing environments
- To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language intents from users, we build ARCADE, a benchmark of 1082 code generation problems using the pandas data analysis framework in data science notebooks
- ARCADE features multiple rounds of NL-to-code problems from the same notebook
- It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction
- To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks
- Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions

# Paper Content

## Introduction
- Data science is the process of extracting insights from data
- Data scientists and machine learning practitioners often use computational notebooks
- As illustrated in Fig. 1, data scientists spend significant amount of time on data wrangling and exploratory data analysis
- Computational notebooks also present unique challenges to LLMs
- ARCADE is a new benchmark for code generation for data wrangling and EDA tasks in computational notebooks
- PACH-INCO is a 62B code language model for Python computational notebooks that significantly outperforms public code LMs on ARCADE
- Further, we explore few-shot prompting strategies to alter the style of model predictions

### Dataset Analysis

### Evaluation by Fuzzy Output Matching
- Recent code generation datasets have shifted from surface form evaluation metrics like BLEU (Yin and Neubig, 2017) to functional correctness, where the execution results of predicted programs on some input examples are matched with the reference output verbatim (Hendrycks et al., 2021;Chen et al., 2021a;Austin et al., 2021), or approximately (for complex output like plots, see Chandel et al., 2022) using unit tests.
- In this paper we aim to evaluate accuracy of code LLMs in synthesizing code for data science notebooks in the wild, i.e., the model is only conditioned on preceding notebook context besides a short user-issued intent ( §2).
- As discussed in §3.2, those intents are often under-specified and have multiple alternative solutions. We therefore use a set of heuristics to approximately match the execu-tion output11 of a predicted program with the annotated reference to determine if they are functionally equivalent. These heuristics primarily cover two scenarios. First, if the output variable is a container type, 12 we canonicalize variables to the same type. Second, we allow for partial matching between complex DataFrame variables.
- Formally, consider a reference DataFrame v with a set of columns {v i }, where each v i is a vector of cell values for the i-th column. We define that v is equivalent with the output DataFrame of a predicted program v, iff for any v i ∈ v, we have v i ∈ v. Intuitively, we consider a predicted program as correct if its output DataFrame contains all the columns (and cell entries) in the reference frame. Intuitively, a predicted program is already useful as long as it covers all the necessary information in the reference.
- In cases of DataFrames, a user could easily create a more compact view of the frame by selecting a subset of target columns. Empirically, we find our evaluation metric is reliable in identifying solutions with alternative output structures, with a relatively low false-negative rate (under 10%).
- We present an error analysis in §6.

## Experiments

### Models and Setup
- PACHINCO is a large code language model that is compared to existing models
- CODEGEN is a family of code LMs that was trained using a combination of NL and code data scraped from GitHub
- INCODER is a code LM that was trained on a mixture of StackOverflow (SO) posts and 159GB of multilingual code data
- PALM is a large code language model that was compared to PACHINCO
- Following prior work, we measure model performance using the pass@k metric

### LM Prompting Strategies
- For a given problem, we conducted two types prompting experiments: prompting using only the notebook context of the problem ( §5.3), and fewshot prompting with extra exemplars as prompt prefix before notebook context ( §5.4), in order to impose more control to the style of predicted code.
- In the basic setup without extra few-shot exemplars, the prompt basically consist of all the prior notebook context, including NL descriptions of schema information and previous rounds of problems.
- Listing 7 shows the complete prompt for u 2 in Fig. 1
- A prompt in this setup is the concatenation of a prompt prefix (with few-shot exemplars) and the notebook context (with prior rounds of problems and NL schema descriptions).
- The part of a prompt that corresponds to notebook context as the same as the previous setting (e.g. Listing 7), except that we insert the preamble # Solution: Let's solve this problem step-by-step. as appropriate after the last cell delimiter.
- For prompt prefix, Listing 1 gives an example prompt prefix for Step-by-Step prompting, while Listing 4 shows the same set of few-shot exemplars for Vanilla Code prompting.
- As mentioned in §5.2, we created three prompt prefixes for each of the four different styles, and report results averaged over these three restarts.
- Listings 1 to 3 show the three groups of prompt prefixes for Step-by-Step, and Listings 4 to 6 show those for Vanilla Code prompting.
- Each prompt prefix has four exemplars, and some exemplars are shared across different prefixes.
- Note that some prompt prefixes in Step-by-Step also contain one simple problem that does not require decomposition and explanation (e.g. Exercise 3, Listing 1).
- We find this to be useful to not bias a model from generate overly complex code solutions for simpler problems.
- We did not put much effort in prompting engineering. Actually, those prompt prefixes were created before we collected 70% of the dataset, # You are a professional data scientist.
- Answer the following questions using pandas and matplotlib.
- Besides the basic setting using the notebook context of a problem, we also explored prompting using additional NL-to-code exemplars as prompt prefix before the notebook context.
- The motivation is to nudge the LM to generate code solutions following different coding and commenting styles, such as documenting the "reasoning path" used by the code.
- We develop four prompting strategies: 1. Vanilla code strategy as in Fig. 4, completion 4a. The predicted code follows the common practice of chaining multiple API calls in a single line. Step-by-step (SbS) code strategy as in Fig. 4, completion 4b, which leads to code with fine-grained decomposition structure. 3. SbS code with preamble, as in Fig. 4, completion 4c, which could further elicit decomposition in predictions. 4. SbS code with preamble and explanation, as in Fig. 4, completion 4d, with inline NL explanations per step.
- Table 2: pass@k evaluation on ARCADE using notebook context as prompts. Grey figures indicate difference to previous line. Each strategy has four exemplars in its prompt prefix, as listed in Appendix N.

### Results
- State-of-the-art code LMs on AR-CADE perform best with truncated prompts up to 900 tokens
- Including more context cells is crucial for good performance
- Pass rates increase when problems are grouped by their preceding context size

### Few-shot Prompting Beyond Test Notebook Contexts
- Our previous set of experiments demonstrate diminishing returns after including more notebook context cells.
- As motivated in §5.2, besides test notebook context, we use few-shot prompting with additional exemplars to teach the model to perform this challenging task, while generating code in different styles.
- Here, we report both functional correctness (Fig. 7) as well as metrics evaluating the style of model-predicted code (Fig. 8) for problems in New Tasks.
- We only evaluate PACHINCO due to that the prompt length (maximal 2, 100 sub-tokens) exceeds the limit of public code LMs.
- With Step-by-Step Prompting (SbS), we observe an improvement on pass@30 over the baseline using only notebook contexts (Tab. 2).
- This is consistent with the general results on other tasks when comparing few-vs zero-shot learning with LLMs (Brown et al., 2020), while we remark that in our case, the "zero"-shot prompting setting (Tab. 2) still contains notebook cells as rich contexts.
- However, as Fig. 7 suggests, SbS prompting is quite helpful for improving pass rate.
- empirically, we observe SbS prompting is especially helpful for problems (at code cells c k ) without adequate preceding notebook context (k is small), For instance, SbS prompting results in 6% absolute improvement for the first two rounds of problems.
- Interestingly, even if we include more test notebook context in the non-exemplar baseline such that the prompt length match with SbS prompting using extra exemplars Curves that appear more to the right represent prompting methods with greater diversity in the samples.
- Step-by-step prompting leads to much greater diversity than the baselines. (Baseline + More Context), SbS prompting is still better, which again suggests the value of extra exemplars to compliment the information in notebook context.
- In line with our intuition, SbS prompting yields code predictions that are better decomposed into more lines of code (LoC ↑ ), with each line being simpler (Tokens/API per Line ↓ ). It also appears to generate slightly more complex solutions, as indicated by the increased pandas API usage.
- Our finding demonstrates the effectiveness of SbS prompting in improving accuracy and also code style.
- In contrast, we also report results from prompting the model using exemplars with "vanilla"-styled code following the common practice of chaining multiple pandas API calls in a single line (Vanilla Code), which only leads to marginal improvements on pass@k over the non-exemplar baseline, while the code style remains consistent.
- Next, on top of SbS prompting, using a preamble at the beginning of a target cell to further encourage the model to solve the problem in a step-by-step fashion (SbS + Preamble) improves pass@k, as well as the level of decomposition in predicted code (LoC ↑ , Tokens/API per Line ↓ ).
- This is consistent with our intuition, which is that preambles help the model focus on the task at hand.
- We also report breakdown results of pass rate on problems with varying level of complexity.
- SbS prompting and its variants are helpful across the board, especially for harder tasks with more than 7 pandas function calls.

## Error Analysis
- To understand the types of errors that LLMs make on ARCADE, especially on challenging problems, we conduct an error analysis on PALM's predictions on the New Tasks split (Tab. Overall, we notice a significant drop in execution errors after two-stage code fine-tuning (PALM →PACHINCO, §4).
- Out of all the incorrect predictions from PACH-INCO under the fuzzy output matching evaluation metric ( §3. Next, we conduct a manual analysis on 50 randomly sampled incorrect predictions (with a focus on executable but incorrect predictions). The cause of these errors can be grouped into the following categories: 1. Complex problems requiring non-trivial reasoning or data transformation steps (43%); 2. Errors in interpreting NL intents, such as missing a requirement specified in the intent (e.g. round to two decimal places) in the code solution (26%); 3. Errors caused by under-specified intents ( §3.2, 19%); 4. False-negatives due to limited coverage of the fuzzy-matching evaluation metric ( §3.3, 6%); 5. Annotation errors (6%).
- The primary source of errors is due to complex problems, which reiterates the motivation of AR-CADE -evaluating code LLMs on challenging data wrangling and EDA tasks.
- Next, a non-trivial amount of errors are caused by under-specified intents, which are common in the setting of prompting LLMs using short instructions ( §3.2), calling for future research to specifically address this issue.
- Finally, our evaluation metric based on fuzzy output matching seems effective in identifying plausible alternative solutions. Still, there are non-trivial cases where there are multiple ways of presenting the outputs (e.g. DataFrames with nested columns or different orientations, Fig. 32). We present more detailed examples of these errors in Appendix I.
- Code with Step-wise Explanations? In §5 we show how prompting PACHINCO to generate code with step-wise explanations is helpful in improving solution diversity (Fig. 10) and also accuracy of self-consistency reranking (Fig. 11).

## Related Works
- Many generalist LMs also have source code as part of their training data mixture
- Those LLMs have registered impressive performance on a variety of benchmarks for code
- Automating Data Science
- Context-driven Code Generation

## Conclusion
- ARCADE is a code generation benchmark for data wrangling and EDA tasks in computational notebooks
- The paper presents ARCADE, a code generation benchmark for data wrangling and EDA tasks in computational notebooks
- The paper also develops PACHINCO, a 62B LM tailed for data science
- PACHINCO outperforms public code LMs on AR-CADE, while being effective in few-shot learning to improve code style and solution diversity
- One idea to write good intents is to keep it concise such that another programmer could quickly understand and implement a solution that executes to the same outputs
- New Tasks For each ML dataset we provided, an annotator creates a Colab notebook with code snippets for some interesting data wrangling and exploratory data analysis tasks using this dataset
- Each code snippet is paired with its natural language intent, simliar to the process of annotating Existing Tasks
- Different from annotating Existing Tasks, we ask them to first create a natural language intent for their task, and then write a code solution in the next cell
- Below is an excerpt from the annotation guideline describing the types of data wranling and EDA tasks to create
- In general, you may create whatever exploratory data analysis tasks that you find interesting for the given datasets
- To come up with interesting tasks, you can think in this way: before training your ML models for the dataset, what kind of data wrangling or EDA tasks would you like to perform on the dataset?
- Below are some more concrete descriptions of such wrangling or EDA tasks
- Data Preprocessing/Wrangling Tasks which involves modifying existing dataframes or creating new ones. Such as normalizing column names, adding new columns, modifying existing columns (e.g., converting string values to date times), generating new dataframes using ops like group_by, and so on.
- Exploratory Data Analysis Tasks that Require Some Wrangling and Preprocessing Answering interesting EDA questions using the given dataset, but some data wrangling steps are required in order to derive the answer. For example, given a dataframe df of user shopping history and credit card expiration dates in the format of df.loc[0]['cc_exp'] = '08/26'.
- To answer the EDA question "How many users have a credit card expiring in 2024?", we need to first convert the expiration year from the string-formatted cc_exp column.
- Jupyter notebooks that are converted to Python files in such format are common in GitHub repositories, which mitigates the domain transfer gap between general Python code and notebookspecific data, and also allows us to prompt public code LLMs, which might have seen similar data during pre-training.
