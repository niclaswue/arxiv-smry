---
title: "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"
date: 2022-12-20T18:58:48.000Z
author: "Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10559v2.webp" # image path/url
    alt: "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10559)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10559).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/why-can-gpt-learn-in-context-language-models).

# Abstract

# Paper Content

## Introduction
- Large pretrained language models, especially in Transformer-based architectures, have shown strong emergent In-Context Learning (ICL) ability.
- ICL just needs several demonstration examples prepended before the original input, and then the model can predict the label for even unseen inputs.
- On numerous downstream tasks, a large GPT model can achieve a quite great performance, which even exceeds some smaller models with supervised finetuning.
- However, although ICL has achieved great success in performance, the working mechanism of it is still an open problem to be investigated.
- In this paper, we explain ICL as a process of meta-optimization and attempt to build connections between GPT-based ICL and finetuning.
- Concentrating on the attention modules, we figure out that the Transformer attention has a dual form of gradient descent based optimization.
- On top of it, we propose a novel perspective to explain ICL: (1) a pretrained GPT serves as a meta-optimizer; (2) it produces meta-gradients according to the demonstration examples through forward computation; (3) the meta-gradients are applied to the original language model through attention to build an ICL model.
- As illustrated in Figure 1, ICL and explicit finetuning share a dual view of gradient descent based optimization. The only difference is that ICL produces meta-gradients through forward computation, while finetuning computes gradients by back-propagation. Therefore, it is reasonable to understand ICL as some kind of implicit finetuning.
- In order to provide empirical evidence to support our understanding, we conduct comprehensive experiments based on real tasks.
- On six classification tasks, we compare the model predictions, attention outputs, and attention scores of pretrained GPT models in the ICL and finetuning settings. As expected, the behavior of ICL is highly similar to explicit finetuning at all of the prediction level, the representation level, and the attention behavior level. These results are strong evidence to prove the reasonability of our understanding that ICL performs implicit finetuning.
- Further, we attempt to take advantage of our understanding of meta-optimization for model designing. To be specific, we design a momentumbased attention, which regards the attention values as meta-gradients and applies the momentum mechanism to them. Experiments on both language modeling and in-context learning show that our momentum-based attention consistently outperforms vanilla attention, which supports our understanding of meta-optimization again from another aspect.
- We note that beyond this preliminary application, our understanding of meta-optimization may have more potential to be used to aid in model designing, which is worth investigating in the future.

## Background
- In-context learning with GPT
- Uses a GPT model to predict the answer
- Predicts the answer by selecting the answer with the highest probability from the candidate answer set

### Dual Form Between Gradient Descent Based Optimization and Attention
- The idea in this paper to explain language models as meta-optimizers is inspired by Aizerman et al. (1964);
- Irie et al. (2022) present that linear layers optimized by gradient descent have a dual form of linear attention.
- Let W 0 , ∆W ∈ R dout×d in be the initialized parameter matrix and the update matrix, respectively, and x ∈ R d in be the input representation.
- A linear layer optimized by gradient descent can be formulated as In the back-propagation algorithm, ∆W is computed by accumulating the outer products of the historic input representations x T i ∈ R d in and the gradients e i ∈ R dout of their corresponding outputs: Combing Equation ( 7) and Equation ( 8), we can derive the dual form between gradient descent based optimization and linear attention: where LinearAttn(V, K, q) denotes the linear attention operation, where we regard the historic output gradients E as values, the historic inputs X as keys, and the current input x as the query.

## In-Context Learning (ICL) Performs Implicit Finetuning
- We qualitatively analyze the Transformer attention under a relaxed linear attention form to figure out a dual form between it and gradient descent based optimization.
- We compare ICL with explicit finetuning and build connections between these two optimization forms.
- Based on these theoretical findings, we propose to understand ICL as a kind of implicit finetuning.

### Transformer Attention as Meta-Optimization
- Let x ∈ R d be the input representation of a query token t, and q = W Q x ∈ R d be the attention query vector.
- In the ICL setting, the attention result of a head is formulated as where W Q , W K , W V ∈ R d ×d are the projection matrices for computing the attention queries, keys, and values, respectively; √ d denotes the scaling factor; X denotes the input representations of query tokens before t; X denotes the input representations of the demonstration tokens; and [X ; X] denotes the matrix concatenation.
- For ease of qualitative analysis, we approximate the standard attention to a relaxed linear attention by removing the softmax operation and the scaling factor: We define W ZSL = W V X (W K X) T as the initialized parameters to be updated since W ZSL q is the attention result in the Zero-Shot Learning (ZSL) setting, where no demonstrations are given.
- Following the reverse direction of Equation ( 9), we derive a dual form of the Transformer attention: As shown in the above equations, the attention to the demonstration tokens is equivalent to parameter updates ∆W ICL that take effect on W ZSL .
- In addition, By analogy with Equation ( 9), we can regard W V X as some meta-gradients, which are used to compute the update matrix ∆W ICL .

### Comparing ICL with Finetuning
- ICL directly takes effect on only the attention keys and values
- ICL and finetuning produce updates (∆W ICL v.s. ∆W FT ) to W ZSL , which can both be regarded as gradient descent
- ICL and finetuning aim at attention

## Experiments

### Experimental Settings
- In our experiments, we use two GPT-like pretrained language models with 1.3B and 2.7B model parameters, respectively, which are released by fairseq.
- For each task, we use the same template to format examples for Zero-Shot Learning (ZSL), ICL, and finetuning.
- The answer prediction processes for ZSL and finetuning are the same with ICL as described in Section 2.1, except that they do not have demonstration examples.
- For ICL, we fix the number of demonstration examples to 32 and tune the random seed for each task to find a set of demonstration examples that achieves the best validation performance.
- For finetuning, we use the same demonstration examples for ICL as the training examples and use SGD as the optimizer.
- For a fair comparison, we fine-tune the model for only one epoch and the training examples are provided in the same order as demonstrated for ICL.
- We tune the learning rate for finetuning and select the one that achieves the best validation performance.

### Metrics
- We design three metrics to measure the similarity between ICL and finetuning at three different levels: the prediction level, the representation level, and the attention behavior level.
- At the prediction level, this metric measures ICL can cover how much behavior of finetuning. We first count N FT , the number of query examples that finetuning can predict correctly but ZSL cannot. Then, among these examples, we count N both , the number that ICL can also predict correctly. Finally, we compute the Rec2FTP score as N both N FT . A higher Rec2FTP score suggests that ICL covers more behavior of finetuning at the prediction level.
- This metric measures the similarity between the effects that ICL and finetuning have on ZSL at the representation level. For a query example, let h (l) X denote the output representation of the last token at the l-th attention layer in the X setting. The updates of ICL and finetuning compared with ZSL are h ZSL , respectively. We compute the cosine between these two updates to get the SimAOU score at the l-th layer. A higher SimAOU score means ICL is more inclined to update the attention output representation in the same direction as finetuning.
- This metric measures the similarity of the attention behavior of ICL and finetuning. For a query example, let m (l,h) X denote the attention weights before softmax of the last token at the h-th attention head in the l-th attention layer in the X setting. For ICL, we omit the attention to the demonstration tokens and only monitor the attention weights to the query input. We compute the cosine between m (l,h) FT and then average the similarity across the attention heads to get the SimAM score at each layer. A higher SimAM score indicates that the attention weights that ICL and finetuning pay to the query tokens are more similar.

### Results
- Validation accuracy is improved in the ZSL, ICL, and finetuning settings
- ICL is better at few-shot scenarios than finetuning
- Rec2FTP scores for two GPT models are similar for ICL and finetuning
- At the representation level, ICL tends to change the attention results in the same direction as finetuning changes
- SimAOU and SimAM are fluctuated at lower layers, and tend to be steadily larger at higher layers
- Using momentum-based attention improves accuracy and in-context learning on language modeling, sentiment analysis, natural language inference, and multi-choice selection

## Conclusion
- According to the demonstration examples, GPT produces meta-gradients for In-Context Learning (ICL) through forward computation.
- ICL works by applying these meta-gradients to the model through attention.
- The meta-optimization process of ICL shares a dual view with finetuning that explicitly updates the model parameters with back-propagated gradients.
- Figure 2: shows that statistics of the SimAOU scores at different layers. The yellow lines denote medians.
- At Different LayersIn order to investigate the similarity between ICL and finetuning more thoroughly, we look into the SimAOU and SimAM scores at different layers. We randomly sample 50 validation examples from each dataset and draw box plots for SimAOU and Figure 3: shows that statistics of the SimAM scores at different layers. The yellow lines denote medians. = {y 1 , y 2 , . . . , y m }, we need to predict a label ŷ conditional on n demonstration examples C = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x n , y n )}, where (x i , y i ) is an input-label pair different from the query one.
- Formally, given a GPT model M, we first compute the probability of each answer y j :
