---
title: "VPIT: Real-time Embedded Single Object 3D Tracking Using Voxel Pseudo Images"
date: 2022-06-06T14:02:06.000Z
author: "Illia Oleksiienko, Paraskevi Nousi, Nikolaos Passalis, Anastasios Tefas, Alexandros Iosifidis"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2206-02619v1.webp" # image path/url
    alt: "VPIT: Real-time Embedded Single Object 3D Tracking Using Voxel Pseudo Images" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.02619)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.02619).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/vpit-real-time-embedded-single-object-3d).

# Abstract
- Voxel pseudo image tracking is a novel 3D single object tracking method that uses voxel pseudo images as input
- The input point cloud is structured by pillar-based voxelization
- The resulting pseudo image is used as an input to a 2D-like Siamese SOT method
- The pseudo image is created in the Bird's-eye View (BEV) coordinates, and therefore the objects in it have constant size

# Paper Content

## I. INTRODUCTION
- Robots are increasingly being used in real-world scenarios
- There is a need to develop methods for understanding the 3D world in order to allow a robot to interact with objects
- Efficient methods for 3D object detection, tracking, and active perception are needed
- Lidar is currently a well-adopted choice for 3D perception methods
- Point cloud data, despite being sparse and irregular, contains much more information needed for 3D perception compared to images
- 3D object detection and tracking methods using point clouds provide the best combination of accuracy and inference speed
- Lidars usually operate at 10-20 FPS, and therefore, a method receiving point cloud data as input can be called real-time if its inference speed is at the rate of the Lidar's data generation, as it will not be able to receive more data to process.
- Even when the method cannot benefit from the FPS, higher than the Lidar's frame rate, perception methods are usually paired with another method that makes use of the perception results, as in planning tasks.
- Methods like those mentioned above do not take into consideration the limited computational capabilities of embedded computing devices, which are typically used in robotics applications, e.g., selfdriving cars, due to a lower power consumption that allows the robotic system to stay active for a longer time.
- This opens a possibility to adapt 2D SOT methods for a 3D task with minor changes.

## II. RELATED WORKS
- SOT in 3D is commonly formulated as an extension of 2D SOT.
- In both cases, an initial position of the object of interest is given, and the method needs to predict the position of the object in all future frames.
- The main difference between 2D and 3D SOT rises from the type of data used as input.
- Camera-based 3D perception methods commonly achieve poor performance due to the increased difficulty of extracting correct spatial information from cameras, and therefore most of the 3D object tracking methods use point cloud data or a combination of point clouds and camera images.
- Point cloud sparsity does not allow using straightforward extensions of 2D SOT methods based on regular CNNs.
- SC3D [13] uses a Siamese approach by encoding a target point cloud shape in the initial frame and searching for regions in a new frame with the smallest cosine distance between target and search encodings.
- After finding a new location, the new object shape is combined with the previous one to increase the quality of comparison in future frames.
- It achieves good tracking results, but its operation is computationally expensive.
- Assuming that an object should not move far away from its current location in consecutive frames, one can define a search region inside its neighborhood for searching it in a new frame.
- This is commonly done by expanding the region around the position of the target.
- P2B [9] uses a point-wise network to process points in target and search regions to create a similarity map and find potential target centers.
- These regions are then used by a voting algorithm to find the best position candidate.
- BAT [14] is a 3D tracking method based on P2B, which uses Box Cloud representations as point features, i.e., a representation which depicts the distances between the points of an object and the center and corners of its 3D bounding box.
- 3D-SiamRPN [5] uses a Siamese point-wise network to create features for target and search point clouds.
- It then uses a cross-correlation algorithm to find points of the target in a new frame.
- An additional region proposal subnetwork is used to regress the final bounding box.
- The F-siamese tracker [15] aims to fuse RGB and point cloud information for 3D tracking and applies a 2D Siamese model to generate 2D proposals from an RGB image, which are then used for 3D frustum generation.
- The proposed frustums are processed with a 3D Siamese model to get the 3D object position.
- Point-Track-Transformer (PTT) [10], [16] creates a transformer module for point-based SOT methods and employs it based on a P2B model, leading to an increased performance.
- 3D Siam-2D [17] uses two Siamese networks, one that creates fast 2D proposals in BEV space, and another one that uses projected 2D proposals to identify which of the proposals belong to the object of interest.
- The loss of information that occurs in BEV projection affects the performance of the 2D Siamese network.
- Although voxel pseudo images lie on the BEV space, they do not have this problem, as each pixel of this image incorporates information about the points in a corresponding voxel via a small neural network, and not the projection alone.
- Recent works on 3D SOT focus on improving speed as well as tracking performance, but most do not take into consideration the limited computational capabilities of embedded devices, like the NVIDIA Jetson series, which are commonly used in robotics applications.
- Large delays in processing incoming frames can lead to dropped frames, which can quickly lead to performance degradation of traditional trackers as larger and larger offsets must be predicted as time progresses.

## III. PROPOSED METHOD
- Modified PointPillars architecture for 3D single object tracking
- Training and inference processes are publicly available in OpenDR toolkit

## A. Model architecture

## B. Training
- We where κ c (•) and σ(•) functions are identical to the ones, described before, and σ a (•) represents an augmentation technique where the center of the search region is shifted from the target center to imitate object movement between frames: ) and y ∼ uniform(− s h −t h 2 , s h −t h 2 ).
- In addition to the proposed augmentation, we use point cloud and ground truth boxes augmentations used in PointPillars, which include point cloud translation, rotation, point and ground truth database sampling.
- For training on KITTI tracking dataset, we select the target and search regions from the same track k and object o k , but the corresponding point clouds and bounding boxes are taken from the different frames, modeling the variance of target object representation in time: where f t and f s are a randomly selected frames from the track k that contain the object of interest o k .
- We balance the number of occurrences of the same object by selecting a constant number of (f t , f s ) samples for each object in the dataset.
- After creating target and search regions, we apply ξ t (•) and ξ s (•) functions by first taking sub-images of those regions and then, depending on the interpolation size parameter, applying bicubic interpolation to have a fixed size of the image: where x is a region to be processed, ρ indicates that either target region t, or search region s is selected, ι r is an interpolation size parameter, which indicates the output size of the bicubic interpolation, and Π(x) creates an AABB pseudo image from the input point cloud that contains the region x.
- These images are processed with the Feature Generation Network, which is the convolutional subnetwork of the PointPillars' RPN, as shown in Fig. 2.
- The resulting features are compared using the crosscorrelation module to create a score map.
- A Binary Cross-Entropy (BCE) loss is used to compare a predicted score map M x to the true label M y as follows: where S(x) = 1/(1 + e −x ) and w n is a scaling weight for each element.
- We follow SiamFC [6] and select the values of w n to equalize the total weight of positive and negative pixels on the ground truth score map.

## C. Inference
- Inference is split into initialization and tracking
- The 3D bounding box of the object of interest is given by either a detection method, a user, or from a dataset
- During tracking, the last target position is used to place a search region, but instead of using only a single search region, we create multiple search regions with identical size and different rotations
- This is done to allow to correct for rotation changes in the object
- The set of 2K + 1 search regions {s τi | i ∈ [−K, K]} is created by altering the initial search region s τ0
- Target features are compared with all search features, and the one with the highest score is selected for a new rotation
- where Λ i is a rotation penalty multiplier, which is 1 for i = 0 and a hyperparameter value in [0, 1] range for all other search regions, µ is a rotation interpolation coefficient and f is a Siamese model
- The score map M , obtained from the Siamese model, is upscaled with bicubic interpolation, increasing its size 16 times by default (score upscale parameter u M )
- Based on the assumption that an object cannot move far from its previous position in consecutive frames, we use a penalty technique for the scores that are far from the center, by multiplying scores with a weighted penalty map
- The penalty map is formed using either Hann window or a 2D Gaussian function
- After the current frame's prediction is ready, the search region is centered on a prediction, assuming that the object's position on the next frame should be close to its position on the previous frame
- In order to make it easier to find the position of an object inside a search region, we apply a linear position extrapolation for the search region by centering it not on a previous prediction, but on a possible new prediction position
- Given a target region from the last frame t τ −1 and the predicted target region t τ , the corresponding search region has its position defined s x,y τ = 2t x,y τ − t x,y τ −1 .
- This approach increases the chances of the target to be in the center of the search region or having a small deviation from it
- When the linear extrapolation is used, the penalty map P (size) (Eq. 13) is created using a 2D Gaussian with a covariance matrix Σ that represents the angle of the extrapolation vector to penalize more positions that are not on the way of the predicted object's movement
- The resulting Gaussian N (µ, Σ 0 ) is rotated by φ = arctan(e), where e is the extrapolation vector
- The creation of such a map for a high-dimensional upscaled score map is a computationally expensive task, and therefore, to optimize it, the penalty maps are hashed by their size and rotation to reuse in future frames

## IV. EXPERIMENTS
- We use KITTI [26] Tracking training dataset split to train and test our model
- We use Precision and Success metrics as defined in One Pass Evaluation [28]
- Precision is computed based on the difference between ground-truth and predicted object centers in 3D
- Success is computed based on the 3D Intersection over Union (IoU) between predicted and ground truth 3D bounding boxes
- We use 1 feature block from the original PointPillars model with 4 layers in a block
- The model is trained for 64, 000 steps with BCE loss, 1 * 10 −5 learning rate and 2 positive label radius.

## A. Comparison with state-of-the-art
- Evaluation results on 1080Ti GPU are given in Table I.
- VPIT is the fastest method and achieves competitive Precision and Success.
- We evaluate official implementations of the fastest methods (P2B, PTT, VPIT) on different high-end and embedded GPU platforms to showcase how the architecture of the device influences the models' FPS.
- When computing FPS, we include all the time the model spends to process the input and create a final result. This excludes time spend to obtain data and to compute Success and Precision values, but includes pre-and post-processing steps.
- As can be seen from Table II, in terms of speed, VPIT outperforms P2B by 67% on 1080Ti GPU, 99% on 2080 GPU and 86% on 2080Ti GPU with 104-core CPU.
- For embedded devices, the difference is bigger: VPIT outperforms P2B by 135% on TX2 and by 98% on Xavier.

## B. Real-time evaluation
- Application of 3D tracking methods is usually done for robotic systems that do not use high-end GPUs due to their high power consumption
- Following [11], we implement a predictive real-time benchmark
- The processing time of the model is higher than zero, which means that the resulting prediction p i at time τi cannot be compared to the label y i from the same frame, and therefore for each label y i from the dataset, it will be compared to the latest prediction p lpr(i)
- The predictive real-time error E pr , based on a regular error E, is computed for each ground truth label y i as follows:
- As shown in [11], the existing model can be improved for a predictive real-time evaluation by predicting the position of the object at a frame i and then predicting its movement during the time period between the frames i and i + 1 using a Kalman Filter [29] or any other similar method.
- Such optimization can be applied to any of the methods that we compare, and therefore, to eliminate the factor of "predictive errors" where the model's error in prediction for the current frame is more leaned towards the prediction for the next frame, we introduce a non-prediction real-time benchmark
- The highest Frame drop of 60 − 70% is seen for P2B and PTT on TX2 for a 20Hz Lidar. In this case, Success values of P2B and PTT are 6 and 4 times lower than during the regular evaluation, respectively, which indicates that these methods cannot work under the aforementioned conditions.
- For every evaluation case, except the Xavier with a 10Hz Lidar, VPIT outperforms P2B and PTT in Success, Precision, FPS and Frame drop, but PTT still maintains the best Success and Precision on Xavier with a 10Hz Lidar.

## C. Ablation study
- We performed experiments to determine the influence of different hyperparameters on the Success and Precision metrics.
- Decreasing the number of feature blocks from the Feature Generation Network, leads to better Success values.
- This can be due to an increasing receptive field with each new block that leads to over smoothing of the resulting feature maps and makes it harder to define a precise position of an object of interest.
- Next, we follow SiamFC [6] and use target and search upscaling with ι t = (127, 127) and ι s = (255, 255).
- This results in constant-sized target and search pseudo-images, corresponding feature and score maps, which allows implementing mini-batch training and reduce the training time.
- However, such approach does not work well for voxel pseudo images, achieving 81% less Success than when using original sizes (ι t = (0, 0) and ι s = (0, 0)).
- Furthermore, following SiamFC [6], positive values of the context amount parameter c result in square target and search regions. In contrast to it, we also use negative c values to add independent context to each dimension of the region, keeping its aspect ratio.
- Positive context leads to better Success with the maximum at c = 0.26, which is 10% higher than the result for c = −0.1.
- Regarding the positive label radius, r = 2 results in the best training procedure, but this hyperparameter has a low influence on the final Success value, as well as the target feature merge scale m tf , window influence η, score upscale u M , search region scale σ s , rotation step ψ α and rotation penalty Λ.
- Next, linear position extrapolation leads to a 30% increase in Success compared to a conventional search region placement and no directional penalty.
- With offset interpolation ω = 0.3, the Success value increases by 24%, compared to the direct application of the model's prediction (ω = 1).
- Finally, a pretrained PointPillars backbone creates features that are capable of differentiating between target class and other object types, but those features cannot be used effectively to differentiate between different objects from the same class. This serves as a good starting point for the SOT task, and then training for 64000 steps results in the best model for selected hyperparameters.

## V. CONCLUSIONS
- The proposed method uses PointPillars' pseudo images as a search space
- The method is a Siamese 2D-like approach
- The method is fast and achieves competitive Precision and Success values
- The method is easy to adapt to a 3D SOT task
