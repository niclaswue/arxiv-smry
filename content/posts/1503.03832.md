---
title: "FaceNet: A Unified Embedding for Face Recognition and Clustering"
date: 2015-03-12T18:10:53.000Z
author: "Florian Schroff, Dmitry Kalenichenko, James Philbin"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1503-03832v3.webp" # image path/url
    alt: "FaceNet: A Unified Embedding for Face Recognition and Clustering" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1503.03832)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1503.03832).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/facenet-a-unified-embedding-for-face).

# Abstract
- Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches.
- In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity.
- Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.
- Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.
- To train, we use triplets of roughly aligned matching/ non-matching face patches generated using a novel online triplet mining method.
- The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.
- On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets.

# Paper Content

## Introduction
- Face verification, recognition and clustering are all based on a deep embedding of the face.
- The embedding is based on a deep convolutional network.
- The network is trained to be a compact 128-D embedding.
- The network is able to handle a lot of variability in the data.
- There are a few different ways to do face clustering, one of which is using hard-positive mining techniques.

## Related Work
- Uses a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions.
- Uses a deep convolutional network inspired by the work of [9]
- Uses a mixed layer that runs several different convolutional and pooling layers in parallel and concatenate their responses.
- Reduced the number of parameters by up to 20 times and have the potential to reduce the number of FLOPS required for comparable performance.
- Uses a non-linear SVM to combine the predictions of different networks.
- Uses a joint Bayesian model that effectively corresponds to a linear transform in the embedding space.
- Uses a combination of classification and verification loss.

## Method
- FaceNet uses a deep convolutional network
- The Zeiler&Fergus [22] style networks and the recent Inception [16] type networks are described
- The triplet loss is used to achieve an embedding that is suitable for face verification, recognition and clustering
- The triplet loss is efficient at scale

### Triplet Loss
- The embedding is represented by f (x) ∈ R d .
- It embeds an image x into a d-dimensional Euclidean space.
- Additionally, we constrain this embedding to live on the d-dimensional hypersphere, i.e. f (x) 2 = 1.
- This loss is motivated in [19] in the context of nearest-neighbor classification.
- Here we want to ensure that an image x a i (anchor) of a specific person is closer to all other images x p i (positive) of the same person than it is to any image x n i (negative) of any other person.
- This is visualized in Figure 3.
- Thus we want, where α is a margin that is enforced between positive and negative pairs.
- T is the set of all possible triplets in the training set and has cardinality N .
- The loss that is being minimized is then (3).

### Triplet Selection
- Triplets must violate the triplet constraint in order for convergence to be fast
- There are two ways to avoid this constraint: generate triplets offline or online
- Offline generation is more efficient, but has a higher chance of bad local minima early on in training
- Semi-hard negatives are selected to mitigate this

### Deep Convolutional Networks
- In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop [8,11] and AdaGrad [5].
- In most experiments we start with a learning rate of 0.05 which we lower to finalize the model.
- The models are initialized from random, similar to [16], and trained on a CPU cluster for 1,000 to 2,000 hours.
- The decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance.
- The margin α is set to 0.2.
- We used two types of architectures and explore their trade-offs in more detail in the experimental section. Their practical differences lie in the difference of parameters and FLOPS.
- The best model may be different depending on the application. E.g. a model running in a datacenter can have many parameters and require a large number of FLOPS, whereas a model running on a mobile phone needs to have few parameters, so that it can fit into memory.
- All our models use rectified linear units as the non-linear activation function.

## Datasets and Evaluation

### Hold-out Test Set
- We keep a hold out set of around one million images
- The FAR and VAL rate are then computed on 100k × 100k image pairs
- Standard error is reported across the five splits

### Personal Photos
- The test set has similar distribution to the training set
- The test set has been manually verified to have very clean labels
- The FAR and VAL rate across all 12k squared pairs of images are computed

### Academic Datasets
- Labeled Faces in the Wild (LFW) is the de-facto academic test set for face verification
- We follow the standard protocol for unrestricted, labeled outside data and report the mean classification accuracy as well as the standard error of the mean.
- Youtube Faces DB is a new dataset that has gained popularity in the face recognition community
- The setup is similar to LFW, but instead of verifying pairs of images, pairs of videos are used.

## Experiments
- The computer science paper mentions that they use between 100M-200M training face thumbnails.
- A face detector is run on each image and a tight bounding box around each face is generated.
- These face thumbnails are resized to the input size of the respective network.

### Computation Accuracy Trade-off
- The five models that are discussed in more detail in the paper are NN1, NN2, NN3, NNS1, NNS2.
- The accuracy of the models is correlated with the number of FLOPS that the model requires.
- The Inception based model NN2 achieves a comparable performance to NN1, but only has a 20th of the parameters.

### Effect of CNN Model
- Traditional Zeiler&Fergus based architecture with 1×1 convolutions [22,9]
- Inception [16] based models that dramatically reduce the model size
- Overall, in the final performance the top models of both architectures perform comparably
- However, some of our Inception based models, such as NN3, still achieve good performance while significantly reducing both the FLOPS and the model size
- The detailed evaluation on our personal photos test set is that the one on the right shows how the image size in pixels effects the validation rate at 10E-3 precision.

### Sensitivity to Image Quality
- The network is robust across a wide range of image sizes
- The network performs well down to a JPEG quality of 20
- Training with lower resolution faces could improve this range further

### Embedding Dimensionality
- We explored various embedding dimensionalities and selected 128 for all experiments
- The larger embeddings perform at least as good as the smaller ones, however, it is possible that they require more training to achieve the same accuracy
- It should be noted, that during training a 128 dimensional float vector is used, but it can be quantized to 128-bytes without loss of accuracy. Thus each face is compactly represented by a 128 dimensional byte vector, which is ideal for large scale clustering and recognition.

### Amount of Training Data
- Table 6 shows the impact of large amounts of training data.
- Due to time constraints this evaluation was run on a smaller model; the effect may be even larger on larger models.
- It is clear that using tens of millions of exemplars results in a clear boost of accuracy on our personal photo test set from section 4.2.
- Compared to only millions of images the relative reduction in error is 60%.
- Using another order of magnitude more images (hundreds of millions) still gives a small boost, but the improvement tapers off.

### Performance on LFW
- We evaluate our model on LFW using the standard protocol for unrestricted, labeled outside data.
- Nine training splits are used to select the L 2 -distance threshold.
- Classification (same or different) is then performed on the tenth test split.
- The selected optimal threshold is 1.242 for all test splits except split eighth (1.256).
- Our model is evaluated in two modes: 1. Fixed center crop of the LFW provided thumbnail. 2. A proprietary face detector (similar to Picasa [3]) is run on the provided LFW thumbnails.
- If it fails to align the face (this happens for two images), the LFW alignment is used.
- Figure 6 gives an overview of all failure cases.
- It shows false accepts on the top as well as false rejects at the bottom.
- We achieve a classification accuracy of 98.87%±0.15 when using the fixed center crop described in (1) and the record breaking 99.63%±0.09 standard error of the mean when using the extra face alignment (2).

### Performance on Youtube Faces DB
- We use the average similarity of all pairs of the first one hundred frames that our face detector detects in each video
- This gives us a classification accuracy of 95.12%±0.39
- Using the first one thousand frames results in 95.18%
- Compared to [17] 91.4% who also evaluate one hundred frames per video we reduce the error rate by almost half.

### Face Clustering
- Compact embedding lends itself to be used for clustering
- Constraints in clustering imposed by the task lead to amazing results
- Figure 7 shows one cluster in a users personal photo collection generated using agglomerative clustering

## Summary
- The findings are interesting
- It is somewhat surprising that it works so well
- Future work can explore how far this idea can be extended
- Presumably there is a limit as to how much the v2 embedding can improve over v1, while still being compatible
- It would be interesting to train small networks that can run on a mobile phone and are compatible to a larger server side model

## Appendix: Harmonic Embedding
- The concept of harmonic embeddings is introduced
- By this, a set of embeddings that are generated by different models but are compatible is denoted
- This compatibility greatly simplifies upgrade paths
- Figure 8 shows results on our 3G dataset that show that the improved model NN2 significantly outperforms the original model NN1

### Harmonic Triplet Loss
- In order to learn the harmonic embedding, we mix embeddings of v1 together with the embeddings v2, that are being learned.
- This is done inside the triplet loss and results in additionally generated triplets that encourage the compatibility between the different embedding versions.
- Figure 9 visualizes the different combinations of triplets that contribute to the triplet loss.
- We initialized the v2 embedding from an independently trained NN2 and retrained the last layer (embedding layer) from random initialization with the compatibility encouraging triplet loss.
- First only the last layer is retrained, then we continue training the whole v2 network with the harmonic loss.
- Figure 10 shows a possible interpretation of how this compatibility may work in practice.
- The vast majority of v2 embeddings may be embedded near the corresponding v1 embedding, however, incorrectly placed v1 embeddings can be perturbed slightly such that their new location in embedding space improves verification accuracy.
- Figure 1. shows the output distances of FaceNet between pairs of faces of the same and a different person in different pose and illumination combinations.
- A distance of 0.0 means the faces are identical, 4.0 corresponds to the opposite spectrum, two different identities. You can see that a threshold of 1.1 would classify every pair correctly.
- Figure 2. Model structure. Our network consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding. This is followed by the triplet loss during training.
- Figure 4. FLOPS vs. Accuracy trade-off. Shown is the trade-off between FLOPS and accuracy for a wide range of different model sizes and architectures.
- Highlighted are the four models that we focus on in our experiments.
- Figure 6. LFW errors. This shows all pairs of images that were incorrectly classified on LFW.
- Only eight of the 13 false rejects shown here are actual errors the other five are mislabeled in LFW.
- Figure 7. Face Clustering. Shown is an exemplar cluster for one user. All these images in the users personal photo collection were clustered together.
- Figure 8. Harmonic Embedding Compatibility. These ROCs show the compatibility of the harmonic embeddings of NN2 to the embeddings of NN1. NN2 is an improved model that performs much better than NN1.
- When comparing embeddings generated by NN1 to the harmonic ones generated by NN2 we can see the compatibility between the two. In fact, the mixed mode performance is still better than NN1 by itself.
- Figure 9. Learning the Harmonic Embedding. In order to learn a harmonic embedding, we generate triplets that mix the v1 embeddings with the v2 embeddings that are being trained.
- The semihard negatives are selected from the whole set of both v1 and v2 embeddings.
- Figure 10. Harmonic Embedding Space. This visualisation sketches a possible interpretation of how harmonic embeddings are able to improve verification accuracy while maintaining compatibility to less accurate embeddings. In this scenario there is one misclassified face, whose embedding is perturbed to the "correct" location in v2.
