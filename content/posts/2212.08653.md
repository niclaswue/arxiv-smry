---
title: "Attentive Mask CLIP"
date: 2022-12-16T18:59:12.000Z
author: "Yifan Yang, Weiquan Huang, Yixuan Wei, Houwen Peng, Xinyang Jiang, Huiqiang Jiang, Fangyun Wei, Yin Wang, Han Hu, Lili Qiu, Yuqing Yang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08653v1.webp" # image path/url
    alt: "Attentive Mask CLIP" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08653)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08653).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/attentive-mask-clip).

# Abstract
- Image token removal is an efficient augmentation strategy for reducing the cost of computing image features
- However, this efficient augmentation strategy has been found to adversely affect the accuracy of CLIP-based training
- We hypothesize that removing a large portion of image tokens may improperly discard the semantic content associated with a given text description, thus constituting an incorrect pairing target in CLIP training. To address this issue, we propose an attentive token removal approach for CLIP training, which retains tokens with a high semantic correlation to the text description. The correlation scores are computed in an online fashion using the EMA version of the visual encoder. Our experiments show that the proposed attentive masking approach performs better than the previous method of random token removal for CLIP training. The approach also makes it efficient to apply multiple augmentation views to the image, as well as introducing instance contrastive learning tasks between these views into the CLIP framework. Compared to other CLIP improvements that combine different pre-training targets such as SLIP and MaskCLIP, our method is not only more effective, but also much more efficient.

# Paper Content

## Introduction
- Large-scale vision-language pre-training such as CLIP [28] and ALIGN [16] have demonstrated remarkable zeroshot image classification and multi-modal retrieval capabilities.
- However, these approaches need large data and large training cost to drive well, for example, the CLIP model is trained using 400 million image-text pairs and the ALIGN model uses more than 1 billion image-text pairs.
- This raises the need for more efficient CLIP training.
- This paper aims to improve the efficiency of CLIP training.
- We note that there is an efficient image augmentation approach named image token removal which drops a large portion of image tokens and thus reduces a lot of computation in image encoder.
- It has shown to perform also effectively on masked image modeling based pre-training approaches [13,15,26,31,36] when combined with the vision Transformer architectures [8].
- We thus seek to introduce this efficient approach into CLIP training to improve the training efficiency.
- However, previous work showed that dropping tokens randomly can harm CLIP performance [9,19], also evidenced in our own experiments; see Table 2.
- More specifically, we observe -2.6% top-1 accuracy drop when we remove 50% random tokens in each image.
- The insight is that the token removal process may incorrectly discard the semantic contents highly relevant to the alt-text, and thus make an incorrect pair for CLIP training.
- To mitigate this issue, in this paper, we propose an attentive token removal strategy, shown in Figure 2.
- The proposed strategy selects the tokens to remove according to the correlation scores with the given text.
- We experiment with several different strategies, and show that the best one is to simply choose the highest scores.
- The correlation scores are computed using the exponential moving average(EMA) version of the visual encoder.
- Specifically, the attention weights to the [CLS] token of the visual encoder is used to represent the correlation scores.
- We also find that using the averaged attention weights of all layers perform better, and thus adopt it as our default method.
- Figure 4 shows the selected tokens, which do well correlate to the associated text semantics.
- Due to the high efficiency of the token removal approach, it allows us to extract multiple masked views from an image while keeping the training as efficient as the original CLIP training, i.e., 2 masked views with 50% token removal ratio.
- In our implementation, we generate each masked view by attentive masking on a randomly cropped sub-image from the original images.
- The random cropping strategy adds certain stochastic effects to different masked views, and thus enables us to apply an auxiliary self-supervised contrastive learning task between the augmentation views in addition to the plain CLIP loss.
- In fact, as has been shown in SLIP [7,27], the auxiliary task can benefit the CLIP train-ing.
- We consider both SimCLR [3] and SimSiam [4] methods for the auxiliary contrastive learning task.
- Note the EMA branch can be also naturally treated as another view, we also apply an online-to-EMA contrastive or consistency loss.
- We adopt the formulation used in BYOL [12] for this purpose.
- The proposed approach is named A-CLIP.
- It is very efficient which only introduces 16% overhead compared to the plain CLIP framework, and is 2.30× and 1.34× faster than previous CLIP improvements which also introduces multiple views and additional self-supervised losses, while being also much more effective: by using ViT-B and YFCC-15M [28,32] dataset, the A-CLIP framework achieves 43.9% top-1 accuracy on ImageNet-1K [30] zero-shot classification, as well as 62.7/42.1 and 38.0/23.2 I2T/T2I retrieval accuracy on Flickr30K [40] and MS COCO [23], which are +1.1%, +5.5/+0.9, and +4.4/+1.3 higher than the SLIP method, and +1.2%, +2.7/+3.3, and +3.9/+2.0 higher than the MaskCLIP method.
- Also note the training cost of our approach can be further reduced by using a lower resolution input for the EMA network, i.e., 2x lower reduced resolution.
- This has little affect on the accuracy of correlation score computing, while marginally reduce the efficacy of online-to-EMA contrastive loss.

## Related Work
- Contrastive language-image pre-training
- An important goal of computer vision is to interpret visual signals using language that humans can understand.
- While the field has long used image classification tasks to learn visual representations which connect visual signals to semantics, recent works of CLIP [28] and ALIGN [16] suggest a new way that can better connect visual signals with linguistic semantics by contrasting image-language pairs.
- In this visuallanguage contrastive learning framework, the training data is more scalable that billions of image-alt-text pairs can be easily collected from Internet.
- At present, the CLIP model has been a mainstream visual learning method, which not only learns transferable representations, but also connect the visual signals to arbitrary semantics.
- There have been extensive follow-up studies [7,11,20,27,35,38,39] to improve the effectiveness via sacrificing training efficiency.
- Our method also aims to improve CLIP pre-training, however, it improves both the effectiveness and efficiency.
- Masking tokens for efficient computation
- Vision Transformers [8,25] treat an image as a sequence of patch tokens, and perform encoding computation on these tokens.
- A masking strategy that removes certain tokens from an image can significantly speed up the computation of the image encoder.
- Dynamic ViTs [29,33] learn to remove tokens for efficient image classification.
- Masked autoencoder [13] randomly masks 75% tokens which significantly speeds up the self-supervised visual representation pre-training based on masked image modeling.
- Our paper extends the idea of token masking to speed up the CLIP training process.
- Comparison with a concurrent work FLIP [22]
- Similar to our method, there is a concurrent work FLIP [22] which also uses token masking to speed up CLIP training.
- Unlike ours, it uses a random masking strategy for CLIP training, like MAE [13].
- When the batch size remains the same and the ViT-B model is used, it achieves inferior zeros-shot accuracy than when trained with full images.
- Our method goes beyond FLIP in the following aspects. First, we propose an attentive masking strategy for CLIP training, which performs significantly more accurately than the random masking baseline.
- Attentive masking is important for CLIP training as its target is heavily defined by semantic texts, while MIM can work well with random masking probably because the target does not have explicit semantic supervision.
- Second, we introduce multiple masked image views into the framework, which allows us to conveniently add auxiliary pre-text tasks such as image-to-image contrast learning.
- Because of the reduced computation by masking, it does not bring computation overhead compared to the plain CLIP model using full images while improves the pre-trained representations.
- With these techniques, our method achieves significant accuracy improvements over the original CLIP model on both zero-shot image classification and multi-modal retrieval, while we note that FLIP is only on-par as the original CLIP model even with additional training tricks.
- Also note that some of the findings in FLIP, particularly on the scaling experiments and the hyper-parameter tuning such as larger batch sizes, the base learning rate tuning, are complementary to ours.
- The complementary findings of FLIP and our method can provide readers with a more complete view of employing masking for CLIP training.
- In the future, we plan to incorporate the findings of FLIP into our method.

## Method

### A Brief Review of CLIP
- CLIP is a visual representation learning approach that uses a large amount of image-to-text pairs.
- The training task treats each image and its associated alt text as a positive pair and pairs of this image to all other alt-texts as negative.
- For all possible pairs within a batch, the method applies an InfoNCE-like loss to classify them as positive or negative.
- The CLIP model shows well connecting images to arbitrary language semantics and achieves remarkably strong zeroshot image recognition accuracy on benchmarks.
- Its learned representation also performs very well when fine-tuned on various down-stream tasks.

### Masking for Efficient CLIP Training
- The naive approach to masking is to randomly remove a portion of image tokens
- The random masking method has been shown to be ineffective
- We propose an attentive masking strategy that can improve accuracy
- The attentive masking strategy is more effective than random masking

### Attentive Mask
- The goal of attentive masking is to keep tokens that are relevant to language description, as illustrated in Figure 1 Left.
- To this end, we note that the representation of the [CLS] token after CLIP training corresponds well to the semantics contained in the associated alt-text, and its attention weights to other image tokens can act a good indicator as the relevance measure of each image token to the linguistic semantics.
- We thus compute the score of the token at location P as: where l denotes the layer index; h denotes the attention head index; f q lh (CLS) denotes the query embedding of the [CLS] token at Layer l and Head h; f k lh (P ) denotes the key embedding of Layer l and Head h for an image token at location P ; C is the number of channels for the query and key embedding.
- The image tokens to mask are selected based on the scores s P .
- We consider three strategies: 1) "Low" strategy. The image tokens with the lowest scores are discarded; 2) "High" strategy. The image tokens with the highest scores are discarded; 3) "Mixed" strategy. A portion of the maintained tokens are the ones with the highest scores, and the other maintained tokens are randomly selected.
- In the experiments, the "low" strategy performs best, and is set as the default in our method.
- To obtain the attentive masked images as described above, we need a network that can generate attention scores for all image areas. We introduce an exponential moving average (EMA) version of the CLIP visual encoder and apply this EMA network on the full image to generate the attention scores.
- Using the EMA network has the following merits: First, the attention scores can be computed online, without a need of an already trained network. Second, the EMA network constitutes another view of the image, so other effective auxiliary tasks such as BYOL [12] can be introduced with little overhead. Third, the EMA network only involves the inference phase during training, which can save 2/3 of the computational cost compared to using a trainable network.
- In order to further improve efficiency and effectiveness, we propose the following two techniques.
- Efficient EMA computation with reduced image resolution
- The EMA network is primarily used for attention score computation to select image tokens to maintain. It does not need to be very accurate. We thus propose to perform the EMA computation at reduced image resolution, for example, using half the original resolution. By reducing the resolution, the EMA computation cost is saved by more than 4×.
- Overall, the EMA computation will correspond to be 1/12 of a regular CLIP visual encoder, which is efficient.
- Shared EMA score map for multiple masked views
- We also consider adding more masked image views for better results. A naive solution would be performing one time of the EMA computation for each view. However, this would waste computational cost. We propose using a shared EMA attention score maps for different image views. Specifically, we first compute the minimum enclosing rectangle for multiple image views. Accordingly, we crop and resize the original image, where the EMA network is applied to produce an attention score map. The selection scores for each image view are computed from this shared attention score map via bilinear interpolation, and with the selection scores, the masking is performed using the "low" strategy described above.

### Overall Framework
- Based on attentive masking, we present an attentive mask CLIP method, dubbed A-CLIP.
- The overall framework is illustrated in Figure 3.
- In the following, we describe several key designs and implementations of the A-CLIP framework.
- The A-CLIP framework can take multiple masked views for better results.
- To make different number of views with roughly the same complexity, we set the token number for each masked view as N/k (k is the number of masked views).
- The final VL loss is the average of all losses between each masked view and the alt-text.
- In the experiments, k = 2, 3 perform best (see Table 6), and k = 2 is set as default.
- Auxiliary self-supervised tasks Our framework can conveniently add several auxiliary self-supervised tasks.
- We consider two tasks: 1) Online-to-EMA contastive task. We adopt the BYOL instantiation [12] for this task, which encourages the feature of a masked view to be the same as the EMA feature.
- 2) Online-to-online contrastive tasks. When multiple masked views are used, we can introduce contrastive losses between the masked views.
- We use the Sim-CLR [3] or SimSiam [4] instantiation for this task.
- Table 3 shows that both auxiliary self-supervised learning tasks can improve the zero-shot and retrieval accuracy.
- EMA inference We find that using the EMA model for zero-shot classification and multi-modal retrieval evaluation is better than using the original network.
- An A-CLIP-eff variant For A-CLIP, we use the full image resolution as the input to the EMA network. We also introduce a variant called A-CLIP-eff, which uses half the resolution for EMA input.
- The new variant is more efficient than the A-CLIP variant, while being marginally worse in accuracy. Specifically, A-CLIP-eff is 0.86× than that of the original CLIP model in training cost, much faster than the A-CLIP variant (1.16× of the original CLIP model), and slightly worse than A-CLIP in terms of zero-shot image classification and multi-modal retrieval accuracy (see Table 1).

## Experiments

### Implementation Details

### Main Results
- CLIP with random masking results in a performance degradation of -2.6%
- The addition of attentive mask can greatly facilitate the integration of mask learning and CLIP, which avoids the mask to bias vision language contrastive task
- EMA-eff, using image with half length and width to reduce computational overhead, achieves competitive results compared to EMA-full
- Mix, a compromise of Low and taking only the top 25% most relevant tokens, adds some randomness to low but does not match performance
- A-CLIP uses masked multi-view input, which allows for a natural and efficient integration of different SSL tasks

### Ablation Study and Analysis
- We hypothesize that the attentive mask input play as strong augmentation to the input images, which can greatly alleviate the over-fitting issue and thus perform better when training is longer.
- To combine more auxiliary self-superivise learning(SSL) tasks as mentioned in Section 3.4, we considered adding stronger data augmentation to learn robuster representation.
- In A-CLIP with or without SSL, we tried with and without stronger data augmentation respectively. Table 5 shows the results.
- For A-CLIP without SSL in group a, blur+color gives a boost of +1.2% on ImageNet-1K zero-shot top-1 accuracy, showing attentive masking as an efficient data augmentation is not in conflict clas-sic color+blur.
- Combining the performance of groups b and c, it can be found that SSL(online) is more dependent on stronger augmentation, but SSL(online-EMA) can bring stable gains without color+blur.
- The difference between them is that one view of SSL(online-EMA) is an image without mask, which we believe allows the model of to learn the output distribution of the complete image.
- Effects of more attentive mask views Our proposed A-CLIP is not limited to two views, and can be extended for multiple views such as 3 and 4.
- Table 6 shows the results of using different views.
- In this comparison, we keep the total number of tokens the same for different k settings. k = 1 means the the original CLIP model which keeps all image tokens.
- For other k, each view selects the most relevant 196/k tokens. So the computation overheads for different k are roughly the same.
- It can be seen that all k = 2, 3, 4 performs significantly better than that of k = 1. k = 2, 3 both perform the best, and are higher than k = 4 (+2.4%).
- We have done some visualization of the attentive mask. Figure 4 showes the original image and the two views inputted in A-CLIP.
- It can be found that we basically keep the significant part of the image. The token that is closer to the language semantics makes the input of A-CLIP effective and valuable.
- Referring to Section 3.3, A-CLIP uses an EMA vision encoder for inference in order to generate a stable attentive mask.
- In Table 7, we find that using EMA for evaluation leads to a performance gain, especially for mask training.
- Effects of masked patch size Referring to the conclusion of SimMIM [37], we experimented with different mask patch sizes. As can be seen in Table 2, it obtained a +0.5% ImageNet zero-shot improvement from change mask size to 32. Due to the continuity and redundancy of images, greater granularity of filtering may lead to better performance.
- Using all layers works better for attentive token selection Table 2 ablates the use of different layers for attentive token selection. Using all layers performs +0.9% higher than that using the last layer.

## Conclusion
- The A-CLIP framework, or "A-CLIP for short", is a more efficient and effective CLIP training method that introduces attentive masks for the image branch.
- Compared to a baseline method using random masking, the attentive mask approach maintains image tokens that are more relevant to the alt-text description and performs much better.
- The A-CLIP framework is also flexible in incorporating multiple masked views as well as several auxiliary self-supervised tasks, which can further boost efficacy.
- The attentive mask also serves as good data augmentation, which benefits more from longer training than the original CLIP method.
- Due to these advantages,...
