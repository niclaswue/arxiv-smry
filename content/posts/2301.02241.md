---
title: "CiT: Curation in Training for Effective Vision-Language Data"
date: 2023-01-05T18:59:57.000Z
author: "Hu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer, Christoph Feichtenhofer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02241v1.webp" # image path/url
    alt: "CiT: Curation in Training for Effective Vision-Language Data" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02241)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02241).


# Abstract
- Large vision-language models are generally applicable to many downstream tasks, but come at an exorbitant training cost
- This paper trades generality for efficiency and presents Curation in Training (CiT), a simple and efficient vision-text learning algorithm that couples a data objective into training
- CiT automatically yields quality data to speed-up contrastive image-text training and alleviates the need for an offline data filtering pipeline, allowing broad data sources (including raw image-text pairs from the web)
- CiT contains two loops: an outer loop curating the training data and an inner loop consuming the curated training data. The text encoder connects the two loops. Given metadata for tasks of interest, e.g., class names, and a large pool of image-text pairs, CiT alternatively selects relevant training data from the pool by measuring the similarity of their text embeddings and embeddings of the metadata. In our experiments, we observe that CiT can speed up training by over an order of magnitude, especially if the raw data size is large.

# Paper Content

## Introduction
- Vision-language models have been successful for fine-tuning and zero-shot transfer to downstream tasks by training on a general-purpose large-scale dataset instead of a small task-level dataset
- General, largescale pre-training is computationally expensive and typically performed on a pre-filtered dataset (e.g. WIT400M used by CLIP [21] is created by searching for image-text pairs with text containing a set of 500,000 queries and [24] uses this model to create another dataset)
- Vanilla CLIP training uses static data from offline human filtering (e.g. cleaned YFCC15M or WIT400M [21]) and optimizes the model
- Instead, our CiT incorporates dynamic data curation into training in two loops: (i) an outer curation loop improving data (for downstream tasks) given the current model; (ii) an inner loop optimizing the model given the curated data.
- The trained text model connects the loops by providing embeddings for curation.
- leased) and is not able to access or change the data pipelines or model architectures.
- Further, work is limited by the prohibitive cost of training on these large image-text datasets (e.g. the CLIP model is trained on WIT400M for 12 days using 256 GPUs).
- In this work, our goal is to empower training with the capability of adjusting the data distribution. Our intention is to dynamically curate the data during training and our key idea is to use the learned text representation of vision-language models to measure relevance of the data w.r.t. the task of interest.
- Given metadata (from downstream tasks e.g. a class name such as "chicken"), we measure its embedding similarity to the training data. This similarity can guide us for the decision of including this data into our training process.
- For example a caption containing the word "giraffe" will have higher embedding similarity to "chicken" than a caption such as "throwback Thursday".
- Driven by this idea, we presents a simple algorithm that incorporates data Curation in Training (CiT), aiming at improving both data efficiency and model performance.
- CiT works as follows. Given a large source of image-text pairs and metadata (e.g. a list of class names used in this paper), CiT alternatively performs curation of the data and training on that curated data.
- As shown in Figure 1, CiT contains two loops: an outer loop to curate data given the current model and an inner loop trains the model given the curated data.
- Similar as Locked image Tuning (LiT [38]), CiT uses pre-trained image and text encoders and freezes the image one.
- The text model connects the two loops by serving curated data to inner loop for training which in turn learns good representations for the outer loop for curation.
- CiT can speed up training by multiple orders of magnitude, especially if the raw data size is large; e.g. when trained on LAION-400M data, CiT reaches similar Ima-geNet zero-shot1 accuracy as OpenCLIP [31], while being 37.7× faster in training.
- Since CiT changes the training data distribution that focuses on one or more tasks of interest, it can even handle image-text pairs from any (noisy) source with unknown distribution.

## Related Work
- Vision-language learning is a type of learning where a machine learns to understand and respond to language in images.
- Contrastive learning was initially popular in vision self-supervision and later adopted for cross-modal learning.
- CLIP [21] populates the idea of contrastive learning from imagetext pairs (used before e.g. in ConVIRT [40]) at scale and shows a strong performance of zero-shot transfer to image classification and retrieval tasks.
- SLIP [20] combines image self-supervision and language supervision.
- LiT [38] shows that when a good pre-trained vision encoder is adopted, it is better to lock (freeze) the well pre-trained vision encoder to protect vision representations from being corrupted by noisy language supervision.
- Flamingo also use pre-trained models for various tasks.
- Large-scale vision-language learning is typically coupled to a data pipeline to yield highquality data for efficient training.
- For example, CC3M [25] heavily filters web crawled pairs and only keeps 0.1% of the raw data.
- Both CC3M and CC12M [3] leverage Google Cloud APIs with models predicting a large number of classes (on the order of 10 5 ) [25] to filter out mismatched image-text pairs.
- YFCC100M is curated from Yahoo Flicker using text fields (such as title, descrip-tion, etc.). This ensures certain data quality but limits the scale.
- Later YFCC100M is further cleaned as YFCC15M to contain English-only image-text pairs by [21].
- Due to the limited scale, CLIP further curates a WebImageText dataset (WIT400M) by formulating queries from Wikipedia and performing searching them online to obtain image-text pairs.
- Florence [37] curates a dataset with the extra multilabel signals to improve supervision.
- ALIGN [12] relaxes CC12M filtering to show that training on 1.8B noisy pairs can achieve CLIP-level performance.
- FLAVA [26] combines existing human annotated datasets of smaller scale for high-quality image-text pairs.
- Different to related research, CiT improves data within the training algorithm, and not as a pre-filtering. We demonstrate that such approach allows us to effectively learn from raw image-text pairs.

## Method
- CLIP pre-training uses a training objective to approximate downstream tasks
- CiT introduces a data proxy to fit the data distribution to downstream tasks
- The data proxy is used in the training loop and the curation loop

### CiT Algorithm
- The curation function takes as input a data set D C and outputs a new data set D that is a subset of D C with the same weighting as D C .
- The training function takes as input a data set D and outputs a new weight vector W that optimizes the error between the predicted values of the data set D and the ground truth values.
- The two loops CiT are executed in parallel.
- The curation loop curates data given the current weights of the model and the training loop optimizing the weights given the curated data.
- The data set D C is the actual training data we aim to curate from the source.
- The function takes as input a data set D and outputs a new weight vector W that optimizes the error between the predicted values of the data set D and the ground truth values.
- The two loops CiT are executed in parallel.

### Training
- The core of CLIP training is the contrastive crossmodal objective proxy to approximates downstream tasks (e.g. higher classification accuracy).
- This objective pulls embeddings of positive image-text pairs closer and pushes negative pairs from other examples in a training batch apart; thus it creates a proxy for classification, which has one example per class and the rest of the batch are other classes described by natural language.
- The training loop is shown in Algorithm 3, with the training data D C , delivered from curation.
- We let m(•; •) denote the image-text model.
- We use sim(x img , x txt ) = x img x txt /( x img x txt ) in line 3 to compute the image-totext cosine similarity, divided by a trainable temperature τ .
- Our CiT training objective has almost the same structure as in CLIP, except that we only use an image-to-text (and no text-to-image) contrastive loss (L img2txt ) in line 4.
- We ablate this loss versus the averaged bidirectional contrastive loss (used by CLIP) in our experiments.
- Line 5 updates the model parameters and line 6 counts training cost.

### Curation
- The data objective of CiT is to curate data using the updated model.
- CiT first obtains the embeddings for the metadata in line 1.
- It sets up the curated set D C for the next round of training and keeps curating data in line 3-7.
- Line 3 gets the next batch of raw image-text pairs.
- Line 4 obtains its text part and line 5 computes the text embedding from the current model.
- Line 6 is the data proxy, which approximates the data distribution for the downstream tasks.
- Lastly, we merge the newly curated subset into the curated set D C .

## Experiments
- We use training data from two categories: clean data that involves human-based offline filter pipelines and raw data that has not undergone cleaning.
- We use a deep learning algorithm to improve the performance of a spam filter.
- The deep learning algorithm is able to improve the performance of the spam filter by using a feature extraction technique.
- The feature extraction technique is able to improve the performance of the spam filter by using a deep learning algorithm.
- The deep learning algorithm is able to improve the performance of the spam filter by using a feature extraction technique.

### Cleaned Training Data
- We use the 15M subset of YFCC100M [29] as the main evaluation dataset
- Except for applying the script from [20] to remove HTML formatting, we do not perform any extra filtering or preprocessing
- In contrast, LiT [38] performs extra filtering such as removing titles that start with "DSC", "IMG" and "Picture", or removing them if more than half of them contain digits
- CC12M. Since YFCC15M may lack enough training data, LiT [38] also combines YFCC15M with Conceptual Captions 12M (CC12M) [3], which is filtered and transformed from image & alt-text pairs from web pages
- CC12M involves cleaning by supervised models from Google Cloud APIs to match the image's prediction over classes with text.
- LAION400M [24] contains 400M English only image-text pairs. It is crawled from 2 and later filtered by a CLIP [21] model. Thus, LAION400M implicitly carries the data filter pipeline of WIT400M on which CLIP has been trained.

### Raw Training Data
- We use the raw YFCC100M (the source of YFCC15M) to compare with YFCC15M.
- Raw Image-Text Crawl.
- To challenge CiT with real-world data, we further collect raw (unfiltered) image-text pairs from Common Crawl.
- We only perform de-duplication and NSFW filtering, but no filtering on image-text association.
- This ended with 1.2B multilingual image-text pairs and 28.56% pairs are English (identified by our language identification system but this information is not used for CiT training).
- As such, about 343M image-text pairs are in English, which are slightly smaller than the scale of WIT400M or LAION400M, but much more noisy.

### Implementation and Training
- Uses 16 Nvidia V100 32GB GPUs for training
- Corresponds to ViT of various sizes
- Uses BERT base -SimCSE with a maximum token length of 32
- Uses pre-trained vision and text encoders
- Can either use the text representation before, or after the projection layer for computing cosine similarity during curation

### Evaluation
- Evaluates zero-shot transfer accuracy of CiT on 26 benchmarks
- Uses YFCC15M as the main data source for training and ImageNet-1K (IN-1K) as the downstream task
- Uses prompts from CLIP for all 26 tasks and additionally uses the extra 2 prompts from LiT for ImageNet
- Performs prompt ensembling by averaging the class embeddings for each class across the prompt templates
- For classification, cosine similarity is computed between an image embedding and the averaged class embeddings and the class with the highest cosine similarity is CiT's prediction
- Performs validation every 500 training steps and stops training if the accuracy does not increase over the previous validation
- Estimates the training time of baselines by re-running them under the same setup as CiT (i.e. 16 GPUs) and maximizes the GPU usage for best throughput

### Choice of Pre-trained Models

### Ablations

### Comparison to prior work on ImageNet
- CiT outperforms LiT on self-supervised MoCo-v3 vision encoders by +5.9% accuracy
- CiT with ViT-B/32 trained with supervised AugReg on IN-21K, CiT yields a +3.4% gain over LiT
- CiT outperforms LiT by +3.4% on YFCC100M
- CiT is faster than OpenCLIP on LAION400M
- CiT can overcome noise in raw image-text pairs to achieve high accuracy in vision-language learning

### Comparison across 26 benchmarks

### Further Analysis
- Samples of Curated Data: We further investigate samples curated by CiT on YFCC100M dataset in Table 8.
- Speed/accuracy trade-off: In Figure 2, we show the speed/accuracy trade-off of CiT vs. LiT [38], corresponding to results in Table 4). We see that CiT achieves a win-win scenario compared to LiT on identical AugReg ViT-B/32 vision encoders: a +3.4% higher accuracy on ImageNet, and a 5× faster total training time (including the curation time).
- Training data is YFCC15M. Models are evaluated at 6 evenly sampled iterations.
- Ratio of Curation: We are interested in the training dynamics of CiT. We use different curation thresholds t and inspect the amount of curated training data. In Figure 3, we see that the ratio of curation which corresponds to the fraction of used training samples from the raw data source, see §3.3, keeps changing over steps for curation/training. Initially, CiT uses more data, e.g. for a threshold of t = 0.5, it peaks at about 75%. In this phase, the latent space of the text encoder is less aligned with the vision latents. Later on during training, CiT starts to produce embeddings that better represent the downstream task, producing a lower ratio.

## Conclusion
- CiT, a novel learning algorithm for efficient pre-training from noisy image-text data
- incorporates a curation process into learning to pull the training data distribution closer to downstream tasks
- our experiments demonstrate both significant accuracy and training time improvements when learning from either public or our own uncurated data from the web
- can train with raw image-text pairs crawled from the web, which would lead to instability for vanilla pre-training objectives
- in this appendix, §A.1 contains implementation details and §A.2 contains further results as well as ablations
- for efficiency, we only load text during the curation loop and the training loop uses the curated indices to reload the full image-text pairs
- our implementation also supports inmemory storage of curated image-text pairs in case the data source is not randomly accessible for (re-)loading curated data
- we use a larger batch size for curation (compared to training) to speed up CiT
- hyperparameters of CiT training are shown in Table 9
- we mostly follow [20,21,38]
- CiT is trained on 16 GPUs with a global batch size of 16,384 (1024 per GPU)
- hyperparameters for CiT curation outlined in §3 of the main paper are shown in Table 10
- we use different thresholds t and minimal ratios γ for each dataset/metadata combination to fit the training into a budget b shown in the table as well
- for YFCC15M and CC12M, whereas for YFCC100M and Raw Img-Text Crawl we use a higher t to focus on high-quality data from the raw data source, in order to roughly meet the budget b
- single GPU Setting. We provide more details on the implementation of the extremely efficient single GPU setup used for zero-shot evaluation on multiple tasks in Table 12
