---
title: "GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models"
date: 2022-10-05T05:20:49.000Z
author: "Chen Liang, Wenguan Wang, Jiaxu Miao, Yi Yang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-02025v1.webp" # image path/url
    alt: "GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.02025)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.02025).


# Abstract
- Prevalent semantic segmentation solutions are a dense discriminative classifier of p(class|pixel feature)
- Going beyond this, we propose GMMSeg, a new family of segmentation models that rely on a dense generative classifier for the joint distribution p(pixel feature,class)
- For each class, GMMSeg builds Gaussian Mixture Models (GMMs) via Expectation-Maximization (EM), so as to capture class-conditional densities
- Meanwhile, the deep dense representation is end-to-end trained in a discriminative manner, i.e., maximizing p(class|pixel feature)
- This endows GMMSeg with the strengths of both generative and discriminative models
- With a variety of segmentation architectures and backbones, GMMSeg outperforms the discriminative counterparts on three closed-set datasets. More impressively, without any modification, GMMSeg even performs well on open-world datasets. We believe this work brings fundamental insights into the related fields.

# Paper Content

## Introduction

## Related Work
- Semantic segmentation is a problem of dividing a image or video into different categories
- Deep-net segmentation solutions are typically built in a dense classification fashion, i.e. learning dense representation and categorization end-to-end
- By directly adopting discriminative softmax for categorization, FCN-style solutions put focus on learning expressive dense representation; they modify the FCN architecture from various aspects, such as enlarging the receptive field
- Later Transformer-style solutions empower attentive networks with local contiguity and multi-level feature aggregation
- Two very recent attentive models formulate the task in an alternative form of mask classification, however, still relying on discriminative softmax
- In [94], though data density is estimated (as a mixture of vMF distributions), it is only used as a supervisory signal for dense embedding learning, and the final prediction is still made by a discriminative classifier -k-NN
- GMMSeg is a hybrid of generative GMM and discriminative representation, getting the best of two worlds
- Although bearing the general idea of trading-off between generative and discriminative classifiers [21-23, 96, 105, 106], none of the previous hybrid algorithms demonstrate their utility in challenging segmentation tasks

## Methodology
- Modern semantic segmentation models are within a dense discriminative classification framework
- This discriminative regime has defects from a probabilistic viewpoint
- We introduce GMMSeg, a new segmentation framework that brings a paradigm shift from the discriminative to generative

### Existing Segmentation Solutions: Dense Discriminative Classifier
- A deep neural network is used for pixel representation learning and softmax for semantic label prediction
- The feature extractor and classifier are jointly trained end-to-end and their corresponding parameters are optimized by minimizing the cross-entropy loss on D
- Softmax is not capable of inferring the data distribution and is notorious with inflating the probability of the predicted class

### GMMSeg: Dense GMM Generative Classification
- Our GMMSeg reformulates the task from a dense generative classification point of view.
- Instead of building posterior p(c|x) directly, generative classifiers predict labels using Bayes rule.
- Specifically, generative classifiers model the joint distribution p(x, c), by estimating the class-conditional distribution p(x|c) along with the class prior p(c).
- Then, following Bayes rule, the posterior is derived as:
- Since the class probabilities p(c) are typically set as a uniform prior (also in our case), estimating the class-conditional distributions (i.e., data densities) p(x|c) is the core and most difficult part of building a generative classifier.
- It is also worth noting that generative classifiers are optimized by approximating the data distribution Π (x,c)∈D p(x|c), which is called generative training [11].
- Although discriminative classifiers demonstrate impressive performance in many application tasks, there are several crucial reasons for using generative rather than discriminative classifiers, which can be succinctly articulated by Feynman's mantra "What I cannot create, I do not understand."
- Surprisingly, generative classifiers have been rarely investigated in modern segmentation models.
- Driven by the belief that generative classifiers are the right way to remove the shortcomings of discriminative approaches, we revisit GMM -one of the most classic generative probabilistic classifiers.
- We couple the generative EM optimization of GMMs with the discriminative learning of the dense feature extractor f -the most successful part of modern segmentation models, leading to a powerful, principled, and dense generative classification based segmentation framework -GMMSeg (Fig. 2).
- Specifically, GMMSeg adopts a weighted mixture of M multivariate Gaussians for modeling the pixel data distribution of each class c in the D-dimensional embedding space: Here m|c ∼ Multinomial(π c ) is the prior probability, i.e., m π cm = 1; µ cm ∈ R D and Σ cm ∈ R D×D are the mean vector and covariance matrix for component m in class c; and ϕ c = {π c , µ c , Σ c }.
- The mixture nature allows GMMSeg to accurately approximate the data densities and to be superior over softmax assuming unimodality for each class.
- Each Gaussian component has an independent covariance structure, enabling a flexible local measure of importance along different feature dimensions.
- To find the optimal parameters of the GMM classifier, i.e., {ϕ * c } C c=1 , a standard approach is EM [114], i.e., maximizing the log likelihood over the feature-label pairs {(x n , c n )} N n=1 in the training dataset D: EM starts with some initial guess at the maximum likelihood parameters ϕ c , and then proceeds to iteratively create successive estimates ϕ (t) c for t = 1, 2,• • • , by repeatedly optimizing a F function [115]: q c [m] = p(m|x, c; ϕ c ) gives the probability that data x is assigned to component m.
- F is defined as: where E qc [•] gives the expectation w.r.t. the distribution over the M components given by q c , and ] defines the entropy of q c .
- Based on Eqs. 4-7, for ∀x n : c n = c, we have: cm N (xn|µ where N c is the number of training samples labeled as c and N cm = n:cn=c q cn [m].
- In E-step, we recompute the posterior q c over the M components given the old parameters ϕ (t−1) .
- In M-step, with the soft cluster assignment q (t) c , the parameters are updated as ϕ (t) c such that the F function is maximized.
- In practice, we find standard EM suffers from slow convergence and delivers unsatisfactory results (cf. §4.3).
- A potential reason is the parameter sensitivity of EM -convergent parameters may change vastly even with slightly different initialization [116].
- Drawing inspiration from recent optimal transport (OT) based clustering algorithms [117,118], we introduce a uniform prior on the mixture weights π c , i.e., ∀c, m : Then E-step in Eq. 6 is performed by restricting the optimization of...

### Implementation Details
- GMMSeg is a general framework that can be built upon any modern segmentation network by replacing softmax with the GMM classifier
- In our experiments (cf. §4.1), we approach GMMSeg on a variety of segmentation models [7,[47][48][49] and backbones [50,51]
- In GMM classifier, a 1×1 conv is used to compress each pixel feature to a 64-dimensional vector, i.e., D = 64, and the covariance matrices Σ ∈ R D×D are constrained to be diagonal, for computational efficiency
- Each class c is represented by a mixture of M = 5 Gaussians (there are a total of 5C Gaussian components for a segmentation task with C semantic classes)
- To improve the diversity of the stored pixel features, we sample a sparse set of 100 pixels per class from each image, instead of directly storing the whole images into the memory
- Pixel prediction is made using Bayes rule (cf. Eq. 3): arg max c p(c|x), where p(c|x) ∝ p(x|c) with the uniform class distribution prior: p(c) = 1/C
- For anomaly segmentation, the pixel-wise uncertainty/anomaly score can be naturally raised as: −max c p(x|c), i.e., the outlier input should reside in low-probability regions [121]

## Experiments
- GMMSeg is effective and robust
- Diagnostic analysis is provided on the core model design.

### Experiments on Semantic Segmentation
- ADE 20K: 20,000 images in 3 categories (stuff, object, and background), with 150 items in each category.
- Cityscapes: 2,975 images in 19 categories.
- COCO-Stuff: 10,000 images in 171 categories.
- GMMSeg is applied to four different architectures: DeepLab V3+, OCRNet, UPerNet, and SegFormer.
- GMMSeg is implemented on MMSegmentation.
- For ADE 20K and COCO-Stuff, sliding window inference is used with 768×768 window size.
- Table 1 demonstrates GMMSeg's quantitative results.
- Fig. 3 illustrates the qualitative comparisons of GMMSeg against SegFormer.

### Experiments on Anomaly Segmentation

### Diagnostic Experiments
- EM algorithm alternates between E-step and M-step for maximum-likelihood inference
- In GMMSeg, in order to blend EM with stochastic gradient descent, we adopt an online version of (Sinkhorn) EM based on momentum update
- Number of Gaussian Components per Class
- Confidence Calibration
- Runtime Analysis

## Conclusion
- GMMSeg is the first generative neural framework for semantic segmentation
- GMMSeg successfully optimizes generative GMM with endto-end discriminative representation learning in a compact and collaborative manner
- GMMSeg is principled and well applicable in both closed-set and open-world settings
- We believe this work provides fundamental insights and can benefit a broad range of application tasks
