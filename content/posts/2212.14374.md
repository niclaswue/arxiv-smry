---
title: "Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks"
date: 2022-12-29T17:11:49.000Z
author: "Vincent Herrmann, Louis Kirsch, Jürgen Schmidhuber"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14374v1.webp" # image path/url
    alt: "Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14374)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14374).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/learning-one-abstract-bit-at-a-time-through).

# Abstract
- Artificial scientists learn to answer given questions and continually invent new questions
- Self-invented experiments in a reinforcement-providing environment lead to effective exploration
- Interesting thought experiments become boring over time

# Paper Content

## Introduction & Previous Work
- artificial curiosity and creativity are important in artificial systems
- artificial Q&A systems are designed to invent and answer questions
- the curiosity reward in artificial Q&A systems is ephemeral, because once something is known, there is no additional intrinsic reward for discovering it again
- in most RL applications, the external reward dominates the total reward

## Self-Invented Experiments Encoded as Neural Networks
- C can design arbitrary computational experiments (including thought experiments) with binary yes/no outcomes.
- Experiments may run for several time steps. However, C will prefer simple experiments whose outcomes still surprise M , until they become boring.
- In general, both the controller C and the model M can be implemented as (potentially multidimensional) LSTMs [11].
- At each time step t = 1, 2, . . ., C's input includes the current sensory input vector in(t), the external reward vector R e (t), and the intrinsic curiosity reward R i (t).
- C may or may not interact directly with the environment through action outputs.
- How does C ask questions and propose experiments? C has an output unit called the START unit. Once it becomes active (> 0.5), C uses a set of extra output units for producing the weight matrix or program θ of a separate RNN or LSTM called E (for Experiment), in fast weight programmer style [45,42,46,9,38,4,21,37,18].
- E takes sensory inputs from the environment and produces actions as outputs. It also has two additional output units, the HALT unit [60] and the RESULT unit.
- Once the weights θ are generated at time step t , E is tested in a trial, interacting with some environment. Once E's HALT unit exceeds 0.5 in a later time step t , the current experiment ends.
- That is, the experiment computes its own runtime [60].
- The experimental outcome r(t ) is 1 if the activation result(t ) of E's RESULT unit exceeds 0.5, and 0 otherwise.
- At time t , so before the experiment is being executed, M has to compute its output pr(t ) ∈ [0, 1] from θ (and the history of C's inputs and actions up to t , which includes all previous experiments their outcomes).
- Here, pr(t ) models M 's (un)certainty that the final binary outcome of the experiment will be 1 (YES) or 0 (NO).
- M is trying to predict this answer before the experiment is executed.
- Since E is an RNN and thus a general computer whose weight matrix can implement any program executable on a traditional computer [71], any computable experiment with a binary outcome can be implemented in its weight matrix (ignoring storage limitations of finite RNNs or other computers).
- That is, by generating an appropriate weight matrix θ, C can ask any scientific question with a computable solution.
- In other words, C can propose any scientific hypothesis that is experimentally verifiable or falsifiable.
- At t , M's previous prediction pr(t ) is compared to the later observed outcome r(t ) of C's experiment (which spans t − t time steps), and C's intrinsic curiosity reward R i (t ) is proportional to M's surprise.
- To calculate it, we interpret pr(t ) as M 's estimated probability of r(t ), given the history of observations so far.
- Then we train M by gradient descent (with regularization to avoid overfitting) for a fixed amount of time to improve all of its previous predictions including the most recent one.
- This yields an updated version of M called M * .
- In general, M * will compute a different prediction PR(t ) of r(t ), given the history up to t − 1.
- At time t , the contribution R IG (t ) to C's curiosity reward is proportional to the apparent resulting information gain, the KL-divergence If M had a...

## Experimental Evaluation
- Automatic generation of interesting experiments
- Evaluation of systems
- Challenges associated with the generation of interesting experiments

### Generating Experiments in a Differentiable Environment
- RL methods such as policy gradients are required
- The environment is fully differentiable and allows for computing analytical policy gradients
- The agent has to navigate through a 2D environment with a fixed external force field
- The states in this environment are the position and velocity of the agent
- The agent's actions are real-valued force vectors applied to itself
- To encourage laziness and a bias towards simple experiments, each time step is associated with a small negative reward
- A sparse large reward (100) is given whenever the agent gets very close to the goal state
- We operate in the single life setting without episodic resets
- Additional information about the force field environment can be found in Appendix A
- Since the environment is deterministic, it is sufficient for C to generate experiments whose results the current M cannot predict
- In many ways, this new setup (depicted in Figure 4 and described in Algorithm 2 in the Appendix) is similar to the one presented in Section 3.

### Pure RNN Thought Experiments
- The previous experimental setup uses feedforward NNs as experiments.
- This section investigates a complementary setup: interesting pure thought experiments are generated for some steps and then used to train a different type of neural network, which is called a RNN without any inputs.
- The RNN is driven by an intrinsic curiosity reward based on information gain.
- This setup is called a "complementary setup."

## Conclusion and Future Work
- The CM framework is extended to arbitrary self-invented computational experiments with binary outcomes
- The model has to predict the outcome of an experiment based solely on the experiment's parameters
- By creating experiments whose outcomes surprise the model, the controller curiously explores its environment and what can be done in it
- Such a system is analogous to a scientist who designs experiments to gain insights about the physical world
- Evaluation of two simple instances of such systems show that self-invented abstract experiments encoded as feedforward networks interacting with a continuous control environment facilitate the discovery of rewarding goal states
- In the second setup, the controller generates pure abstract thought experiments in the form of RNNs and observes that over time, newly generated experiments result in less intrinsic information gain reward
- These two empirical setups should be seen as initial steps towards more capable systems such as the one proposed in Section 2
