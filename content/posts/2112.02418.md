---
title: "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone"
date: 2021-12-04T19:50:29.000Z
author: "Edresson Casanova, Julian Weber, Christopher Shulby, Arnaldo Candido Junior, Eren Gölge, Moacir Antonelli Ponti"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2112-02418v3.webp" # image path/url
    alt: "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2112.02418)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2112.02418).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/yourtts-towards-zero-shot-multi-speaker-tts).

# Abstract
- YourTTS brings the power of a multilingual approach to the task of zero-shot
- Our method builds upon the VITS model and adds several novel modifications for zero-shot
- We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable
- Additionally, our approach achieves promising results in a target language with
- Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech

# Paper Content

## Introduction
- TTS systems have significantly advanced in recent years with deep learning approaches
- Most TTS systems were tailored from a single speaker's voice, but there is current interest in synthesizing voices for new speakers (not seen during training), employing only a few seconds of speech
- This approach is called zero-shot multi-speaker TTS (ZS-TTS)
- ZS-TTS using deep learning was first proposed by [5] which extended the DeepVoice 3 method [6]
- Meanwhile, Tacotron 2 [7] was adapted using external speaker embeddings extracted from a trained speaker encoder using a generalized end-to-end loss (GE2E) [8]
- Similarly, Tacotron 2 was used with a different speaker embeddings methods [2]
- The authors also showed that a gender-dependent model improves the similarity for unseen speakers [2]
- In this context, Attentron [3] proposed a finegrained encoder with an attention mechanism for extracting detailed styles from various reference samples and a coarsegrained encoder
- As a result of using several reference samples, they achieved better voice similarity for unseen speakers
- ZSM-SS [11] is a Transformer-based architecture with a normalization architecture and an external speaker encoder based on Wav2vec 2.0 [12]
- The authors conditioned the normalization architecture with speaker embeddings, pitch, and energy
- Despite promising results, the authors did not compare the proposed model with any of the related works mentioned above
- SC-GlowTTS [4] was the first application of flow-based models in ZS-TTS
- It improved voice similarity for unseen speakers in training with respect to previous studies while maintaining comparable quality
- Despite these advances, the similarity gap between observed and unobserved speakers during training is still an open research question
- ZS-TTS models still require a considerable amount of speakers for training, making it difficult to obtain high-quality models in low-resource languages
- Furthermore, according to [13], the quality of current ZS-TTS models is not sufficiently good, especially for target speakers with speech characteristics that differ from those seen in training
- When one limits the number and variety of training speakers, it also further hinders the model generalization for unseen voices
- In parallel with the ZS-TTS, multilingual TTS has also evolved aiming at learning models for multiple languages at the same time [15,16,17,18]
- Some of these models are particularly interesting as they allow for code-switching, i.e. changing the target language for some part of a sentence, while keeping the same voice [17]
- YourTTS with several novel ideas focused on zero-shot multi-speaker and multilingual training is proposed

## YourTTS Model

## Experiments

### Speaker Encoder
- The speaker encoder used in the SC-GlowTTS paper [4] reached an EER of 5.244
- The H/ASP model [29] publicly available, that was trained with the Prototypical Angular [30] plus Softmax loss functions in the VoxCeleb 2 [31] dataset was chosen for achieving state-of-the-art results in Vox-Celeb 1 [32] test subset.

### Audio datasets
- We investigated 3 languages, using one dataset per language to train the model.
- For all datasets, pre-processing was carried out in order to have samples of similar loudness and to remove long periods of silence.
- All the audios to 16Khz and applied voice activity detection (VAD) using Webrtcvad toolkit 4 to trim the trailing silences. Additionally, we normalized all audio to -27dB using the RMS-based normalization from the Python package ffmpeg-normalize.
- English: VCTK [14] dataset, which contains 44 hours of speech and 109 speakers, sampled at 48KHz. We divided the VCTK dataset into: train, development (containing the same speakers as the train set) and test. For the test set, we selected 11 speakers that are neither in the development nor the training set; following the proposal by [1] and [4], we selected 1 representative from each accent totaling 7 women and 4 men (speakers 225, 234, 238, 245, 248, 261, 294, 302, 326, 335 and 347). Additionally, in some experiments we used the subsets train-clean-100 and train-clean-360 of the LibriTTS dataset [34] seeking to increase the number of speakers in the training of the models.
- Portuguese: TTS-Portuguese Corpus [35], a singlespeaker dataset of the Brazilian Portuguese language with around 10 hours of speech, sampled at 48KHz. As the authors did not use a studio, the dataset contains ambient noise. We used the FullSubNet model [36] as denoiser and resampled the data to 16KHz.
- French: fr FR set of the M-AILABS dataset [37], which is based on LibriVox. It consists of 2 female (104h) and 3 male speakers (71h) sampled at 16KHz. To evaluate the zero-shot multi-speaker capabilities of our model in English, we use the 11 VCTK speakers reserved for testing. To further test its performance outside of the VCTK domain, we select 10 speakers (5F/5M) from subset test-clean of LibriTTS dataset [34].
- For Portuguese we select samples from the Multilingual LibriSpeech (MLS) [33] dataset.
- For French, no evaluation dataset was used, due to the reasons described in Section 4.

### Experimental setup
- The four experiments were carried out with four different datasets
- In experiment 1, a monolingual VCTK dataset was used
- In experiments 2 and 3, a bilingual VCTK and TTS-Portuguese dataset was used
- In experiment 4, a model was trained from the VCTK dataset with 1151 additional English speakers
- After training, the models were tested on new speakers and found to not generalize well

## Results and Discussion
- Mean Opinion Score (MOS)
- To compare the similarity between the synthesized voice and the original speaker, we calculate the Speaker Encoder Cosine Similarity (SECS)
- Following previous works [3,4], we compute SECS using the speaker encoder of the Resemblyzer [43] package, allowing for comparison with those studies.
- We also report the Similarity MOS (Sim-MOS) following the works of [1], [3], and [4].
- Although the experiments involve 3 languages, due to the high cost of the MOS metrics, only two languages were used to compute such metrics: English, which has the largest number of speakers, and Portuguese, which has the smallest number.
- For the calculation of MOS and the Sim-MOS in the English language, we use 276 and 200 native English contributors, respectively.
- For the Portuguese language, we use 90 native Portuguese contributors for both metrics.
- During evaluation we use the fifth sentence of the VCTK dataset (i.e, speakerID 005.txt) as reference audio for the extraction of speaker embeddings, since all test speakers uttered it and because it is a long sentence (20 words).
- For the Lib-riTTS and MLS Portuguese, we randomly draw one sample per speaker considering only those with 5 seconds or more, to guarantee a reference with sufficient duration.
- For the calculation of MOS, SECS, and Sim-MOS in English, we select 55 sentences randomly from the test-clean subset of the LibriTTS dataset, considering only sentences with more than 20 words.
- For Portuguese we use the translation of these 55 sentences.
- During inference, we synthesize 5 sentences per speaker in order to ensure coverage of all speakers and a good number of sentences.
- As ground truth for all test subsets, we randomly select 5 audios for each of the test speakers.
- For the SECS and Sim-MOS ground truth, we compared such randomly selected 5 audios per speaker with the reference audios used for the extraction of speaker embeddings during synthesis

### VCTK dataset
- For the VCTK dataset, the best similarity results were obtained with experiments 1 (monolingual) and 2 + SCL (bilingual).
- According to the Sim-MOS, the use of SCL did not bring any improvements; however, the confidence intervals of all experiments overlap, making this analysis inconclusive.
- On the other hand, according to SECS, using SCL improved the similarity in 2 out of 3 experiments.
- Also, for experiment 2, both metrics agree on the positive effect of SCL in similarity.
- Another noteworthy result is that SECS for all of our experiments on the VCTK dataset are higher than the ground truth.
- This can be explained by characteristics of the VCTK dataset itself which has, for example, significant breathing sounds in most audios.
- The speaker encoder may not be able to handle these features, hereby lowering the SECS of the ground truth.
- Overall, in our best experiments with VCTK, the similarity (SECS and Sim-MOS) and quality (MOS) results are similar to the ground truth.

### LibriTTS dataset
- We achieved the best LibriTTS similarity in experiment 4.
- This result can be explained by the use of more speakers (∼ 1.2k) than any other experiments.
- On the other hand, MOS achieved the best result for the monolingual case.
- We believe that this was mainly due to the quality of the training datasets.

### Portuguese MLS dataset
- The highest MOS metric was achieved by experiment 3+SCL, with MOS 4.11±0.07
- It is interesting to observe that the model trained in Portuguese with a single-speaker dataset of medium quality, manages to reach a good quality in the zero-shot multi-speaker synthesis.
- Experiment 3 is the best experiment according to Sim-MOS (3.19±0.10) however, with an overlap with other ones considering the confidence intervals.
- In this dataset, Sim-MOS and SECS do not agree: based on the SECS metric, the model with higher similarity was obtained in experiment 4+SCL.
- We believe this is due to the variety in the LibriTTS dataset.
- The dataset is also composed of audiobooks, therefore tending to have similar recording characteristics and prosody to the MLS dataset.
- We believe that this difference between SECS and Sim-MOS can be explained by the confidence intervals of Sim-MOS.
- Finally, Sim-MOS achieved in this dataset is relevant, considering that our model was trained with only one male speaker in
- Analyzing the metrics by gender, the MOS for experiment 4 considering only male and female speakers are respectively 4.14 ± 0.11 and 3.79 ± 0.12.
- Also, the Sim-MOS for male and female speakers are respectively 3.29 ± 0.14 and 2.84 ± 0.14.
- Therefore, the performance of our model in Portuguese is affected by gender.
- We believe that this happened because our model was not trained with female Portuguese speakers.
- Despite that, our model was able to produce female speech in the Portuguese language.

### Speaker Consistency Loss
- The use of Speaker Consistency Loss (SCL) improved similarity measured by SECS.
- However, for the Sim-MOS the confidence intervals between the experiments are inconclusive to assert that the SCL improves similarity.
- Nevertheless, we believe that SCL can help the generalization in recording characteristics not seen in training.
- For example, in experiment 1, the model did not see the recording characteristics of the Lib-riTTS dataset in training but during testing on this dataset, both the SECS and Sim-MOS metrics showed an improvement in similarity thanks to SCL.
- On the other hand, it seems that using SCL slightly decreases the quality of generated audio.
- We believe this is because with the use of SCL, our model learns to generate recording characteristics present in the reference audio, producing more distortion and noise.
- However, it should be noted that in our tests with high-quality reference samples, the model is able to generate high-quality speech.

## Zero-Shot Voice Conversion
- As in the SC-GlowTTS model, we do not provide any information about the speaker's identity to the encoder, so the distribution predicted by the encoder is forced to be speaker independent.
- Therefore, YourTTS can convert voices using the model's Posterior Encoder, decoder and the HiFi-GAN Generator.
- Since we conditioned YourTTS with external speaker embeddings, it enables our model to mimic the voice of unseen speakers in a zero-shot voice conversion setting.
- In [44], the authors reported the MOS and Sim-MOS metrics for the AutoVC [45] and NoiseVC [44] models for 10 VCTK speakers not seen during training.
- To compare our results, we selected 8 speakers (4M/4F) from the VCTK test subset.
- Although [44] uses 10 speakers, due to gender balance, we were forced to use only 8 speakers.
- Furthermore, to analyze the generalization of the model for the Portuguese language, and to verify the result achieved by our model in a language where the model was trained with only one speaker, we used the 8 speakers (4M/4F) from the test subset of the MLS Portuguese dataset.

### Intra-lingual results
- For zero-shot voice conversion from one English-speaker to another English-speaker, the model achieved a MOS of 4.20±0.05
- For zero-shot voice conversion from one Portuguese speaker to another Portuguese speaker, the model achieved a MOS of 3.64±0.09
- The model performs significantly worse in voice transfer similarity between female speakers (3.35±0.19) compared to transfers between male speakers (3.80±0.15). This can be explained by the lack of female speakers for the Portuguese language during the training of our model.

### Cross-lingual results
- Apparently, the transfer between English and Portuguese speakers works as well as the transfer between Portuguese speakers.
- However, for the transfer of a Portuguese speaker to an English speaker (pt-en) the MOS scores drop in quality.
- This was especially due to the low quality of voice conversion from Portuguese male speakers to English female speakers.
- In general, as discussed above, due to the lack of female speakers in the training of the model, the transfer to female speakers achieves poor results.
- In this case, the challenge is even greater as it is necessary to convert audios from a male speaker in Portuguese to the voice of a English female speaker.
- However, for transfers involving Portuguese, the absence of female voices in the training of the model hindered generalization.

## Speaker Adaptation
- The different recording conditions are a challenge for the generalization of the zero-shot multi-speaker TTS models.
- Speakers who have a voice that differs greatly from those seen in training also become a challenge [13].
- Nevertheless, to show the potential of our model for adaptation to new speakers/recording conditions, we selected samples from 20 to 61 seconds of speech for 2 Portuguese and 2 English speakers (1M/1F) in the Common Voice [38] dataset.
- Using these 4 speakers, we perform fine-tuning on the checkpoint from experiment 4 with Speaker Consistency Loss individually for each speaker.
- During fine-tuning, to ensure that multilingual synthesis is not impaired, we use all the datasets used in experiment 4.
- However, we use Weighted random sampling [41] to guarantee that samples from adapted speakers appear in a quarter of the batch.
- The model is trained that way for 1500 steps.
- For evaluation, we use the same approach described in Section 4.
- Table 3 shows the gender, total duration in seconds and number of samples used during the training for each speaker, and the metrics SECS, MOS and Sim-MOS for the ground truth (GT), zero-shot multi-speaker TTS mode (ZS), and the finetuning (FT) with speaker samples.
- In general, our model's fine-tuning with less than 1 minute of speech from speakers who have recording characteristics not seen during training achieved very promising results, significantly improving similarity in all experiments.
- In English, the results of our model in zero-shot multispeaker TTS mode are already good and after fine-tuning both male and female speakers achieved Sim-MOS comparable to the ground truth.
- The fine-tuned model achieves greater SECS than the ground truth, which was already observed in previous experiments.
- We believe that this phenomenon can be explained by the model learning to copy the recording characteristics and reference sample's distortions, giving an advantage over other real speaker samples.
- In Portuguese, compared to zero-shot, fine-tuning seems to trade a bit of naturalness for a much better similarity.
- For the male speaker, the Sim-MOS increased from 3.35±0.12 to 4.19±0.07 after fine-tuning with just 31 seconds of speech for that speaker.
- For the female speaker, the similarity improvement was even more impressive, going from 2.77±0.15 in zeroshot mode to 4.43±0.06 after the fine-tuning with just 20 seconds of speech from that speaker.
- Although our model manages to achieve high similarity using only seconds of the target speaker's speech, Table 3 seems to presents a direct relationship between the amount of speech used and the naturalness of speech (MOS).
- With approximately 1 minute of speech in the speaker's voice our model can copy the speaker's speech characteristics, even increasing the naturalness compared to zero-shot mode.
- On the other hand, using 44 seconds or less of speech reduces the quality/naturalness of the generated speech when compared to the zero-shot or ground truth model. Therefore, although our model shows good results in copying the speaker's speech characteristics using only 20 seconds of speech, more than 45 seconds of speech are more adequate to allow higher quality.
- Finally, we also noticed that voice conversion improves significantly after fine-tuning the model, mainly in Portuguese and French where few speakers are used in training.

## Conclusions, limitations and future work
- YourTTS achieved SOTA results in zero-shot multi-speaker TTS and zero-shot voice conversion in the VCTK dataset
- Furthermore, we show that our model can achieve promising results in a target language using only a single speaker dataset
- For speakers who have both a voice and recording conditions that differ greatly from those seen in training, our model can be adjusted to a new voice using less than 1 minute of speech
- However, our model exhibits some limitations, for example instability in the stochastic duration predictor which, for some speakers and sentences, generates unnatural durations
- Also, mispronunciations occur for some words, especially in Portuguese
- Unlike [35,46,19], we do not use phonetic transcriptions, making our model more prone to such problems
- For Portuguese voice conversion, the speaker's gender significantly influences the model's performance, due to the absence of female voices in training
- For Speaker Adaptation, although our model shows good results in copying the speaker's speech characteristics using only 20 seconds of speech, more than 45 seconds of speech are more adequate to allow higher quality
- In future work, we intend to seek improvements to the duration predictor of the YourTTS model as well as training in more languages
- Furthermore, we intend to explore the application of this model for data augmentation in the training of automatic speech recognition models in low-resource settings
