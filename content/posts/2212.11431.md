---
title: "Local Policy Improvement for Recommender Systems"
date: 2022-12-22T00:47:40.000Z
author: "Dawen Liang, Nikos Vlassis"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11431v1.webp" # image path/url
    alt: "Local Policy Improvement for Recommender Systems" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11431)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11431).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/local-policy-improvement-for-recommender).

# Abstract
- Recommender systems aim to answer the question of what items a user is likely to interact with next.
- Historically, the problem of predicting what a user will do next has been framed as a predictive task via (self-)supervised learning.
- However, in recent years, more emphasis has been placed on approaching the recommendation problem from a policy optimization perspective- learning a policy that maximizes some reward function.
- However, it is commonly the case in recommender systems that we are only able to train a new policy given data collected from a previously-deployed policy.
- One way to address this policy mismatch is through importance sampling correction, which unfortunately comes with its own limitations.
- In this paper, we suggest an alternative approach- local policy improvement without off-policy correction- which is drawn from a number of related results in the fields of causal inference, bandits, and reinforcement learning.
- We argue that this local policy improvement paradigm is particularly well suited for recommender systems, given that in practice the previously-deployed policy is typically of reasonably high quality, and furthermore it tends to be re-trained frequently and gets continuously updated.

# Paper Content

## INTRODUCTION
- Recommender systems are ubiquitous and are used in a variety of applications
- In the recommendation problem, we observe how a set of users interact with a set of items, and our goal is to present these users a set of previously unseen items that they will enjoy
- Historically, the problem has been framed primarily as a prediction task via (self-)supervised learning
- However, more research is focusing on viewing recommendation as a form of intervention, which naturally leads to a policy optimization approach
- One challenge in taking the policy optimization approach is that modern recommender systems are mostly off-policy, meaning we only observe feedback on items that have been recommended by the previously-deployed policy
- In this paper, we take a different approach by locally optimizing a new policy
- We draw from results in a number of fields including causal inference, bandits, and rl, and present a suite of methods that computes and optimizes a lower bound of the expected reward of the new policy
- In essence, we can perform policy optimization in an off-policy fashion without resorting to importance sampling correction, and hence avoid the issue of large estimation variance
- We argue that this type of local policy improvement is particularly well-suited for recommender systems

## BACKGROUND AND RELATED WORK
- offline learning is used to make decisions
- sequential recommendation is used to recommend items
- notational convention is used
- random, data-driven estimates have the form •

## Offline learning for decision making
- Uses an offline dataset of triplets (, , ) representing the observed context, action taken (with A being the action space), and resulting reward
- Defines the objective of offline learning for decision making as argmax   ().
- Is difficult to optimize because it is essentially asking a counterfactual question: what would have happened had we deployed policy  instead of the logging policy ?
- Re-writes Eq. 1 using importance sampling: which effectively re-weights the data generated from  to pretend it is sampled from .
- Suggests the following inverse propensity scoring (ips) estimator for  ():

## Sequential recommendation as offline decision making
- So far, we have mostly talked about offline learning for decision making in a general setting.
- Now we contextualize it and connect to recommender systems, in particular sequential recommendation.
- As standard in recommender systems literature, we assume a set of users  ∈ U interacting with a catalog of items  ∈ A.
- For each user , we have observed an interaction sequence   = { 1 ,  2 , . . . ,  |  | } (we will drop the subscript  when there is no ambiguity) which can be an ordered or unordered list of items.
- The goal of sequential recommendation is to build a model  (• | ) which takes input the interaction sequence  and predicts the next item a user will likely interact with.
- Given the sequential nature of the interaction, it is common in practice to use a sequential model (e.g., convolutional or recurrent neural networks, transformers, etc.) as  (• | ) [12,16,39,43,53] and train such a model via maximum likelihood given a collection of user-item interactions generated from some previously-deployed recommender system.
- Similar to how we define the dataset in offline learning for decision making in Section 2.1, we can assume the previously-deployed recommender system executes the "logging policy"  (we do not need the analytical form for , just assume that it exists).
- Furthermore, user-item interactions naturally induce some form of rewards (both positive and negative).
- For instance, in e-commerce use cases, the positive rewards can be clicks, add to cart, purchases, etc. The negative rewards are generally more implicit [13,23,27], which can present unique challenges to recommender systems [51,54].
- Even the standard sequential recommendation setting can be interpreted as having a 0-1 binary reward. Nevertheless, the specific definition of rewards is an important yet orthogonal question and is beyond the scope of this paper.
- From this perspective, we can re-interpret sequential recommendation as an offline decision making problem, where the goal becomes the following: Given an offline dataset of triplets {(  ,   ,   )} generated by previously-deployed recommender system , design a new policy  so that when given as input an interaction sequence  from a user, the recommender system recommends items  ∈ A following the policy  (• | ) to maximize the expected reward  ().
- Again, we can employ the same architectural choice for  (• | ) as we would for  (• | ) for the standard sequential recommendation problem [12,16,39,43,53].

## LOCAL POLICY IMPROVEMENT
- The problem of local policy improvement is addressed by drawing from various research threads
- The lower bound of the expected reward function is easy to estimate from data and does not involve density ratios
- To the best of our knowledge, the proposed methodology has not been used before in the context of recommendation systems

## The contextual bandit setting

## The rl setting
- We extend the above results to rl setting with a Markov decision process (mdp).
- Besides the context/state  ∈ X (to be consistent with the previous section, we use the word "context" and "state" interchangeably in the mdp setting), action  ∈ A, and (immediate) reward  (, ), we also assume a discount factor  ∈ (0, 1), a starting distribution  0 (), and a state transition model
- In an rl setting, we are often presented with trajectories sampled from a policy  as {( 0 ,  0 ,  0 ), ( 1 ,  1 ,  1 ), . . .
- We follow the standard notation of action-value   (, ), state-value   (), and advantage   (, ), defined as as the discounted stationary distribution of states  under policy  [40], where P ( =   | ) is the probability of being in state  after following  for  timesteps.
- The expected (discounted) cumulative reward of a policy  can be written as
- Recall in Eq. 8, we can compute the difference on the expected reward between two policies, a similar result exists for rl [1,15] which if we can maximize would again lead to an improved policy.
- However, the expectation over   is problematic since we can neither compute it nor sample from it.
- Many modern rl algorithms [28,47] optimize instead max subject to  ≈  and we can sample from   .
- From here, we can equivalently derive the same local policy improvement objective as in Section 3.1 by introducing  F  || as a penalty term.
- This result has been known as relative entropy policy search [29] in the optimal control / rl literature.
- Similar ideas are also explored in Schulman et al. [35,36].
- It may appear that we have overloaded the definition of the expected reward  () and advantage   in both Section 3.1 and this section.
- It is in fact an intentional choice since we can generalize our definition here to a contextual bandit setting by assuming  = 0 and the state  ∼  (•) are sampled independent of the previous state and action.
- To distinguish, we use lpi cb and lpi rl to refer to the contextual bandit and mdp settings, respectively.

## Practical considerations and recipes
- The local policy improvement perspective has been presented in various contexts within the bandit/rl/optimal control literature over the years [4,25,28,29,47].
- In previous work, the logging policy has mostly been used as a way to debias the learning and evaluation through ips [7,11,34].
- To the best of our knowledge, there has not been an application of the local policy improvement approach in the recommender systems literature.
- This is rather surprising; we would argue that the local policy improvement perspective fits particularly well for recommender systems.
- In a typical recommender system, we deploy a production/logging policy, which generates new user interaction data.
- These data is collected and used to train a new model, and this process repeats on a regular basis.
- In this case, it is reasonable to assume the logging policy is likely of reasonable quality.
- Therefore, it is especially relevant if we can always train a new policy that is an improved version of the previously-deployed policy.
- A general architecture.
- Figure 2a demonstrates how a typical recommender system interaction sequences can be formulated into the policy optimization perspective presented in this paper.
- The backbone of the architecture is a sequential model, which can be a convolutional/recurrent neural network or a transformer [12,16,39,43,53].
- The input at each step is  output corresponds to action   .
- For certain architectures, if the input sequence is too long, making predictions and computing loss function across the entire sequence might be infeasible.
- Therefore, we allow the loss to be only computed over the most recent  actions where  < |  | (as demonstrated by the dashed boxes in Figure 2a, meaning they can be skipped in the loss function).
- We use SASRec [16] as the backbone architecture for all the experiments.

## and the
- We have two separate objective functions to optimize: the local policy improvement part Llpi and the temporal-difference loss Ltd.
- To parametrize the policy, we take a similar approach to some earlier work [7,50,51] with a multi-head architecture shown in Figure 2b, which suggests the following general objective: where  balances the two parts of the objective.
- The auxiliary prediction head is primarily optimized by td loss in our case, e.g., it can be an estimated action-value Q (, ) or state-value V ().
- It also acts as the weighting function (e.g., exp( Â (  ,  )/)) to the policy head.
- This architecture also supports other related work [7,50,51] by setting the auxiliary prediction head accordingly.

## EXPERIMENTS
- Evaluation of the performance of the proposed local policy improvement approach to recommender systems is difficult because recommender systems operate under the partial feedback setup in which we only observe feedback on items that have been recommended by the deployed policy.
- It is argued [8] that offline (off-policy) evaluation [9,30,44,45] in general is more challenging than off-policy learning.
- Online AB testing is often considered the gold standard in terms of evaluating a recommendation policy. However, even in AB testing, a recommendation policy is rarely deployed as is; instead the results generated by a policy are often provided to a downstream model which might re-rank the items or construct a slate of items while considering other factors, e.g., business logic, and/or the diversity of the items in a slate.
- In this paper, we still primarily evaluate various policies following the standard procedure in recommender systems literature by looking at how heldout interactions are ranked by the new policy.
- We take this pragmatic approach since in practice before deploying a new recommendation policy online, traditional recommendation ranking metrics can still give us a rough guidance (especially when proper exploration strategies are adopted [6] and accounted for, which is beyond the scope of this paper).
- Additionally, as we demonstrate in this section, when we look at both the traditional ranking metrics as well as the divergence between the new policy and the logging policy, we are able to get a more holistic picture regarding the different approaches and make an explicit explorationexploitation trade-off.

## The mdp setting
- In the mdp setting, we assume the user context/state follows a transition model.
- Much of the experimental setup is still similar to Section 4.1 Datasets: We conduct experiments on two real-world e-commence datasets which have been used in recent work on applying rl to recommender systems: RC15 and RetailRocket.
- Both contain sequences of clicks and purchases/add-to-cart.
- The attributes of the datasets after preprocessing are in Table 2.
- We use the same train/validation/test data split as provided by Xin et al. [50,51,52].
- The only difference is in how we batch the sequences, as described in Section 3.3.
- Experimental setup.
- To easily compare with the related approaches, we use the following two recommendation metrics: hit rate (HR) and normalized discounted cumulative gain (nDCG).
- HR@ is similar to recall@, measuring whether the heldout item is in the top-of-the-recommended items.
- nDCG@ also considers the rank of the heldout item, but it assigns higher scores 1/log 2 (rank + 1) to higher positions.
- Following Xin et al. [50,51,52], we define both click-based and purchase-based metrics, and assign a higher reward ( p = 1.0) to actions leading to purchases compared to actions leading to only clicks ( c = 0.2).
- In addition to HR@ and nDCG@, we again measure the JS divergence between the learned policy π and the estimated logging policy μ. Specifically, we can compute an average over the dataset to estimate Baselines.
- In additional to the existing baselines: the estimated logging policy μ and the reward-weighted cross-entropy L  (), we also consider the following baselines: • Self-supervised Q-learning (sqn) [50]: it optimizes an unweighted cross-entropy function Lce () = 1   =1 log  (  |   ) regularized by td loss: Lsqn = Lce −  • Ltd . • Self-supervised Actor-Critic (sac) [50]: similar to sqn, this method optimizes an action-value-weighted cross-entropy function (akin to actor-critic) Lqce () = 1   =1 Q (  ,   ) log  (  |   ) regularized by the td loss: Lsac = Lqce −  • Ltd . • Policy Gradient (pg): similar to reward-weighted cross-entropy, but instead of immediate reward we use cumulative reward (referred to as "reward-to-go" [5,14]) as weight at each timestep. pg is by design an on-policy method. Here we treat it as a policy which optimizes a weighted cross-entropy objective.
- Off-policy Policy Gradient (ips-pg): similar to pg, but we apply an single-step important-sampling correction (  |  ) [7] which is shown to be biased but with much less variance.
- There has been some more recent approaches. Specifically, Xin et al. [52] considers a prompt-based approach to sequential recommendation.
- It is not obvious how we can parametrize such a policy so that we can compute the divergence between the learned policy and the logging policy, which is one of the main focuses in this paper.
- In addition, advantage function is also discussed in Xin et al. [51]. However, the way it is computed differs from the actual definition of the advantage function. We leave incorporating the modified-advantage used in Xin...

## CONCLUSION
