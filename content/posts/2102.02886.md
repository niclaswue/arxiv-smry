---
title: "Ivy: Templated Deep Learning for Inter-Framework Portability"
date: 2021-02-04T20:58:37.000Z
author: "Daniel Lenton, Fabio Pardo, Fabian Falck, Stephen James, Ronald Clark"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2102-02886v3.webp" # image path/url
    alt: "Ivy: Templated Deep Learning for Inter-Framework Portability" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2102.02886)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2102.02886).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter).

# Abstract
- Ivy is a templated Deep Learning (DL) framework
- It abstracts existing DL frameworks
- It unifies the core functions of these frameworks
- New high-level framework-agnostic functions and classes can be implemented as compositions of the unified low-level Ivy functions
- Ivy currently supports TensorFlow, PyTorch, MXNet, Jax and NumPy
- We release four pure-Ivy libraries for mechanics, 3D vision, robotics, and differentiable environments
- Through our evaluations, we show that Ivy can significantly reduce lines of code with a runtime overhead of less than 1% in most cases

# Paper Content

## INTRODUCTION
- There is a trade-off between run-time efficiency and ease of development in software projects.
- At a high level, this trade-off is intuitive; programming solutions with more abstractions remove complexity, but also necessarily remove control, and the ability to perform task-specific optimizations.
- Effective frameworks must find a middle ground between these two competing factors, where the right abstractions are needed to make development as quick and easy as possible, whilst also enabling customized implementations for maximum runtime efficiency and control.
- In the context of Deep Learning (DL), Python has emerged as the front-runner language for research and development.
- Most DL frameworks depend on efficient precompiled C++ code in the backend, which is a clear example of finding an effective balance between these competing factors.
- The Python interface makes prototyping code quick and easy, and the pre-compiled C++ operations and CUDA kernels in the backend make model inference fast.
- While users of most DL frameworks are still given the option for C++ and CUDA development of custom operations, the most common use case is for developers to implement their projects as compositions of operations in pure Python.
- Ivy abstracts existing DL frameworks such that their functional Application Programming Interfaces (APIs) all exhibit consistent call signatures, syntax and input-output behaviour.
- In doing so, Ivy effectively moves existing DL frameworks one layer down the abstraction stack to the Ivy "backend".
- As with the abstracted C++ backend in DL frameworks, we find the benefits of the Ivy abstraction generally outweigh the costs.
- New functions written in Ivy are instantly portable to TensorFlow, PyTorch, MXNet, Jax, and NumPy, enabling an inter-framework "drag-and-drop" approach not currently possible among modern DL frameworks.
- If a new Python DL framework was introduced in future, adding this framework to the Ivy backend would then make all existing Ivy code instantly compatible with the new framework.
- Ivy offers the potential for creating "lifelong" framework-agnostic DL libraries, which are jointly usable by present and future DL developers in present and future frameworks.

## Towards General Differentiable Programming
- DL models increasingly use a hybrid of neural networks and parameter-free, "handdesigned" components
- Many works which combine DL with SfM for geometric reconstructions also utilize core image projection and warping functions in the graph
- Gradient based optimization also pre-dates DL in many applied fields

## A Templated Framework
- Template metaprogramming refers to compile-time polymorphism, enabling source code to compile against different data types
- Template methods are a behavioral design pattern for object oriented programming, reducing lines of code by delegating low-level implementations of general abstract functions to more specific child classes
- Ivy takes inspiration from this general concept, and uses templates at the level of DL frameworks
- Ivy enables functions, layers and libraries to be implemented once, with simultaneous support for all prominent modern Python DL frameworks
- Unlike Keras (Chollet et al., 2015), we do not attempt to abstract high level classes
- Aside from this being more difficult to maintain, this level of abstraction removes control from users
- Instead, we abstract only the core tensor functions, which are often semantically similar, but syntactically unique

## RELATED WORK

## Deep Learning Frameworks
- DL progress has been rapid over the past decade
- Many frameworks were designed specifically for matrix and tensor operations
- In Python, one of the most widely used packages is NumPy, which established itself as a standard in scientific computing
- NumPy is a general matrix library, but with many function implementations highly optimized in C
- Since the beginning of the new DL era, a number of libraries with automatic differentiation have been utilized
- An early and widely used library was Caffe, written in C++
- The Microsoft Cognitive Toolkit (CNTK) was also written in C++
- Both of these are now deprecated
- Python has become the front-runner language for DL interfaces
- TensorFlow, Theano, Chainer, MXNet, PyTorch and JAX are all examples of DL frameworks primarily for Python development
- Despite the variety in frameworks, the set of fundamental tensor operations remains finite and well defined

## Deep Learning Libraries
- Many field-specific libraries exist
- DLTK (Pawlowski et al., 2017) provides a TensorFlow toolkit for medical image analysis
- PyTorch3D (Ravi et al., 2020) implements a library for DL with 3D data
- PyTorch Geometric (Fey & Lenssen, 2019) provides methods for deep learning on graphs and other irregular structures
- ZhuSuan (Shi et al., 2017) is a TensorFlow library designed for Bayesian DL
- Officially supported framework extensions are also becoming common, such as GluonCV and GluonNLP (Guo et al., 2020) for MXNet, TensorFlow Graphics (Valentin et al., 2019), Probability (Dillon et al., 2017), and Quantum (Broughton et al., 2020) for TensorFlow, and torchvision and torchtext for PyTorch (Paszke et al., 2019)
- However, the frameworks these libraries are built for can quickly become obsoleted
- Furthermore, none of these libraries address the code shareability barrier for researchers working in different frameworks
- As of yet, there is no solution for building large framework-agnostic libraries for users of both present and future DL Python frameworks

## Deep Learning Abstractions
- Keras provided abstractions at the level of classes and models, enabling users to prototype quickly with higher level objects and standard training pipelines.
- However, for researchers adopting non-standard pipelines, this level of abstraction can remove too much control.
- In contrast, Ivy simplifies and reduces the abstraction to the core tensor APIs, enabling new libraries and layers to be built on top of Ivy's functional API in a highly scalable, maintainable and customized manner.
- TensorLy (Kossaifi et al., 2016) is closer in scope to Ivy, also offering a framework agnostic functional API. However, many functions such as gather, scatter and neural network functions such as convolutions are not provided by TensorLy. The library instead focuses on general tensor operations such as decomposition, regression and sparse tensor handling. The TensorLy backend focuses on the functions necessary to support these operations.
- Ivy offers a much more comprehensive API, applicable to a wide variety of fields, which is showcased in the applied libraries for mechanics, vision, robotics, and differentiable environments.

## IVY CORE
- Ivy provides an overview of the core API
- Ivy functions are unit tested against each backend framework
- The Ivy API can easily be extended to include additional functions

## Framework-Specific Namespaces
- Almost all of the functions in the core Ivy API exist in the native frameworks in some form.
- Ivy wraps these native functions to provide consistent syntax and call signatures, and in some cases also extend functionality to achieve this goal.
- This is necessary in cases where the native functions are lacking, for example ivy.torch.gather_nd is implemented by wrapping the less general torch.gather.
- In general, the input-output behaviour for each Ivy function is selected to be the most general variant among the backends, whilst following the most common syntax.
- The framework-specific functions with the updated Ivy syntax and call signatures are all accessible via frameworkspecific namespaces such as ivy.tensorflow and ivy. torch, see Figure 2.
- Each of these namespaces behave like the functional API of the original framework, but with the necessary changes to bring inter-framework unification.
- Due to the semantic similarity between all DL frameworks, these changes are very minor for most functions, with many changes being purely syntactic, which enables direct bindings to the native functions.
- Other functions require simple re-arrangement of the arguments, and sometimes extra processing of optional arguments to unify default behaviour.
- We show how Ivy wraps PyTorch functions with varying extents of modification below.
- A full runtime analysis of the Ivy overhead for each core function averaged across the backend frameworks is given in Section 7.2, and frameworkspecific overheads are provided in Appendix A.4.

## Framework-Agnostic Namespace
- framework-specific namespaces provide interframework unification of syntax and call signatures
- framework-agnostic Ivy namespace is used to create framework agnostic code
- every function in framework-specific namespaces is also implemented in the framework-agnostic namespace
- how user specifies which backend framework to use at runtime is a remaining question

## Local Framework Specification

## Global Framework Setting
- A framework can be set globally for all future function calls
- The framework can be specified either as a string or as the backend namespace
- The backend can then be unset using ivy.unset_framework()
- With most projects using a single framework at runtime, this is the recommended framework selection mode
- The framework can also be set globally for a block of code by using the with command

## Ivy Classes
- Ivy is a fully functional framework at its core
- Ivy provides classes which build directly on these functional primitives
- Common trainable neural network layers and stateful optimizers are provided, which derive from the base classes ivy.Module and ivy.Optimizer respectively
- Importantly, these high level classes do not abstract high-level classes from the backend frameworks
- New classes are built directly on top of Ivy's framework-agnostic functional API, making the new classes stable, maintainable and easily extensible

## IVY LIBRARIES
- Ivy provides an API that is framework agnostic
- Many high-level framework-agnostic Ivy libraries are possible
- Every function in these libraries are unit tested
- All support arbitrary batch dimensions of the inputs
- Brief overviews of these four libraries are provided
- The functions in these libraries can all be integrated directly into arbitrary computation graphs for end-to-end gradientbased learning

## A SPECTRUM OF USERS
- Ivy contributors use Ivy Core to develop an Ivy library
- Ivy library users exist on the other end of the spectrum and use Ivy libraries to supplement their own projects in their own preferred native framework
- Ivy creators exist somewhat in the middle of the spectrum and use both Ivy core and the Ivy libraries to implement substantial parts of their own personal project in Ivy
- Once this project is released online, the project can be forked by others and integrated directly into new projects, regardless of their frameworks

## END-TO-END INTEGRATION
- The functions from all Ivy libraries can be integrated into arbitrary computation graphs, such as neural networks, for gradient-based end-to-end training.
- This is useful for many areas of intersectional research, which explore the integration of conventional parameter-free computation within neural-network based deep learning.
- The libraries are also applicable to gradient-based methods outside of deep learning.
- We explore once such example in this section, which combines the different Ivy libraries in an intersectional application. Specifically, we explore the combined application of the mechanics, vision and robotics libraries to gradient-based motion planning of a drone in a scene with obstacles, see Fig 4.
- This takes on a similar formulation to a variety of existing works (Ratliff et al., 2009;Schulman et al., 2014).
- The public method drone.sample_body is then called, receiving the trajectory of matrix poses m traj , to produce body point trajectories b traj ∈ R 100×5×3 in world space.
- The scene is represented as a collection of bounding boxes, one for each object, and the method ivy_vision. cuboid_signed_distances is used to convert this scene description into a single scene-wide signed distance function (SDF).
- This SDF is then queried using the body point trajectories b traj and summed, the lengths of each trajectory in b traj are also summed, and the sum of lengths and negative sum of signed-distances are combined to create the motion planning cost function.

## FRAMEWORK EVALUATIONS
- Ivy abstracts away the underlying backend library, making development time faster
- The overhead of wrapping the backend functions brings all backend frameworks into syntactic and behavioural alignment

## Line of Code Analysis
- Ivy makes it possible to write a library once, with joint support of all DL frameworks
- Currently supports 5 backend frameworks, which means all Ivy libraries use only 20% of the code that would be required compared to the alternative of creating framework-specific libraries
- Offers a variety of commonly used functions in different areas of applied DL
- This avoids the need for Ivy users to implement these functions themselves, reducing lines of code in their own projects
- To quantify these points with a concrete example, we analyze the lines of code required to implement the motion planning pipeline from Sec 6, both with and without Ivy and it's libraries
- We consider the lines of code required from the perspective of the Ivy user, wishing to implement this demo in a manner that supports all frameworks
- We first assume access to both Ivy and it's libraries, which results in 100 LoC
- We next assume that the libraries do still exist, but Ivy does not exist, and so we assume the libraries are implemented in each of the native frameworks PyTorch, TensorFlow, JAX, and MXNet
- This would mean four separate motion planning demo scripts would be required in order to support all frameworks, bringing the total LoC to 100 × 4 = 400
- Numpy is not included in this case, as it does not support automatic gradients, which are required for this demo
- We next consider the LoC assuming that Ivy does exist, but the Ivy libraries do not exist.
- Table 1 quantifies the LoC for each of the functions used in the example from Section 6, outlined in Figure 4.
- Table 1. Lines of code for the different Ivy library functions used in the motion planning example from Section 6.
- Therefore, without the existence of the Ivy libraries, each function would need to be implemented as part of the demo, and the total demo LoC increases to 100 + 53 + 133 + 108 + 61 = 455.
- Finally, we consider the case where neither Ivy nor the Ivy libraries exist.
- Taking the previous result for no Ivy libraries 455 as a starting point, the demo would now also need to be repeated for each specific framework, bringing the total LoC to 455×4 = 1820.

## Ivy Core Runtime Analysis
- In order to assess the overhead introduced by the Ivy abstraction, we perform a runtime analysis for each core function using all possible backend frameworks, and assess how much inference time is consumed by the Ivy abstraction in both eager mode and compiled mode.
- Ivy code can be compiled using ivy.compile_fn(), which wraps the compilation tools from the native framework.
- Our analysis only considers 53 of the 101 core functions implemented at the time of writing, as the remaining 48 Ivy functions incur no overhead for any of the backend frameworks.
- Each function is called with all inputs of flattened size 1 × 10 4 where possible, and is executed on the CPU.
- To perform this analysis, we separate the lines of code for each Ivy function into 3 code groups: In eager mode, by using the method time. perf_counter() from the time module between adjacent code groups; and in compiled mode, by using the native compiler.
- While the absolute runtimes of eager functions will be slower than compiled functions, we find that the relative runtimes between different tensor operations in eager mode is a good approximation for their relative runtimes in compiled mode.
- Our analysis focuses on the proportionate overhead of Ivy, and not the absolute compiled runtimes, and so this approximation is still informative for our analysis.
- The runtime analysis for each core function averaged across the backend frameworks is presented in Figure 6.
- Framework-specific runtimes are presented in Appendix A.4.
- By combining the method usage frequencies for each library (see Appendix A.1) with the Ivy overhead runtimes, we assess the Ivy overhead when using each of the four Ivy libraries in both eager mode and compiled mode.
- We compute these values separately for each backend framework.
- The results are presented in Table 3.

## CONCLUSION AND FUTURE WORK
- Ivy is a templated deep learning framework supporting TensorFlow, PyTorch, MXNet, Jax, and Numpy.
- Ivy offers the potential for creating frameworkagnostic DL libraries, which are usable in both present and hypothetical future frameworks.
- We provide four initial Ivy libraries for mechanics, 3D vision, robotics, and differentiable environments.
- We welcome developers to join the Ivy community by writing their own functions, layers and libraries in Ivy, maximizing their direct audience and helping to accelerate DL research through the creation of "lifelong" inter-framework codebases.
- Regarding the future vision for Ivy, we will continue extending the derived libraries, as well as adding new libraries for additional research fields.
- We also will continue developing Ivy Core, to remain compatible with all the latest DL framework developments, and add support for new Python frameworks as and when they arrive.
- We will strive to support the community of open DL research through our framework, and continue to encourage collaboration and contributions from the community.
