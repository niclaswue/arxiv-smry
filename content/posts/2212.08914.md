---
title: "Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark"
date: 2022-12-17T16:32:15.000Z
author: "Xiaofeng Wang, Zheng Zhu, Yunpeng Zhang, Guan Huang, Yun Ye, Wenbo Xu, Ziwei Chen, Xingang Wang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08914v1.webp" # image path/url
    alt: "Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08914)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08914).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/are-we-ready-for-vision-centric-driving).

# Abstract
- In recent years, vision-centric perception has flourished in various autonomous driving tasks
- However, the latency of vision-centric approaches is too high for practical deployment
- To bridge the gap between ideal research and real-world applications, it is necessary to quantify the trade-off between performance and efficiency
- Traditionally, autonomous-driving perception benchmarks perform the offline evaluation, neglecting the inference time delay
- To mitigate the problem, we propose the Autonomous-driving StreAming Perception (ASAP) benchmark
- On the basis of the 2Hz annotated nuScenes dataset, we first propose an annotation-extending pipeline to generate high-frame-rate labels for the 12Hz raw images
- Referring to the practical deployment, the Streaming Perception Under constRained-computation (SPUR) evaluation protocol is further constructed
- In the ASAP benchmark, comprehensive experiment results reveal that the model rank alters under different constraints, suggesting that the model latency and computation budget should be considered as design choices to optimize the practical deployment

# Paper Content

## Introduction
- The ASAP benchmark is a quantitative evaluation of the accuracy-latency trade-off of camera-based perception methods
- An annotation-extending pipeline is proposed to increase the annotation frame rate of the nuScenes dataset from 2Hz to 12Hz
- Simple baselines are established in the ASAP benchmark, which alleviates the influence of inference delay and consistently improves the streaming performances across different hardware.

## Related Work

### Autonomous-Driving Benchmark
- The release of various benchmarks has led to immense progress in autonomous-driving perception
- Among these benchmarks, CamVid [3], Cityscapes [14], Mapillary Vistas [45], Apolloscape [30], BDD100K [75] and CityPerson [76] focus on the 2D annotation (segmentation masks or detection boxes)
- To facilitate 3D perception in autonomous driving, several benchmarks collect multi-modal data (RGB images, Li-DAR, RADAR, GPS/IMU) with comprehensive 3D annotations
- Among these 3D annotated datasets, nuScenes [4], Waymo [57], Argoverse [5], A2D2 [20], Lyft L5 [53] provide surround-view image data
- Especially in the nuScenes dataset [4], a vision-centric 3D perception trend [28,29,[36][37][38]40,41,48,64,69] demonstrates that camera-based 3D detectors can achieve promising accuracy
- Nevertheless, these benchmarks evaluate perception methods in an offline manner, neglecting the inference time delay

### Vision-Centric Driving Perception
- Compared with the costly LiDAR, cameras can be deployed with much lower budgets
- Besides, cameras-based methods own the desirable merits to extract rich semantic information from dense color and texture information
- e.g., 3D detection, semantic map construction, and depth estimation

### Streaming Perception
- The concept of streaming perception is first proposed
- Faced with model latency, Kalman filter [31], dynamic scheduling [34], and reinforcement learning [21] are utilized to alleviate the problems caused by inference time delay
- To further enhance the streaming performance, [71] simplifies the streaming perception to the task of predicting the next frame by an efficient detector [18]
- To investigate the streaming perception in LiDAR-based 3D detection, [25] proposes to split the full-scan LiDAR points into multiple slices and process the streaming LiDAR slices with a recurrent neural network
- Following the same setting, [17] preserves past slice features and concatenates them with current slice data
- The polar-pillar representation is utilized in the streaming perception

## The ASAP benchmark
- The concept of ASAP is introduced
- The difficulty to evaluate streaming algorithms on the original nuScenes dataset is analyzed
- The high frame-rate nuScenes-H dataset is introduced
- The SPUR evaluation protocol is presented to assess the streaming performance under different computational resources
- Simple baselines to alleviate the inference time delay in streaming detection are proposed

### Autonomous-Driving Streaming Perception
- The ASAP benchmark conducts the evaluation in an online manner
- The key insight is to perform the evaluation at every input timestamp even if the processing of the current sample is not complete
- The streaming evaluation is illustrated in Fig. 2
- For frame A and frame B, the previous predictions are evaluated

### nuScenes-H

### SPUR Evaluation Protocol
- The Streaming Perception Under constRained-computation (SPUR) evaluation protocol is designed to comprehensively investigate the streaming performance of various 3D detectors.
- In the streaming evaluation, the predicted bounding boxes are displaced from the ground-truth locations due to inference time delay, especially for fast-moving objects. Consequently, the majority of the anticipated true positives are slow-moving or static objects, and the AVE metric only measures the velocity error of true positive objects, which makes the streaming velocity error even lower than the offline velocity error.
- To address the issue, we calculate AVE as the original offline metric, and other metrics are measured with the streaming evaluation, which are termed mAP-S, ATE-S, ASE-S, AOE-S, and AAE-S.

### ASAP Baselines
- The ASAP benchmark evaluates the most recent predictions if the current computation is not finished, resulting in a mismatch between the previously processed observation and the current one.
- In this subsection, we discuss how to mitigate the mismatch problem induced by the inference time delay.
- Naturally, forecasting the future state emerges as a simple solution to compensate for the delay.
- To make future state estimations, we establish the velocity-based baseline that updates future states by the predicted object motion.
- Besides, we investigate a learning-based baseline that directly estimates the future locations of objects (illustrated in Fig. 5).
- Velocity-based updating baseline.
- Object velocity estimation is an essential task in the original nuScenes benchmark, and various 3D detectors [28, 36-38, 41, 48, 64] have been investigated to produce accurate velocity estimations.
- We empirically find that simply using the constant velocity motion model can benefit streaming 3D detection: where T r(•) and V (•) represent the predicted object translation and velocity, and t i , t i+1 denote the previous input timestamp and the current evaluation timestamp.
- Such a velocity-based updating strategy is straightforward, but the velocity estimations are independent in each frame, neglecting that the predicted velocity should be consistent and change smoothly.
- To alleviate the problem, we associate predictions in consecutive frames and refine the predictions using the first-order Kalman filter [31]. Specifically, IoUbased greedy matching is applied to associate 3D bounding boxes across frames.
- And state representation in the Kalman filter is {x, y, z, ẋ, ẏ}, where (x, y, z) denotes the center location of the bounding box, and ẋ, ẏ is the estimated velocity in the BEV plane.
- The updated state representations are leveraged to refine per-frame predictions.
- For objects that do not have correspondence in sequential frames, we simply use the constant velocity motion model to update the locations.
- Notably, the velocity-based updating strategy can be applied to any modern 3D detector (e.g., in the experiment, BEVDepth-Sv is built upon BEVDepth [37]).
- Besides, the updating pipeline is lightweight (∼10ms on CPU), which has a negligible impact on streaming delays.
- Learning-based forecasting baseline.
- The above baseline exploits velocity as the intermediate surrogate to predict future states.
- From another perspective, the streaming 3D detector should be inherently predictive of the future.
- Therefore, we craft a simple framework for directly forecasting the future locations of objects.
- Specifically, a 3D-future detector is built upon BEVDepth [37] (termed BEVDepth-Sf), where the algorithm leverages history frames as input and forecasts the detection results of the next frame.
- The model architecture and training strategy are similar to those of [37], except that the loss is calculated using annotations from the subsequent frame.
- For samples that do not have annotations for future frames, we discard them in the training phase.

## Streaming Evaluation on ASAP Benchmark
- The experiment setup is given
- Subsequently, the computation-constrained assessment is discussed
- Finally, the association between streaming performance and input size/backbone selection is analyzed.

### Experiment Setup
- In the ASAP benchmark, vision-centric perception is instantiated on camera-based 3D detection
- The extended nuScenes-H dataset is leveraged to evaluate 3D detectors
- Following [34], the streaming evaluation is conducted with a hardware-dependent simulator
- For measuring the inference time of monocular paradigms [61,62], we set the batch size as 6

### Computation-Constrained Assessment
- All the 3D detectors suffer from performance drops on the ASAP benchmark
- The streaming performance degrades continually as the computation power is increasingly constrained
- Future state estimation can compensate for the inference time delay, which improves the streaming performance

### Analysis on Input Size and Backbone Selection
- Experiments are conducted to investigate the association between streaming performance and input size/backbone selection
- For the offline evaluation, BEVDepth@ResNet101 relatively improves BEVDepth@ResNet50 by 3.4%.
- Besides, the im- plementation of high-resolution inputs and stronger backbones may hinder streaming performance due to high latency.

## Conclusion
- The ASAP benchmark is proposed to evaluate the online performance of vision-centric driving perception approaches
- The ASAP benchmark is based on modern GPUs (e.g., NVIDIA RTX3090, NVIDIA RTX2070S, NVIDIA GTX1060)
- The ASAP benchmark is used to compare the streaming performance of seven modern 3D detectors
- The proposed baselines built upon [28,29,38,40,61,62] consistently enhance the streaming performance
