---
title: "Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data"
date: 2023-01-02T05:00:11.000Z
author: "Jumin Lee, Woobin Im, Sebin Lee, Sung-Eui Yoon"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-00527v1_9boSEH14U.jpg" # image path/url
    alt: "Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00527)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00527).


# Abstract
- models.
- Uses discrete and latent diffusion for 3D categorical data on a scene-scale
- Perform semantic scene completion by learning a conditional distribution

# Paper Content

## Introduction
- Learning to generate 3D data has received much attention
- For instance, a 3D generative model with a diffusion probabilistic model [2] has shown its effectiveness in 3D completion [2] and text-to-3D generation [1,3]
- While recent models have focused on 3D object generation, we aim beyond a single object by generating a 3D scene with multiple objects. In Fig. 1b, we show a sample scene from our generative model, where we observe the plausible placement of the objects, as well as their correct shapes.
- Compared to the existing object-scale model [1] (Fig. 1a), our scene-scale model can be used in a broader application, such as semantic scene completion (Sec. 4.3), where we complete a scene given a sparse LiDAR point cloud.
- We base our scene-scale 3D generation method on a diffusion model, which has shown remarkable performance in modeling complex real-world data, such as realistic 2D images [4][5][6] and 3D objects [1][2][3].
- We develop and evaluate diffusion models learning a scene-scale 3D categorical distribution. First, we utilize categorical data for a voxel entity since we have multiple objects in contrast to the existing work [1][2][3], so each category tells each voxel belongs to which category. Thus, we extend discrete diffusion models for 2D categorical data [7,8] into 3D categorical data (Sec. 3.1).
- Second, we validate the latent diffusion model for the 3D scene-scale generation, which can reduce training and testing computational cost (Sec. 3.2).
- Third, we propose to perform semantic scene completion (SSC) by learning a conditional distribution using our generative models, where the condition is a partial observation of the scene (Sec. 3.1). That is, we demonstrate that our model can complete a rea-sonable scene in a realistic scenario with a sparse and partial observation.
- Lastly, we show the effectiveness of our method in terms of the unconditional and conditional (SSC) generation tasks on the CarlaSC dataset [9] (Sec. 4). Especially, we show that our generative model can outperform a discriminative model in the SSC task.

## Related Work

### Semantic Scene Completion
- We use a discrete diffusion model for conditional 3D segmentation map generation
- As a baseline model against the diffusion model, we train a network with an identical architecture by discriminative learning without a diffusion process
- We optimize the baseline with a loss term L = − k w k x k log(x k ), where w k is a weight for each semantic class
- We visualize results from the baseline and our discrete diffusion model in Fig. 4
- Despite the complexities of the networks being identical, our discrete diffusion model improves mIoU (i.e., class-wise IoU) up to 5.89%p than the baseline model
- Especially, our method achieves outstanding results in small objects and fewer frequency categories like 'pedestrian', 'pole', 'vehicles,' and 'other'

### Diffusion Models
- Recent advances in diffusion models have shown that a deep model can learn more diverse data distribution by a diffusion process [5].
- A diffusion process is introduced to adopt a simple distribution (e.g., Gaussian) to learn a complex distribution [4].
- Especially, diffusion models show impressive results for image generation [6] and conditional generation [22,23] on high resolution compared to GANs.
- GANs are known to suffer from the mode collapse problem and struggle to capture complex scenes with multiple objects [24].
- On the other hand, diffusion models have a capacity to escape mode collapse [6] and generate complex scenes [23,25] since likelihood-based methods achieve better coverage of full data distribution.
- Diffusion models have been studied to a large extent in high-dimensional continuous data. However, they often lack the capacity to deal with discrete data (e.g., text and segmentation maps) since the discreteness of data is not fully covered by continuous representations.
- To tackle such discreteness, discrete diffusion models have been studied for various applications, such as text generation [7,8] and lowdimensional segmentation maps generation [7].
- Since both continuous and discrete diffusion models estimate the density of image pixels, a higher image resolution means higher computation.
- To address this issue, latent diffusion models [23,26] operate a diffusion process on the latent space of a lower dimension.
- To work on the compressed latent space, Vector-Quantized Variational Auto-Encoder (VQ-VAE) [27] is employed.
- Latent diffusion models consist of two stages: VQ-VAE and diffusion. VQ-VAE trains an encoder to compress the image into a latent space. Equipped with VQ-VAE, autoregressive models [28,29] have shown impressive performance.
- Recent advances in latent diffusion models further improve the generative performance by ameliorating the unidirectional bias and accumulated prediction error in existing models [23,26].
- Our work introduces an extension of discrete diffusion models for high-resolution 3D categorical voxel data. Specifically, we show the effectiveness of a diffusion model in terms of unconditional and conditional generation tasks, where the condition is a partial observation of a scene (i.e., SSC).
- Further, we propose a latent diffusion models for 3D categorical data to reduce the computation load caused by high-resolution segmentation maps.

### Diffusion Models for 3D Data
- Diffusion models have been used for 3D data
- Until recently, research has been mainly conducted for 3D point clouds with xyz-coordinates
- PVD [2] applies continuous diffusion on point-voxel representations for object shape generation and completion without additional shape encoders
- LION [3] uses latent diffusion for object shape completion (i.e., conditional generation) with additional shape encoders
- In this paper, we aim to learn 3D categorical data (i.e., 3D semantic segmentation maps) with a diffusion model
- The study of object generation has shown promising results, but as far as we know, our work is the first to generate a 3D scene with multiple objects using a diffusion model
- Concretely, our work explores discrete and latent diffusion models to learn a distribution of volumetric semantic scene segmentation maps

## Method
- Our goal is to learn a data distribution p(x)
- 3D segmentation maps are samples from the data distribution p(x), which is the categorical distribution Cat(k 0 , k 1 , • • • , k M ) with M +1 probabilities of the free label k 0 and M main categories.
- The discrete diffusion models could learn data distribution by recovering the noised data, which is destroyed through the successive transition of the label [8].
- Our method aims to learn a distribution of voxelized 3D segmentation maps with discrete diffusion (Sec. 3.1). Specifically, it includes unconditional and conditional generation, where the latter corresponds to the SSC task.

### Discrete Diffusion Models
- Forward process gradually adds noise to data
- In the forward process in discrete diffusion, an original segmentation map x 0 is gradually corrupted into a t-step noised segmentation map x t with 1 ≤ t ≤ T
- Each forward step can be defined by a Markov uniform transition matrix Q t [8]
- Based on the Markov property, we can derive the t-step noised segmentation map x t straight from the original segmentation map x 0 , q(x t |x 0 ), with a cumulative transition matrix
- In the reverse process parametrized by θ, a learnable model is used to reverse a noised segmentation map by p θ (x t−1 |x t )
- Specifically, we use a reparametrization trick [5] to make the model predict a denoised map x0 and subsequently get the reverse process p θ (x t−1 |x t )
- We optimize a joint loss that consists of the KL divergence of the forward process q(x t−1 |x t , x 0 ) from the reverse process p θ (x t−1 |x t ); of the original segmentation map q(x 0 ) from the reconstructed one p θ (x t−1 |x t ) for an auxiliary loss: where w 0 is an auxiliary loss weight.

### Latent Diffusion Models
- Latent diffusion models project the 3D segmentation maps into a smaller latent space and operate a diffusion process on the latent space instead of the highdimensional input space.
- A latent diffusion takes advantage of a lower training computational cost and a faster inference by processing diffusion on a lower dimensional space.
- To encode a 3D segmentation map into a latent representation, we use Vector Quantized Variational AutoEncoder (VQ-VAE) [27].
- VQ-VAE extends the VAE by adding a discrete learnable codebook E = {e n } N n=1 ∈ R N ×d , where N is the size of the codebook and d is the dimension of the codes.
- The encoder E encodes 3D segmentation maps x into a latent z = E(x), and the quantizer V Q(•) maps the latent z into a quantized latent z q , which is the closest codebook entry e n .
- Note that the latent z ∈ R h×w×z×d has a smaller spatial resolution than the segmentation map x.
- Then the decoder D reconstructs the 3D segmentation maps from the quantized latent, x = D(V Q(E(x))).

## Experiments
- In section 4.2, the authors empirically study the effectiveness of diffusion models on 3D voxel segmentation maps.
- In section 4.3, the authors study the conditional data distribution given a sparse occupancy map.

### Implementation Details
- The CarlaSC dataset is a synthetic outdoor driving dataset
- The dataset consists of 24 scenes in 8 dynamic maps
- The experiments are deployed on two NVIDIA GTX 3090 GPUs
- The diffusion models and VQ-VAE are trained using a batch size of 8 and 4 time steps, respectively
- The hyper-parameters of the diffusion models and VQ-VAE are set with T = 100 timesteps
- The encoder-decoder structure in Cylinder3D is used for time embedding and discreteness of the data
- The encoder-decoder structure in Cylinder3D with the vector quantizer module is used for VQ-VAE

### 3D Segmentation Maps Generation
- We use the discrete and the latent diffusion models for 3D segmentation map generation
- Fig. 3 shows the qualitative results of the generation
- As seen in the figure, both the discrete and latent models learn the categorical distribution as they produce a variety of reasonable scenes
- Note that our models are learned on a large-scale data distribution like the 3D scene with multiple objects; this is worth noting since recent 3D diffusion models for point clouds have been performed on an object scale
- In Tab. 2, the quantitative comparison is shown in Tab. 2
- The bigger the codebook size is, the higher the performance is, but it saturates around 1,100
- That is because most of the codes are not updated, and the update of the codebook can lapse into a local optimum

## Methods
- The IoU mIoU LMSCNet SS [16] has an 85.98% similarity score to the SSCNet Full [17]
- The MotionSC (T=1) [9] has an 86.5% similarity score to the SSCNet Full

## Conclusion
- In this work, we demonstrate the extension of the diffusion model to scene-scale 3D categorical data beyond generating a single object.
- We empirically show that our models have impressive generative power to craft various scenes through a discrete and latent diffusion process.
- Additionally, our method provides an alternative view for the SSC task, showing superior performance compared to a discriminative counterpart.
- We believe that our work can be a useful road map for generating 3D data with a diffusion model.
