---
title: "(QA)$^2$: Question Answering with Questionable Assumptions"
date: 2022-12-20T05:25:12.000Z
author: "Najoung Kim, Phu Mon Htut, Samuel R. Bowman, Jackson Petty"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10003v1.webp" # image path/url
    alt: "(QA)$^2$: Question Answering with Questionable Assumptions" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10003)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10003).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/qa-2-question-answering-with-questionable).

# Abstract
- Naturally-occurring information-seeking questions often contain questionable assumptions - Questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers to information-seeking questions.
- For instance, the question "When did Marie Curie discover Uranium?" cannot be answered as a typical when question without addressing the false assumption "Marie Curie discovered Uranium".

# Paper Content

## Introduction
- Questionable assumptions are false or unverifiable assumptions in information-seeking questions
- Questionable assumptions pose a unique challenge to QA because the type of answer requested by the form of the question is often the wrong answer
- To address the false or unverifiable assumption in a question, the question must be addressed in some way

## Questions with Questionable Assumptions
- Questions contain questionable assumptions.
- Questionable assumptions are false or unverifiable assumptions that are likely to be believed by the speaker.
- Assumptions under this definition relate to the epistemic bias of the speaker.

### Data Collection and Annotation

### Dataset Statistics
- The final dataset contains 602 annotated questions
- Half of which contain questions without any questionable assumptions (valid), and half of which contain questionable assumptions
- Out of the 301 questionable assumptions, 251 were false and 50 were unverifiable
- 32 questions (16 with questionable assumptions, 16 valid) are set aside as an adaptation set
- We deliberately keep the size of the adaptation set small because we intend (QA) 2 to serve primarily as a diagnostic evaluation set. Questions in (QA) 2 are annotated with fields containing the following information: the abstractive answer, extractive evidence that supports the abstractive answer, URL(s) of the extractive evidence, whether it contains any questionable assumptions, what the questionable assumption(s) is/are if any, and whether the status of the questionable assumption is temporally dependent.

### Discussion
- We observe that questions with questionable assumptions in the wild often do not stand out as bad questions immediately, as do examples like Which linguist invented the lightbulb? (Kim et al., 2021) and examples highlighted on social media such as Which rocket launched the Statue of Liberty into the ocean? (Munroe, 2020).
- Instead, it is often the case that detectability of questionable assumptions is dependent on narrower domains of factual knowledge. For instance, to recognize that How did Daisy kill her mother in Barefoot? contains a false assumption that Daisy killed her mother in Barefoot, one needs to know the plot of the movie-such knowledge is likely to be known by a much narrower audience compared to the aforementioned cases.

## Evaluation
- In addition to end-to-end QA, we propose two simpler subtasks that are in binary classification format-questionable assumption detection and verification-to investigate where major challenges may lie.
- These binary classification tasks can easily be posed in question-answering format, which then can be subject to zero-shot evaluation using the QA model being tested, or evaluated in a few-shot or in-context learning setup using the adaptation set.
- Since we used human rater judgments to evaluate model answers on end-to-end abstractive QA (more details in Section 4.1), these binary classification tasks can also serve as automated metrics that are easier to employ during model development.

### End-to-End Abstractive QA
- The first set of evaluation is a full end-to-end abstractive QA, where the input is the question and the expected output is an adequate answer to the question.
- Since there are many valid ways to address questionable assumptions (e.g., pointing out the problematic assumption, repairing the assumption and answering the repaired question, asking a clarification question...), automatic metrics that compare against a single gold answer may not be the best quantitative evaluation.
- Hence, we used judgments from human raters for human evaluation-specifically, we ask if a modelgenerated answer is an acceptable answer to the question, when the gold annotated answer is provided to the raters.
- We randomly sampled 100 questions from the evaluation set for this rating, and recruited five raters to provide judgments on the acceptability of the model generated answers.
- Then, we took the majority voted option (acceptable or unacceptable) as the final aggregated judgment, and reported the % acceptable as the final performance.
- The raters were recruited on Prolific.

### Questionable Assumption Detection
- The task of detecting a questionable assumption can be posed in question-answering format
- The task can be tailored to the model being evaluated
- The task can be posed in a variety of ways

### Questionable Assumption Verification
- The task of decomposing a problem into smaller pieces in order to find assumptions
- The task of identifying assumptions of a question
- The task of asking if a model is capable of answering a factual Yes/No question correctly
- Annotating questions with a gold answer to indicate if the question is valid

## Experiments

### Models
- We applied evaluations described in Section 4 to a range of models, including models specialized for QA and general-purpose language models that have shown competitive performance on QA.
- We experimented with several evaluation settings including zero-shot (E2E, detection, verification), in-context learning (E2E, detection, verification) and few-shot tuning (detection, verification).
- While our dataset contains up-to-date information as of December 2022, the models tested here have been trained on data with varying endpoints ranging between December 2018 and June 2021.
- For this reason, we expect all models to be insensitive to any factual updates after June 2021.
- We performed a zero-shot evaluation of two opendomain QA models: Macaw (Tafjord and Clark, 2021) and REALM (Guu et al., 2020), on our three evaluation tasks.
- Macaw is a generative QA model built on UnifiedQA (Khashabi et al., 2020) that can flexibly adapt to a variety of QA formats. One advantage of Macaw is the ability to optionally generate explanations-we specify an explanation slot in end-to-end QA to investigate whether the model explanations can provide satisfactory solutions to questionable assumptions.
- REALM is a language augmented with a Wikipedia-based knowledge retriever, which then can be finetuned on QA datasets. We expect models with a retrieval component to have an advantage in the assumption verification task (Section 4.3), since the verification step relies on being able to accurately recall and deploy factual knowledge.
- We used a publicly available checkpoint of REALM finetuned on an open-domain adaptation of NQ proposed in Lee et al. (2019).
- We furthermore evaluated general-purpose language models that have shown competitive performance on QA through prompt-based approaches. Specifically, we tested GPT-3 (Brown et al., 2020, davinci, text-davinci-002, text-davinci-003), T0 (Sanh et al., 2022), and Flan-T5 (Chung et al., 2022) in a zero-shot setting.
- Flan-T5 and the best-performing GPT-3 model were also evaluated in an in-context setting. Additionally, we tested T-Few (Liu et al., 2022), a parameter-efficient few-shot adaptation method applied to T0.
- Step-by-Step Prompting with Task Decomposition In addition to in-context demonstrations that show the models examples of end-to-end QA with questionable assumptions, we develop an additional prompting strategy that combines Let's think step by step (Kojima et al., 2022) and task decomposition (Khot et al., 2022) and applied it to the best performing model in the in-context setting.

## Results and Discussion
- Overall, end-to-end QA is quite challenging
- text-davinci-003 is the best performing model, but it's not perfect
- models generally underperform when questions have questionable assumptions

### Subtasks
- Most models performed around chance on the detection task
- The best performing model was GPT-3 text-davinci-003 using step-by-step task decomposition prompting with 57% accuracy
- Verification performance was generally higher compared to detection performance

### Error Analysis
- 59% of the time, the computer produced a factually wrong answer.
- 27% of the time, the computer didn't produce an answer.
- 10% of the time, the computer produced an uninformative or underinformative answer.

## Related Work
- Questions with questionable assumptions have been a part of benchmark QA datasets for some time
- More recently, Kim et al. (2021) have pointed out that treating questions with questionable assumptions under the banner of unanswerable is unsatisfactory
- They have shown that even under a closed-book setup, assumption verification is a challenging task
- We extend their analysis to open-domain settings and come to several similar conclusions: QA with questionable assumptions is hard, and even when the difficulty of identifying assumptions is removed, factual verification itself remains challenging
- We view (QA) 2 as a robustness check for questionable assumptions for QA models, in the spirit of Ribeiro et al. (2020)
- The closest work in terms of motivation is the challenge set of Tafjord and Clark (2021), in that we intend (QA) 2 to be used as an evaluation-only dataset (with minimal adaptation if necessary)
- The challenge set of Tafjord and Clark ( 2021) is also a small-scale, evaluationfocused open-domain QA dataset like ours, but we substantially extend the coverage specifically for questions with questionable assumptions, given that their challenge set only contains 3 examples of presupposition failure

## Conclusion
