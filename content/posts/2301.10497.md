---
title: "E(n)-equivariant Graph Neural Cellular Automata"
date: 2023-01-25T10:17:07.000Z
author: "Gennaro Gala, Daniele Grattarola, Erik Quaeghebeur"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-10497v1.webp" # image path/url
    alt: "E(n)-equivariant Graph Neural Cellular Automata" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.10497)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.10497).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/e-n-equivariant-graph-neural-cellular).

# Abstract
- Cellular automata (CAs) are computational models based on local interactions of cells
- Graph CAs (GCAs) are similar to Graph Neural Networks (GNNs)
- Graph Neural CAs (GNCAs) can be trained to approximate the transition rule of any GCA
- Existing GNCAs are anisotropic, meaning they don't treat related instances the same
- E(n)-GNCAs are proposed as a class of isotropic automata that are lightweight and can handle large graphs
- E(n)-GNCAs are successfully applied to three tasks: pattern formation, graph auto-encoding, and simulation of E(n)-equivariant dynamical systems

# Paper Content

## Introduction
- Collective intelligence is a growing area of machine learning research
- Collective intelligence is inspired by real biological systems
- Cellular automata are used to study collective intelligence and morphogenesis
- Neural nets have been used to learn and design CA rules
- Neural Cellular Automata have been applied for self-organizing systems, image generation and classification, and reinforcement learning
- Graph Neural Networks are universal engines for learning transition rules
- E(n)-GNCAs respect isometries in the state space
- E(n)-GNCAs are showcased on three different tasks: pattern formation, graph auto-encoding, and simulation of E(n)-equivariant dynamical systems

## Preliminaries and related work
- A graph consists of nodes and edges
- An adjacency matrix can be used to define a graph
- Each node can have a state, such as location, velocity, and features
- Each edge can have an attribute

### Graph (neural) cellular automata
- Graph Cellular Automata (GCA) is a triple (G, S, τ ) where G is a graph and S is a state space.
- τ is a local transition rule to update the state of each node.
- Anisotropy and Isotropy are important properties for CAs.
- Neural Cellular Automata (NCA) is a neural net with parameters θ representing a transition rule.
- NCAs can be viewed as Recurrent Neural Networks (RNNs).
- RNNs and CAs are known to be Turing complete.

### Graph neural networks
- GNNs are used for representation learning on graphs
- GNNs use a message-passing scheme to update node features
- Message-passing layers are composed of a message function, a permutation-invariant operation, and a node update function
- All operators must be differentiable to allow for end-to-end optimization with stochastic gradient descent

### E(n)-equivariant graph neural networks
- EGNNs are a class of GNNs designed to be permutation equivariant and translation, rotation and reflection equivariant.
- EGNNs use node coordinates, node features and optional edge attributes.
- EGNNs use a sequence of EGCs to process the data.
- EGNNs can use attention weights and velocities.

## E(n)-equivariant graph neural cas
- Our work builds on the connection between isotropic GCAs and EGNNs.
- We consider a setting in which a parametrised transition rule is implemented with a single E(n)-equivariant Graph Convolution (EGC).
- A layered EGNN is also a possible and viable approach to modeling transition rules.
- We introduce E(n)-Equivariant Graph Neural Cellular Automata, E(n)-GNCAs for short.
- The overall state configuration S of an E(n)-GNCA is defined as S = [X, H], or S = [X, H, V] when velocity is available.
- The transition rules we consider are 1-step Markovian.
- The map ψ is a rigid transformation (aka isometry).
- Output coordinates X and output node features H are respectively E(n)-equivariant and E(n)-invariant to rigid transformations of input coordinates X.
- Global propagation of information arises from spatially localized interactions of nodes.

## Experiments
- Showcased broad and successful applicability of E(n)-GNCAs in 3 tasks: pattern formation, graph autoencoding, and simulation of E(n)-Equivariant dynamical systems
- Used h = 16 (hidden state dimension) and m = 32 (message dimension) throughout experiments, resulting in 5K parameters regardless of coordinate dimension n
- Code available at github.com/gengala/egnca
- Used PyTorch, PyTorch Geometric, and PyTorch Lightning software packages

### Pattern formation
- Inspired by prior work on CA morphogenesis
- Show how E(n)-GNCAs can be trained to converge to a given fixed target state
- Target is a sparse geometric graph G that visually defines a recognisable 2D or 3D shape
- Optimise the model by minimising the MSE between coordinates reached by the model and target coordinates
- Minimise the MSE between nodes' distance in the model's final configuration and the target one
- E(n)-invariant objective provides a weaker supervision signal
- Optimizing for Equation 13 results in learning a transition rule τ θ such that [X , H ] = τ t θ ([X, 1]) and X = ψ( X) for any arbitrary rigid transformation ψ(•)
- Use pool as a replay memory and perturb point clouds in the batch to promote regeneration
- Evaluate the quality of the learned transition rule using sample entropy (SE) and correlation dimension (CD)

### Graph autoencoding with cellular automata
- E(n)-GNCAs can be used as Graph AutoEncoders
- Task is to learn node representations to reconstruct adjacency matrices
- Five datasets of featureless graphs used
- Pool of states created for each training graph
- Model trained by minimizing binary cross-entropy
- 3D demo experiment to show persistent autoencoding, conditional generation of 3D point clouds and graph drawing abilities
- E(n)-GNCAs can scale to higher Euclidean spaces and larger graphs
- Compared to layered EGNNs as a suitable baseline
- E(n)-GNCAs are multi-target

### Simulation of e(n)-equivariant dynamical system
- E(n)-GNCAs can be used to simulate E(n)-equivariant dynamical systems
- The Boids Algorithm is a 1-step Markovian and distributed multi-agent system
- The underlying graph G changes dynamically through time
- The dynamical system can be formulated as a GCA
- A dataset of 500 trajectories was created using the ground-truth simulator
- Attention weights and equations 9 and 10 were used to explicitly account for velocities
- The MSE of the estimated velocities was optimized with the ground truth ones

## Discussion
