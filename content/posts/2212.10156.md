---
title: "Goal-oriented Autonomous Driving"
date: 2022-12-20T10:47:53.000Z
author: "Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10156v1.webp" # image path/url
    alt: "Goal-oriented Autonomous Driving" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10156)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10156).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/goal-oriented-autonomous-driving).

# Abstract
- Modern autonomous driving system is characterized as modular tasks in sequential order,
- As sensors and hardware get improved, there is trending popularity to devise a system that can perform a wide diversity of tasks to fulfill higher-level intelligence
- Contemporary approaches resort to either deploying standalone models for individual tasks, or designing a multi-task paradigm with separate heads. These might suffer from accumulative error or negative transfer effect. Instead, we argue that a favorable algorithm framework should be devised and optimized in pursuit of the ultimate goal, i.e. planning of the self-driving-car. Oriented at this goal, we revisit the key components within perception and prediction.
- We analyze each module and prioritize the tasks hierarchically, such that all these tasks contribute to planning (the goal). To this end, we introduce UniAD, the first comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective.
- Tasks are communicated with unified query design to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven to surpass previous state-of-the-arts by a large margin in all aspects.

# Paper Content

## Introduction
- With the successful development of deep learning, autonomous driving algorithms are assembled with a series of tasks, including detection, tracking, mapping in perception; and motion and occupancy forecast in prediction.
- The multi-task learning scheme shares a backbone with multiple heads.
- Previous attempts either adopt a direct optimization on planning in (c.1) or devise the system with partial components in (c.2).
- Instead, we argue in (c.3) that a desirable system should be goal-oriented as well as to prioritize tasks in a hierarchical manner -facilitating the ultimate target of planning.
- As depicted in Fig. 1(a), most industry solutions deploy standalone models for each task independently [59,62], as long as the resource bandwidth of on-board chip allows.
- Although such a design simplifies the R&D difficulty across teams, it bares the risk of information loss, across modules, error accumulation and feature misalignment due to the isolation of optimization targets [50,57,72].
- A more elegant design is to incorporate a wide span of tasks into a multi-task learning (MTL) paradigm as shown in Fig. 1(b).
- This is a popular practice in many domains, including general vision [69,81,95], autonomous driv- NMP [88] NEAT [19] BEVerse [92] (c.1) [14,16,68,85] (c.2) PnPNet † [50] ViP3D † [30] P3 [72] MP3 [11] ST-P3 [37] LAV [15] (c.3) UniAD (ours)
- The key component to hit the ground running is the unified query design across nodes. As such, UniAD enjoys flexible representations to exchange knowledge from a global perspective.
- We instantiate UniAD on the challenging benchmark for realistic scenarios. Through extensive ablations, we verify the superiority of our method over all previous state-of-the-arts.
- We hope this work would shed some light on the targetdriven design for the autonomous driving system, providing a starting point for coordinating various tasks.

## Methodology
- UniAD is comprised of four transformer decoder-based perception and prediction modules
- Each module is goal oriented, leveraging the benefits of joint optimization from preceding nodes
- The pipeline is pipelined via Transformer structure, with the unified query as veins running in the loop
- Map over occupancy is for visual purpose only

### Perception: Tracking and Mapping
- TrackFormer performs joint detection and multiobject tracking without non-differentiable postprocessing
- Inspired by [87,91], we take a similar query design
- Besides the conventional detection queries utilized in object detection, additional track queries are introduced to model the life cycle of tracked agents throughout the scene
- At each time step, initialized detection queries detect newborn agents while stored track queries keep capturing information from previously observed ones
- Both detection queries and track queries enhance themselves by attending to BEV feature B
- As the scene is updated in a sequence, track queries are also continuously updated by an attention module until the agent disappears completely (untracked in a certain time period)
- In this way, the track query fully aggregates the temporal in-formation of the corresponding agent
- Similar to [8], Track-Former contains N layers and the final output query Q A provides rich historical knowledge of N a valid agents for downstream prediction tasks
- Note that one particular egovehicle query is included in the query set to explicitly model the self-driving vehicle itself for future usage, which is always assigned to the ground truth of ego vehicle and not involved in bipartite matching during training

### Prediction: Motion Forecasting
- Most recent researchers have proven the effectiveness of transformer structure on the motion task
- Inspired by which we propose MotionFormer under end-toend scenarios
- With information-enriched queries for dynamic agents Q A and static map Q M from TrackFormer and MapFormer respectively, it predicts all agents' multimodal future movements (several possible trajectories) in a scene-centric manner
- This paradigm produces multiagent trajectories in the global frame in a single pass, which greatly saves the computational cost of aligning the whole scene to each agent's coordinate
- Meanwhile, in order to preserve the ego identity in the scene-level setting and make the ego feature engage in social interactions with other agents, we have an ego-vehicle query from Track-Former passing through the motion node
- Formally, the output is X = {x i,k ∈ R T×2 |i = 1, . . . , N a ; k = 1, . . . , K} where i indexes the agent, k indexes the modality and T is the length of prediction horizon
- MotionFormer is composed of N layers and built via the cross-attention mechanism
- In each layer, we aim at capturing three types of interactions: agent-agent, agentmap and agent-goal point
- For each motion query Q i,k (defined later, and we omit subscripts i, k of motion queries for simplicity), its interactions between other agents Q a or map elements Q m could be formulated as:
- In order to forecast the intended position as well, it is essential for the task to focus on the intended position
- Benefiting from environmental dynamics in BEV, we devise an agent-goal point attention module via deformable attention [96]
- where xl−1 T is the endpoint of the prediction from previous layer, a deformable attention module DeformAttn(q,r,x) takes in the query q, reference point r and dense feature x
- By sparsely sampling the encoded BEV feature near the agent's goal point, we refine the predicted trajectory with its surrounding information
- All three interactions are finally fused via concatenation and a multi-layer perceptron (MLP), resulting in query context Q ctx which is sent to the proceeding layer or decoded as prediction results
- Motion queries are comprised of two components: the query context Q ctx produced by the last layer and the query position Q pos
- Since the scenecentric paradigm brings extra degrees of freedom and uncertainty which lifts the challenge for prediction, it is crucial to encode the positional information into motion queries
- Specifically, Q pos captures the positional knowledge in fourfolds as Eq. ( 3): (1) the position of scene-level anchors I s ; (2) the position of agent-level anchors I a ; (3) current location of agent i and (4) the predicted goal point
- The scene-level anchor represents prior movement statistics in a global view, while the agent-level anchor captures the possible intention in local coordinate
- They both are clustered by k-means algorithm on the endpoints of ground-truth trajectories, to narrow down the uncertainty of prediction
- Non-linear Optimization is used to optimize the target trajectories
- Brutally regressing the ground-truth trajectories from an imperfect detection position or heading angle may cause unrealistic results with large curvature and acceleration which are hard for vehicles to follow
- To tackle this, we adopt a non-linear smoother [7] to optimize the target trajectories

### Prediction: Occupancy Prediction
- Occupancy grid map is a discretized BEV representation where each cell holds a belief indicating whether it is occupied, and occupancy prediction task is designed to discover how the grid map changes in the future.
- Previous approaches predict results with the help of RNNs for temporally expanding future predictions from observed BEV features [34,37,92]. However, they generate per-agent occupancy masks through highly hand-crafted clustering postprocessing.
- To address this, we present OccFormer to incorporate both scene-level and agent-level semantics in two aspects: (1) a dense scene feature acquires agent-level features via an exquisitely designed attention module when unrolling to future horizons; (2) we produce instance-wise occupancy easily by a matrix multiplication between agent-level features and dense scene features without heavy post-processing.
- Specifically, OccFormer is composed of T o sequential blocks where T o indicates the prediction horizon and is typically smaller than T in the motion task due to the high computational cost of densely represented occupancy.
- Each block takes the informed agent features G t and the state (dense feature) F t−1 from previous layer as input, and generates F t for timestep t considering both instance-and scene-level information.
- To produce agent feature G t with dynamics and spatial priors, motion queries from Motion-Former are max-pooled in the modality dimension to get a more compact representation denoted as Q X ∈ R Na×D , with D as the feature dimension. Then we fuse the upstream track query Q A , current position embedding P A and Q X together via a temporal-specific MLP: where [•] denotes concatenation.
- For the scene-level knowledge, the BEV feature B is downscaled to 1 /4 resolution for training efficiency to serve as the first block input F 0 .
- To further reduce the computational cost, each block follows a downsample-upsample manner with an attention module in between (described below), conducting pixel-agent interaction at 1 /8-scaled feature F t ds .
- Pixel-agent interaction. It is designed to unify the sceneand agent-level understanding when we predict future occupancy.
- Instead of taking instance-level embeddings as queries and attending to dense features in conventional detection or segmentation transformers, we employ the dense scene feature F t ds as queries, instance-level features as keys and values to update the dense feature over time.
- Detailedly, We perform self-attention with F t ds to learn a long-range per-pixel interaction, followed by a cross-attention with the agent feature G t . Moreover, to encourage the pixel-agent correspondence, we constrain the cross-attention by an attention mask, which restricts each pixel to only look at the agent occupying it at timestep t, inspired by [17].
- The update process of the dense feature is formulated as:
- As the attention mask O t m is semantically the same with occupancy, it is also generated via a matrix multiplication between an additional agent-level feature denoted as mask feature M t = MLP(G t ) and the dense feature F t ds .
- After the interaction process in Eq. ( 7), D t ds is upsampled to 1 /4 size of original BEV.
- We further add D t ds with a residual connection from the input F t−1 for training stability, and the resultant feature F t is passed to the next block.

### Planning
- Planning without HD maps or predefined routes usually requires a high-level command to indicate the direction to go
- We follow this spirit and divide the rough navigation signal into three bins, i.e., turn left, turn right and keep forward.
- Note that the ego-vehicle query from Mo-tionFormer already encodes ego-vehicle's feasible behaviors, thus in the planning stage, we further endow it with the command intention, leading to an enhanced "plan query".
- It predicts the final trajectory τ by attending to BEV features again for road and traffic information.
- To further ensure safety, we introduce an optimization strategy based on Newton's method during inference: where τ * denotes the ultimate plan, Ô is a classical instance-agnostic occupancy map generated by merging all agents' predictions, and the target function is calculated by: Here the l 2 cost pulls the trajectory toward the original planned goal while the collision term D pushes it away from obstacles on the occupancy map, which is modeled as Gaussian distributions for positions around each waypoint τ t .

### Joint Learning
- UniAD is trained in two stages
- In the first stage, jointly training the tracking and mapping modules is done
- In the second stage, the model is optimized end-to-end
- The two stage training is found to be more stable
- The shared matching policy is adopted in the tracking and online mapping stage
- For the tracking task, candidates from newborn queries are paired with ground truth objects through bipartite matching
- Predictions from track queries inherit the assigned ground truth index from previous frames
- The matching results in the tracking module are further utilized in motion and occupancy tasks

## Experiments
- We conduct experiments on the challenging nuScenes benchmark.
- Due to space limit, some ablative experiments, visualizations, the full suite of details on metrics and protocols are provided in the Supplementary.

### Main Results
- We compare with prior state-of-the-arts, both modularized and end-to-end based approaches.
- The main metric for each task is marked in gray background in Tables.
- As for multi-object tracking in Table 2, UniAD yields a significant improvement of +6.5 and +14.2 AMOTA(%) compared to MUTR3D [91] and a prediction-target method ViP3D [30] respectively.
- Moreover, UniAD achieves the lowest ID switch score showing the temporal consistency for each tracklet.
- For online mapping in Table 3, UniAD shows excellent ability to segment road elements, especially in lanes (+7.4 IoU(%) compared to BEVFormer) which are crucial for downstream agentroad interaction in motion forecasting.
- Note that our tracking is slightly inferior to state-of-the-art Immortal Tracker, which is designed by tracking-by-detection paradigm; and the performance on some semantic classes in mapping falls behind c.f. some perception-oriented works.
- We argue that UniAD is goal-oriented to benefit final planning -not targeting at top-1 performance within single task.
- Prediction results. Motion forecasting results are shown in Table 4, where UniAD remarkably outperforms previous vision-based end-to-end methods and reduces errors by 38.3% and 65.4% on minADE compared to PnPNetvision [50] and ViP3D [30] respectively.
- In terms of occupancy prediction reported in Table 5, UniAD gets notable advances in nearby areas, yielding +4.0 and +2.0 on IoUnear(%) compared to FIERY [34] and BEVerse [92] with heavy augmentations, respectively.
- Planning results. Benefiting from rich spatial-temporal information in both the ego-vehicle query and occupancy, UniAD reduces planning L2 error and collision rate by 51.2% and 56.3% compared to ST-P3 [37], in terms of the average value for the planning horizon.
- Moreover, it notably outperforms several LiDAR-based counterparts, which is often deemed challenging for perception tasks.

### Ablation Study
- To validate our goal-oriented design philosophy, we conduct extensive ablations as shown in Table 7 to prove the effectiveness and necessity of preceding modules.
- To achieve the ultimate goal of planning, we analyze the two types of prediction tasks in our framework. In Exp.10-12, only when motion forecasting and occupancy prediction modules are introduced simultaneously (Exp.12), both metrics of the planning L2 and collision rate achieve the best result, compared to naive end-to-end planning (Exp.10, Fig. 1(c.1)). Thus we conclude that both these two prediction tasks are required for a safe planning objective.
- Taking a step back, in Exp.7-9, we show the cooperative effect of our two designated types of prediction. The performance of both tasks boosts when they are closely integrated (Exp.9, -3.5% minADE, -5.8% minFDE, -1.3 MR(%), +2.4 IoU-f., +2.4 VPQ-f.), which demonstrates the necessity to unify these agent-and scene-level representations.
- Meanwhile, in order to realize a superior motion forecasting performance, we explore how perception modules could contribute in Exp.4-6. Notably, incorporating both tracking and mapping nodes brings remarkable improvement to forecasting results (-9.7% minADE, -12.9% minFDE, -2.3 MR(%)).
- We also present Exp.1-3, which indicate training perception sub-tasks together leads to comparable results to a single task. Additionally, compared with naive multi-task learning (Exp.0, Fig. 1(b)), our hierarchical design (Exp.12) outperforms it by a significant margin in all essential metrics (-15.2% minADE, -17.0% minFDE, -3.2 MR(%)), +4.9 IoUf., +5.9 VPQ-f., -0.15m avg.L2, -0.51 avg.Col.(%)), showing the superiority of our goal-oriented hierarchical design.

## Conclusion and Future Work
- We propose a hierarchical, goal-oriented pipeline for autonomous driving.
- We provide detailed analysis on the necessity of each module within perception and prediction.
- To unify tasks, a query-based design is proposed to connect all nodes in UniAD, benefiting from richer representations for agent interaction in the environment.
- Extensive experiments verify the proposed method in all aspects.
- Limitations and future work are discussed.
