---
title: "Goal-oriented Autonomous Driving"
date: 2022-12-20T10:47:53.000Z
author: "Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2212-10156v1_DA5GoHrgY.jpg" # image path/url
    alt: "Goal-oriented Autonomous Driving" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10156)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10156).


# Abstract
- Modern autonomous driving system is characterized as modular tasks in sequential order
- As sensors and hardware get improved, there is trending popularity to devise a system that can perform a wide diversity of tasks
- Contemporary approaches resort to either deploying standalone models for individual tasks, or designing a multi-task paradigm with separate heads
- These might suffer from accumulative error or negative transfer effect. Instead, we argue that a favorable algorithm framework should be devised and optimized in pursuit of the ultimate goal, i.e. planning of the self-driving-car. Oriented at this goal, we revisit the key components within perception and prediction.
- We analyze each module and prioritize the tasks hierarchically, such that all these tasks contribute to planning (the goal). To this end, we introduce UniAD, the first comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective.
- Tasks are communicated with unified query design to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven to surpass previous state-of-the-arts by a large margin in all aspects.

# Paper Content

## Introduction
- With the successful development of deep learning, autonomous driving algorithms are assembled with a series of tasks, including detection, tracking, mapping in perception; and motion and occupancy forecast in prediction.
- The multi-task learning scheme shares a backbone with multiple heads.
- Previous attempts either adopt a direct optimization on planning in (c.1) or devise the system with partial components in (c.2).
- Instead, we argue in (c.3) that a desirable system should be goal-oriented as well as to prioritize tasks in a hierarchical manner -facilitating the ultimate target of planning.
- As depicted in Fig. 1(a), most industry solutions deploy standalone models for each task independently [59,62], as long as the resource bandwidth of on-board chip allows.
- Although such a design simplifies the R&D difficulty across teams, it bares the risk of information loss, across modules, error accumulation and feature misalignment due to the isolation of optimization targets [50,57,72].
- A more elegant design is to incorporate a wide span of tasks into a multi-task learning (MTL) paradigm as shown in Fig. 1(b).
- This is a popular practice in many domains, including general vision [69,81,95], autonomous driv- NMP [88] NEAT [19] BEVerse [92] (c.1) [14,16,68,85] (c.2) PnPNet † [50] ViP3D † [30] P3 [72] MP3 [11] ST-P3 [37] LAV [15] (c.3) UniAD (ours)
- The key component to hit the ground running is the unified query design across nodes. As such, UniAD enjoys flexible representations to exchange knowledge from a global perspective.
- We instantiate UniAD on the challenging benchmark for realistic scenarios. Through extensive ablations, we verify the superiority of our method over all previous state-of-the-arts.
- We hope this work would shed some light on the targetdriven design for the autonomous driving system, providing a starting point for coordinating various tasks.

## Methodology
- The UniAD pipeline is comprised of four transformer decoder-based perception and prediction modules
- Each module is goal oriented, leveraging the benefits of joint optimization from preceding nodes
- The map over occupancy is for visual purpose only
- The track queries inquire about the agents' information from BEV feature B to detect newborn objects and track previously existed ones
- MapFormer updates map query features targeting to make panoptic segmentation for road elements such as lanes and dividers
- With valid agent features and map features, MotionFormer captures interactions among agents and maps and forecasts per-agent future trajectory
- Since the action of agents in the scene can significantly impact others, this module produces scene-centric joint predictions
- OccFormer employs the BEV feature as queries, agent-level features as keys and values to explore future scene representations and predict occupancy grid maps with agent identity preserved
- Planner ultimately decodes the enriched ego-vehicle query from MotionFormer into the planning result and keeps itself away from occupied regions predicted by OccFormer

### Perception: Tracking and Mapping
- TrackFormer performs joint detection and multiobject tracking without non-differentiable postprocessing
- Inspired by [87,91], we take a similar query design
- Besides the conventional detection queries utilized in object detection, additional track queries are introduced
- At each time step, initialized detection queries detect newborn agents while stored track queries keep capturing information from previously observed ones
- Both detection queries and track queries enhance themselves by attending to BEV feature B
- As the scene is updated in a sequence, track queries are also continuously updated by an attention module until the agent disappears completely (untracked in a certain time period)
- In this way, the track query fully aggregates the temporal in-formation of the corresponding agent
- Similar to [8], Track-Former contains N layers and the final output query Q A provides rich historical knowledge of N a valid agents for downstream prediction tasks
- Note that one particular egovehicle query is included in the query set to explicitly model the self-driving vehicle itself for future usage, which is always assigned to the ground truth of ego vehicle and not involved in bipartite matching during training

### Prediction: Motion Forecasting
- Most recent researchers have proven the effectiveness of transformer structure on the motion task [55,60,61,74,86],
- inspired by which we propose MotionFormer under end-to-end scenarios.
- With information-enriched queries for dynamic agents Q A and static map Q M from TrackFormer and MapFormer respectively, it predicts all agents' multimodal future movements (several possible trajectories) in a scene-centric manner.
- This paradigm produces multiagent trajectories in the global frame in a single pass, which greatly saves the computational cost of aligning the whole scene to each agent's coordinate [44].
- Meanwhile, in order to preserve the ego identity in the scene-level setting and make the ego feature engage in social interactions with other agents, we have an ego-vehicle query from Track-Former passing through the motion node.
- Formally, the output is X = {x i,k ∈ R T×2 |i = 1, . . . , N a ; k = 1, . . . , K}, where i indexes the agent, k indexes the modality and T is the length of prediction horizon.
- MotionFormer. It is composed of N layers and built via the cross-attention mechanism. In each layer, we aim at capturing three types of interactions: agent-agent, agentmap and agent-goal point.
- For each motion query Q i,k (defined later, and we omit subscripts i, k of motion queries for simplicity), its interactions between other agents Q a or map elements Q m could be formulated as:
- where MHCA, MHSA represents multi-head cross-attention and multi-head self-attention [80] respectively.
- In the meantime, it is essential for the forecasting task to focus on the intended position as well.
- Benefiting from environmental dynamics in BEV, we devise an agent-goal point attention module via deformable attention [96] as follows:
- where xl−1 T is the endpoint of the prediction from previous layer, a deformable attention module DeformAttn(q,r,x) takes in the query q, reference point r and dense feature x.
- By sparsely sampling the encoded BEV feature near the agent's goal point, we refine the predicted trajectory with its surrounding information.
- All three interactions are finally fused via concatenation and a multi-layer perceptron (MLP), resulting in query context Q ctx which is sent to the proceeding layer or decoded as prediction results.
- Motion queries. The input queries for each layer of Mo-tionFormer referred to as motion queries are comprised of two components: the query context Q ctx produced by the last layer and the query position Q pos .
- Since the scenecentric paradigm brings extra degrees of freedom and uncertainty which lifts the challenge for prediction, it is crucial to encode the positional information into motion queries. Specifically, Q pos captures the positional knowledge in fourfolds as Eq. ( 3):
- (1) the position of scene-level anchors I s ;
- (2) the position of agent-level anchors I a ;
- (3) current location of agent i and (4) the predicted goal point.
- Here the sinusoidal position encoding PE(•) followed by an MLP is utilized to encode the positional points and x0 T is set as I s at the first layer (subscripts i, k are also omitted).
- The scene-level anchor represents prior movement statistics in a global view, while the agent-level anchor captures the possible intention in local coordinate. They both are clustered by k-means algorithm on the endpoints of ground-truth trajectories, to narrow down the uncertainty of prediction.
- Unlike the prior knowledge, the start point provides customized positional embedding for each agent, and the predicted endpoint serves as a dynamic anchor optimized layerby-layer in a coarse-to-fine fashion.
- Non-linear Optimization. Unlike the conventional motion forecasting works which have access to ground truth perceptual results e.g. bounding boxes, we consider the prediction uncertainty from prior stages in our end-to-end paradigm.
- Brutally regressing the ground-truth trajectories from an imperfect detection position or heading angle may cause unrealistic results with large curvature and acceleration which are hard for vehicles to follow.
- To tackle this, we adopt a non-linear smoother [7] to optimize the target trajectories: x * = arg min where x and x * denote the original and optimized target trajectory, x is generated by multiple-shooting [3], and the cost function is as follows:...

### Prediction: Occupancy Prediction
- Occupancy grid map is a discretized representation of the environment where each cell holds a belief indicating whether it is occupied or not.
- The occupancy prediction task is designed to discover how the grid map changes in the future.
- Previous approaches predict results with the help of RNNs for temporally expanding future predictions from observed BEV features. However, they generate per-agent occupancy masks through highly hand-crafted clustering postprocessing.
- To address this, we present OccFormer to incorporate both scene-level and agent-level semantics in two aspects: (1) a dense scene feature acquires agent-level features via an exquisitely designed attention module when unrolling to future horizons; (2) we produce instance-wise occupancy easily by a matrix multiplication between agent-level features and dense scene features without heavy post-processing.
- Specifically, OccFormer is composed of T o sequential blocks where T o indicates the prediction horizon and is typically smaller than T in the motion task due to the high computational cost of densely represented occupancy.
- Each block takes the informed agent features G t and the state (dense feature) F t−1 from previous layer as input, and generates F t for timestep t considering both instance-and scene-level information.
- To produce agent feature G t with dynamics and spatial priors, motion queries from Motion-Former are max-pooled in the modality dimension to get a more compact representation denoted as Q X ∈ R Na×D , with D as the feature dimension. Then we fuse the upstream track query Q A , current position embedding P A and Q X together via a temporal-specific MLP:
- where [•] denotes concatenation.
- For the scene-level knowledge, the BEV feature B is downscaled to 1 /4 resolution for training efficiency to serve as the first block input F 0 .
- To further reduce the computational cost, each block follows a downsample-upsample manner with an attention module in between (described below), conducting pixel-agent interaction at 1 /8-scaled feature F t ds .
- Pixel-agent interaction. It is designed to unify the sceneand agent-level understanding when we predict future occupancy.
- Instead of taking instance-level embeddings as queries and attending to dense features in conventional detection or segmentation transformers, we employ the dense scene feature F t ds as queries, instance-level features as keys and values to update the dense feature over time.
- Detailedly, We perform self-attention with F t ds to learn a long-range per-pixel interaction, followed by a cross-attention with the agent feature G t . Moreover, to encourage the pixel-agent correspondence, we constrain the cross-attention by an attention mask, which restricts each pixel to only look at the agent occupying it at timestep t, inspired by [17].
- The update process of the dense feature is formulated as:
- As the attention mask O t m is semantically the same with occupancy, it is also generated via a matrix multiplication between an additional agent-level feature denoted as mask feature M t = MLP(G t ) and the dense feature F t ds .
- After the interaction process in Eq. ( 7), D t ds is upsampled to 1 /4 size of original BEV.
- We further add D t ds with a residual connection from the input F t−1 for training stability, and the resultant feature F t is passed to the next block.

### Planning
- Planning without HD maps or predefined routes usually requires a high-level command to indicate the direction to go
- We follow this spirit and divide the rough navigation signal into three bins, i.e., turn left, turn right and keep forward.
- Note that the ego-vehicle query from Mo-tionFormer already encodes ego-vehicle's feasible behaviors, thus in the planning stage, we further endow it with the command intention, leading to an enhanced "plan query".
- It predicts the final trajectory τ by attending to BEV features again for road and traffic information.
- To further ensure safety, we introduce an optimization strategy based on Newton's method during inference: where τ * denotes the ultimate plan, Ô is a classical instance-agnostic occupancy map generated by merging all agents' predictions, and the target function is calculated by:
- Here the l 2 cost pulls the trajectory toward the original planned goal while the collision term D pushes it away from obstacles on the occupancy map, which is modeled as Gaussian distributions for positions around each waypoint τ t.

### Joint Learning
- UniAD is trained in two stages
- In the first stage, UniAD jointly trains the tracking and mapping modules
- In the second stage, UniAD optimizes the model end-to-end
- The two stage training is found to be more stable
- UniAD is referred to the Supplementary for details of each loss

## Experiments
- We conduct experiments on the challenging nuScenes benchmark.
- Due to space limit, some ablative experiments, visualizations, the full suite of details on metrics and protocols are provided in the Supplementary.

### Main Results
- The main metric for each task is marked in gray background in Tables.
- For multi-object tracking in Table 2, UniAD yields a significant improvement of +6.5 and +14.2 AMOTA(%) compared to MUTR3D [91] and a prediction-target method ViP3D [30] respectively.
- Moreover, UniAD achieves the lowest ID switch score showing the temporal consistency for each tracklet.
- For online mapping in Table 3, UniAD shows excellent ability to segment road elements, especially in lanes (+7.4 IoU(%) compared to BEVFormer) which are crucial for downstream agentroad interaction in motion forecasting.
- Note that our tracking is slightly inferior to state-of-the-art Immortal Tracker, which is designed by tracking-by-detection paradigm; and the performance on some semantic classes in mapping falls behind c.f. some perception-oriented works.
- We argue that UniAD is goal-oriented to benefit final planning -not targeting at top-1 performance within single task.
- For motion forecasting results, UniAD remarkably outperforms previous vision-based end-to-end methods and reduces errors by 38.3% and 65.4% on minADE compared to PnPNetvision [50] and ViP3D [30] respectively.
- In terms of occupancy prediction reported in Table 5, UniAD gets notable advances in nearby areas, yielding +4.0 and +2.0 on IoUnear(%) compared to FIERY [34] and BEVerse [92] with heavy augmentations, respectively.
- Planning results. Benefiting from rich spatial-temporal information in both the ego-vehicle query and occupancy, UniAD reduces planning L2 error and collision rate by 51.2% and 56.3% compared to ST-P3 [37], in terms of the average value for the planning horizon.
- Moreover, it notably outperforms several LiDAR-based counterparts, which is often deemed challenging for perception tasks.

### Ablation Study
- To validate our goal-oriented design philosophy, we conduct extensive ablations
- To achieve the ultimate goal of planning, we analyze the two types of prediction tasks in our framework
- In Exp.10-12, only when motion forecasting and occupancy prediction modules are introduced simultaneously (Exp.12), both metrics of the planning L2 and collision rate achieve the best result, compared to naive end-to-end planning (Exp.10, Fig. 1(c.1))
- Thus we conclude that both these two prediction tasks are required for a safe planning objective
- Taking a step back, in Exp.7-9, we show the cooperative effect of our two designated types of prediction
- The performance of both tasks boosts when they are closely integrated (Exp.9, -3.5% minADE, -5.8% minFDE, -1.3 MR(%), +2.4 IoU-f., +2.4 VPQ-f.), which demonstrates the necessity to unify these agent-and scene-level representations
- Meanwhile, in order to realize a superior motion forecasting performance, we explore how perception modules could contribute in Exp.4-6. Notably, incorporating both tracking and mapping nodes brings remarkable improvement to forecasting results (-9.7% minADE, -12.9% minFDE, -2.3 MR(%))
- We also present Exp.1-3, which indicate training perception sub-tasks together leads to comparable results to a single task. Additionally, compared with naive multi-task learning (Exp.0, Fig. 1(b)), our hierarchical design (Exp.12) outperforms it by a significant margin in all essential metrics (-15.2% minADE, -17.0% minFDE, -3.2 MR(%)), +4.9 IoUf., +5.9 VPQ-f., -0.15m avg.L2, -0.51 avg.Col.(%)), showing the superiority of our goal-oriented hierarchical design

## Conclusion and Future Work
- We discuss the system-level design for the autonomous driving algorithm framework
- A hierarchical, goal-oriented pipeline is proposed toward the ultimate pursuit for planning, namely UniAD
- We provide detailed analysis on the necessity of each module within perception and prediction
- To unify tasks, a query-based design is proposed to connect all nodes in UniAD, benefiting from richer representations for agent interaction in the environment
- Extensive experiments verify the proposed method in all aspects
- Limitations and future work are discussed

## A. Task Definition
- Detects surrounding objects (coordinates, length, width, height, etc.) at each time stamp
- Tracks the correspondences between different objects across time stamps and associates them temporally (i.e., assigns a consistent track ID for each agent)
- In the paper, uses multi-object tracking in some cases to denote the detection and tracking process
- The final output is a series of associated 3D boxes in each frame, and their corresponding features Q A are forwarded to the motion module
- Additionally, note that we have one special query named ego-vehicle query for downstream tasks, which would not be included in the predictionground truth matching process and it regresses the location of ego-vehicle accordingly
- Online mapping
- Map intuitively embodies the geometric and semantic information of the environment
- In UniAD, models the online map into four categories: lanes, drivable area, dividers and pedestrian crossings, and segments them in the BEV map
- Similar to Q A , the map queries Q M would be further utilized in the motion forecasting module to model the agent-map interaction
- Motion forecasting
- Bridging perception and planning, prediction plays an important role in the whole autonomous driving system to ensure final safety
- Typically, motion forecasting is an independently developed module that predicts agents' future trajectories with detected bounding boxes and HD maps
- And the bounding boxes are ground truth annotations in most current motion datasets [27], which is not realistic in onboard scenarios
- While in this paper, the motion forecasting module takes previously encoded sparse queries (i.e., Q A and Q M ) and dense BEV features B as inputs, and forecasts K plausible trajectories in future T timesteps for each agent
- Besides, to be compatible with our end-to-end and scene-centric scenarios, we predict trajectories as offset according to each agent's current positions
- The agent features before the last decoding MLPs, which have encoded both the historical and future information will be sent to the occupancy module for scene-level future understanding
- Planning
- As an ultimate goal, the planning module takes all upstream results into consideration
- Traditional planning methods in the industry often are rule-based, formulated by "if-else" state machines conditioned on various scenarios which are described with prior detection and prediction results
- In our learning-based model, we take the upstream ego-vehicle query, and the dense BEV feature B as input, and predict one trajectory τ for total T p timesteps
- Then, the trajectory τ is optimized with the upstream predicted future occupancy Ô to avoid collision and ensure final safety.

## B. The Necessity of Each Task
- In terms of perception, tracking in the loop as does in PnPNet and ViP3D is proven to complement spatial-temporal features and provide history tracks for occluded agents
- With the aid of high-definition (HD) maps and motion forecasting, planning become more accurate toward higher-level intelligence
- However, such information is expensive to construct and prone to be outdated, raising the demand for online mapping without HD maps
- As for prediction, motion forecasting generates long-term future behaviors and preserves agent identity in form of sparse waypoint outputs
- However, there exists the challenge to integrate nondifferentiable box representation into subsequent planning module
- Some recent literature investigates another type of prediction task named as occupancy prediction to assist end-to-end planning
- However, the lack of agent identity and dynamics in occupancy makes it impractical to model social interactions for safe planning
- The large computational consumption of modeling multi-step dense features also leads to a much shorter temporal horizon compared to motion forecasting
- Therefore, to benefit from the two complementary types of prediction tasks for safe planning, we incorporate both agentcentric motion and whole-scene occupancy in UniAD

### C.1. Joint perception and prediction
- Joint learning of perception and prediction is proposed to avoid the cascading error in traditional modularindependence pipelines
- Similar to the motion forecasting task alone, it usually has two types of output representations: agent-level bounding boxes and scene-level occupancy grid maps
- Pioneering work FaF [57] predicts boxes in the future and aggregates past information to produce tracklets. [10] extends it to reason about intentions and [25,28] further predict future states in a refinement fashion.
- Some exploit detection first and utilize agent features in the second prediction stage [9,47,65]
- Note that history information is ignored, PnPNet [50] enriches it by estimating tracking association scores to avert the non-differentiable optimization process. Yet, all these methods rely on nonmaximum suppression (NMS) in detection which still leads to information loss.
- ViP3D [30] which is closely related to our work, employs agent queries in [91] to forecasting, taking HD map as another input. We follow the philosophy of [30,91] in agent track queries, but also develop non-linear optimization on target trajectories to alleviate the potential inaccurate perception problem. Moreover, we introduce an ego-vehicle query for better capturing the ego behaviors in the dynamic environment, and incorporate online mapping to prevent the localization risk or high construction cost with HD map.
- The alternative representation, namely the occupancy grid map, discretizes the BEV map into grid cells which hold a belief indicating if it is occupied. Wu et al. [84] estimates a dense motion field, while it could not capture multi-modal behaviors. Fishing Net [32] also predicts deterministic future BEV semantic segmentation with multiple sensors. To address this, [72] proposes non-parametric distribution of future semantic occupancy and FIERY [34] devises the first paradigm for multi-view cameras. A few methods improve the performance of FIERY with more sophisticated uncertainty modeling [1,37,92]
- Notably, this representation could easily extend to motion planning for collision avoidance [11,37,72], while it loses the agent identity characteristic and takes a heavy burden to computation which may constrain the prediction horizon. In contrast, we leverage agent-level information to occupancy prediction and ensure accurate and safe planning through unifying these two modes.

### C.2. Joint prediction and planning
- PRECOG [71] proposes a recurrent model that conditions forecasting on the goal position of the ego vehicle
- PiP [75] generates agents' motion considering complete presumed planning trajectories. However, producing a rough future trajectory is still challenging in the real world
- [54] presents a deep structured model to derive both prediction and planning from the same set of learnable costs
- [38,39] couple the prediction model with classic optimization methods
- Meanwhile, some motion forecasting methods implicitly include the planning task by producing their future trajectories simultaneously
- Similarly, we encode possible behaviors of ego vehicle in the scene-centric motion forecasting module, but the interpretable occupancy map is utilized to further optimize the plan to stay safe.

### C.3. End-to-end motion planning
- End-to-end motion planning has been an active research domain for many years
- Pomerleau uses a single neural network that directly predicts control signals
- Subsequent studies make great advances especially in closed-loop simulation with deeper networks
- Multi-modal inputs
- Multi-task learning
- Reinforcement learning
- Distillation from certain privilege knowledge
- Some works jointly decode planning and BEV semantic predictions to enhance interpretability
- PLOP adopts a polynomial formulation to provide smooth planning results for both ego vehicle and neighbors
- Cui et al. introduces a contingency planner with diverse set of future predictions and LAV trains the planner with all vehicles' trajectories to provide richer training data
- NMP and its variant estimate a cost volume to select the plan with minimal cost besides deterministic future perception
- Though they risk producing inconsistent results between two modules, the cost map design is intuitive to recover the final plan in complex scenarios
- Inspired by [88], most recent works propose models that construct costs with both learned occupancy prediction and hand-crafted penalties
- Contrary to these approaches, we leverage the ego-motion information without sophisticated cost design and present the first attempt that incorporates the tracking module along with two genres of prediction representations simultaneously in an end-to-end model
- We provide a lookup table of notations and their shapes mentioned in this paper in Table 12 for reference
- We inherit most of the detection designs from BEV-Former [48] which takes a BEV encoder to transform image features into BEV feature B and adopts a Deformable DETR head [96] to perform detection on B
- To further conduct end-to-end tracking without heavy post association, we introduce another group of queries named track queries as in MOTR [87] which continuously tracks previously observed instances according to its assigned track ID
- Tracking process:
- At the beginning (i.e., first frame) of each training sequence, all queries are considered detection queries and predict all newborn objects, which is actually the same as BEVFormer. Detection queries are matched to the ground truth by the Hungarian algorithm [8].
- They will be stored and updated via the query interaction module (QIM) for the next timestamp serving as track queries following MOTR [87].
- In the next timestamp, track queries will be directly matched with a part of ground-truth objects according to the corresponding track ID, and detection queries will be matched with the remaining ground-truth objects (newborn objects).
- To stabilize training, we adopt the 3D IoU metric to filter the matched queries. Only those predictions having the 3D IoU with ground-truth boxes larger than a certain threshold (0.5 in practice) will be stored and updated.
- Inference stage:
- Different from the training stage, each frame of a sequence is sent to the network sequentially, meaning that track queries could exist for a longer horizon than the training time.
- Another difference emerging in the inference stage is about query updating, that we use classification scores to filter the queries (0.4 for detection queries and 0.35 for track queries in practice) instead of the 3D IoU metric since the ground truth is not available.
- Besides, to avoid the interruption of tracklets caused by short-time occlusion, we use a lifecycle mechanism for the tracklets in the inference stage. Specifically, for each track query, it will be considered to disappear completely and be removed only when its corresponding classification score is smaller than 0.35 for a continuous period (2s in practice).
- Following [49], we decompose the map query set into thing queries and stuff queries. The thing queries model instance-wise map elements (i.e., lanes, boundaries, and pedestrian crossings) and are matched with ground truth via bipartite matching, while the stuff query is only in charge of semantic elements (i.e., drivable area) and is processed with a class-fixed assignment.
- We set the total number of thing queries to 300 and only 1 stuff query for the drivable area.
- We stack 6 location decoder layers and 4 mask decoder layers (we follow the structure of those layers as in [49]).
- We empirically choose thing queries after the location decoder as our map queries Q M for downstream tasks.
