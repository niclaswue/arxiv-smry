---
title: "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"
date: 2023-01-05T15:37:15.000Z
author: "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02111v1.webp" # image path/url
    alt: "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02111)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02111).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot).

# Abstract
- We introduce a language modeling approach for text to speech synthesis
- Specifically, we train a neural codec language model (called Vall-E) using
- During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of
- Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech
- Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis.

# Paper Content

## Introduction
- Uses large and diverse data to train a model with strong generalization
- outperforms the state-of-the-art zero-shot TTS system on speech naturalness and speaker similarity

## Related Work
- Cascaded TTS systems usually leverage a pipeline with an acoustic model and a vocoder
- End-to-end TTS models are proposed to jointly optimize the acoustic model and vocoder
- In real scenarios, it is highly desirable to customize a TTS system to an arbitrary voice with rare enrolled recordings
- Most work is done in the context of cascaded TTS systems
- To improve the quality of unseen speakers, advanced speaker embedding models can be employed
- Diffusion model based TTS is also extended to zero-shot TTS and achieved good results
- Compared to previous work, our work follows the line of cascaded TTS but first uses audio codec code as intermediate representations
- Spoken generative pre-trained models: Self-supervised learning is widely investigated in the field of speech understanding and speech-to-speech generation
- AudioLM [Borsos et al., 2022] trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.

### VALL-E 4.1 Problem Formulation: Regarding TTS as Conditional Codec Language Modeling
- Given a dataset of audio samples, encoded into discrete acoustic codes
- Uses a pre-trained neural codec model to encode each audio sample into a code
- Each acoustic code matrix represents a set of codes for a specific frame, and the column vector of each acoustic code matrix represents the code sequence for that code
- Uses a optimization objective to train the neural codec to generate the best acoustic code for a given phoneme sequence and acoustic prompt
- Inference is done by estimating the acoustic code matrix for a given phoneme sequence and a 3-second enrolled recording of an unseen speaker

### Training: Conditional Codec Language Modeling
- The neural speech codec model allows us to operate on discrete audio representations.
- Due to residual quantization in the neural codec model, the tokens have a hierarchical structure: tokens from previous quantizers recover acoustic properties like speaker identity, while the consecutive quantizers learn fine acoustic details.
- Each quantizer is trained to model the residual from the previous quantizers.
- Motivated by this, we design two conditional language models in a hierarchical manner.
- For the discrete tokens from the first quantizer c :,1 , we train an autoregressive (AR) decoder-only language model.
- It is conditioned on the phoneme sequence x and the acoustic prompt C:,1 , formulated as
- Since VALL-E is a decoder-only LM, the concatenation of c:,1 and c :,1 is a whole sequence, and we do not distinguish them or insert a specific token in training.
- Only c :,1 is predicted while the prefix c:,1 is given during inference.
- For the discrete tokens from the second to the last quantizers, c :,j∈[2,8] , we train a non-autoregressive (NAR) language model.
- Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix C is used as an acoustic prompt. Thus, the model is
- Figure 3: The structure of the conditional codec language modeling, which is built in a hierarchical manner.
- In practice, the NAR decoder will be called seven times to generate codes in seven quantizers.
- The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed.
- On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse.
- In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction.
- On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from O(T ) to O(1).
- Overall, the prediction of C can be modeled as:
- The autoregressive language model generates the tokens from the first quantizer.
- It comprises a phoneme embedding W x , an acoustic embedding W a , a transformer decoder, and a prediction layer.
- In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model. Thus, the model input is the concatenation of x and c :,1 , and two special <EOS> tokens are appended after each of them.
- We compute sinuous position embedding separately for prompt and input tokens.
- For the causal transformer model, each token c t,1 can attend to (x, c ≤t,1 ) as illustrated in the left part of Figure 3.
- The model is optimized to maximize the probability of the next token in the first codebook.
- We share the parameters of the output projection layer with the parameters of the acoustic embedding W a .
- In the AR model, we do not explicitly extract an audio clip as the prompt in training.
- The training process is pure casual language model training.
- In this way, any prefix sequence c <t,1 is treated as a prompt for the latter part of the sequence c ≥t,1 .
- During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together.

### Inference: In-Context Learning via Prompting
- In-context learning is a surprising ability of the text-based language model
- For TTS, if the model can synthesize high-quality speech for unseen speakers without fine-tuning, the model is believed to have in-context learning capability
- However, the in-context learning capability of existing TTS systems is not strong, because they either require additional fine-tuning or degrade dramatically for unseen speakers
- For language models, prompting is necessary to enable in-context learning in the zero-shot scenario
- We design prompts and inference as follows. We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt. Both prompts are used in the AR and NAR models.
- For the AR model, we use samplingbased decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop. Furthermore, the sampling-based method could significantly increase the diversity of the output.
- For the NAR model, we use greedy decoding to choose the token with the highest probability. Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences.
- The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases: VALL-E: Our main interest is to generate given content for unseen speakers. The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription. We prepend the transcription phoneme of the enrolled speech to the phoneme sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech c:,1 as an acoustic prefix. With the phoneme prompt and the acoustic prefix, VALL-E generates the acoustic tokens for the given text cloning the voice of this speaker. In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations. The inference process is the same as setting VALL-E, except that the enrolled speech and the generated speech are semantically continuous.

## Experiment

### Experiment Setup

### LibriSpeech Evaluation

### VCTK Evaluation
- Our model is able to synthesize speech with higher speaker similarity
- Compared to the baseline, VALL-E outperforms the baseline even if the baseline has seen 97 speakers in training
- By comparing different lengths of the prompt, we can see our model is able to generate more similar speech when the prompt becomes longer

### Qualitative Analysis
- Previous TTS systems have a strong one-one mapping between input text and output waveform, because mel spectrum generation is based on reconstruction for each step without randomness.
- VALL-E uses the sampling-based method to generate discrete tokens, which results in diverse outputs for the same input text due to the randomness in inference.
- Considering the diversity feature of VALL-E, it is an ideal candidate to generate pseudo-data for speech recognition.
- Acoustic environment maintenance: When the acoustic prompt has reverberation, VALL-E could synthesize speech with reverberation as well, whereas the baseline outputs clean speech.
- Speaker's emotion maintenance: VALL-E can preserve the emotion in the prompt at a zero-shot setting.

## Conclusion, Limitations, and Future Work
- We introduced VALL-E, a language model approach for TTS with audio codec codes as intermediate representations.
- We pre-train VALL-E with 60K hours of speech data, and show the in-context learning capability in zero-shot scenarios.
- We achieve new state-of-the-art zero-shot TTS results on LibriSpeech and VCTK.
- Furthermore, VALL-E could keep the acoustic environment and speaker's emotion in synthesis, and provide diverse outputs in different sampling-based decoding processes.
- Despite making significant progress, VALL-E still suffers from several issues.
- We observe that some words may be unclear, missed, or duplicated in speech synthesis. It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue.
- The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling.
- In the future, we would like to leverage these techniques to solve the issue.
