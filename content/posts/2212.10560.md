---
title: "Self-Instruct: Aligning Language Model with Self Generated Instructions"
date: 2022-12-20T18:59:19.000Z
author: "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10560v1.webp" # image path/url
    alt: "Self-Instruct: Aligning Language Model with Self Generated Instructions" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10560)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10560).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/self-instruct-aligning-language-model-with).

# Abstract
- Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks
- Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.
- We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model.
- Applying our method to vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT_001, which is trained with private user data and human annotations.
- For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT_001.

# Paper Content

## Introduction
- Large pre-trained language models (LM) are a key component of newer engines that are more powerful than text-davinci-001
- Human-written instruction data is a limiting factor for the quality of these newer engines
- Semi-automated process for instruction-tuning a pretrained LM using instructional signals from the model itself is introduced
- The resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions

## Related Work
- Language models can follow general language instructions if annotated "instructional" data is available
- This annotated data is often provided by human annotators, which poses a bottleneck for progress
- Our work aims to reduce the dependence on human annotators and create a large-scale, public dataset
- Bootstrapping with limited resources can be done using language models

## Method
- Annotating large-scale instruction data can be challenging for humans because it requires creativity to come up with novel tasks and expertise for writing the labeled instances for each task.
- In this section, we detail our process for SELF-INSTRUCT, which refers to the pipeline of generating tasks with a vanilla pretrained language model itself and then conducting instruction tuning with this generated data in order to align the language model to follow instructions better.
- This pipeline is depicted in Figure 1.

### Defining Instruction Data
- The instruction data we want to generate contains a set of instructions, each of which defines a task in natural language.
- Each task has one or more input-output instances.
- A model is expected to produce the output, given the task instruction and the instance input: (, ) = , for (, ) âˆˆ (, ).
- Note that the instruction and instance input does not have a strict boundary in many cases.

### Automatic Instruction Data Generation
- The pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data.
- Based on a finding that large pretrained language models can be prompted to generate new and novel instructions when presented with some existing instructions in the context.
- We propose to generate a diverse set of instructions in a bootstrapping fashion.
- We initiate the task pool with 175 tasks (1 instruction and 1 instance for each task) written by our authors.
- For every step, we sample 8 task instructions from this pool as in-context examples.
- Of the 8 instructions, 6 are from the human-written tasks, and 2 are from the model-generated tasks in previous steps to promote diversity.
- The prompting template is shown in Table 6.
- Classification Task Identification.
- Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.
- We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks.
- The prompting template is shown in Table 7.
- Instance Generation.
- Given the instructions and their task type, we generate instances for each instruction independently.
- This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output.
- We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.
- A natural way to do this is the Input-first Approach, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output.
- This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks.
- The prompting template is shown in Table 8.
- However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input).
- Therefore, we additionally propose an Output-first Approach for classification tasks, where we first generate the possible class labels, and then condition the input generation on each class label.
- The prompting template is shown in Table 9.
- To encourage diversity, a new instruction is added to the task pool only when its ROUGE-L overlap with any existing instruction is less than 0.7.
- We also exclude instructions that contain some specific keywords (e.g., images, pictures, graphs) that usually can not be processed by language models.

### Finetuning the LM to Follow Instructions
- After the creation of the large-scale instruction data, we use this data to finetune the original language model.
- To do this, we concatenate the instruction and instance input as a prompt and train the model to generate the instance output in a standard supervised way.
- To make the model robust to different formats, we use multiple templates to encode the instruction and instance input together.

### Diversity
- The verb-noun structure is found in 26,559 out of the 52,445 generated instructions
- The verb is the closest to the root of the parse tree and has its first direct noun object
- Other instructions usually contain more complex clauses or are framed as questions

### Quality
- The quantity and diversity of the generated data is impressive
- The quality of the generated data is uncertain, but most of the generated instructions are meaningful
- The generated instances may contain more noise, but most of them are still in the correct format or even partially correct

## Experimental Results
- We describe our models and other baselines
- We conduct experiments to measure and compare the quality of models under various instruction tuning setups
- We first describe our models and other baselines, followed by our experiments
- We use the OpenAI finetuning API to finetune the GPT3 model
- The resulting model is denoted as GPT3 SELF-INST

### Baselines
- Off-the-shelf language models are evaluated for their ability to follow human instructions
- Both T5-LM and GPT3 are finetuned from the T5 checkpoints and are publicly available
- SELF-INSTRUCT boosts the instruction-following ability of GPT3 by a large margin
- Models trained on SUPERNI training set still achieve better performance on its evaluation set

### Experiment 2: Generalization to User-oriented Instructions on Novel Tasks
- The comprehensiveness of SUPERNI in collecting existing NLP tasks is not enough to create a good set of instructions for practical use.
- To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications.
- We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance.
- We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.).
- In total, we create 252 instructions with 1 instance per instruction.
- We believe it can serve as a testbed for evaluating how instructionbased models handle diverse and unfamiliar instructions.

### Broader Impact
- The burden now falls on academia to better understand the source of success in these models and strive for bet- ter -yet open -models.
- If true, this may create barriers to access for those who may not have large computing resources.
- Relatedly, one observed challenge in this process is the algorithm's difficulty in producing balanced labels, which reflected models' prior biases.
- We introduce SELF-INSTRUCT, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it.
- Our method conducts instruction-tuning of the original model on the pruned subset of generated samples.
- On experimenting with vanilla GPT3, we observe a 33% absolute improvement over the original model on SUPER-NATURALINSTRUCTIONS.
- This performance is on par with InstructGPT 001 , which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT 001 .
