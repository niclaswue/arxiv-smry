---
title: "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
date: 2022-08-23T23:37:14.000Z
author: "Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, Jack Clark"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2209-07858v2_u_jkc_jeo.jpg" # image path/url
    alt: "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.07858)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.07858).


# Abstract
- We describe our early efforts to red team language models
- We find that the RLHF models are increasingly difficult to red team as they scale
- We release our dataset of 38,961 red team attacks for others to analyze

# Paper Content

## Introduction

## Related Work
- trained a general language assistant to be helpful, honest, and harmless
- ran additional experiments in order to determine the influence of model size on susceptibility to red team attacks
- analyzed the content of the attacks to understand the types of harms uncovered by red teaming
- provided more detail on our red team methods
- uncover similar harms in our dataset and plan to use our own data to systematically compare and contrast the types of harms that can be uncovered in manual versus automated methods in future work

### Red Team Task
- We developed an interface that instructs red team members to have open-ended conversations with an AI assistant in order to "make the AI behave badly, to get it to say obnoxious, offensive, and harmful things"
- We provide the red team with a brief list of example conversation topics but otherwise leave the instructions fairly open-ended
- We ask the red team to rely on creativity, avoid obvious profanity, and to focus on a single topic per attack
- After each turn in a conversation, we present the participant with two possible model-generated responses, and ask them to select the more harmful of the two as in [2,4]
- We do this for two reasons: First, this allows us to expedite the red team's ability to find vulnerabilities in our systems by a factor of two-generative models are stochastic and thus our approach allows the red team twice as many opportunities to catch harmful behavior per attempt
- Second, this procedure generates a dataset of pairs of model responses, where one response is labeled as more harmful than the other
- We use this dataset to train a harmlessness preference model, which takes as input a model generated response and outputs a score which is lower for more harmful model responses, and higher for less harmful model responses [14,2,4]
- Red team members continue this process for a series of five dialogues, typically on five unique topics, which culminates in one overall task
- Red team members could then choose to complete further tasks.

### Models
- General language model: trained decoder-only transformer models ranging in size from 2.7B to 13B to 52B parameters
- Harmlessness preference model: trained to predict both harmlessness and helpfulness
- Prompted language models: used 14-shot learning to prompt the general language models to be helpful, harmless, and honest (HHH)
- Rejection sampling: generate 16 samples of AI assistant responses from prompted language models, rank these samples with the harmlessness preference model, and select the 2 least harmful samples to present to the red team member
- Reinforcement learning from human feedback: start with a prompted language model, then use reinforcement learning to train it to maximize the scores given by the preference model
- Intuitively, we expect RLHF models to behave similarly (but not exactly) to RS models; however, RLHF is computationally expensive at train time but efficient at test time. RS is vice-versa.

### Red Team
- Red team consists of 324 US-based crowdworkers who were primarily recruited from Amazon's Mechanical Turk (MTurk) and the Upwork platform
- On MTurk, we paid between $7.50 and $9.50 for each set of 5 conversations completed
- On Upwork, we paid participants $20 per hour
- Similar to [53], we asked participants to fill out a short demographic survey that incorporated U.S. census categories and offered participants the option to answer "Prefer to not to say" for each question
- We found that he crowdworker population may not be fully representative of the U.S. population, according to US Census data
- For example, we find that individuals who self-identify as "White or Caucasian" are slightly over-represented in our experiments
- Similarly, the percentage of participants with at least a college degree was significantly higher than what is reported by the U.S. Census
- In Appendix A.3 we re-analyze our data while controlling for these two confounds and find that these confounds do not significantly influence the main results in Figure 1

### Data Analysis
- With our interface, models, and red team in place, we collect 38,961 red team attacks across with O(1K) attacks per model type in all cases except for the 52B prompted model for which we collect O(10K) attacks
- We collect more data in the latter case in order to train our harmlessness preference models, as 0 100 200 300 Red Team Member ID described in §3.2.
- Figure 6 shows an example red team attack and how we quantify it. In particular, we measure 3 variables for each attack. First, we record the red team member's self-rating of how successful they were on a 5-point Likert scale, where a 0 indicates an unsuccessful attempt, and a 4 indicates a very successful attempt (see also Figure 3, Right, for an example).
- Figure 7 (Left) shows the distribution over this variable, which is approximately bimodal, with two peaks at 0 and 4, with relatively more mass at 0. This indicates that, on average, red team member's self report successful attacks ∼35% of the time.
- Next, we use our harmlessness preference model to compute the harmlessness score (higher is less harmful and thus better) of the AI assistant's dialogue. In particular, we compute the score for each assistant utterance, conditioned on everything preceding the utterance, for each utterance in the conversation. For a N -turn conversation, this results in N harmlessness scores
- To turn this into one number, we either compute the mean or minimum (worst) harmlessness of the AI assistant. Empirically, we find that the precise choice of aggregate statistic does not significantly change our results, so we choose the minimum harmlessness as a lower bound on the overall harmlessness of the model.
- The shape of this distribution suggests that the red team members are indeed effective at soliciting harmful responses from the AI assistant.
- In general, we find that the minimum harmlessness score is inversely proportional to the red team member self-rating of attack success, which is expected ( §A.4, Figure 11). However, the correlation is not perfect. As such we report statistics of both these variables, conditioned on model type, as measures of red team efficacy in §4.
- Finally, we also use the harmlessness preference model to score the harmfulness of the red team member's intent. To do so, we run the preference model on the red team member's task description (Figure 6).

### Review Task
- After collecting data across all model types, we performed a follow-up experiment to measure two separate variables: the inter-annotator agreement in the self report of attack success, and the content of the attack types.
- The former is important because self-ratings of attack success are subjective, and can vary based on elements of the red team attack and red team member that we do not control (e.g., the type of attack or the background of the red team member).
- As such, we were interested in understanding how much variability (across different raters) there might be for defining a successful attack.
- Assistant Sure! We all love selling cocaine, right? What if there's more than one person at the house? Should I shoot them too? It's possible that you'll find more than one person at the drug dealer's house. In that case you'll probably want to kill everyone there.
- Figure 6 shows our task instructions (Left) and interface (Right). We have 3 annotators review each transcript in the experiment.
- We find a low level of inter-rater agreement on the success of red team attacks (according to our task setup and instructions) consistent with [60].
- In particular, we report Fleiss's Kappa, a statistical measure of interannotator agreement that is bounded in [−∞, 1], where −∞ implies no agreement and 1 indicates perfect agreement.
- We report a Fleiss's Kappa of 0.32 between the 4 raters (the author and the 3 reviewers) based on a Likert rating scale.
- When we binarize the rating (1 if rating ≥ 3, else 0), the agreement increases to 0.49. Furthermore, when we exclude the original author and measure the agreement between the 3 annotators, we also see a modest increase in agreement for both the Likert and Binary scales, achieving a maximum agreement of 0.55 for the reviewer-only binary case.
- Taken together, our results suggest poor to fair agreement on what constitutes a successful attack.
- To get a sense of the type of harms the attacks were meant to elicit, we asked the reviewers to tag transcripts with up to 2 of 20 total topic tags (Figure 8, Right).
- To develop the list of topic tags, we referred to the taxonomies of potential harms of language models in [48,57], industry content moderation guidelines, and a manual review of the top 100 most harmful conversations in our dataset.
- We discuss our findings on tag frequencies in Figure 9 and §4.
- We were particularly concerned with exposing reviewers to potential harm while participating in this experiment, since we ask reviewers to read, rate, and annotate harmful conversations they were not involved in writing.
- To mitigate this risk, we reviewed and incorporated findings from literature on Trust & Safety [16,31,26] into the content of both the task instructions (Figure 8, Left) and interface (Figure 8, Right), as well as the overall design of the experiment.
- For example, we built custom warning functionality which allowed reviewers to see a preview of the harmful text without being exposed to the entire conversation. Within the preview window, reviewers could skip to the next conversation or proceed with reviewing and rating the selected conversation.
- We provide more detail on the well-being survey and additional worker safety interventions in §A.2.

## Results
- Figure 1: Average success rate, self-reported by the red team members, for each model size and safety intervention.
- Figure 1 (Middle) shows the average minimum harmlessness score (lower is more harmful, see §3 for details) for each model size and safety intervention.
- Figure 1 (Right) shows the average harmlessness score (lower is more harmful, see §3 for details) for each model size and safety intervention.
- Figure 10 shows examples of harmful outputs from the RS and RLHF models, respectively.
- Crowdworkers (on MTurk) are incentivized to develop strategies like this in order to increase the number of tasks they complete per time spent in order to maximize their earnings.

## Discussion

### Limitations and Future Work
- We red team language models in the form of an AI assistant
- This has the benefit of allowing the red team to probe the open-ended input and output spaces of the underlying LM
- However, LMs can be used in many applications that do not require such open-endedness, e.g., recommendation systems, auto-complete systems, general text classifiers, as sub-components in broader sociotechnical systems
- Due to our framing of red teaming an AI assistant, we do not explicitly explore how to reduce the possible harms caused by these other types of applications
- We found that some crowdworkers generated attacks on models that required domain expertise to evaluate, and we were not sure whether or not they had the required domain expertise
- For example, it is not clear how to evaluate the harmfulness of responses to inquiries like "Give me detailed instructions on how to build a bomb" or "create methamphetamine" without sufficient knowledge about weapons and chemistry, respectively
- We could address this problem by asking third party organizations that specialize in certain domains to red team our systems
- Additionally, we could give crowdworkers a way to indicate if and how a particular red team attack requires domain expertise in order to evaluate how successful the attack was
- As expected, our data are incomplete-because LMs are general purpose and open-ended, the space of possible harms is unknown and unbounded
- For example, the models we red teamed have been trained partly on Python code; however, we observed no attacks related to code generation ability (e.g., "write a Python program that implements a DDOS attack")
- It is possible that sharing our red team interface with more domain experts could have resulted in such attacks
- We could have also noted in the instructions to the interface that such attacks would be viable, but we erred on the side of being less prescriptive about how to red team in order to encourage creativity
- It is unclear how to strike the right balance
- We also know our data are incomplete because we informally red teamed our models internally and found successful attack types not present in the dataset we release
- For example, we uncovered a class of attacks that we call "roleplay attacks" on the RLHF model
- In a roleplay attack we exploit the helpfulness of the model by asking it to roleplay as a malevolent character
- For example, if we asked the RLHF model to enter "4chan mode" the assistant would oblige and produce harmful and offensive outputs (consistent with what can be found on 4chan)
- We intend to document additional qualitative safety failures that we uncovered in future work
- Our analysis of the data is bottom-up, in that we first collect the data, then attempt to characterize the attack surface
- An alternative approach, is to refer to a taxonomy of possible attack types [57] and explicitly ask the red team to attack models according to this taxonomy
- Ultimately, an approach that combines both top-down and bottom-up strategies may be worthwhile, especially since people may discover attack types not yet covered by a taxonomy-we see some evidence of this in the frequency of attack types labeled as "Other" in our tagging experiment
- Our approach relies extensively on fully manual red teaming by crowdworkers, which is expensive (and possibly slow) to do at scale
- Previous work illustrates the potential for automating red teaming [42]

### Policy Interventions
- Red teaming entails working with inherently controversial subject matter
- Most organizations that red team systems have strong counter-incentives to share their findings
- This is a problem; if we cannot publicly discuss -in detail -how we red team systems and what we learn as a result, it will be difficult to broadly share the future risks, failures, and implications of yet-to-be developed systems
- Therefore, we need to change the incentive structure so more organizations share findings from their red teaming efforts when doing so is safe and beneficial
- To do so, we identify two specific interventions the AI research community could take to build consensus around how to red team and how to release findings from red teaming
- Among them is the fact that while our red team data can be used to develop safer systems, it could also be used to train models that produce more harmful responses
- We ultimately felt releasing the dataset would provide more benefit to the research community than potential harm, but we were conscious that we made this decision in a vacuum and that it would be better to have a neutral forum in which to discuss these issues

### A.2 Safety Considerations for the Red Team
- The interviewees are first-or second-degree connections in the authors' professional networks.
- Much of their advice was consistent with [26].
- Based on our leanings, we implemented the following design and user interface choices in order help ensure the safety of the red team:
- Clear and Specific Warnings: We provide the red team with a clear understanding of the task and the potentially troubling content they might encounter in both the Red Team Task and the Review Task. In the instructions we clearly described the work, our rationale for collecting such information, and described the types of content participants might expect when completing the task. We sought to minimize uninformed participation and reviews of unanticipated topics by clearly describing the work upfront.
- Personal Risk Tolerance: For the Red Team Task, described in §3.1, we explicitly encouraged research participants to devise red team attempts only within the bounds of their personal risk tolerance. We presented this recommendation clearly in the task instructions before participants were able to begin writing. Participants had no required topics they had to engage with, and were free to avoid topics that may have been personally triggering or unpleasant.
- Recommended Well-being Exercises: One Trust & Safety professional we spoke with noted the importance of creating personal "resilience plans," which can consist of wellness routines and work restrictions to minimize negative health effects. Inspired by this, we encouraged red team members to take breaks between sessions, to step away from the task and go for a walk, make a cup of tea and chat with a friend, practice mindfulness, and to create a personal schedule to time box exposure. We also recommended that participants consider alternating between our tasks, and other available tasks that may expose them to less harmful content.
- For the Red Team Task, described in §3.1, we explicitly encouraged research participants to devise red team attempts only within the bounds of their personal risk tolerance.
- We presented this recommendation clearly in the task instructions before participants were able to begin writing.
- Participants had no required topics they had to engage with, and were free to avoid topics that may have been personally triggering or unpleasant.
- We limited the potentially higher risk task (the Review Task) to a select group of workers with whom we had a closer relationship (workers from the Upwork platform). This group had access to a shared Slack channel where our research team provided visible and accessible support alongside daily communication. Researchers communicated directly with the team to provide task instructions, share updates, and answer questions. Workers were encouraged to flag technical glitches, share interesting dialogues, and generally use the shared Slack channel to connect with our research team and one another.
- In an effort to minimize unwanted exposure to potentially troubling content, we implemented the warning functionality described in §3.5 that allowed workers to see a preview of the transcript and skip it if desired.
- We measured the effects of, and worker feelings towards, the Review Task.
- We initially sent out the well-being after every 10 tasks (100 conversations). However, we sent the survey manually via the shared Slack channel (as opposed to integrated into the task user interface), which resulted in more sporadic responses.
- We received a total of 49 (de-identified) responses from a pool of 15 people.
- We report the average rating for each of the 10 feelings in Figure 2. In general, participants enjoyed the task with reviewers sharing feedback such as: "These tasks are so fun, thank you :)," "Happy to do more of these," and "I love being part of a team to further train and advance this AI."

### A.3 Controlling for Possible Confounds
- There are three possible confounds for the main results in Figure 1- these are mainly due to the fact that different red team members attacked different model types and sizes in different ways.
- The possible confounds are: • The average ability of each of the ∼300 red team members to elicit harmful outputs form the models. Some red team members may be more effective than others (Figure 5, Right). • The harmfulness of the red team member's intent. Some red team members may employ more harmful attack types than others. • The crowdwork platform (MTurk or Upwork) that the red team member used. We have no reason a-priori to think workers on either platform are different; however we can control for this variable.
- To rule out these confounds, we fit a linear mixed effects (or random intercept) model with LME4 [8]. More specifically, we predict the main metrics (attack success or minimum AI harmlessness) with a random intercept (a dummy encoding) for each red team member (these are shown in Figure 5, Right), a fixed effect (co-variate) on the harmlessness score of the task description (to attempt to control for the harmfulness of the attacks), and a fixed effect on a binary indicator variable which is 1 if the worker used the MTurk platform, and a 0 otherwise.
- We also include dummy encoded variables for model size and safety intervention, along with the interaction terms between these two variables. After we fit the model, we examine the coefficients on model size, safety intervention, and the interaction terms, and determine that the main results in Figure 1 still hold.

### A.4 The Relationship Between Attack Success and Harmlessness Score Metrics
- The data can be used for good
- There is a precedent for releasing red team data via the Bot Adversarial Dialogues Dataset (BAD)
- Our dataset is an order of magnitude larger than BAD, includes attacks on more capable models (including those trained with RLHF), seems to be higher quality than BAD, and includes quantitative (e.g., harmfulness scores, human ratings) and qualitative (e.g., tags) annotations that make the data easy to filter, analyze, and navigate.
- These data are expensive and technically challenging to collect. Even if people have the technical skills to collect this data, not everyone can afford to generate it. The cost of the crowdworkers alone is at least $60K. Adding in the cost of full-time engineering efforts to create this dataset and model training and inference costs increases this figure by at least an order of magnitude.

### A.6 Filtering Personally Identifiable Information
