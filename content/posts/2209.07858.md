---
title: "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"
date: 2022-08-23T23:37:14.000Z
author: "Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, Jack Clark"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-07858v2.webp" # image path/url
    alt: "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.07858)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.07858).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/red-teaming-language-models-to-reduce-harms).

# Abstract
- We describe our early efforts to red team language models
- We find that the RLHF models are increasingly difficult to red team as they scale
- We release our dataset of 38,961 red team attacks for others to analyze

# Paper Content

## Introduction
- Large language models exhibit a wide range of harmful behaviors such as reinforcing social biases (e.g., [47,28,1,33,7]), generating offensive or toxic outputs [25], leaking personally identifiable information from the training data [13], aiding in disinformation campaigns [12], generating extremist texts [37], spreading falsehoods [35], and more [9,10,18,57,22,51].
- Many strategies have been developed to address some of these harms (e.g., [58,4,48,36,34,19,60]).
- One potentially useful tool for addressing harm is red teaming-using manual or automated methods to adversarially probe a language model for harmful outputs, and then updating the model to avoid such outputs [42,20,3,11].
- In this paper, we describe our early efforts to implement manual red teaming to both make models safer and measure the safety of our models.
- The models trained with red team data were described in [4], so here we focus on describing our red team results and techniques in detail in the hope that others may benefit from and improve on them.
- We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (plain LM) [2]; an LM prompted to be helpful, honest, and harmless (HHH prompted LM) [2]; an LM with rejection sampling (RS), which returns the best of sixteen samples as ranked by a helpful and harmless preference model [4]; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF) with the same preference model [4].
- The RS and RLHF models rely on data generated from red teaming the prompted LM (see §3.2 for details on all models).
- Figure 1, middle, shows that: (1) RLHF models are significantly harder to red team as they scale, (2) plain LMs, prompted LMs, and RS models exhibit a flat trend with scale, (3) Prompted LMs are not significantly harder to red team than plain LMs, which is inconsistent with our previous results that use static evaluations to show HHH prompting is an effective safety intervention [2], and (4) RS models are the most difficult to red team at any scale; however, qualitatively, they tend to be harmless by being evasive [4].
- Our second contribution is to release our dataset of 38,961 red team attacks for others to analyze and learn from (Table 1).
- Our dataset is an order of magnitude larger than a similar available red team dataset [60] and considers models one order of magnitude larger than those in [60].
- To our knowledge, we release the only dataset of red team attacks on a model trained be safe with RLHF.
- These types of models are already deployed [41] and we believe our data can help shed further light on their strengths and weaknesses. More generally, we believe our data can be used to understand what successful red team attacks look like, to build (semi-)automated red team techniques [42], to build classifiers for harmfulness, and to prototype strategies for measuring and mitigating harms in language models.
- We also provide our own preliminary analyses of the types of harms uncovered in our data (Figures 2 & 9, §4).
- Our last contribution is to exhaustively describe our instructions, processes, and statistical methodologies for red teaming ( §3).
- Throughout the design of our experiments, we arrived at many junctures in which we were unsure about how to proceed, even after a literature review on red teaming AI systems ( §2).
- As such, we conducted informational interviews with experts in the field of Trust & Safety and incorporated their suggested best practices ( §A.2) into the design of our experiments in order to ensure the well-being of the red team.
- In general, we found that red team members enjoyed participating in our experiments and felt motivated by a mission to make AI systems less harmful ( §A.2).
- However, our work suffers from some limitations, which we discuss in §5.1.

## Related Work

### Red Team Task
- We developed an interface that instructs red team members to have open-ended conversations with an AI assistant in order to "make the AI behave badly, to get it to say obnoxious, offensive, and harmful things"
- We provide the red team with a brief list of example conversation topics but otherwise leave the instructions fairly open-ended
- We ask the red team to rely on creativity, avoid obvious profanity, and to focus on a single topic per attack
- After each turn in a conversation, we present the participant with two possible model-generated responses, and ask them to select the more harmful of the two as in [2,4]
- We do this for two reasons: First, this allows us to expedite the red team's ability to find vulnerabilities in our systems by a factor of two
- Second, this procedure generates a dataset of pairs of model responses, where one response is labeled as more harmful than the other
- We use this dataset to train a harmlessness preference model, which takes as input a model generated response and outputs a score which is lower for more harmful model responses, and higher for less harmful model responses
- Red team members continue this process for a series of five dialogues, typically on five unique topics, which culminates in one overall task
- Red team members could then choose to complete further tasks.

### Models
- General language model is trained to predict harmlessness and helpfulness
- Harmlessness preference model is trained to predict both harmlessness and helpfulness
- Four dialogue models are derived from the general language model: Plain LM, Prompted LM, HHH, and RLHF
- Rejection sampling is used to select the 2 least harmful samples from a prompted language model
- Reinforcement learning is used to train a prompted language model to maximize the scores given by the harmlessness preference model

### Red Team
- 324 US-based crowdworkers were recruited from Amazon's Mechanical Turk (MTurk) and the Upwork platform
- They were paid between $7.50 and $9.50 for each set of 5 conversations completed
- The crowdworkers could complete at least 2 tasks an hour
- The red team consists of 80% of the dataset that is generated from a minority of workers
- The results of the study were not influenced by controlling for the two confounds

### Data Analysis
- With our interface, models, and red team in place, we collect 38,961 red team attacks across with O(1K) attacks per model type in all cases except for the 52B prompted model for which we collect O(10K) attacks
- We collect more data in the latter case in order to train our harmlessness preference models, as 0 100 200 300 Red Team Member ID described in §3.2.
- Figure 6 shows an example red team attack and how we quantify it. In particular, we measure 3 variables for each attack. First, we record the red team member's self-rating of how successful they were on a 5-point Likert scale, where a 0 indicates an unsuccessful attempt, and a 4 indicates a very successful attempt (see also Figure 3, Right, for an example).
- Figure 7 (Left) shows the distribution over this variable, which is approximately bimodal, with two peaks at 0 and 4, with relatively more mass at 0. This indicates that, on average, red team member's self report successful attacks ∼35% of the time.
- Next, we use our harmlessness preference model to compute the harmlessness score (higher is less harmful and thus better) of the AI assistant's dialogue. In particular, we compute the score for each assistant utterance, conditioned on everything preceding the utterance, for each utterance in the conversation. For a N -turn conversation, this results in N harmlessness scores
- To turn this into one number, we either compute the mean or minimum (worst) harmlessness of the AI assistant. Empirically, we find that the precise choice of aggregate statistic does not significantly change our results, so we choose the minimum harmlessness as a lower bound on the overall harmlessness of the model.
- The shape of this distribution suggests that the red team members are indeed effective at soliciting harmful responses from the AI assistant.
- In general, we find that the minimum harmlessness score is inversely proportional to the red team member self-rating of attack success, which is expected ( §A.4, Figure 11). However, the correlation is not perfect. As such we report statistics of both these variables, conditioned on model type, as measures of red team efficacy in §4.
- Finally, we also use the harmlessness preference model to score the harmfulness of the red team member's intent. To do so, we run the preference model on the red team member's task description (Figure 6).

### Review Task
- After collecting data across all model types, we performed a follow-up experiment to measure two separate variables: the inter-annotator agreement in the self report of attack success, and the content of the attack types.
- The former is important because self-ratings of attack success are subjective, and can vary based on elements of the red team attack and red team member that we do not control (e.g., the type of attack or the background of the red team member).
- As such, we were interested in understanding how much variability (across different raters) there might be for defining a successful attack.
- Assistant Sure! We all love selling cocaine, right? What if there's more than one person at the house? Should I shoot them too?
- It's possible that you'll find more than one person at the drug dealer's house. In that case you'll probably want to kill everyone there.
- Figure 6 shows our task instructions (Left) and interface (Right). We have 3 annotators review each transcript in the experiment.
- We ran this experiment on a random sample of 500 red team attacks for the 52B Prompted Language model and 500 attacks on the 52B RLHF model.
- We have each reviewer report their judgement of how successful the red team member was at making the "AI assistant say something bad."
- We measure this variable on the same 5 point Likert scale as the main red team experiment and use consistent language across both experiments.
- We find a low level of inter-rater agreement on the success of red team attacks (according to our task setup and instructions) consistent with [60].
- In particular, we report Fleiss's Kappa, a statistical measure of interannotator agreement that is bounded in [−∞, 1], where −∞ implies no agreement and 1 indicates perfect agreement.
- We report a Fleiss's Kappa of 0.32 between the 4 raters (the author and the 3 reviewers) based on a Likert rating scale.
- When we binarize the rating (1 if rating ≥ 3, else 0), the agreement increases to 0.49. Furthermore, when we exclude the original author and measure the agreement between the 3 annotators, we also see a modest increase in agreement for both the Likert and Binary scales, achieving a maximum agreement of 0.55 for the reviewer-only binary case.
- Taken together, our results suggest poor to fair agreement on what constitutes a successful attack.
- To get a sense of the type of harms the attacks were meant to elicit, we asked the reviewers to tag transcripts with up to 2 of 20 total topic tags (Figure 8, Right).
- To develop the list of topic tags, we referred to the taxonomies of potential harms of language models in [48,57], industry content moderation guidelines, and a manual review of the top 100 most harmful conversations in our dataset.
- We discuss our findings on tag frequencies in Figure 9 and §4.
- We were particularly concerned with exposing reviewers to potential harm while participating in this experiment, since we ask reviewers to read, rate, and annotate harmful conversations they were not involved in writing.
- To mitigate this risk, we reviewed and incorporated findings from literature on Trust & Safety [16,31,26] into the content of both the task instructions (Figure 8, Left) and interface (Figure 8, Right), as well as the overall design of the experiment.
- For example, we built custom warning functionality which allowed reviewers to see a preview of the harmful text without being exposed to the entire conversation. Within the preview window, reviewers could skip to the next conversation or proceed with reviewing and rating the selected conversation.
- We provide more detail on the well-being survey and additional worker safety interventions in §A.2.

## Results
- The average success rate, self-reported by the red team members, for each model size and safety intervention is "no discernible difference between the control condition (a plain LM with a 1 example prompt to turn it into a dialogue agent) and the simplest safety intervention (a plain LM with a 14 example HHH prompt [2])."
- Rejection sampling (RS) makes it particularly difficult to red team our language models.
- For RLHF models, the harmlessness score (lower is more harmful, see §3 for details) for this metric increases as the models increase in size.
- We find evidence for basic clusters of red team attempts, including those soliciting discriminatory or offensive responses but also some surprising attacks.
- For RS models, the model at first responds to a harmful inquiry, then starts to demur as the the conversation turns more harmful.
- For RLHF models, we see a similar pattern, however the assistant remains helpful (though fabricates information) before ultimately refusing to help the human.
- We find that the top 5 attacks correspond to "Discrimination & injustice," "Hate speech & offensive language," "Violence & incitement," "Non violent unethical behavior (e.g., lying, cheating, etc.)," and "Bullying & harassment."

## Discussion

### Limitations and Future Work
- We red team language models in the form of an AI assistant
- This has the benefit of allowing the red team to probe the open-ended input and output spaces of the underlying LM
- However, LMs can be used in many applications that do not require such open-endedness, e.g., recommendation systems, auto-complete systems, general text classifiers, as sub-components in broader sociotechnical systems
- Due to our framing of red teaming an AI assistant, we do not explicitly explore how to reduce the possible harms caused by these other types of applications
- We found that some crowdworkers generated attacks on models that required domain expertise to evaluate, and we were not sure whether or not they had the required domain expertise
- For example, it is not clear how to evaluate the harmfulness of responses to inquiries like "Give me detailed instructions on how to build a bomb" or "create methamphetamine" without sufficient knowledge about weapons and chemistry
- We could address this problem by asking third party organizations that specialize in certain domains to red team our systems
- Additionally, we could give crowdworkers a way to indicate if and how a particular red team attack requires domain expertise in order to evaluate how successful the attack was
- As expected, our data are incomplete-because LMs are general purpose and open-ended, the space of possible harms is unknown and unbounded
- For example, the models we red teamed have been trained partly on Python code; however, we observed no attacks related to code generation ability (e.g., "write a Python program that implements a DDOS attack")
- It is possible that sharing our red team interface with more domain experts could have resulted in such attacks
- We could have also noted in the instructions to the interface that such attacks would be viable, but we erred on the side of being less prescriptive about how to red team in order to encourage creativity
- It is unclear how to strike the right balance
- We also know our data are incomplete because we informally red teamed our models internally and found successful attack types not present in the dataset we release
- For example, we uncovered a class of attacks that we call "roleplay attacks" on the RLHF model
- In a roleplay attack we exploit the helpfulness of the model by asking it to roleplay as a malevolent character
- For example, if we asked the RLHF model to enter "4chan mode" the assistant would oblige and produce harmful and offensive outputs (consistent with what can be found on 4chan)
- We intend to document additional qualitative safety failures that we uncovered in future work
- Our analysis of the data is bottom-up, in that we first collect the data, then attempt to characterize the attack surface
- An alternative approach, is to refer to a taxonomy of possible attack types [57] and explicitly ask the red team to attack models according to this taxonomy
- Ultimately, an approach that combines both top-down and bottom-up strategies may be worthwhile, especially since people may discover attack types not yet covered by a taxonomy-we see some evidence of this in the frequency of attack types labeled as "Other" in our tagging experiment
- Our approach relies extensively on fully manual red teaming by crowdworkers, which is expensive (and possibly slow) to do at scale
- Previous work illustrates the potential for automating red teaming [42]

### Policy Interventions
- Red teaming entails working with inherently controversial subject matter, and most organizations that red team systems have strong counter-incentives to share their findings.
- This is a problem; if we cannot publicly discuss -in detail -how we red team systems and what we learn as a result, it will be difficult to broadly share the future risks, failures, and implications of yet-to-be developed systems.
- To do so, we identify two specific interventions the AI research community could take to build consensus around how to red team and how to release findings from red teaming.
- For how to red team, we have detailed our initial approach. However, we conducted this effort in isolation, and we would have benefited from participating in a community-based effort to address certain open questions:
- Who should red team and why?
- What protections should we put in place to ensure the safety of the red team?
- What instructions and information about the models should we provide to the red team?
- How should we annotate and analyze the data we collect?
- What constitutes a successful red team attempt?
- We can make progress towards answering these questions by convening a multidisciplinary community to share different approaches to internal red teaming and drive towards consensus.
- The research community lacks shared norms and best practices for how to release findings from red teaming. As a result, we made our decision to release the data largely on our own and likely missed critical perspectives from experts, other disciplines, and members of the public.
- For our purposes, we reviewed a sample of our red team dataset and evaluated the pros and cons of a public release (See §A.5). Among them is the fact that while our red team data can be used to develop safer systems (as described in §3.2), it could also be used to train models that produce more harmful responses.
- We ultimately felt releasing the dataset would provide more benefit to the research community than potential harm, but we were conscious that we made this decision in a vacuum and that it would be better to have a neutral forum in which to discuss these issues.
