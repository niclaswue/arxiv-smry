---
title: "MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue"
date: 2022-12-20T17:34:25.000Z
author: "Nikita Moghe, Evgeniia Razumovskaia, Liane Guillou, Ivan Vulić, Anna Korhonen, Alexandra Birch"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10455v1.webp" # image path/url
    alt: "MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10455)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10455).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multi3nlu-a-multilingual-multi-intent-multi).

# Abstract
- Task-oriented dialogue systems have been used in a range of domains
- TOD systems are typically constructed for a single domain or language
- Extension to other languages is restricted by lack of available training data
- To support work on Natural Language Understanding in TOD across multiple languages
- MULTI3NLU++ extends the English-only NLU++ dataset to include manual translations into a range of high, medium and low resource languages
- MULTI3NLU++ inherits the multi-intent property of NLU++

# Paper Content

## Introduction
- Task-oriented dialogue systems, which are used to automate telephone-based and online customer service tasks, have been used to automate travel, finance and banking, and hotel booking tasks.
- The Natural Language Understanding (NLU) module performs two crucial tasks: intent detection and slot labelling. In the intent detection task the aim is to identify or classify the goal of the user's utterance from several pre-defined classes (or intents) (Tur et al., 2010). These intents are then used by the policy module (? Young et al., 2013) to decide the next conversational move by the conversational agent in order to mimic the flow of a human-human dialogue.
- In the slot labelling task the aim is to label each token in an utterance with a label that describes the type of semantic information represented by the token.
- Although intent detection models can reach impressive performance and have been deployed in many commercial systems (Altinok, 2018;Li et al., 2019), they are still unable to fully capture the variety and complexity of natural human interactions, and as such do not meet the requirements for deployment in more complex industry settings (Casanueva et al., 2022).
- To address all of the limitations discussed above, we propose MULTI 3 NLU ++ , a multilingual, multi-intent, multi-domain dataset for training and evaluating TOD systems.
- MULTI 3 NLU ++ includes manual translations of the 3,080 utterances in NLU++ to four languages of diverse typology and data availability: Spanish, Marathi, Turkish, and Amharic. The selection of languages covers a range of language families and scripts and includes high, medium, and lowresource languages.
- Capturing the language diversity is particularly important if we wish to design multilingual TOD systems that are robust to the variety of expressions across languages to represent the same value or concept.

## Dataset Collection
- We collected Spanish, a widely-spoken Romance language, as our high-resource language.
- Marathi, an Indo-Aryan language predominantly spoken in the Indian state of Maharashtra, is our medium-resource language.
- Turkish is an agglutinative Turkic language and may be regarded as a low-resource language from a Machine Translation perspective, or as a medium-resource language based on the amount of training data in XLM-R (Conneau et al., 2020).
- Amharic, an Ethiopian Semitic language belonging to the Afro-Asiatic language family, is our low-resource lan-guage.
- Spanish and Turkish, like English, are written in Latin script, Marathi is written in Devanagari, and Amharic in Ge'ez script.
- Manual Translation. The use of crowd-sourcing agencies to collect multilingual datasets has often resulted in crowd workers using a Machine Translation API (plus post-editing) to complete the task, or even simply transliterating the given sentence (Goyal et al., 2022).
- In the case of post-editing MT output, translations may exhibit post-editese -post-edited sentences are often simplified, normalised, or exhibit a higher degree of interference from the source language than manual translations (Toral, 2019).
- We wished to preserve the register of the original utterances, in particular with respect to the colloquial nature of many of the utterances in the original English NLU++ dataset.
- After the pilot, we asked the same translators to complete the translation of the remaining utterances in the dataset.
- We ran an automatic checker to ensure that the slot values marked by the translators were present within their translated sentences.
- Further corrections such as incorrect annotations were also communicated to the translators.
- The data collection process was carried out over five months and involved (i) the selection of translation agencies, (ii) running pilot translations, (iii) providing feedback to the translators, and (iv) the full-fledged data collection phase.
- The professional translators were compensated at £0.06/word for Spanish and £0.07/word for the remaining languages.
- The total cost of MULTI NLU ++ is £7,611; please refer to Appendix B for further details.

## Baseline Experiments
- MULTI 3 NLU ++ is a multilingual dialogue NLU dataset covering both intent detection and slot labelling tasks.
- In this section we benchmark several state-of-the-art approaches on the dataset to provide reference points for future dataset use as well as to demonstrate various aspects of multilingual dialogue NLU systems which can be evaluated using the dataset, such as cross-lingual and cross-domain generalisation.
- We provide baseline numbers for intent detection and analyse the performance across languages and methods, and for different sizes of training data.
- We follow the main experimental setups from Casanueva et al. (2022) where possible, but we extend them to multilingual contexts.
- Training Data Setups. We follow an N-fold crossvalidation setup. As every result of cross-validation is an average of N folds, the results are more stable, even for very low-resource setups.
- The experiments were run in three setups: low, mid and large.
- The low data setup corresponds to 20-fold cross-validation, where the model is trained on 1 20 th of the dataset and tested on the remaining 19 folds.
- The mid and large setups correspond to 10-fold cross-validation, where in mid setup the model is trained on 1 10 th of the data and tested on the other 9 folds, and vice versa for the large setup.
- Domain Setups. MULTI 3 NLU ++ contains training and evaluation data for two domains: BANKING and HOTELS. It thus enables evaluation of NLU systems in three domain setups: in-domain, crossdomain and all-domain.
- In the in-domain setup, the model is trained and evaluated on the same domain, that is, we are testing how well the model can generalise to unseen user utterances while operating in the same intent space as the training data and without any domain distribution shift.
- In the cross-domain setup, the model is trained on one domain and tested on the other domain. Here, we evaluate on the union of label sets of two domains rather than on the intersection as done by Casanueva et al. (2022).
- In this setup we are testing how well a model can generalise to a new, unseen domain including intents unseen in training.
- In the all-domain setup, we train and evaluate the models on data from both domains. In this setup, we are testing how models perform on the larger label set (where some labels are shared between the domains) when examples are provided for all classes.
- Language Setups. MULTI 3 NLU ++, offering comparable sets of annotated data points across languages, allows for systematic comparisons of multilingual dialogue NLU systems on languages with different amounts of resources and diverse typological properties.
- We evaluate NLU systems in two setups: in-language and cross-lingually.
- Crosslingual benchmarking is conducted with the established approaches: (i) direct transfer using multilingually pretrained large language models (e.g., XLM-R; Conneau et al. (2020)), and (ii) transfer via translation, i.e., when either the test utterances are translated into the source language (Translate-Test) or the training utterances are translated into the target language (Translate-Train).
- We source our translations from the M2M100 translation model (Fan et al., 2021).

### Classification-Based Methods
- The experiment evaluates two standard classification approaches to intent detection: (i) MLP-based with a fixed encoder; and (ii) full-model fine-tuning.
- Prior work has demonstrated that strong intent detection results can be attained without finetuning the full encoder model both in monolingual (Casanueva et al., 2020) and multilingual setups (Gerz et al., 2021).
- The idea is to use a fixed efficient sentence encoder to encode sentences and train only the multi-layer perceptron (MLP) classifier on top of that to identify the intents.
- As we are dealing with multi-label classification, we use a sigmoid layer on top of the classifier, similar to Casanueva et al. (2022).
- Intent classes for which the probability scores are higher than a threshold are considered active.
- As in Casanueva et al. (2022), we use the threshold of 0.3.
- In the experiments we evaluate two state-of-the-art multilingual sentence encoders: 1) mpnet, a multilingual sentence encoder trained using multilingual knowledge distillation (Reimers and Gurevych (2020)); 2) LaBSE, a language-agnostic BERT sentence encoder (Feng et al., 2022) which was trained using dual encoder training.
- LaBSE was especially tailored to the low-resource languages.
- The models were loaded from the sentence transformers library (Reimers and Gurevych, 2019).
- In the full finetuning setup, currently standard in multilingual NLP, a multilingually pretrained encoder is finetuned on task-specific data.
- In our case, XLM-Roberta (Conneau et al., 2020) is finetuned for multi-label intent detection.
- Hyperparameters
- All classification models were trained with the same hyperparameters.
- We use the same hyperparameters as in Casanueva et al. (2022).
- The MLP classifier for the MLP-based approach consists of 1 hidden layer with 512 nodes and with tanh activation function.
- The learning rate was set to 0.003, with linear weight decay.
- In full finetuning, the model is trained with 2e − 5 linear rate.
- For all setups, the models were trained for 500 epochs with the AdamW optimiser.
- We used a batch size of 32.
- In all setups, the evaluation metric was micro-F-1 score.
- Results
- Table 2 presents the comparison between the full fine-tuning and MLP-based baselines.
- We compare the fully finetuned XLM-R encoder with the MLP-based setup using two underlying sentence encoders, LaBSE (Feng et al., 2022) and mpnet (Reimers and Gurevych, 2019).
- The results in the comparison are for the in-language in-domain 10-Fold cross-validation setup.
- The results demonstrate that for the few-shot setup the MLP-based approach works consistently better than full finetuning across domains and languages.
- We assume that the reason is that the MLP-based approach is more parameter efficient than full fine-tuning making it more suited for few-shot setups.
- Thus, further in this section we focus on MLP-based models, due to their computational efficiency and strong performance.
- The main MLP-based results are presented in tables 3 and 4 for in-domain intent detection for the in-language and zero-shot cross-lingual setups, respectively.
- When we compare the performance on low-and high-resource languages, although the sizes and content of the training data are the same across languages, we observe a large gap between the performances of the same models on Spanish and Amharic.
- Additionally, the results in Table 3 reveal the properties of the multilingual sentence encoders with respect to the intent detection task.
- While mpnet performs consistently better on Spanish (our high-resource language), LaBSE is a much stronger encoder for low-resource Amharic.
- The differences are especially pronounced in the lowdata setups (20-Fold and 10-Fold).
- While for Spanish this difference can be recuperated with the training data size (cf. Large setup), for Amharic these differences persist and...

### Question Answering Baseline
- We implement baselines for intent detection using question-answering (QA) models (Namazifar et al., 2021).
- To formulate intent detection as an extractive question-answering task, the utterance is appended with "yes. no. [UTTER-ANCE]" and acts as the context.
- Then all the intent labels are converted into questions as "Is the intent to ask about [INTENT_DESCRIPTION]" where INTENT_DESCRIPTION is the free-form descrip-tion of the intent.
- The QA model must learn to predict the span as "yes" if the specific intent is present in the current utterance and "no" otherwise.
- Thus, if an utterance has multiple intents, the corresponding questions should extract the span as "yes".
- The overhead is that every utterance will be accompanied by X questions where X is the number of intent labels in the current setup.
- To build an extractive QA model for our task, we first fine-tune a multilingual language model with a general-purpose QA dataset such as SQuAD v1 (Rajpurkar et al., 2016) and then with our dataset.
- This sequential fine-tuning process has shown impressive performance on recent monolingual intent detection datasets (Namazifar et al., 2021).
- We investigate if extractive QA can act as a strong baseline for multilingual multi-label intent detection.
- As SQuAD is available in English, we only consider experiments where the transfer is from English to another language (unless specified otherwise).
- Hyperparameters We follow the same hyperparameters as Casanueva et al. (2022) -fine-tune for five epochs with learning rate of 3e-5, weight decay is 0, and batch size is 32.
- We fine-tune the general-purpose QA model with English data and report the zero-shot performance on the remaining languages across all of the training data setups in Table 5.
- The performance on English multi-intent detection is lower than the results reported in Casanueva et al. (2022) as the underlying QA model is trained with a multilingual language model.
- We notice a drastic drop in the zero-shot transfer performance across all languages and domains. This is in line with recent findings that zero-shot transfer is harder for dialogue tasks (Ding et al., 2022;Hung et al., 2022;Majewska et al., 2022) as opposed to other crosslingual tasks (Hu et al., 2020).
- We also find that the drop in performance across the languages is inversely correlated with the amount of training data present per language during the pre-training of the XLM-R model (Conneau et al., 2020).
- We consider two different setups for evaluating the zero-shot performance: the questions and yes/no spans are in English (Q: en) versus questions and yes/no spans are in the target language (Q: tgt).
- We find that using questions in target language drastically improves the performance of Marathi (avg 10.8%) and then Amharic (avg 3%).
- Spanish has a mixed performance while Turkish shows a consistent drop in performance. This suggests that transfer learning could only rely on shallow pattern matching between the scripts of the question and the utterance than improved cross-lingual understanding.
- Going by the results in Table 10, the performance of QA-based intent detection is lower than MLP-based intent detection models, in contrast with the observation for monolingual multi-label intent detection models (Casanueva et al., 2022).

## Discussion and Future Work
- Languages: High-and Low-Resource Language models such as multilingual BERT and XLM-R have been pretrained on over 100 languages.
- However, their representational power is uneven for high-and low-resource languages (Lauscher et al., 2020;Ebrahimi et al., 2022).
- MULTI 3 NLU ++ includes the same training and evaluation data for all languages, allowing us to systematically analyse the models' performance on dialogue NLU for both high-and low-resource languages.
- We compare the performance for different languages across domains for the in-domain setup.
- As the results in Fig. 2 demonstrate, the overall trend in performance is the same across languages: with more training data, we gain higher overall performance.
- Interestingly, the absolute numbers are indicative of the resources available in pre-training for a given language.
- For example, Amharic (am) has the lowest performance while Spanish (es) has the highest performance.
- We consider the intent detection task in the cross-domain inlanguage setting. Here, the models trained on the HOTELS domain are tested on BANKING and vice versa.
- We evaluate the models on data from the same language used in training.
- The results in Fig. 3 corroborate the findings from the in-domain experiments: the lower-resource the language is, the lower the performance on the task is.
- It is noticeable that the performance in the cross-domain setup is much lower than for the in-domain setup, additionally exposing the complexity of MULTI 3 NLU ++ .
- Additionally, in Fig. 3 we observe that for the cross-domain setup, highresource languages benefit more from the increase in training data size than low-resource languages.
- This shows that the gap in performance on low-and high-resource languages is rooted not only in the amount of training data available, but also in the representational power of multilingual models for low-resource languages.
- Future Directions
- We plan to expand the dataset to include paraphrases of the translated utterances.
- We will use paraphrase to mitigate the effects of translationese, i.e. artifacts present in the translation output that were transferred from the source language during the translation process, which has been shown to result in the overestimation of system performance at evaluation time for various conversational tasks (Majewska et al., 2022).
- We propose the adoption of the instructions for the manual paraphrase task in Freitag et al. (2020), adapting them for the task of paraphrasing utterances in the dialogue domain.
- Another possible direction for future work would be to further diversify the dataset by including additional languages, i.e. with a focus on increasing coverage of language families, branches, and/or scripts, or properties that pose particular challenges in multilingual settings (e.g. free word order in Machine Translation, etc.).

## Conclusion
- We collected MULTI 3 NLU ++
- The dataset incorporates core properties of from its predecessor NLU++ (Casanueva et al., 2022): a multi-intent and slot-rich ontology, a mixture of generic and domain-specific intents for reusability, and utterances that are based on complex scenarios.
- We investigate these properties in a multilingual setting for Spanish, Marathi, Turkish, and Amharic.
- We implemented MLP-based and QA-based intent detection baselines across different data setups, transfer learning setups, and multilingual models.
- From a wide range of observations, we highlight that zero-shot performance improves when the source language has lower resources.
- Multilingual question-answering models rely on shallower heuristics than cross-lingual language understanding.
- We propose that the community use this dataset to advance research in multilingual taskoriented dialogue.
