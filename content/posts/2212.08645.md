---
title: "Efficient Conditionally Invariant Representation Learning"
date: 2022-12-16T18:39:32.000Z
author: "Roman Pogodin, Namrata Deka, Yazhe Li, Danica J. Sutherland, Victor Veitch, Arthur Gretton"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08645v1.webp" # image path/url
    alt: "Efficient Conditionally Invariant Representation Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08645)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08645).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/efficient-conditionally-invariant).

# Abstract
- Conditional Independence Regression Covariances (CIRCE) is a measure of conditional independence for multivariate continuous-valued variables
- CIRCE can be used as a regularizer in settings where we wish to learn neural features of data to estimate a target, while being conditionally independent of a distractor
- CIRCE is superior to previous methods on challenging benchmarks

# Paper Content

## INTRODUCTION
- Fairness: Z is some protected attribute (e.g., race or sex) and the condition ϕ(X) ⊥ ⊥ Z | Y is the equalized odds condition (Mehrabi et al., 2019).
- Domain invariant learning: Z is a label for the environment in which the data was collected (e.g., if we collect data from multiple hospitals, Z i labels the hospital that the ith datapoint is from). The condition ϕ(X) ⊥ ⊥ Z | Y is sometimes used as a target for invariant learning (e.g., Long et al., 2018;Tachet des Combes et al., 2020;Goel et al., 2020;Jiang & Veitch, 2022).
- Causal representation learning: Neural networks may learn undesirable "shortcuts" for their tasks -e.g., classifying images based on the texture of the background. To mitigate this issue, various schemes have been proposed to force the network to use causally relevant factors in its decision (e.g., Veitch et al., 2021;Makar et al., 2022;Puli et al., 2022).
- The structural causal assumptions used in such approaches imply conditional independence relationships between the features we would like the network to use, and observed metadata that we may wish to be invariant to. These approaches then try to learn causally structured representations by enforcing this conditional independence in a learned representation.
- In this paper, we will be largely agnostic to the motivating application, instead concerning ourselves with how to learn a representation ϕ that satisfies the target condition. Our interest is in the (common) case where X is some high-dimensional structured data -e.g., text, images, or video -and we would like to model the relationship between X and (the relatively low-dimensional) Y, Z using a neural network representation ϕ(X).
- There are a number of existing techniques for learning conditionally invariant representations using neural networks (e.g., in all the motivating applications mentioned above). Usually, however, they rely on the labels Y being categorical with a small number of categories. We develop a method for conditionally invariant representation learning that is effective even when the labels Y and attributes Z are continuous or moderately high-dimensional.
- To understand the challenge, it is helpful to contrast with the task of learning a representation ϕ satisfying the marginal independence ϕ(X) ⊥ ⊥ Z. To accomplish this, we might define a neural network to predict Y in the usual manner, interpret the penultimate layer as the representation ϕ, and then add a regularization term that penalizes some measure of dependence between ϕ(X) and Z. As ϕ changes at each step, we'd typically compute an estimate based on the samples in each mini-batch (e.g., Beutel et al., 2019;Veitch et al., 2021). The challenge for extending this procedure to conditional invariance is simply that it's considerably harder to measure.
- More precisely, as conditioning on Y "splits" the available data,1 we require large samples to assess conditional independence. When regularizing neural network training, however, we only have the samples available in each mini-batch: often not enough for a reliable estimate.
- The main contribution of this paper is a technique that reduces the problem of learning a conditionally independent representation to the problem of learning a marginally independent representation, following a characterization of conditional independence due to Daudin (1980). We first construct a particular statistic ζ(Y, Z) such that enforcing the marginal independence ϕ(X)  (Song et al., 2009;Grunewalder et al., 2012;Park & Muandet, 2020;...

## EFFICIENT CONDITIONAL INDEPENDENCE REGULARIZER
- General-purpose characterization of conditional independence
- CIRCE: conditional independence criterion based on this characterization
- Zero if and only if conditional independence holds (under certain required conditions)
- finite sample estimate with convergence guarantees
- strategies for efficient estimation from data

## CONDITIONAL INDEPENDENCE
- Definition 2.1 (Daudin, 1980)
- Proposition 2.2 (Daudin, 1980)
- Lemma B.1 (Daudin, 1980)
- The reduction to g not depending on Y is crucial for our method: when we are learning the representation ϕ(X), then evaluating the conditional expectations E X [g(ϕ(X), Y ) | Y ] from Proposition 2.2 on every minibatch in gradient descent requires impractically many samples, but does not depend on X and so can be pre-computed before training the network.

## CONDITIONAL INDEPENDENCE REGRESSION COVARIANCE (CIRCE)
- characterization (4) of conditional independence is still impractical, as it requires checking all pairs of square-integrable functions g and h.
- we will now transform this condition into an easy-toestimate measure that characterizes conditional independence, using kernel methods.
- a kernel k(x, x ) is a symmetric positive-definite function k : X ×X → R.
- a kernel can be represented as an inner product k(x, x ) = φ(x), φ(x ) H for a feature vector φ(x) ∈ H, where H is a reproducing kernel Hilbert space (RKHS).
- these are spaces H of functions f : X → R, with the key reproducing property φ(x), f H = f (x) for any f ∈ H.
- for M points we denote K X • a row vector of φ(x i ), such that K Xx is an M × 1 matrix with k(x i , x) entries and K XX is an M × M matrix with k(x i , x j ) entries.
- for two separable Hilbert spaces G, F, a Hilbert-Schmidt operator A : G → F is a linear operator with a finite Hilbert-Schmidt norm where {g j } j∈J is an orthonormal basis of G (for finite-dimensional Euclidean spaces, obtained from a linear kernel, A is just a matrix and A HS its Frobenius norm).
- the Hilbert space HS(G, F) includes in particular the rank-one operators ψ ⊗ φ for ψ ∈ F, φ ∈ G, representing outer products, See Gretton (2022, Lecture 5) for further details.
- we next introduce a kernelized operator which (for RKHS functions g and h) reproduces the condition in (4), which we call the Conditional Independence Regression CovariancE (CIRCE).
- definition 2.4 (CIRCE operator) gives rise to the same expression as in (4),
- the assumption that the kernels are bounded in Definition 2.4 guarantees Bochner integrability (Steinwart & Christmann, 2008, Def. A.5.20), which allows us to exchange expectations with inner products as above: the argument is identical to that of Gretton (2022, Lecture 5) for the case of the unconditional feature covariance.
- for unbounded kernels, Bochner integrability can still hold under appropriate conditions on the distributions over which we take expectations, e.g. a linear kernel works if the mean exists, and energy distance kernels may have well-defined feature (conditional) covariances when relevant moments exist (Sejdinovic et al., 2013).
- our goal now is to define a kernel statistic which is zero iff the CIRCE operator C c XZ|Y is zero.
- one option would be to seek the functions, subject to a bound such as g G ≤ 1 and f F ≤ 1, that maximize (8); this would correspond to computing the largest singular value of C c XZ|Y .
- for unconditional covariances, the equivalent statistic corresponds to the Constrained Covariance, whose computation requires solving an eigenvalue problem (e.g. Gretton et al., 2005a, Lemma 3).
- we instead follow the same procedure as for unconditional kernel dependence measures, and replace the spectral norm with the Hilbert-Schmidt norm (Gretton et al., 2005b): both are zero when C c XZ|Y is zero, but as we will see in Section 2.3 below, the Hilbert-Schmidt norm has a simple closed-form empirical expression, requiring no optimization.
- next, we show that for rich enough RKHSes G, F (including, for instance, those with a Gaussian kernel), the Hilbert-Schmidt norm of C c XZ|Y characterizes conditional independence.
- the "if" direction is immediate...

## EMPIRICAL CIRCE ESTIMATE AND ITS USE AS A CONDITIONAL INDEPENDENCE

## REGULARIZER
- To estimate CIRCE, we first need to estimate the conditional expectation
- The CIRCE operator can be written as
- We need two datasets to compute the estimator: a holdout set of size M used to estimate conditional expectations, and the main set of size B (e.g., a mini-batch).
- The holdout dataset is used to estimate conditional expectation µ ZY | Y with kernel ridge regression. This requires choosing the ridge parameter λ and the kernel parameters for Y . We obtain both of these using leave-one-out crossvalidation; we derive a closed form expression for the error by generalizing the result of Bachmann et al. (2022) to RKHS-valued "labels" for regression (see Theorem C.1).
- The following theorem defines an empirical estimator of the Hilbert-Schmidt norm of the empirical CIRCE operator, and establishes the consistency of this statistic as the number of training samples B, M increases.
- The proof and a formal description of the conditions may be found in Appendix C.2
- Theorem 2.7 states that the following estimator of CIRCE for B points and M holdout points (for the conditional expectation): ) when the regression in Equation ( 30) is wellspecified.
- K XX and K Y Y are kernel matrices of X and Y ; elements of K c ZZ are defined as ] characterizes how well-specified the solution is and p ∈ (0, 1] describes the eigenvalue decay rate of the covariance operator over Y .
- The notation O p (A) roughly states that with any constant probability, the estimator is O(A).
- Remark. For the smoothly well-specified case we have β = 2, and for a Gaussian kernel p is arbitrarily close to zero, giving a rate O p (1/ √ B + 1/M 1/4 ).
- The 1/M 1/4 rate comes from conditional expectation estimation, where it is minimax-optimal for the well-specified case (Li et al., 2022).
- Using kernels whose eigenvalues decay slower than the Gaussian's would slow the convergence rate (see Li et al., 2022, Theorem 2).
- The algorithm is summarized in Algorithm 2.
- We can further improve the computational complexity for large training sets with random Fourier features (Rahimi & Recht, 2007); see Appendix D.
- Holdout data Leave-one-out (Theorem C.1) for λ (ridge parameter) and σ y (parameters of Y kernel):
- We can use of our empirical CIRCE as a regularizer for conditionally independent regularization learning, where the goal is to learn representations that are conditionally independent of a known distractor Z.
- We switch from X to an encoder ϕ θ (X).
- If the task is to predict Y using some loss L(ϕ θ (X), Y ), the CIRCE regularized loss with the regularization weight γ > 0 is as follows:

## RELATED WORK
- Kernel-based measures of conditional independence are reviewed
- The conditional kernel cross-covariance was first introduced as a measure of conditional dependence
- Following this work, a kernel-based conditional independence test (KCI) was proposed by Zhang et al. (2011)
- The latter test relies on satisfying Proposition 2.2 leading to a statistic 4 that requires regression of ϕ(X) on Y in every minibatch
- More recently, Quinzan et al. (2022) introduced a variant of the Hilbert-Schmidt Conditional Independence Criterion (HSCIC; Park & Muandet, 2020) as a regularizer to learn a generalized notion of counterfactually-invariant representations (Veitch et al., 2021)
- Estimating HSCIC(X, Z|Y ) from finite samples requires estimating the conditional mean-embeddings µ X,Z|Y , µ X|Y and µ Z|Y via regressions
- HSCIC requires three times as many regressions as CIRCE, of which two must be done online in minibatches to account for the conditional cross-covariance terms involving X.
- We will compare against HSCIC in experiements, being representative of this class of methods, and having been employed successfully in a setting similar to ours.
- Alternative measures of conditional independence make use of additional normalization over the measures described above.
- The Hilbert-Schmidt norm of the normalized cross-covariance was introduced as a test statistic for conditional independence by Fukumizu et al. (2008)
- Huang et al. (2020) proposed using the ratio of the maximum mean discrepancy (MMD) between P X|ZY and P X|Y , and the MMD between the Dirac measure at X and P X|Y , as a measure of the conditional dependence between X and Z given Y .
- The additional normalization terms in these statistics can result in favourable asymptotic properties when used in statistical testing.
- This comes at the cost of increased computational complexity, and reduced numerical stability when used as regularizers on minibatches.
- Another approach, due to Shah & Peters (2020), is the Generalized Covariance Measure (GCM). This is a normalized version of the covariance between residuals from kernel-ridge regressions of X on Y and Z on Y (in the multivariate case, a maximum over covariances between univariate regressions is taken).
- As with the approaches discussed above, the GCM also involves multiple regressions -one of which (regressing X on Y ) cannot be done offline.
- Since the regressions are univariate, and since GCM simply regresses Z and X on Y (instead of ψ(Z, Y ) and φ(X) on Y ), we anticipate that GCM might provide better regularization than HSCIC on minibatches.

## EXPERIMENTS
- We conduct experiments addressing two settings: (1) synthetic data of moderate dimension, to study effectiveness of CIRCE at enforcing conditional independence under established settings (as envisaged for instance in econometrics or epidemiology); and (2) high dimensional image data, with the goal of learning image representations that are robust to domain shifts.
- We compare performance over all experiments with HSCIC (Quinzan et al., 2022) and GCM (Shah & Peters, 2020).
- We report in-domain MSE loss, and measure the level of counterfactual invariance of the predictor using the VCF (Quinzan et al., 2022, eq. 4; lower is better).
- Given X = (A, Y, Z), Multivariate Cases
- We present results on 2 multivariate cases: case 1 has high dimensional Z and case 2 has high dimensional Y .
- For each multivariate case, we vary the number of dimensions d = {2, 5, 10, 20}.
- To visualize the trade-offs between in-domain performance and invariant representation, we plot the Pareto front of MSE loss and VCF.
- With high dimensional Z (Figure 2A), CIRCE and HSCIC have a similar trade-off profile, however it is notable that GCM needs to sacrifice more in-domain performance to achieve the same level of invariance. This may be because the GCM statistic is a maximum over normalized covariances of univariate residuals, which can be less effective in a multivariate setting.
- For high dimensional Y (Figure 2B), the regression from Y to ψ(Z) is much harder. We observe that HSCIC becomes less efficient with increasing d until at d = 20 it fails completely, while GCM still sacrifices more in-domain performance than CIRCE.

## DSPRITES
- Of the six independent generative factors in d-Sprites, we choose the y-coordinate of the object as our target Y and the x-coordinate of the object in the image as our distractor variable Z.
- Our neural network consists of three convolutional layers interleaved with max pooling and leaky ReLU activations, followed by three fully-connected layers with 128, 64, 1 unit(s) respectively.
- Linear dependence
- We sample images from the dataset as per the linear relation
- We then translate all sampled images (both in-domain and OOD) vertically by ξ y , resulting in an observed object coordinate of (Z, Y + ξ y ).
- In this case, linear residual methods, such as GCM, are able to sufficiently handle the dependence as the residual will also penalize the network's dependence on the observed x-coordinate to predict Y .
- Whereas CIRCE which uses a feature map ψ(Z) can capture higher order features.
- Results are shown in Figure 5: we see again that CIRCE performs best, followed by HSCIC, with GCM doing poorly. Curiously, GCM performance does still improve slightly on OOD data as regularization increases -we conjecture that the encoder ϕ(X) may extract non-linear features of the coordinates.

## EXTENDED YALE-B
- CIRCE is a better regressor than HSCIC on the natural image dataset of Extended Yale-B Faces
- GCM suffers from numerical instability in this example, which leads to poor performance

## DISCUSSION
- Introduced CIRCE: a kernel-based measure of conditional independence
- Can be used as a regularizer to enforce conditional independence between a network's predictions and a pre-specified variable with respect to which invariance is desired
- Well suited to minibatch training
- By contrast, alternative conditional independence regularizers require an additional regression step on each minibatch, resulting in a higher variance criterion which can be less effective in complex learning tasks

## APPENDICES A CONDITIONAL INDEPENDENCE DEFINITIONS

## Sufficient condition: E[gh
- The theorem states that if two conditions are met, then the function is integrable.
- The first condition is that the function is a simple function.
- The second condition is that the function is integrable.
- The theorem states that if two conditions are met, then the function is dense in a certain space.
- The first condition is that the function is integrable.
- The second condition is that the function is dense in a certain space.
- The theorem states that if two conditions are met, then the function is L 2 -universal.
- The first condition is that the function is L 2 -universal.
- The second condition is that the function is integrable.
- The theorem states that if two conditions are met, then the function is CIRCE.
- The first condition is that the function is CIRCE.

## C PROOFS FOR ESTIMATORS C.1 ESTIMATING THE CONDITIONAL MEAN EMBEDDING
- We will construct an estimate of the term E [ψ(Z, Y ) | Y ] that appears inside CIRCE, as a function of Y .
- The estimate is well-specified when there exists a Hilbert-Schmidt operator , where H Y is the RKHS on Y with feature map ψ(y) (Li et al., 2022).
- We now consider the case relevant to our setting, where Q := (Z, Y ).
- We define6 ψ(Z, Y ) = ψ(Z) ⊗ ψ(Y ), which for radial basis kernels (e.g. Gaussian, Laplace) is L 2 -universal for (Z, Y ).
- We then write | Y ] can be found with kernel ridge regression (Grunewalder et al., 2012;Li et al., 2022): has O(1/B) bias and O p (1/ √ B) deviation from the mean for any fixed probability of the deviation.
- Proof. The bias is straightforward: For the variance, first note that our estimator has bounded differences. Denote K T T = K Y Y K c ZZ and t = (y, z), if we switch one datapoint (x i , t i ) to (x i , t i ) and denote the vectors with switch coordinates as X i , T i ≤ (2 + 4(B − 1))K x max K t max ≤ (4B − 2)K x max K y max K c z max . Therefore, for any index i
- We can now use McDiarmid's inequality (McDiarmid, 1989) with meaning that for any > 0 (SRC) F ∈ [G] β for β ∈ [1, 2] (note that β < 1 would include the misspecified setting).
- Lemma C.4. Consider the well-specified case of conditional expectation estimation (see Li et al., 2022).
- For bounded kernels over X, Z, Y and a (β, p)-kernel over Y , F (y) = E [ψ(Z) | Y ] (y), bounded F ≤ C F , and M points used to estimate F , define the conditional expectation estimate as where λ M = Θ(1/M β+p ).
- Then, the estimator Tr K XX Kc ZZ /(B(B − 1)) of the "true" CIRCE estimator (i.e., with the actual conditional expectation) deviates from the true value as O p (1/M (β−1)/(2(β+p)) ).
