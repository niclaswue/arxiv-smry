---
title: "NVAutoNet: Fast and Accurate 360$^{\circ}$ 3D Perception For Self Driving"
date: 2023-03-23T00:55:48.000Z
author: "Trung Pham, Mehran Maghoumi, Wanli Jiang, Bala Siva Sashank Jujjavarapu, Mehdi Sajjadi Xin Liu and 6 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-12976v1.webp" # image path/url
    alt: "NVAutoNet: Fast and Accurate 360$^{\circ}$ 3D Perception For Self Driving" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.12976)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.12976).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/nvautonet-fast-and-accurate-360-circ-3d).

# Abstract
- Autonomous vehicles need 3D world perception in real-time
- Introduce an end-to-end surround camera perception system
- System takes time-synced camera images as input and produces 3D signals
- Network is modular and end-to-end
- Network training is done in one single stage
- High accuracy while running at 53 fps on NVIDIA Orin SoC
- Robust to sensor mounting variations
- Can be quickly customized for different vehicle types
- Successfully deployed and tested on real roads

# Paper Content

## Introduction
- Problem of reconstructing 3D world from single camera is difficult.
- Autonomous vehicles need precise 3D world perception.
- Consumer vehicle industry has relied on single camera and radar for ADAS.
- Multi-camera and multi-radar systems becoming norm for major consumer vehicle manufactures.
- NVAutoNet designed to address accurate 3D estimation, camera independence, scalability, and efficiency.
- NVAutoNet uses hardware-aware NAS for high accuracy and low latency.

## Related work
- No previous work achieved all the goals listed
- Summary of related methods to components of work

### Multi-camera fusion and bev perception.
- Monocular camera perception tasks have been established in computer vision.
- Multi-camera perception has recently attracted interest.
- Modern methods use an early fusion approach, where all input images from multiple cameras are injected into a single perception model.
- Late fusion approach requires a shared representation independent of cameras.
- BEV representation is a common choice and is preferable by downstream modules.

### Perspective to 3d view transformation.
- Transforming perspective view to 3D view is an ill-posed problem
- Classical methods assume a flat-world model or require depth information
- Data driven and machine learning based view transformation approaches are becoming more popular
- Recent works use MLP operations on columns independently
- Transformers architectures have been used for view transformation tasks
- Disadvantages of Transformers based methods include sparse set of BEV queries and high resource requirements

### Perception for autonomous driving
- Autonomous driving requires the perception module to detect and recognize objects from the environment
- Three main tasks are selected to demonstrate the performance of the perception system: obstacles, freespaces and parking spaces
- 3D object detection using 2D images is challenging due to lack of depth information
- Prior methods often relied on external sub-networks, such as depth estimation
- End-to-end methods have become more popular
- Detection of free, driveable space is important for improving autonomous driving safety
- Most existing methods focus on detecting 2D freespace from monocular cameras

## Nvautonet

### Overview
- Inputs to NVAutoNet are collections of N view camera images and camera parameters
- 2D image features are extracted and uplifted to BEV features
- BEV features from multiple images are fused into a single BEV feature map
- BEV encoder extracts high-level BEV features
- Outputs are 3D signals such as positions, sizes and orientations of obstacles, parking spaces, free spaces

### 2d image feature extractors
- Input images are fed to a feature extractor to compute feature maps
- Feature extractors are based on convolutional architectures
- Network configurations are optimized for latency and accuracy
- Feature maps are up-sampled and merged to form multi-level semantically rich feature maps
- Different input images can share the same feature extractor

### Image-to-bev transformation and fusion
- BEV representation is commonly used in self-driving
- BEV plane passes through vehicle's rig center and is orthogonal to Z axis
- BEV grid G bev is discretized with dimensions W bev Ã— H bev
- Ego vehicle's position is at BEV grid's center
- MLP network is used to train camera-to-BEV transformation function
- Geometric relationship between row/column positions in image plane and radial/angular positions in BEV plane
- Polynomial curves from projecting individual image columns onto BEV plane
- MLP based BEV view transformation method consumes camera intrinsic/extrinsic parameters
- BEV transformation network transforms image column features into pseudo BEV features
- BEV features are summed up to produce fused BEV feature map
- BEV grid resolution affects detection range and accuracy
- Irregular BEV grid with Polar coordinates is designed for efficient representation

### Bev feature extractor
- Adopt a CNN backbone to extract high-level features from fused BEV feature map
- Goal of 3D object detection is to localize, classify and estimate dimensions and orientations of objects in 3D space
- Objects are represented by category and 3D cuboid with 9 DOF
- Set prediction approach removes need for NMS post-processing
- Network includes four lightweight heads to predict class distributions and 3D cuboid parameters
- Training loss is computed in two-step fashion with matching cost function considering classification, position, dimension, and orientation costs
- Regression loss is composed of position, shape and orientation losses

### 3d freespace
- 3D obstacle detection covers classifiable vehicles and vulnerable road users
- Autonomous vehicles must drive safely and avoid obstacles
- Freespace region is represented as a radial distance map
- Radial distance map is composed of equiangular bins and radial distance values
- 3D freespace detection network has a shared 'neck' and two separate heads
- Training losses include a polar intersection-over-union loss, similarity loss, and focal loss

### 3d parking space
- Autonomous driving requires the ability to localize and classify parking spaces.
- Parking spaces are represented as oriented rectangles with parameters.
- Three different parking profiles are supported: angled, parallel and perpendicular.
- The parking detection network consists of classification and regression heads.
- The classification head predicts per-profile confidence scores.
- The regression head predicts the parking space oriented bounding boxes.
- The training loss is similar to the obstacle detection task.

## Multi-task learning and loss balancing
- NVAutoNet is a multi-task network that learns multiple tasks at the same time.
- The same training recipe is not necessarily optimal for all tasks.
- Loss weights are assigned to each task to represent their importance.
- Loss weights are calculated by taking the inverse of the loss sum and scaling it by a task loss prior.

## Experimental evaluation
- NVAutoNet is optimized for both accuracy and latency
- NVAutoNet is designed for real self-driving applications with a detection range of up to 200 meters

### Datasets and evaluation metrics
- Datasets consist of real, simulated and augmented reality data
- 2.2M training scenes, 400K validation scenes and 177K testing scenes
- Lidar data used to generate ground truth labels
- Noisy labels due to view point differences between Lidar and camera sensors
- Calculate obstacle detection metrics based on TP, FP and FN
- Calculate precision, recall, F1-score, AP and mAP KPIs
- Calculate position, orientation and shape errors
- Calculate relative gap, absolute gap, success rate and smoothness metrics for 3D Freespaces
- Calculate precision, recall, F1 and AP metrics for 3D Parking Spaces
- Calculate mean IoU values for true positive detections

### Latency performance
- NVAutoNet was exported using NVIDIA TensorRT and timed on NVIDIA Orin.
- NVAutoNet latency was 18.7 ms (8 cameras input), leading to a frame-rate of 53 FPS.

### Quantitative performance
- Table 5 reports obstacle detection accuracy results
- BEVFormer achieved 0.58 mAP, better than our 0.46 mAP
- Detection accuracy drops significantly with distance
- Most methods in NuScenes benchmark are heavily optimized for accuracy
- FastBEV is real-time but has low accuracy

### Generalization to different vehicle lines
- Tested NVAutoNet, originally developed for cars, on trucks
- Differences in intrinsic and extrinsic parameters between the two platforms
- Model redesign and large-scale data collection not required when deploying perception model on different platforms
- Model finetuing using small training dataset sufficient
- Model-C (fine-tuned from Model-B) significantly outperforms Model-A and Model-B

## Conclusion and future work
- Existing BEV perception models require a lot of computational resources and are not suitable for self-driving applications.
- NVAutoNet is optimized for accuracy and latency balance.
- NVAutoNet is able to run faster than real-time at 53 FPS on NVIDIA Orin SoC.
- NVAutoNet is used for 3D detection heads, parking space detection, obstacles and freespaces detection.
