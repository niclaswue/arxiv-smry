---
title: "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective"
date: 2022-10-16T17:24:06.000Z
author: "Ping Yang, Junjie Wang, Ruyi Gan, Xinyu Zhu, Lin Zhang, Ziwei Wu, Xinyu Gao, Jiaxing Zhang, Tetsuya Sakai"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-08590v2.webp" # image path/url
    alt: "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.08590)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.08590).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/zero-shot-learners-for-natural-language).

# Abstract
- We propose a new paradigm for zero-shot learners that is format agnostic,
- i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis.
- Zero-shot learning aims to train a model on a given task such that it can address new learning tasks without any additional training.
- Our approach converts zero-shot learning into multiple-choice tasks,
- avoiding problems in commonly used large-scale generative models such as FLAN.
- It not only adds generalization ability to models but also significantly reduces the number of parameters.
- Our method shares the merits of efficient training and deployment.
- Our approach shows state-of-the-art performance on several benchmarks and produces satisfactory results on tasks such as natural language inference and text classification.
- Our model achieves this success with only 235M parameters, which is substantially smaller than state-of-the-art models with billions of parameters.

# Paper Content

## Introduction
- Large-scale language models have made substantial progress in a wide variety of tasks, including text classification, natural language inference, and commonsense reasoning.
- Zero-shot learning aims to predict labels on datasets from novel domains, and most solutions can be framed in the prompt tuning framework.
- However, these frameworks suffer from their inherent problems, and thus limit their potential in zero-shot learners.
- One immediate problem is that these models are often hard to be trained, making the deployment and consumption difficult.
- Secondly, manual processing is required when addressing zero-shot problems. For instance, T0 builds 2, 073 prompts to handle more than 170 tasks.
- Lastly, existing models employ a single direction paradigm, either auto-regressive models or sequence-to-sequence, resulting in inadequate usage of information from both directions.
- As an example, PMLM tries to implement a zero-shot learner, which is shown in Figure 1 (c). Note that recent work (Liu et al., 2019a) state that PMLM is more suitable than PLM for Natural Language Understanding (NLU) tasks. However, it has to be fine-tuned on the task-specific samples to initialize the classifier instead of randomly initializing the classifier. Therefore, the ability of PMLM is limited when dealing with zeroshot scenarios.
- To address the aforementioned problems, we introduce a light-weight framework, called Unified Multiple Choice model (UniMC), proposing a novel MC tuning.
- The proposed MC tuning has the following advantages: i) parameter updating only happens in the MC training phase, and ii) facilitating the deployment.
- To reduce the manual processing, we only formulate one candidate option prompt format and one question prompt format.
- Note that we also consider the case without any question prompt format. Under this setting, we can treat labels as options rather than building verbalizer maps and providing its text information to the models as before.
- We therefore can learn the information from labels directly.
- To this end, we convert the problematic classifiers to options.
- One immediate question is how to choose an option efficiently and unambiguously. Therefore, as shown in Section 3.2, we develop an option-mask tokens [O-MASK] to predict "yes" or "no" before each option.
- A two-step process is introduced to output the desired options. First, similar to Masked Language Modeling (MLM) (Devlin et al., 2019), we conduct Option MLM (O-MLM) to recover the "yes" or "no" for each option. Next, we propose an Option Prediction (OP) method to compute proper options.
- With extensive experiments on multiple challenging benchmarks, we demonstrate that our approach's performance outperforms state-of-the-art baselines, while reducing the model size with two orders, as shown in Figure 2.

## Related Work
- NLP tasks often have diverse formats due to the fast emergence of datasets
- Recent research shows the necessity of unifying formats to fix the gap across various tasks
- T0 builds an application to map original NLP datasets into target templates with custom prompts
- FLAN groups multiple datasets into 12 task clusters
- Then designs 10 unique instruction templates to unify formats
- Despite effective, this focuses on generative styles and thus cannot be adapted to vast label-based models

### Label Information
- Label semantics is an important information source for solving few-shot tasks
- The L-TapNet framework integrates label information with manually designed prompts for inputs to solve few-shot slot tagging tasks
- LSAP obtains powerful few-shot performance by introducing label semantics into the pre-training and fine-tuning phases of the PLMs

### Zero-Shot Learning
- Large-scale Pre-trained Language Models (PLMs) have shown impressive performance across various few-shot tasks
- However, they have limited competence when dealing with zero-shot tasks, which have broader applications in practice
- Recent efforts try to mitigate this issue from different perspectives
- FLAN (Wei et al., 2021) designs specific instruction templates for each task and utilizes over 60 labeled datasets to "fine-tune" a 137B language model
- T0 (Sanh et al., 2021) unifies all tasks into a source-target format by collecting a large variety of prompt templates, specifically 2, 073 manually constructed prompts, and trains the model with multi-task learning
- Along this line, ZeroPrompt (Xu et al., 2022) applies over 1, 000 supervised datasets and proposes the genetic prompt search method to find prompts for new tasks
- However, these methods cost significant laborious efforts, such as prompt engineering and template designing
- Moreover, the pre-training and tuning phases of large-scale PLMs take enormous amounts of computational resources, therefore, new tasks may suffer great difficulty in deploying

## Approaches
- UniMC is a unified framework for training and inference in deep learning
- The UniMC framework is composed of three modules: a training module, an inference module, and a validation module
- The training module is responsible for training the deep learning models
- The inference module is responsible for inference of the deep learning models
- The validation module is responsible for validation of the deep learning models
- The UniMC framework is composed of three modules: a training module, an inference module, and a validation module
- The training module is responsible for training the deep learning models
- The inference module is responsible for inference of the deep learning models
- The validation module is responsible for validation of the deep learning models

### MC tuning
- The backbones are often pre-trained models, resulting in excellent skill in capturing the commonsense knowledge.
- Intuitively, we can employ these as base modules by taking advantage of their high volume knowledge.
- More specifically, we use the outputs of pre-trained models as the initial states for the following MC tasks, leading to a twostage tuning paradigm.
- In the MC training phase, we train the models with MC tasks and gain a great initialization for selecting a correct option.
- In the zero-shot phase, we apply the unified MC models to unseen zero-shot tasks.
- We now introduce the proposed option masked language modeling (O-MLM) and option prediction (OP) methods in detail.
- Masked Language Modeling (MLM) is a pretraining task in BERT (Devlin et al., 2019) for selfsupervised learning, where T is the random perturbed token from T ; m(T ) and T \m(T ) are the masked tokens from T and the reset tokens, respectively.
- In practice, we randomly replace tokens in the passage sequence x with special tokens [MASK], as opposed to the whole sequences used in standard BERT.
- The main difference between O-MLM and MLM is the way of masking. We always mask the [O-MASK] tokens to predict "yes" or "no", as shown in Figure 4 (b).
- Therefore, the loss L Oâˆ’MLM and L MLM share the same style.
- Once the prediction probabilities of "yes" or "no" is obtained, we next introduce the OP to teach the model for learning MC tasks, which is shown in Figure 4 (b).
- To learn the mutually exclusive characteristics between options, OP takes the logits yes" for each option sequence to generate label distributions.
- OP aims to compute a cross-entropy loss with ground truth label distribution Y :
- Recent studies show that including mixed tasks in a batch will improve the generalization ability of neural networks (Aghajanyan et al., 2021).
- When facing mixed tasks, we mask the output logits except for [O-MASK] during the Softmax operation to compute the OP loss in a mini-batch, as shown in Figure 6.
- The logit masking approach allows our UniMC to handle MC tasks with different number of options in a single batch.

## Experiments

### Experimental Setup
- We follow the preparation in T0 (Sanh et al., 2021) to cluster the label-based NLP datasets into 6 groups.
- In particular, we collect publicly available datasets.
- We apply accuracy in all datasets.
- For computing the overall average accuracy, we take the average accuracy for each task and then calculate the arithmetic mean for them.
- In the experiments, we compare our method with the state-of-the-art baselines, including: GPT2 (Radford et al., 2019), GPT3 * (Zhao et al., 2021), T0 (Sanh et al., 2021), FLAN (Wei et al., 2021), PaLM (Chowdhery et al., 2022), GaLM (Du et al., 2021) and UnifiedQA (Khashabi et al., 2020).
- We report the accuracy of each method to measure their performance.
- We only present the average outcomes if the baseline is conducted in multiple runs.
- Besides, we include the random guessing as a naive baseline for the comparison.
- In our model, we use the ALBERT-xxlarge-V2 (Lan et al., 2020) as backbone models by taking its light-weighted parameters.
- For fair comparison, we set the maximum length token as 512 in all experiments as in (Lan et al., 2020).
- We run only one epoch by following the setting in FLAN (Wei et al., 2021).
- We set the number of samples for each task up to 20K, aiming to prevent the model from being dominated by specific tasks.
- Besides, we repeat the experiment 5 times by using different seeds.
- We run all our experiments on 8 NVIDA A100 GPUs.

### Main Results
- UniMC achieves the best performance in all datasets
- UniMC achieves these competitive results with as few as 235M parameters as opposed to hundred billions of parameters in other baselines
- These results confirm the effectiveness of unifying formats as a multiple choice style
- Besides, a bi-directional structure in UniMC strengths its ability in capturing information as opposed to the previous onedirectional structures
- Text classification task aims to select a label/class for given texts. This is similar to the objective of MC task in nature. Therefore, we conduct a zeroshot text classification experiment to verify our model's capability. As shown in Table 2, UniMC outperforms previous SOTA models by a large margin. In particular, we know that Dbpedia includes 13 categories, adding a significant challenge to the classification task. Fortunately, UniMC has a builtin advantage in dealing with multiple classes due to the similarity between choices and classes, leading up to 48.9% improvement.
- We now present our main results from the Natural Language Inference (NLI) task in Table 1. UniMC achieves the best performance in all datasets, demonstrating its capability of NLI. In particular, UniMC achieves these competitive results with as few as 235M parameters as opposed to hundred billions of parameters in other baselines
- These results confirm the effectiveness of unifying formats as a multiple choice style
- Besides, a bi-directional structure in UniMC strengths its ability in capturing information as opposed to the previous onedirectional structures
- In the NLI task, UniMC achieves better performance than FLAN in general, which is consistent with the results in Table 1.
- We also select tasks like the commonsense reasoning, the coreference resolution, and the sentiment analysis to further explore the generalization ability of ours. UniMC gets an obvious advantage in COPA, Hellaswag, Winogrande, WSC, DPR when evaluating the common sense and coreference tasks. Beyond these two tasks, we find that the construction of datasets plays a critical role to the performance. In general, these datasets can be grouped into two categories: the understanding and generation styles. UniMC tends to show better performance on datasets that more close to the understanding style.
- For the option prompts, we present the experimental results in Table 6. We would like to emphasize that option prompts are necessary for our UniMC, therefore, we cannot remove this component as in the above experiment. Instead, we design different option prompts to demonstrate their effects. We observe that different prompts show limited performance variations, indicating the robustness of our UniMC to option prompts.
- Since FLAN and PaLM are not open-sourced, we choose one of the most powerful models, e.g., UnifiedQA-T5, as the baseline to ensure the fairness in comparison. In the experiment, we find that UnifiedQA-T5 is sensitive to option prompts, which have up to 8.3 standard variation (Std).
- We design the prompt to frame the input sequences to make all datasets fit into UniMC directly. However, some recent methods need extra processes, such as adopting an option with a context (question and passage) into a sequence and aggregate multiple different sequences to get an answer (Sun et al., 2021). To fix this gap, we design two strategies to control the flow of the information as in Section 3.1.2.
- We summarize the performance of these two in Table 7. We observe that AMM adds the greatest improvement to results, which is much better than UIE.
