---
title: "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective"
date: 2022-10-16T17:24:06.000Z
author: "Ping Yang, Junjie Wang, Ruyi Gan, Xinyu Zhu, Lin Zhang, Ziwei Wu, Xinyu Gao, Jiaxing Zhang, Tetsuya Sakai"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-08590v2.webp" # image path/url
    alt: "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.08590)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.08590).


# Abstract
- We propose a new paradigm for zero-shot learners that is format agnostic,
- i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis.
- Zero-shot learning aims to train a model on a given task such that it can address new learning tasks without any additional training.
- Our approach converts zero-shot learning into multiple-choice tasks,
- avoiding problems in commonly used large-scale generative models such as FLAN.
- It not only adds generalization ability to models but also significantly reduces the number of parameters.
- Our method shares the merits of efficient training and deployment.
- Our approach shows state-of-the-art performance on several benchmarks and produces satisfactory results on tasks such as natural language inference and text classification.
- Our model achieves this success with only 235M parameters, which is substantially smaller than state-of-the-art models with billions of parameters.

# Paper Content

## Introduction
- Large-scale language models have made substantial progress in a wide variety of tasks, including text classification, natural language inference, and commonsense reasoning.
- Zero-shot learning aims to predict labels on datasets from novel domains, and most solutions can be framed in the prompt tuning framework.
- However, these frameworks suffer from their inherent problems, and thus limit their potential in zero-shot learners.
- One immediate problem is that these models are often hard to be trained, making the deployment and consumption difficult.
- Secondly, manual processing is required when addressing zero-shot problems. For instance, T0 builds 2, 073 prompts to handle more than 170 tasks.
- Lastly, existing models employ a single direction paradigm, either auto-regressive models or sequence-to-sequence, resulting in inadequate usage of information from both directions.
- As an example, PMLM tries to implement a zero-shot learner, which is shown in Figure 1 (c). Note that recent work (Liu et al., 2019a) state that PMLM is more suitable than PLM for Natural Language Understanding (NLU) tasks. However, it has to be fine-tuned on the task-specific samples to initialize the classifier instead of randomly initializing the classifier. Therefore, the ability of PMLM is limited when dealing with zeroshot scenarios.
- To address the aforementioned problems, we introduce a light-weight framework, called Unified Multiple Choice model (UniMC), proposing a novel MC tuning.
- The proposed MC tuning has the following advantages: i) parameter updating only happens in the MC training phase, and ii) facilitating the deployment.
- To reduce the manual processing, we only formulate one candidate option prompt format and one question prompt format.
- Note that we also consider the case without any question prompt format. Under this setting, we can treat labels as options rather than building verbalizer maps and providing its text information to the models as before.
- We therefore can learn the information from labels directly.
- To this end, we convert the problematic classifiers to options.
- One immediate question is how to choose an option efficiently and unambiguously. Therefore, as shown in Section 3.2, we develop an option-mask tokens [O-MASK] to predict "yes" or "no" before each option.
- A two-step process is introduced to output the desired options. First, similar to Masked Language Modeling (MLM) (Devlin et al., 2019), we conduct Option MLM (O-MLM) to recover the "yes" or "no" for each option. Next, we propose an Option Prediction (OP) method to compute proper options.
- With extensive experiments on multiple challenging benchmarks, we demonstrate that our approach's performance outperforms state-of-the-art baselines, while reducing the model size with two orders, as shown in Figure 2.

## Related Work
- NLP tasks often have diverse formats due to the fast emergence of datasets
- Recent research shows the necessity of unifying formats to fix the gap across various tasks
- T0 builds an application to map original NLP datasets into target templates with custom prompts
- FLAN groups multiple datasets into 12 task clusters
- Then designs 10 unique instruction templates to unify formats
- Despite effective, this focuses on generative styles and thus cannot be adapted to vast label-based models

### Label Information
- Label semantics is an important information source for solving few-shot tasks
- The L-TapNet framework integrates label information with manually designed prompts for inputs
- LSAP obtains powerful few-shot performance by introducing label semantics into the pre-training and fine-tuning phases of the PLMs

### Zero-Shot Learning
- Large-scale pre-trained language models (PLMs) have shown impressive performance across various few-shot tasks
- However, they have limited competence when dealing with zero-shot tasks, which have broader applications in practice
- Recent efforts try to mitigate this issue from different perspectives
- FLAN (Wei et al., 2021) designs specific instruction templates for each task and utilizes over 60 labeled datasets to "fine-tune" a 137B language model
- T0 (Sanh et al., 2021) unifies all tasks into a source-target format by collecting a large variety of prompt templates, specifically 2, 073 manually constructed prompts, and trains the model with multi-task learning
- Along this line, ZeroPrompt (Xu et al., 2022) applies over 1, 000 supervised datasets and proposes the genetic prompt search method to find prompts for new tasks
- However, these methods cost significant laborious efforts, such as prompt engineering and template designing
- Moreover, the pre-training and tuning phases of large-scale PLMs take enormous amounts of computational resources, therefore, new tasks may suffer great difficulty in deploying

## Approaches
- UniMC is a unified model-based learning framework
- UniMC is composed of three modules: a model generator, a model evaluator, and a model fusion module
- The model generator generates a model from a data set
- The model evaluator evaluates the model on new data sets
- The model fusion module combines the best features of the models generated by the model generator and the model evaluator
- The UniMC framework is composed of three modules: a model generator, a model evaluator, and a model fusion module
- The model generator generates a model from a data set
- The model evaluator evaluates the model on new data sets
- The model fusion module combines the best features of the models generated by the model generator and the model evaluator

### Unified Input
- A unified input format will facilitate the generalization of models, promoting the sharing of knowledge across different tasks.
- To achieve this, we frame all tasks' objectives together as a multiplechoice (MC) problem, as shown in Figure 3.
- A MC problem often consists of three components, including options, question, and passage.
- We can often get the passage component effortlessly by using the raw question directly or providing a corresponding question when it is missing.
- The transformation of options depends on whether or not we can get a straightforward expression of classes.
- On the one hand, we can convert all classification tasks into options directly as it has specific information for choices.
- On the other hand, we have to construct an option prompt to generate particular choices.
- Details of this transformation can be found in Appendix A.
- In effect, these allow us to abandon label indices as in classification tasks, which include much less information than our used options.
- In our framework, we employ BERT-like PMLMs as the backbone, such as ALBERT (Lan et al., 2020) and RoBERTa (Liu et al., 2019b), to integrate the bidirectional modeled input x inp .
- In additional, the discussion of backbone models is in Appendix B.
- Instead of using the original embedding methods directly, we develop a new solution for the segment id, position id, and attention mask matrix to fit multiple choice tasks, simultaneously.
- Tokenization: In this framework, the key to achieve the ability of addressing MC tasks is to set up a proper option.
- We thus introduce option-mask.
- It's a cookie-cutter movie, a cut-and-paste job.
- What is sentiment of the review?
- Options: [1] it's great. [2] it's terrible. [C] no it's great. yes it's terrible.

### MC tuning
- The backbones are often pre-trained models, resulting in excellent skill in capturing the commonsense knowledge.
- Intuitively, we can employ these as base modules by taking advantage of their high volume knowledge.
- More specifically, we use the outputs of pre-trained models as the initial states for the following MC tasks, leading to a twostage tuning paradigm.
- In the MC training phase, we train the models with MC tasks and gain a great initialization for selecting a correct option.
- In the zero-shot phase, we apply the unified MC models to unseen zero-shot tasks.
- We now introduce the proposed option masked language modeling (O-MLM) and option prediction (OP) methods in detail.
- Masked Language Modeling (MLM) is a pretraining task in BERT (Devlin et al., 2019) for selfsupervised learning, where T is the random perturbed token from T ; m(T ) and T \m(T ) are the masked tokens from T and the reset tokens, respectively.
- In practice, we randomly replace tokens in the passage sequence x with special tokens [MASK], as opposed to the whole sequences used in standard BERT.
- The main difference between O-MLM and MLM is the way of masking. We always mask the [O-MASK] tokens to predict "yes" or "no", as shown in Figure 4 (b).
- Therefore, the loss L Oâˆ’MLM and L MLM share the same style.
- Once the prediction probabilities of "yes" or "no" is obtained, we next introduce the OP to teach the model for learning MC tasks, which is shown in Figure 4 (b).
- To learn the mutually exclusive characteristics between options, OP takes the logits yes" for each option sequence to generate label distributions. OP aims to compute a cross-entropy loss with ground truth label distribution Y :
- Recent studies show that including mixed tasks in a batch will improve the generalization ability of neural networks (Aghajanyan et al., 2021).
- When facing mixed tasks, we mask the output logits except for [O-MASK] during the Softmax operation to compute the OP loss in a mini-batch, as shown in Figure 6.
- The logit masking approach allows our UniMC to handle MC tasks with different number of options in a single batch.

## Experiments

### Experimental Setup
- We follow the preparation in T0 (Sanh et al., 2021) to cluster the label-based NLP datasets into 6 groups.
- In particular, we collect publicly available datasets.
- We apply accuracy in all datasets.
- For computing the overall average accuracy, we take the average accuracy for each task and then calculate the arithmetic mean for them.
- In the experiments, we compare our method with the state-of-the-art baselines, including: GPT2 (Radford et al., 2019), GPT3 * (Zhao et al., 2021), T0 (Sanh et al., 2021), FLAN (Wei et al., 2021), PaLM (Chowdhery et al., 2022), GaLM (Du et al., 2021) and UnifiedQA (Khashabi et al., 2020).
- We report the accuracy of each method to measure their performance.
- We only present the average outcomes if the baseline is conducted in multiple runs.
- Besides, we include the random guessing as a naive baseline for the comparison.

### Main Results
- UniMC outperforms other baselines in all datasets
- UniMC is effective with as few as 235M parameters
- FLAN is a well-known model for zero-shot option or label-related tasks, but UniMC outperforms FLAN in a comprehensive comparison
- The construction of datasets is less important than the task style

### Ablation Studies
- The MC training is necessary for the prompt effect
- The prompt effect is caused by the way of data construction
- Different prompts show limited performance variations, indicating the robustness of the UniMC to option prompts
- The model size has no significant effect on the prompt effect

## Conclusions
- Zero-shot learning is a new paradigm that introduces flexibility and generalization ability to learners.
- We propose O-MLM and OP in both MC training and zero-shot phase, aiming to capture information from both directions.
- Our UniMC achieves better performances over SOTA models that a few hundred times larger than our model.
- Our experiments demonstrate the effectiveness and generalization ability of UniMC on zero-shot tasks.

## Limitations
- Framework for zero-shot tasks
- Introducing additional artificial information
- Reducing manual processing to the minimum
- Exploring how to employ question prompts
- Non-trivial to decide whether a prompt is required
- Only compare with limited baselines
- In experiments, only a few comparative experiments are implemented

## Ethical Considerations
- Natural language processing is an important technology
- In this work, we develop a novel zero-shot NLP approach to enhance the generalization ability of NLP
- As discussed in (Schramowski et al., 2022(Schramowski et al., , 2019;;Blodgett et al., 2020), language models might contain human-like biases, which might embed in both the parameters of the models and outputs
- Furthermore, we note the potential abuse of zero-shot models because these are often being integrated into applications without justification
- We encourage open debating on its utilization, such as the task selection and the deployment, hoping to reduce the chance of any misconduct

### B.3 Results on all datasets
- UniMC achieves the best performance on 11 out of 17 datasets
- PLMs outperform UniMC in the tasks of commonsense reasoning and coreference resolution in Hallawag, Winogrand, and WSC
- PLMs benefit from unsupervised language modeling on a large-scale text corpus
- For question prompts, we conduct experiments on four challenge tasks by showing perfor-
- Zero-shot results in sentiment analysis task. "Std" indicates Standard Deviation. The best average results are in bold. The more stable performance is underlined.
- Zero-shot performance with different strategies to control the flow between options. "UIE" indicates Updating Id Embeddings, including segment id and position id. "AMM" means Attention Mask Matrix. "Improve" shows the accuracy improvement from Random Guessing.
