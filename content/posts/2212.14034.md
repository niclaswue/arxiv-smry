---
title: "Cramming: Training a Language Model on a Single GPU in One Day"
date: 2022-12-28T18:59:28.000Z
author: "Jonas Geiping, Tom Goldstein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14034v1.webp" # image path/url
    alt: "Cramming: Training a Language Model on a Single GPU in One Day" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14034)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14034).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/cramming-training-a-language-model-on-a).

# Abstract
- Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners.
- We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario.
- We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings.

# Paper Content

## SCALING UP AND SCALING DOWN
- Large-scale training of machine learning models with transformer architectures has lead to groundbreaking improvements in many sub-fields of natural language processing including language understanding and natural language generation (Vaswani et al., 2017;Dosovitskiy et al., 2021;Radford et al., 2019).
- The nowadays accepted (but historically surprising) key behavior of these systems is that they reliably scale -they continuously improve in performance when the number of model parameters and amount of data grow.
- These increases in performance are well-described by various power laws as studied by Kaplan et al. (2020).
- This sets up a dominant paradigm in which scaling is the key to performance improvement (Sutton, 2019).
- The power of scale has set off a race to produce extremely large models, which in turn has created an environment where few researchers or practitioners feel that they are capable of training a language model.
- The original BERT model Devlin et al. (2019), which became a cornerstone transformer for many practical applications in natural language understanding, already required a significant amount of computation to train.
- Yet, the reproduction and improvements in Liu et al. (2019) further increased its performance by cranking up the level of computation by orders of magnitude.
- As these pre-trained checkpoints became popular for a range of downstream applications (Wolf et al., 2020), the competition for the largest language model became a focal point for industrial labs.
- This led to training runs that improved the performance of pretrained language models at the expense of computation at the zettaFLOP scale (Raffel et al., 2020;Yang et al., 2020;Zaheer et al., 2021) and later at the extremely large yottaFLOP scale (Brown et al., 2020;Black et al., 2022;Chowdhery et al., 2022;Rae et al., 2022).
- Our goal is to turn this trend on its head and investigate how to best scale down language model training and what trade-offs emerge when doing so: What downstream performance can be achieved by a modest researcher when training from scratch with a single GPU for a single day?
- The ability to train a language model to the performance level of BERT with such modest resources has several interesting implications.
- For one, if scaled-down model pretraining is a viable analogue Preprint of large-compute pretraining, then this opens up a host of further academic investigations that are currently hard to realize for large-scale models.
- For example, research questions about the differences between existing and new pre-training tasks, tracing model predictions to data points (Ilyas et al., 2022), security questions such as membership inference (Carlini et al., 2022) and data poisoning (Geiping et al., 2021), and a wide range of empirical investigations into topics such as stability or generalization that arise during training (Nagarajan & Kolter, 2019;Jiang et al., 2019).
- At the same time, we can imagine situations in which legal requirements make it unclear whether models trained on public data with uncertain origin are permissible, and where a practitioner is interested in retraining their language models using a specialized or trustworthy data source (Wilka et al., 2017;Gold & Latonero, 2017).
- In addition, we are motivated to benchmark the overall conceptual progress of research in this area over the last years, beyond simply turning the scaling knob.

## TYING OUR HANDS BEHIND OUR BACK: A SETUP WITH LIMITED COMPUTE
- A transformer-based language model of arbitrary size is trained with masked-language modeling, completely from scratch.
- Existing pretrained models cannot be included in any part of the pipeline.
- Any raw text (excluding downstream data) can be included for training. This means that one can achieve speedups by making judicious choices about how and when to sample data, provided the sampling mechanism does not require a pre-trained model.
- The downloading and pre-processing of raw data is exempted from the total compute budget. Pre-processing may include CPU-based tokenizer construction, tokenization, and filtering, but cannot include representation learning (e.g. pre-training a word embedding is not allowed, unless it is counted towards the final runtime).
- Training proceeds on a single GPU for 24 hours.
- Downstream performance is evaluated on GLUE (Wang et al., 2018). Downstream finetuning on GLUE is limited to brief training with only the training data of the downstream task (we consider 5 epochs or less) and needs to work with hyperparameters set globally for all GLUE tasks. Downstream finetuning is excluded from the total compute budget.

## Preprint

## RELATED WORK ON EFFICIENT TRANSFORMERS
- It takes 11 days to train BERT on TPUs
- After the original training run on TPUs, improvements have reduced the upper limit to 3-4 days
- Most improvements do not reliably materialize gains in final accuracy
- Tay et al. (2021) found that only a few modifications outperform the original architecture
- The meta-study investigating improvements in preparation for extreme-scale training found that most improvements do not reliably materialize gains in final accuracy

## Scaling Laws
- Kaplan et al. (2020) found that the size of a model (the number of parameters in non-embedding layers) is the only factor that strongly predicts performance
- For a fixed compute budget, an optimal model size can be derived, but performance is only mildly connected to model size -larger models processes less data per unit of compute, but improve faster by almost the same margin
- The precise coefficients and shape of these scaling laws continue to be iterated on (Hoffmann et al., 2022) and adapted for related settings (Bansal et al., 2022;Clark et al., 2022;Bahri et al., 2021b)

## INVESTIGATIONS
- Implement and test a considerable number of proposed modifications to the setup of Devlin et al. (2019)
- Clarify the common implementation and initial data setup
- Investigate architectural, training and dataset improvements

## IMPLEMENTATION DETAILS
- We implement everything in PyTorch
- We keep everything on the implementation level of the PyTorch framework
- We only allow automated operator fusion (Sarofeen et al., 2022) that can be applied to all components
- We run all experiments and ablation studies with the same setup of automated mixed precision (Micikevicius et al., 2018) for standard 16-and 32-bit floating point precision (over full 32-bit float, scaled 16-bit (Rasley et al., 2020) and pure bfloat16 (Wang & Kanwar, 2019))

## Initial Data Setup
- We use a recent dump of the English Wikipedia and English bookcorpus to study Devlin et al.'s (2019) work
- We strip accents and non-ascii characters from the text, create a tokenizer, and use WordPiece with a vocabulary size of 2 15 = 32768
- We found no significant change in performance with BPE or SentencePiece with Unigrams
- Smaller vocabulary sizes resulted in worse performance, while larger vocabulary sizes were not reliably better
- We pack tokenized data into randomized sequences of length 128 and separate unrelated fragments by <sep>
- The performance impact from dropping this separator was minimal
- No impact was observed from including a <cls> token in pretraining

## MODIFYING THE ARCHITECTURE
- A large number of layers (50,000)
- A large number of neurons (100,000)
- A large number of activations (1 million)
- A large number of weights (100,000)
- A large number of biases (1 million)
- A large number of epochs (10,000)
- A large number of layers (50,000)
- A large number of neurons (100,000)
- A large number of activations (1 million)
- A large number of weights (100,000)
- A large number of biases (1 million)
- A large number of epochs (10,000)
- A large number of layers (50,000)
- A large number of neurons (100,000)
- A large number of activations (1 million)
- A large number of weights (100,000)
- A large number of biases (1 million)
- A large number of epochs (10,000)

## MODIFYING THE TRAINING SETUP
- We study the impact of training hyper-parameters on the BERT-base architecture.
- We keep Adam (Kingma & Ba, 2015) as the optimizer of choice, with weight decay of 0.01 as described in (Loshchilov & Hutter, 2017).
- We find that a simple one-cycle learning rate (Smith & Topin, 2018) with a peak learning rate of 10 −3 leads to minimal pretraining loss within our budget.
- We find that the optimal batch size in this setting is around 1536 for minimal pretraining loss, but 4032 for maximal downstream performance for the 2080ti, i.e. we accumulate gradients and only perform an update every 16 and 42 forward/backward passes, respectively.
- For the larger A4000 and A6000 cards, this corresponds to a micro-batch size of 128/256 and final batch size of 4096, which we again accumulate.
- Fortunately, we can find small speedups by using an aggressive batch size schedule; we increase the number of averaged micro-batches linearly over the course of training.
- We also experiment with automatic and adaptive batching rules (De et al., 2017;Bollapragada et al., 2018a;b), but find that the best results from these adaptive schedules resemble the fixed linear schedule.

## OPTIMIZING THE DATASET
- Scaling laws create a barrier to making major gains with architectural modifications
- However, scaling laws do not preclude us from training on better data
- We can improve our downstream performance by filtering, sorting and increasing the batch size
- Larger vocabulary sizes correlate with larger average GLUE score, although the effect is plateauing for the MNLI task around the original 32768 vocabulary size

## FINETUNING PERFORMANCE ON GLUE
- Systematically evaluates performance on the GLUE benchmark of Wang et al. (2018), minus WNLI as in Devlin et al. (2019).
- Finds that performance is surprisingly decent, especially for the larger datasets of MNLI, QQP, QNLI and SST-2, where downstream finetuning can smooth out the remaining differences between the full BERT model and the crammed variants.
- For BERT-base, finds substantial gains over both a naive BERT and training with limited budget.

## ABLATION -WHICH CHANGES REALLY MATTERED?
- The paper discusses changes to the BERT algorithm.
- The paper discusses how to make minimal modifications to the BERT algorithm.
- The paper discusses how to make more aggressive learning rate schedules.

## WHAT HAPPENS WHEN TRAINING LONGER?
- We train models on 8 A6000 GPUs for 48 hours
- The discussed recipe does immediately generalize to larger compute budgets
- Yet, in other tasks, such as CoLA, the new models barely improve even in the larger compute regime

## LIMITATIONS
- In this work, we limited our investigation to transformer-based architectures trained with MLM objectives.
- However, we do think that the general task of cramming posed in Section 2 is interesting even when relaxing these constraints.
- There have been a number of modifications proposed to the objective in particular (Joshi et al., 2020;Bao et al., 2020;Bajaj et al., 2022;Tay et al., 2022b).
- While Artetxe et al. (2022) and Wang et al. (2022) find MLM still to hold up well as a pretraining objective, other suggestions such as ELECTRA (Clark et al., 2019;2020;He et al., 2021) could be employed which might be beneficial for crammed models.
- Also, the optimal architecture might not be transformer-based (Merity, 2019;Fusco et al., 2022;Peng, 2021).

## CONCLUSIONS
- Transformer-based language models can achieve decent downstream performance on GLUE when crammed into a setting with very limited compute.
- Many implications of Kaplan et al. (2020) still hold in this regime, and for examples improvements through larger models are evened out by their slower speed.
- We hope that this work can provide a baseline for explorations of the question of cramming we formalize in Section 2 and cast an additional light on a number of improvements and tricks proposed for transformer architectures in recent years.
- 3 for a day on a single GPU with mixed precision.
- Batch size is 4036 and dataset is bookcorpus-wikipedia.
- Downstream evaluation as described in Section 5.
- All values for pretraining on an A4000.
- Note the discrepancy between optimal pretraining batch size and optimal batch size for evaluation on MNLI when ramp-up is used, but also note that differences are overall barely significant.
- A4000 performance is actually less clear from this whitepaper, and estimated to be 88.45 TFLOP/s, based on it containing 192 tensor cores, compared to 336 for the A6000.
- For the RTX2080ti, the whitepaper at https://images.nvi dia.com/aem-dam/en-zz/Solutions/design-visualization/technologie s/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf reports 53.8 "peak FP16 Tensor TFLOPS with FP32 Accumulate" for the reference edition.
- All total exaFLOP numbers are then computed based on these TFLOP/s numbers over the training time period described in each work.
- Figure 1: Various Transformer architectures and shapes, showing MLM loss versus number of tokens ingested.
- Left: Global view.
- Right: Zoom onto 10e8 or more tokens.
- All models trained with the same budget.
- We see that improvements through architectural reshaping are minimal; while there are some fluctuations in loss early in training, the rates of loss decay during most of training differ by a multiplicative constant (horizontal shift due to logarithmic horizontal axis) that depends strongly on the model size and not model type.
- Figure 2: Learning Rate Schedules.
- Although globally many schedule result in similar behavior, we see in the zoom in the middle, that differences do exist.
- The right side shows the corresponding learning rate schedules.
- Both triangular-shaped one-cycle schedules have better end-time behavior, possibly due to the quick annealing.
- Figure 3: Vocabulary Size versus GLUE Score and MNLI Accuracy for models trained in the cramming regime on bookcorpus-wikipedia data.
- Figure 4: Extended version of Figure 2, including additional learning rate schedules.
- Figure 5: Partial variations of batch sizes with and without linear ramp-up.
- All experiments run with the training setup described in Section 4.3 for a day on a single GPU with mixed precision.
- Batch size is 4036 and dataset is bookcorpus-wikipedia.
- Downstream evaluation as described in Section 5.
- All values for pretraining on an A4000.
- Note the discrepancy between optimal pretraining batch size and optimal batch size for evaluation on MNLI when ramp-up is used, but also note that differences are overall barely significant.
