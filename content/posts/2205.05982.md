---
title: "Vectorized and performance-portable Quicksort"
date: 2022-05-12T09:41:31.000Z
author: "Mark Blacher, Joachim Giesen, Peter Sanders, Jan Wassenberg"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2205-05982v1.webp" # image path/url
    alt: "Vectorized and performance-portable Quicksort" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.05982)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.05982).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/vectorized-and-performance-portable-quicksort).

# Abstract
- Recent works showed that implementations of Quicksort using vector CPU instructions can outperform the non-vectorized algorithms in widespread use.
- However, these implementations are typically single-threaded, implemented for a particular instruction set, and restricted to a small set of key types.
- We lift these three restrictions: our proposed 'vqsort' algorithm integrates into the state-of-the-art parallel sorter 'ips4o', with a geometric mean speedup of 1.59.
- The same implementation works on seven instruction sets (including SVE and RISC-V V) across four platforms. It also supports floating-point and 16-128 bit integer keys.

# Paper Content

## Introduction
- Due to fundamental properties of current and expected future CPUs, including the per-instruction energy cost, it is important for software to be designed to utilize SIMD and/or vector extensions.
- An instructive example and the focus of this article is sorting, which is an important part of many applications, including information retrieval.
- Replacing Quicksort from a standard library by a vectorized Mergesort implementation can reduce energy usage by a factor of six.
- However, vectorized sorting is not used much in practice.
- There are, however, some explanations for the so far limited adoption of vectorized sorting.
- For instance, Intel lists some 11,000 vector instructions and variants.
- Developing SIMD software involves specialized domain expertise, including knowledge of the various instruction sets, which is not a trivial requirement.
- For linear algebra or 'vertical' algorithms, where the SIMD elements (lanes) are independent, autovectorization, that is, synthesizing vector instructions directly from C++ code by the compiler, is an appealing option.
- However, re-ordering vector lanes, which is fundamental to sorting, is infeasible via autovectorization.
- In the absence of viable compiler or language support, we use an abstraction layer, called Highway, over platform-specific intrinsics (functions that map to vector instructions).
- In C++, this is fairly straightforward as wrapper functions like, for instance, Reverse are easier to use than calling the corresponding _mm512_permutexvar_epi16 intrinsic directly.
- Many such libraries have been developed.
- However, some difficulties arise when choosing a set of functions efficiently implementable on x86, Arm NEON, Arm SVE, and RISC-V V.
- The latter two involve 'scalable vectors' whose sizes are unknown at compile time, which currently rules out some common C++ implementation techniques like wrapping vectors in a class to enable member functions and specifying the vector size as a compile-time constant.
- Furthermore, heterogeneous cloud servers and client devices offer different instruction sets, requiring the application to decide at runtime which instruction set is available for use.
- To the best of our knowledge, our Highway C++ library is the only library that can handle 'scalable vectors' and check for the best available instruction set at runtime.
- Application code is expressed by using calls to Highway functions, also known as ops.
- This single implementation is automatically compiled for each requested target by using the preprocessor to re-include the code, and #pragma statements to set the target architecture.
- Highway then chooses the appropriate version at runtime.

## Vectorized quicksort
- Quicksort is a simple algorithm that recursively sorts arrays by partitioning them with respect to some pivot element.
- The performance of Quicksort crucially depends on the choice of the pivot element.
- In this section we provide a portable partitioning, faster than AVX-512specific code, and a vectorized, cache-aware, robust pivot sampling.
- For small array sizes it pays off to switch to alternative sorting algorithms/strategies such as sorting networks.
- Here, we provide a vectorized sorting network which sorts 256 keys in several hundred CPU cycles.
- Furthermore we support reverse sort order and 128-bit keys.
- A simplified C++ implementation of Quicksort is shown in Algorithm 1.
- After calling Partition, which returns the starting index of the second partition, the remainder of Algorithm 1 is concerned with recursing to both partitions.
- It is desirable to have the recursive function end with a call to itself. This enables so-called 'tail recursion', for which a jump instruction suffices, avoiding the overhead of parameter passing and setting up a stack frame.
- Note that we call ChoosePivot before recursing, rather than inside Recurse. This allows us to choose a safe pivot whenever a degenerate (empty) partition is detected.
- With some advance knowledge of the pivot and partitioning schemes (pivots are always one of the input keys, and Partition moves to the left any key equal to the pivot), we are assured the left partition is never empty. Conversely, the right partition is only empty if the pivot is equal to the last value in sort order.
- If we again choose the same pivot, there is even a risk of infinite recursion. Thus we must handle this case separately.
- The most likely cause is that all keys in the current range are equal. This is quite common in information retrieval applications, in which keys are often drawn from a small subset of the possible values.
- We check for this by scanning through the keys and computing their minimum and maximum value. This can be vectorized by accumulating per-lane min and max, then 'reducing' them to a single min/max using Highway's Min/MaxOfLanes.
- Eq is a Highway op that returns a mask indicating whether the inputs are equal, and AllTrue indicates whether all lanes of the mask are true.
- If the min and max are equal, then all keys are also equal, and thus already sorted, so we do not recurse further. Otherwise, the pivot was an unlucky choice.
- Because we recursively use the median of three sampling, approximately one third of the input must have been equal to the largest value. We only observe this to happen with lower-entropy keys (e.g. uniform random 16-bit integers within 32 or 64-bit elements).
- It happens less frequently with vectors of size 16, which imply NBaseCase = 256: large enough that a narrow range of values within such a partition is unlikely.
- We therefore use a simple heuristic that still guarantees forward progress: choosing the first key in sort order as the pivot will partition off at least some keys, otherwise, they are all equal, which was handled above.
- Having hoisted ChoosePivot out of the recursion, we also first check whether the input is small enough to be handled directly in BaseCase (Section 2.3).
- If so, the pivot would not be used anyway, and guaranteeing a minimum input size is helpful for both Partition and ChoosePivot.

### Partition
- Partitioning the input array is defined as moving elements which compare less than or equal to the pivot argument before the other elements.
- Partition follows the basic approach of Bramas [12]: an in-place bidirectional scan using an AVX-512 instruction represented by the CompressStore Highway op.
- This accepts a vector and a mask as input and writes to contiguous memory all vector lanes whose corresponding mask bits are true.
- To partition, we simply CompressStore elements at the left array side with the mask obtained by comparing inputs to pivot, and again on the right side with the negated mask, advancing the write positions according to the number of elements written, and stopping once they meet.
- Inputs are loaded from the left or right side to maintain the invariant that all elements from the current loop iteration could be stored either on the left or right sides.
- This entails checking the 'capacity' (difference between the read and write positions) on either side, and loading from the one with less.
- To establish the invariant before the loop, we begin by loading the first and last vectors of the input to registers, to be partitioned after the loop.
- In contrast to Bramas' explicit usage of AVX-512 instructions [12], the Highway op is portable.
- For AVX-512, CompressStore maps directly to an instruction except for 16-bit elements, which would require the not yet widely available VBMI2 instruction set.
- On Arm SVE and RISC-V, the op stores the result of a Compress instruction to memory.
- For instruction sets without per-lane masking, Highway emulates this operation by reordering the vector according to a pattern loaded from a table, where the index is the concatenation of the mask bits [11].
- We also find it is crucial for performance to unroll the partition loop [6], possibly due to the conditional branch for deciding whether to load the next elements from the left or right end of the array.
- We also find branchless computations of the next address to be slower on a Skylake CPU. Perhaps this is because the branch predictor sometimes guesses correctly, thus reducing latency.
- Unrolling simply repeats each step in the loop, in our case only four times as a compromise between code size, number of registers required, and sufficient latency hiding.
- However, four vectors may exceed the minimum guaranteed input size NBaseCase (smaller inputs are handled by BaseCase). Thus we require an additional loop that partitions small arrays.
- This is by definition not time-critical, so we adopt a simple approach: overwriting the input via CompressStore with the mask, and again CompressStore with negated mask to a buffer.
- Finally, we can append the buffer contents to the current write position in the input.
- We must again handle inputs which are not multiples of the vector size.
- As an extra complication, the Highway op CompressStore is allowed to overwrite memory after the valid lanes.
- Such overwriting is fine for the padded buffer, but unacceptable for the writes to the original input, for which we use the similar CompressBlendedStore op which avoids such overwriting, either with masked stores, or by non-atomically 'blending' the valid result with the previous contents of memory following it.
- Note that we also use this small loop (PartitionToMultipleOfUnroll in the code) to handle remainders.

### Pivot selection
- The ChoosePivot function returns the pivot that will be passed to Recurse and thence to Partition.
- Many published Quicksort implementations use medians of constant-sized samples, but this traditional approach would benefit from some adaptation for vectors and caches.
- Loading elements from random array indices is possible using vector Gather ops, but these are expensive and emulated on the SSE4 and NEON instruction sets.
- We instead load nine 64-byte chunks from random 64-byte-aligned offsets, and recursively reduce their elements to a single median using medians of three as described below.
- The 64-byte chunk size typically corresponds to the L1 cache line size.
- Note that it would be onerous to detect the actual cache line size, and unnecessary for correctness.
- For each element index within a chunk, we determine the median of three elements at same index within groups of three chunks we loaded.
- Medians of three can be obtained with a sorting network consisting of four conditional swaps: (0,2) (0,1) (1,2).
- The first grouping in this notation corresponds to replacing elements 0 and 2 with their minimum and maximum, respectively. The latter two groupings only require a total of two swaps because it suffices to correctly order element 1, the median.
- We choose a sample size of three because sorting network size is superlinear in the input size (e.g. already nine swaps for five inputs).
- When implemented using vector ops, this network is able to produce independent results per lane, independently of the vector width. Thus we can iterate up to the chunk size in units of the vector size, storing the resulting medians to a buffer.
- Note that such a loop pattern is typical of vector-length-agnostic code, which is preferred for the sake of portability.
- Random bits are generated using a variant of SFC64, chosen because it would support guaranteed-unique streams, though we did not use this capability.
- To obtain offsets, we use a division-free modulo algorithm, which only requires a single random draw per value.
- Some numbers are generated less frequently than others, but the range of numbers, i.e. chunk offsets, for sorting 2 30 elements is 2 24 , implying a bias of only 2 −8 .
- We perform the above loads and median three times for a total of nine chunks loaded and 192 bytes of medians.
- Given expected input sizes from 2 20 to 2 30 , this corresponds to roughly log(n) samples.
- We then reduce the buffer to a single median, starting with the above approach to store the median of three vectors from the input buffer to a second buffer.
- Once there are fewer input elements than the vector size, we load single elements into vectors and again compute medians with the same approach, but only store the first lane.
- The remaining zero, one or two input elements are ignored.
- Finally, we swap the buffers, recurse until fewer than three medians remain, and choose the first to be the pivot.
- Note that this constant-sized sampling strategy may lead to O(N ) recursions of the main Quicksort [15] in the worst case.

### Base case
- We now handle small arrays separately as a 'base case' of the recursion, a common optimization for Quicksort [6,23].
- Sorting networks built upon vector instructions can have much lower constant factors than other algorithms because they execute fewer instructions and avoid conditional branches.
- For moderate input sizes, this outweighs their higher O(n • log 2 (n)) complexity.
- With 256 or 512-bit vectors and 16-32 registers commonly available, it is feasible to sort 64-256 elements within registers -an order of magnitude more than the five-element network found in LLVM's Quicksort.
- However, vector instructions entail handling input arrays that do not evenly divide the vector size.
- Although instruction sets typically provide some capability for only loading/storing valid lanes, AMD's x86 implementation does not guarantee it can safely be used:"Exception and trap behavior for elements not selected for loading or storing from/to memory is implementation dependent. For instance, a given implementation may signal a data breakpoint or a page fault for doublewords that are zeromasked and not actually written" [24].
- Thus the function BaseCase begins by copying the input range to buf using the SafeCopyN op, which either uses masking or non-vector instructions to handle any remainder elements.
- To ensure correct results, we then pad the buffer with neutral elements (the last value in sort order) such that they remain in place while sorting.
- Our vectorized sorting network (Section 3) can then load entire aligned vectors from the buffer, and store the sorted results there.
- Finally, we again copy these outputs to the original array. Note that the buffer size is O(1) with respect to the overall input to Quicksort.
- Our sorting network reshapes n inputs into a matrix of r = 16 rows and the smallest power of two c ≤ 16 columns, such that r • c ≥ n and c elements fit within a vector. Thus NBaseCase is r • c and the buffer size must be at least 256 elements, plus two vectors for padding in case vectors are larger than c.
- We also reuse this buffer in PartitionToMultipleOfUnroll and ChoosePivot. Thus it must also fit at least nine vectors or four chunks plus two vectors.
- Because RISC-V (and to a lesser extent Arm SVE) vectors may be large, the buffer size may exceed the limit for stack allocation.
- Unfortunately, the C++ standard forbids std::sort from allocating memory dynamically. Thus vqsort cannot be used as a drop-in replacement on those platforms.

### Sort order and 128-bit keys
- User-specified comparators interact poorly with runtime dispatch (choosing the sort implementation based on CPU capabilities).
- We implement the latter by calling the best available implementation through an indirect pointer.
- Unlike function templates such as std::sort, this would not allow us to inline user-specified functions.
- We expect that calling back to a comparator through another function pointer would be expensive.
- If custom comparisons are required, they can be inserted into a patched version of the vqsort source code, and exposed as a different sort function.
- However, we do generalize comparisons to enable sorting in ascending or descending order, which can be selected using a type-tag argument: SortAscending or SortDescending.
- Our vqsort implementation is agnostic to the sort order because it builds upon an abstraction layer: OrderAscending and OrderDescending.
- These define Compare, First, FirstValue for padding, and FirstOfLanes (which is equivalent to the result of First applied to successive lanes, but implemented using Highway's MinOfLanes reduction op).
- For every First* there is also a corresponding Last*.
- Recall that 128-bit keys (or 64-bit keys with 64-bit associated data) are helpful for some information retrieval applications.
- SIMD/vector instruction sets generally do not support 128-bit lanes natively.
- We can choose to split them into 64-bit halves, such that one vector holds all lower halves of some keys, or take advantage of the fact that Highway guarantees at least 128-bit vectors to treat pairs of 64-bit lanes as unsigned 128-bit numbers.
- The former is likely more efficient, but may require major changes to the memory layout of applications.
- Thus we pursue the latter and find it to be about 0.7 times as fast as native 64-bit sorts on x86, which is surprising given that 128-bit comparisons require at least five instructions (taken care of by Highway).
- We reproduce the x86 implementation in Algorithm 2 for illustration; the functions called there are all Highway ops.
- This returns true in both lanes iff the upper lane is less, or the upper lane is equal and the lower lane is less.
- The additional cost of pairs as Algorithm 2 128-bit comparison using pairs of 64-bit lanes V eqHL = VecFromMask(d, Eq(a, b)); V ltHL = VecFromMask(d, Lt(a, b)); V ltLX = ShiftLeftLanes<1>(ltHL); V vecHx = OrAnd(ltHL, eqHL, ltLX); return InterleaveUpper(d, vecHx, vecHx);
- opposed to separate halves is due to the required interactions between the upper and lower lanes.

## Sorting networks
- sorting arrays in the 'base case' (n ≤ 256) is done with sorting networks with a minimum number of compare-and-exchange modules
- the number of rows in the matrix is not necessary, any reasonable number of rows can be used
- the basic idea behind merging sorted columns or sorted submatrices is to permute the values of vectors so that the two nodes of each compare-and-exchange module are placed under the same index in two different vectors
- after a permutation, the two nodes of a module are vertically aligned between two different vectors

### More bandwidth-friendly algorithms
- Quicksort only splits N inputs into two partitions, requiring about log 2 (N ) recursions.
- If we instead scatter inputs into K partitions, the base of the logarithm changes to K, which seems promising.
- However, our method of compressing vector lanes and storing to each partition seems unsuitable for K ≥ 8 and current vector lengths of 512-bits.
- Given 64-bit keys, we would only be writing one key on average to each partition, and CompressStore can only execute every other clock cycle on Skylake.
- Thus the throughput would be limited to 64 bytes per 32 cycles, or 6 GB/s at 3 GHz, which is far below the Skylake L3 cache bandwidth as seen in Figure 3.
- That leaves K = 4, which was previously found to be helpful [30] in a non-vectorized context.
- That algorithm can benefit from conditional branches, which allow some comparisons to be skipped.
- However, we prototyped vectorized compress with K = 4 and found it to reach about half the speed of K = 2, thus negating the gain from halving the number of recursions.
- If vectorized multi-pivot is unhelpful, what about the extreme case of Samplesort, which is essentially a very large (K = 256) generalization?
- As expected, ips4o scales better than vqsort (Figure 5a) because it requires fewer passes over the data, thus reducing pressure on the shared memory system.
- However, in absolute terms, it is slower in aggregate for less than 19 threads.
- For a possible explanation, we note that ips4o executes nearly five times as many instructions (5.2T versus 1.1T) because its current form does not take advantage of vector instructions.
- As mentioned in the introduction, it may be possible to accelerate ips4o using vector instructions for scattering keys, or also slightly accelerating the comparisons used to classify keys into buckets.
- However, this appears to be difficult given our  goal of a portable and vector-length-independent algorithm.
- We instead pursue a simpler approach.
- Given that ips4o scales better, but has lower single-core throughput, we can switch to vqsort after several initial recursions of ips4o.
- To this end, we simply change baseCaseSort to call vqsort, set IPS4OML_BASE_CASE_SIZE to 8192, and tweak IPS4OML_BLOCK_SIZE and IPS4OML_UNROLL_CLASSIFIER to 1024 and 6, respectively.
- This improves scalability (Figure 5b) and the geometric mean of speedups relative to ips4o is 1.18.

### Performance portability
- Performance portability is achievable on recent x86 CPUs and other platforms with weaker vector units
- vqsort appears to be practical and useful on multiple architectures and instruction sets

## Limitations
- To facilitate vectorization, we imposed a constraint on sort keys, namely, that they be 16/32/64-bit integers, floating-point numbers, or pairs of 64bit numbers representing a 128-bit integer.
- This constraint excludes some applications that need to sort tuples or large items with custom comparators.
- However, we argue that sorting numbers is still in widespread use: 'columnar' databases store the values of each column contiguously, and fields are often encoded as numbers.
- Database query engines typically require a stable sort (order-preserving among equivalent keys), which Quicksort is not, but can be made so by appending a unique (row) identifier to the least-significant bits of the key.
- Thus we have one important target application, typically involving 64-bit numbers.
- To understand the impact, we surveyed uses of sorting in Google's production workloads and found that sorting numbers is actually more costly in total than sorting strings or user-defined types including tuples.
- Our methodology starts by searching in Google's entire source code depot for occurrences of std::sort and the wrapper function absl::c_sort.
- A small fraction of these are excluded based on their filename (e.g. nonsource files) or path (e.g. compiler test suites).
- We then exclude the vast majority whose directories do not account for a relevant number of samples in Google-wide CPU usage measurements.
- This leaves several hundred occurrences, which are still too numerous for manual inspection.
- We further filter out calls (about half) which have an extra comparator argument.
- Note that some of them may define a lexicographical ordering within 128 or fewer bits of data, which could be supported by vqsort.
- However, this would be laborious to prove, so we exclude them from our analysis.
- We then manually inspect the code, finding that the total CPU time for sort calls with up to 128-bit keys outnumbers the total for other sorts (e.g. strings and tuples) by a factor of two.
- Although we are surprised by this result, the straightforward and mostly automated methodology makes us reasonably confident that the analysis is valid.
- However, there is one major caveat: we only find calls to the standard library sort.
- Other potential sort-like algorithms such as tournament sort are not included in the analysis.
- We remark that vectorizing sorts with custom comparators is still possible.

## Conclusions
- We used the Highway cross-platform abstraction layer to implement vqsort and utilize the most efficient instructions available on the current CPU.
- The algorithm features a new recursive sorting network for up to 256 elements that mitigates the previously reported [23] problem of excessive code size.
- Our measurements indicate that vqsort makes very good use of AVX-512 instructions.
- To the best of our knowledge, it is the fastest sort for individual (non-tuple) keys on AVX2 and AVX-512, outperforming the standard library's sort by factors of 10 to 20.
- When using Arm NEON on Apple M1 hardware, we observe a 3-8x speedup versus the standard library.
- We are currently only able to test the code for SVE and RISC-V via emulators which do not predict performance. SVE benchmarking may be feasible once Amazon's Graviton3 CPUs are publicly accessible. However, we can reasonably expect good results because both provide native compress instructions.
- Integrating vsort into the state-of-the art parallel sorter ips4o [13] yields a speedup of 1.59 (geometric mean). In contrast to previous works, which can be seen as proofs of concept, we have focused on practical usability (support for 32/64-bit floatingpoint and 16/32/64/128-bit integer keys in ascending or descending order), and performance portability (supporting seven instruction sets with close-to native performance).
