---
title: "Image-and-Language Understanding from Pixels Only"
date: 2022-12-15T18:52:08.000Z
author: "Michael Tschannen, Basil Mustafa, Neil Houlsby"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08045v1.webp" # image path/url
    alt: "Image-and-Language Understanding from Pixels Only" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08045)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08045).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/image-and-language-understanding-from-pixels).

# Abstract
- CLIP is a multimodal model that consists of many task- and modality-specific pieces
- CLIPPO is a model that uses a single encoder that processes both regular images and text rendered as images
- CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP, with half the number of parameters and no text-specific tower or embedding
- When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling)

# Paper Content

## Introduction
- Large-scale multimodal training of Transformer-based models has led to improvements in the state-of-the-art in different domains
- In particular, a single large pre-trained model can outperform task-specific expert models
- However, large multimodal models often use modality or dataset-specific encoders and decoders, and accordingly lead to involved protocols
- For example, such models frequently involve training different parts of the model in separate phases on their respective datasets, with dataset-specific preprocessing, or transferring different parts in a task-specific manner
- Such modality and task-specific components can lead to additional engineering complexity, and poses challenges when introducing new pretraining losses or downstream tasks
- Developing a single end-to-end model that can process any modality, or combination of modalities, would be a valuable step for multimodal learning
- In this paper, we explore the use of a pure pixel-based model for multimodal learning of text and images
- Our model is a single Vision Transformer that processes visual input, or text, or both together, all rendered as RGB images
- We train our model using only a single task: contrastive learning, as popularized by CLIP [50] and ALIGN [30]
- We therefore call our model CLIP-Pixels Only (CLIPPO).
- We find that CLIPPO performs similarly to CLIP (within 1-2%) on the main tasks CLIP was designed for-image classification and text/image retrieval-despite not having modality-specific towers.
- Surprisingly, CLIPPO can perform complex language understanding tasks to a decent level without any left-to-right language modelling, masked language modelling, or explicit word-level losses.
- In particular, on the GLUE benchmark [66] CLIPPO outperforms classic NLP baselines, such as ELMO+BiLSTM+attention, outperforms prior pixel-based masked language models [54], and approaches the score of BERT [14].
- Interestingly, CLIPPO can also obtain good performance on VQA when simply rendering the image and text together, despite never having been pre-trained on such data.

## Related work
- Multimodal and contrastive pretraining is closely related to CLIPPO
- CLIPPO, ALIGN, and LIMoE explore large-scale contrastive training
- Different methods are used to generate text pairs for multimodal training, including word deletion, span deletion, reordering, synonym substitution, and next-sentence-prediction
- Many models rely on text extracted using an off-the-shelf OCR pipeline in combination with document images
- Contrastive training in NLP is closely related to CLIPPO

## Contrastive language-image pretraining with pixels
- Contrastive language-image pretraining has emerged as a powerful, scalable paradigm to train versatile vision models on web-scale data sets
- Concretely, this approach relies on image/alt-text pairs which can be automatically collected at large scale from the web
- Thereby, the textual descriptions are usually noisy, and can e.g. consist of single keywords, sets of keywords, or potentially lengthy descriptions with many attributes describing the image content
- Using this data, two encoders are jointly trained, namely a text encoder embedding the alt-texts and an image encoder embedding the corresponding images into a shared latent space.
- These two encoders are trained with a contrastive loss, encouraging the embeddings of corresponding images and alt-text to be similar, and at the same time to be dissimilar from all other image and alt-text embeddings
- Once trained, such an encoder pair can be used in many ways: It can be specialized to classifying a fixed set of visual concepts via their textual descriptions (zero-shot classification); the embeddings can be used to retrieve images given a textual description and vice-versa; or the vision encoder can be transferred in supervised fashion to a downstream task by fine-tuning on a labeled data set or by training a head on top of the frozen image encoder representation.
- In principle, the text encoder can be used as a standalone text embedding, but this application-to our knowledge-has not been explored in-depth, with some authors citing the low quality of the alt-texts leading to weak language modeling performance of the text encoder
- Previous works [42,47] have shown that the image and text encoder can be can be realized with a single shared transformer model (henceforth referred to as single tower model, or 1T-CLIP), where the images are embedded using a patch embedding, and the tokenized text is embedded using a separate word embedding.
- Alongside multimodal versatility, CLIPPO alleviates common hurdles with text processing, namely the development of an appropriate tokenizer and vocabulary.
- This is particularly interesting in the context of a massively multilingual setup, where the text encoder has to handle dozens of languages.
- We find that CLIPPO trained on image/alt-text pairs performs comparably with its 1T-CLIP counterpart on common image and image-language benchmarks, and is com-petitive with strong baseline language models on the GLUE benchmark

## Experiments

### Training details and models
- We use a single training setup for all our baselines and visual text models.
- This setup was tuned to to produce good results for standard image/alt-text contrastive training as in [50] (using exactly the same loss function as [50], following the pseudocode in [50, Fig. 3]) and we found that it readily transfers to 1T-CLIP and CLIPPO (including variants with text/text co-training).
- We use the default architecture for 1T-CLIP and CLIPPO, a ViT-B/16.
- We perform a subset of experiments with a ViT-L/16 architecture to study the effect of scale (we equip both models a MAP head [37] to pool embeddings).
- In all cases, the representation dimension used for the contrastive loss is 768.
- We set the batch size to 10,240 and train the main models for 250k steps, using a minimum 100k training steps for ablations.
- For models co-trained with a certain percentage of text/text data, we scale the number of iterations such that the number of image/alt-text pairs seen matches the number of iterations of the corresponding model without text/text data (e.g. when 50% of the data is text/text pairs we increase the number of iterations from 250k to 500k).
- The contrastive loss is computed across the full batch.
- We use the Adafactor optimizer [60] with a learning rate of 10 −3 and decoupled weight decay with weight 10 −4 .
- Baseline CLIP-style models are trained using the T5en SentencePiece tokenizer [51]; we use the abbreviation CLIP * for the two tower model from [50] trained from scratch using the setup described above, to avoid confusion with the model released by [50].
- A sequence length of 196 is used, as this matches the number of visual text "tokens" CLIPPO can process with patch size 16 has at 224px resolution (which we use throughout unless noted otherwise).
- Visual text
- For visual text rendering [54,57]
- Table 1. Vision and vision-language cross-modal results.
- We report ImageNet-1k 10-shot linear transfer validation accuracy (I1k 10s.), ImageNet-1k zero-shot transfer validation accuracy (I1k 0s.), image-to-text and text-to-image retrieval recall@1 on MS-COCO (C I→T and C T→I) and on Flickr30k (F T→I and F I→T).
- CLIPPO and 1T-CLIP incur a minor drop in these evaluations compared to CLIP * , while only using about half of the model parameters.
- Co-training with text pairs from C4 (models with + xx%C4) degrades performance on some cross-modal tasks (but leads to improved language understanding capabilities, see Table 2).

### Evaluations and metrics
- Uses standard metrics from the literature
- Classifies images with embedded textual descriptions and compares these with image embeddings
- Reports the accuracy on ImageNet-1k
- Tests the low-data transfer performance of the models by means of the linear adaptation protocol from [15]
- Evaluates the language understanding capabilities on the General Language Understanding Evaluation (GLUE) benchmark
- Transfers baselines and CLIPPO models by attaching a 2-hidden layer MLP with 768 units to their representation and following precisely the fine-tuning protocol from BERT

### Vision and vision-language understanding
- CLIPPO outperforms CLIP * and 1T-CLIP on a variety of benchmarks
- The difference in performance between CLIPPO and 1T-CLIP is small for a B/16 backbone at 100k training steps
- The multilingual CLIPPO model performs somewhat worse than the corresponding 1T-CLIP
- Increasing the model size to L/16 does not improve performance
- However, when trained with sentence pairs, CLIPPO outperforms 1T-CLIP

### Multilingual vision-language understanding
- For typical language models, tokenizer choice can be a challenging process
- CLIPPO bypasses this issue, removing any language-related bias stemming from unbalanced or restrictive tokenizers
- We consider multilingual image/text retrieval on Crossmodal3600 and compare CLIPPO, trained on WebLI with multilingual alt-texts, against 1T-CLIP with a number of SentencePiece tokenizers; one trained from 300M WebLI multilingual alt-texts, English (T5-en) and multilingual (T5-all) tokenizers from T5, and a multilingual tokenizer (mT5) from mT5
- On average, CLIPPO achieves comparable retrieval performance to these baselines
- In the case of mT5, the use of extra data to create the specialized vocabulary can boosts performance above that of CLIPPO; the leveraging of such extra parameters and data in the multilingual context will be an interesting future direction for CLIPPO
- If a tokenizer is well suited to a particular dataset, it will tokenize to shorter sequencesthis is especially the case when byte fallback [36] is enabled
- Senten-cePiece tokenizers have the advantageous ability to tokenize entire-possibly quite long-words to single tokens
- CLIPPO cannot learn any such compression, but benefits from equal treatment of all languages and words: it will by definition generalize equally well to all data, as its tokenization schema has not been trained on a specific dataset
- We analyze 20,000 samples for each of the 104 C4 languages. Each CLIPPO token is assumed to be a 16 × 16 patch; though in typical computations all approaches considered here would pad to a fixed length, we compute CLIPPO's sequence length according to the last patch which contains rendered text
- Fig. 4 shows the fraction of C4 languages where CLIPPO processes tokens more efficiently than the vocabularies discussed above
- We conservatively define "more efficient" as producing a shorter token sequence for over 75% of examples
- Even so, CLIPPO is indeed more efficient across the majority of languages
- Per-language breakdowns of multilingual retrieval performance and tokenization efficiency are further discussed in Appendix C.4
- bLI performs competitively with the BiLSTM+Attn+ELMo baseline which relies on deep word embeddings trained on a large language corpus
- Also, it can be seen that CLIPPO along with 1T-CLIP outperform the language encoder trained using standard contrastive language vision pretraining (CLIP * ). This indicates that multimodal training in a single encoder benefits language understanding
- Finally, training CLIPPO only on sentence pairs leads to a model which outperforms PIXEL by a significant margin, albeit our model has seen more sentence pairs than PIXEL, so PIXEL might improve as well when training longer

### Language understanding
- To corroborate that contrastive NSP is a sensible objective to improve language understanding in the context of CLIPPO, we train CLIPPO without any image/alt-text data on pairs of parallel translated sentences.
- We evaluate the resulting text representations on GLUE.

### Modality gap analysis
- Liang et al. discovered that text and image embeddings of CLIP-style models form two distinct clusters
- The modality gap decreases significantly when training contrastively with sentence pairs

## Discussion and limitations
- CLIPPO outperforms the 1T-CLIP baseline across many of the considered tasks
- CLIPPO has less than half the parameters of a comparable CLIP *
- To achieve language understanding performance competitive with PIXEL and BERT on GLUE, contrastive co-training with text pairs is necessary
- While adding 25% C4 data to the batch seems to strike a good balance across all tasks considered, it does induce a non-negligible drop in zero-shot image classification and image/text retrieval
- CLIPPO currently relies on cleanly rendered text as an input and its capabilities to handle text from documents or web pages without further adaption is limited
- CLIPPO obtains strong multilingual image/text retrieval performance without requiring the development of an appropriate tokenizer

## Conclusion
- We introduced CLIPPO, a joint model for processing image and text through the lens of vision.
- This reduces design choices and parameter count, can improve language understanding, and increases generality across multiple languages.
- We also explored methods of enhancing language understanding, where traditional image/text contrastive models trained on web data fall short (e.g. poor grammatical understanding on CoLA).
- We demonstrate this is possible by co-training with text pairs, with CLIPPO models outperforming strong NLP baselines while maintaining solid image understanding capabilities.
- Although CLIPPO suffers somewhat when co-training on multiple tasks, and future work aiming to further harmonize the co-training setup to ameliorate the trade-off would enhance the models significantly.
- Deeper understanding of the design choices made in rendering text as images, and the impact on performance, is another interesting avenue to explore.
