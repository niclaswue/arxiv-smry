---
title: "SuperGF: Unifying Local and Global Features for Visual Localization"
date: 2022-12-23T13:48:07.000Z
author: "Wenzheng Song, Ran Yan, Boshu Lei, Takayuki Okatani"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13105v1.webp" # image path/url
    alt: "SuperGF: Unifying Local and Global Features for Visual Localization" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13105)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13105).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/supergf-unifying-local-and-global-features).

# Abstract
- Advanced visual localization techniques encompass image retrieval challenges and 6 Degree-of-Freedom (DoF) camera pose estimation, such as hierarchical localization.
- Previous methods have achieved this through resource-intensive or accuracy-reducing means, such as combinatorial pipelines or multi-task distillation.
- In this study, we present a novel method called SuperGF, which effectively unifies local and global features for visual localization, leading to a higher trade-off between localization accuracy and computational efficiency.
- Specifically, SuperGF is a transformer-based aggregation model that operates directly on image-matching-specific local features and generates global features for retrieval.
- We conduct experimental evaluations of our method in terms of both accuracy and efficiency, demonstrating its advantages over other methods.
- We also provide implementations of SuperGF using various types of local features, including dense and sparse learning-based or hand-crafted descriptors.

# Paper Content

## Introduction
- Visual localization is the problem of estimating the 6 Degree-of-Freedom (DoF) camera pose from which a given image was taken relative to a reference scene representation.
- Advanced visual localization approaches are hierarchical, encapsulating image retrieval problems and 6-DoF camera pose estimation [35,42,50].
- i.e., given a query image of the current view, related candidates are determined by performing image retrieval in a database; then, perform local feature matching between the query image and related NetVLAD [1] and SuperPoint [10], respectively.
- candidates to establish point correspondences between 2D to 3D, solving a PnP and estimate 6 DoF camera pose of the query image.
- Naturally, two types of image features are needed to perform such a hierarchical visual localization approach, i.e., global image features for retrieval and local image features for image-matching.
- However, existing studies struggle to unify these two types of image features.
- For instance, the image retrieval task also involves these two types of features for the two-stage retrieval strategy [5], i.e., global features are used for large-scale coarse retrieval to get prior candidates, then re-rank it based on the number of inliers obtained from local feature matching with geometry verification.
- However, such local features tend to deviate from needs from 6-DoF pose estimation. Specifically, local features yielded by a retrieval model always tend to be semantically rich and lose spatial invariance [14,26].
- Most studies employ two models for the hierarchical visual localization., i.e., one for image retrieval, such as NetVLAD [1], and another for image matching, such as SIFT [24] or SuperPoint [10].
- However, such a method is not competitive as far as efficiency is concerned due to the absence of shared computing.
- In addition, it is difficult to trade off efficiency and accuracy in model selection. Specifically, accurate image retrieval can improve the accuracy and efficiency of localization within a certain range. How-ever, advanced image retrieval models tend to be heavy to pursue higher accuracy, which causes an increase in computational cost. Still, these improvements do not result in corresponding gains for localization.
- A recent study unifies global and local features for visual localization using multitask distillation, i.e., HF-Net [35]. It uses a CNN that jointly estimates local and global features with a shared encoder. It improves computational efficiency by using a compression model, i.e., Mo-bileNetVLAD (MNV).
- However, the distillation inevitably harms inference accuracy, especially in challenging cases, e.g., night scenes.
- One possible reason is that they do not consider the conflict between the two tasks, i.e., image retrieval and image-matching, at the feature level; see Figure 1.
- Specifically, learning global features for image retrieval tends to induce features in the backbone to be more semantic and less localizable, eventually making it more sparse. Conversely, learning local features for imagematching emphasizes feature invariance, which leads the features to focus more on local information and lacks global semantics.
- This issue has been demonstrated earlier [5] but needs to be handled more appropriately.
- Most recently, transformers have been used for feature extraction in computer vision tasks and have led to state-of-the-art results [26,47].
- It benefits from the desirable property of the self-attention mechanism, which can break through spatial constraints to establish global correlations, and aggregate task-relevant features naturally.
- In this paper, we propose a novel holistic model, Su-perGF, which unifies global and local features for visual localization.
- It works directly on the local features generated by the image-matching model and aggregates to a global image feature, similar to BoW [29,30] or Fisher Vector [33].
- A transformer is adopted to perform feature aggregation, which is more accurate and resource-friendly.
- It integrates global contextual information and establishes global correlations between feature tokens by the self-attention mechanism; thus, semantic cues can be learned automatically from local features of the image-matching model.
- As a result, the transformer module can bridge the feature-level gap between the two tasks and yield robust global features for image retrieval in an efficient manner.

## Related Work

### Approaches for Visual Localization
- Structure-based localization: relies on estimating correspondences between 2D keypoints in the query and 3D points in a sparse model
- Image-based localization: tries to predict the location of a given query image by transferring the geotag of the most similar image retrieved from a geotagged database
- Hierarchical localization: divides the problem into a global, coarse search followed by a fine pose estimation
- Taira et al. applied learning-based features to camera pose estimation but in a dense, expensive manner
- HF-Net integrated learning-based models of image retrieval and image-matching

### Global and Local Image Features
- Hand-crafted local features are limited in invariance
- Recent features emerging from convolutional neural networks (CNN) exhibit unrivaled robustness at a low computing cost
- However, it tends to be task-specific
- Transformers have been adopted for feature extraction in computer vision fields and achieved state-of-art performances

### Overview
- SuperGF is a feature aggregation framework that aggregates local image features into glocal image features with rich semantic information.
- Figure 2 illustrates the framework of SuperGF.
- In the case of the sparse version, only local descriptors where keypoints are taken into count are aggregated.
- In the case of the dense version, we adopt dense descriptors, i.e., dense SIFT or feature map output by the encoder of SuperPoint.
- Then, we process the input of local features into tokens.
- In the case of the dense version, we perform the local descriptors by a tokenizer consisting of a one-layer CNN with downsampling operations.
- Then, we perform position embedding for these tokens.
- In the case of the sparse version, the input of local features is a variable-length sequence.
- To save computational costs, we downsample them by performing clustering.
- Before that, we integrate three features of descriptors, keypoints, and confidence scores by performing positional encoding.
- Finally, these tokens generated by the previous step are input into the three layers of the vision transformer encoder.
- And a global image feature is aggregated from the transformer's output using a GeM pooling layer.

### Local Feature Processing
- The feature map of local descriptors of an input image can be denoted as 16.
- The tokenizer produces a sequence of flattened 2D patterns (H 16 ×W 16 , D) as input tokens for transformer encoder layers.
- The clustering module is based on slot attention and can be represented by function Φ(•) : R i×d → R N ×D .
- The function φ receives three inputs, i.e., key, value, and query, represented by K, Q, and V , respectively.
- The function ψ generates the K, Q, and V , passing through layer normalization and fed to three linear projection functions that project them to dimensions d k , d q , and d v , respectively.

### Aggregate to Global Image Features
- SuperGF employs vision transformer encoders to achieve feature aggregation
- In practice, we follow the implementation of the vision transformer encoder, which consists of a stack of Multi-headed Self-Attention (MSA) and Multi-Layer Perceptron (MLP) modules
- T out is aggregated to global image features by performing Generalized Mean (GeM) pooling
- D is given by

### Training Strategy
- In most studies, image retrieval or Visual Place Recognition (VPR) is treated as a binary problem.
- In the VPR task, the similarity between images in VPR cannot be strictly defined in a binary fashion.
- Thus as we observed, in most VPR datasets, there is a large ambiguity between the positive and negative samples of the training data.
- Moreover, VPR is essentially a problem of ranking, not classification.
- For training a VPR model, metric learning losses distinguish between positive and negative samples by defining a margin, which easily leads to a distance ambiguity.
- This results in the performance being very sensitive to the loss settings so it is often difficult to achieve optimal performance by optimizing a metric learning loss directly.
- Considering the above, we adopt a listwise loss.
- i.e., Average Precision (AP) loss [7,19,31], to train our model.
- The AP loss train models by optimizing ranking results directly.
- We follow the implementation of AP loss as Chen et al. [31].
- As illustrated in Fig. 3 where Let's define the function AP (•) denotes the ranking metric AP for D Q , The final loss function can be defined as:
- where S ∈ [0, 1] denotes the label of image similarity scores between I Q and I P ∪ I N .
- In the case of the sparse version, we further adopt an attention decorrelation loss which aims at reducing the spatial correlation between attention maps.
- It makes the output of the slot attention clustering module, i.e., ζ, as complementary as possible.
- Specifically, we encourage them to attend to different local features, i.e., different locations of the image.
- Let's define α = [α 1 , ..., α N ],
- The attention decorrelation loss is given by:
- where i, j ∈ {1, ..., N }.
- In practice, We train our sparse version model with the above two losses jointly.
- First, we aim to design robust global image descriptors that can be used for place recognition for SuperPiont dependencies.
- we fixed encoder during the training process.
- We perform image-level supervision using the AP loss.
- In practice, we adopt the data annotation provided by Vallina et al. [23] as the soft label, i.e., S for our model training, which represents Field-of-View (FoV) overlap between query and database images.
- Specifically, for each query image, we split the database images into three kinds based on overlap, i.e., positive samples where S ∈ [0.5, 1], soft negative samples where S ∈ (0, 0.5), and negative samples where S = 0.
- For each query image I Q , we compute the AP loss between it with 105 images in the database, which contains 1 positive sample, 2 soft negative samples 100 negative samples.
- each iteration the training process, we load 4 batches compute and optimize the parameters of our model.
- We observe that choosing more negative samples for each query image tends to result in better performance. However, selecting too many negative samples takes up more computational resources and provides limited gain to the model.

## Experiments
- SuperGF outperforms state-of-the-art methods on several benchmark datasets
- The proposed SuperGF can be used to generate global image features
- The performance of SuperGF can be improved by using more accurate feature representations

### Implementation details
- latent embedding dimension 512
- hidden dimension 1024
- trained on MSLS training set
- selected 10,000 query samples randomly from all sub-cities
- trained on 350 epochs in total
- adopted the AdamW optimizer
- set the parameter of weight decay = 10 −4 in training
- set an initial learning rate to 10 −4 and it will finally decline to 10 −6
- performed the retrieval based on the L 2 distance between global image features to obtain the prior candidates
- further perform geometry verification by matching sparse descriptors provided by the front-end model
- in practice, given an image pair of the query and database, we obtain keypoints and sparse local descriptors for both
- then, we perform feature matching to obtain initial point correspondences, using Nearest Neighbor searching (NN) or Su-perGlue
- finally, we estimate the homography with RANSAC as a robust estimator and re-rank the prior candidates based on the number of inliers
- keep the top 100 candidates retrieved by global image features and perform the geometry verification

### Evaluation Datasets
- Evaluation on public benchmark datasets
- Resizing images to 640 × 480 during evaluation
- All images are different challenging appearances

### Metrics
- The Recall@N metric is used to measure the accuracy of localization
- For the localization dataset, i.e., the RobotCar Seasons dataset, different pose recall thresholds are used for each sequence
- The default pose recall thresholds are used for all datasets.

### Compared Methods
- We compare SuperGF against several models on both VPR and visual localization tasks
- We compare methods in the VPR task considering both two approaches, i.e., single-pass and twostage retrieval
- For the experiment of single-pass retrieval, we compare with a widely used baseline, i.e., NetVLAD
- For the experiment of two-stage retrieval, we compare with DELG
- We compare SuperGF with two state-of-the-art methods, i.e., Patch-NetVLAD and TransVPR

## Results and Discussion

### Single-pass Retrieval
- Table 1: Single-pass retrieval results with three metrics of recall, where 'R-' means 'Recall'.
- Nordland test: measures the average precision of the retrieved objects
- Tokyo247 test: measures the average recall of the retrieved objects
- Method: R@1, R@5, R@10
- R@1: the performance of single-pass retrieval only based on global image features is highly relevant to the dataset, which denotes global features as a compact representation
- R@5: the performance of single-pass retrieval only based on global image features is highly relevant to the dataset, which exhibits undesirable generalizability, especially for high-precision retrievals, such as R@1 and R@5
- R@10: the performance of single-pass retrieval only based on global image features is highly relevant to the dataset, which exhibits undesirable generalizability, especially for high-precision retrievals, such as R@1 and R@5
- Using learned local features for the aggregation, i.e., SuperGF-denseSP and SuperGF-sparseSP, show significantly better results than using hand-craft local features, i.e., SuperGF-denseSIFT and SuperGF-sparseSIFT
- Using dense local features show better results than using sparse local features, especially for hand-craft local features

### Two-stage Retrieval
- Table 2 shows the experimental results for two-stage retrieval
- Compared to single-pass retrieval, the robustness is significantly improved by performing re-ranking based on geometry verification
- Using sparse local features designed for imagematching for the re-ranking show on par even better performance than using those task-specific local features
- It is noteworthy that the former performs matching sparse local features, which is more competitive in terms of efficiency than the latter, which performs matching local features densely

### Visual Localization
- Methods that intermediate a retrieval step, structure-based methods, i.e., AS and CSL, show competitive results on the dusk sequence, where the accuracy tends to saturate.
- In contrast, methods based on a hierarchical approach tend to work significantly better than structure-based methods, which suffer from the increased ambiguity of the matches.
- NV+SIFT shows slightly better results than NV+SP on easy cases, such as dusk.
- But in contrast, NV+SP shows significantly better results than NV+SIFT in those challenging cases, especially at night.
- It indicates that the SIFT descriptors perform better than Su-perPoint on camera pose estimation in normal cases, but the conclusion is the opposite in the situation of challenging cases.
- By comparing our method with baselines, we can observe that although the two-stage localization fuses image retrieval and camera pose estimation, it is clear that the latter plays a dominant role.

### Latency and Memory
- The computational time and memory requirements for compared methods of extracting glocal image features to process a single query image
- In practice, we unify the input image size as 480 × 640 for all methods
- SuperGF-sparse uses minimal resources but generates SOTA-level global image features for image retrieve
- SuperGF is affected by the adopted local descriptor significantly

## Summary and Conclusion
- Transformer-based method for global feature extraction
- Advantage over other methods in terms of accuracy and efficiency
- Can be used for image retrieval or image matching
- Can be used with learning-based or hand-craft descriptors
- Can be used with dense or sparse local features
