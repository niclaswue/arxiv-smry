---
title: "Visconde: Multi-document QA with GPT-3 and Neural Reranking"
date: 2022-12-19T17:39:07.000Z
author: "Jayr Pereira, Robson Fidalgo, Roberto Lotufo, Rodrigo Nogueira"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09656v1.webp" # image path/url
    alt: "Visconde: Multi-document QA with GPT-3 and Neural Reranking" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09656)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09656).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/visconde-multi-document-qa-with-gpt-3-and).

# Abstract
- The paper proposes a question-answering system that can answer questions whose supporting evidence is spread over multiple (potentially long) documents.
- The system, called Visconde, uses a three-step pipeline to perform the task: decompose, retrieve, and aggregate.
- The first step decomposes the question into simpler questions using a few-shot large language model (LLM).
- Then, a state-of-the-art search engine is used to retrieve candidate passages from a large collection for each decomposed question.
- In the final step, we use the LLM in a few-shot setting to aggregate the contents of the passages into the final answer.
- The system is evaluated on three datasets: IIRC, Qasper, and StrategyQA.
- Results suggest that current retrievers are the main bottleneck and that readers are already performing at the human level as long as relevant passages are provided.

# Paper Content

## Introduction
- Question answering tasks that use relatively short contexts (e.g., a paragraph) have seen remarkable progress in multiple domains
- Most of these approaches rely on fine-tuning large language models on supervised datasets, which may be available for a variaty of domains
- Other approaches use Transformers for long sequences like LongT5 to process the context document and the question at once
- The few-shot capability of LLMs may reduce the costs for solving QA tasks, as it allows one to implement QA systems for different domains without needing a specific annotated dataset
- In addition, recent studies showed that adding a chain-of-thought (CoT) reasoning step before answering significantly improves LLMs' zero or few-shot effectiveness on diverse QA benchmarks
- Thus, we argue future work on multi-document QA should focus on improving retrievers.

## Related Work
- Most approaches for multi-document QA are typically based on a retriever followed by a reader component
- Recent studies used dense retrievers [30,7] or commercial search engines [17] for this task.
- For the reader component, some studies used sequence-to-sequence models to generate natural language answers [18,34,12], numerical reasoning models adapted to reason also over text [8,30,7], or LLMs to aggregate information from multiple documents [21,17].
- Recent work enriches this pipeline by adding components to perform query decomposition [24,4,6,1,11] or evidence retrieved by a web search engine [21,24,17].
- Our work is similar to these, but we focus on evaluating the limitations of this method and found that the retrieval component needs more work.

## Our Method: Visconde
- Visconde is a multi-document QA system that has three main steps: Question decomposition, Document Retrieval, and Aggregation.
- As illustrated in Figure 1, the system first decomposes the user question when necessary and searches for relevant documents to answer the subquestions.
- The retrieved documents are the basis for generating an explanation and a final answer using an LLM.
- In Figure 1 we show an example of a question that needs to be decomposed (Q) extracted from the IIRC dataset [7] and the subquestions generated by the model (Q1 and Q2).
- Document Retrieval: For document retrieval, we used a strategy divided into three main steps: 1) document indexing -we create an inverted index using Pyserini [19]; 2) candidates retrieval -we use the Pyserini implementation of the BM25 algorithm to retrieve candidate documents.
- Document rerankingwe rerank the top-1000 documents retrieved using a sequence-to-sequence model designed for reranking, the monoT5 model [22], an adaptation of the T5 model [25].
- monoT5 receives as input a sequence with the document and the query, and a softmax function is applied only on the logits calculated by T5 to the tokens true and false. The log probability of the token true is used as the document relevance score given the question. The output is a list of documents ranked by the relevance scores.
- Aggregation: We use GPT-3 (text-davinci-002) as a few-shot learner for the aggregation step.
- Different studies have shown that the effectiveness of LLM can be improved by inducing it to first generate reasoning steps before answering a question [32,31,16,3].
- We use CoT to induce the LLM to reason over multiple documents and answer a question, as shown in the example in Figure 2.
- In our prompt, each example has a list of context documents (e.g., [Document 1]), a question, an evidence paragraph, and the answer.
- The context documents of the target example are the top-k documents from the retriever step.
- When the question is decomposed, we use the top-k documents from each subquestion.
- For the target example, an evidence paragraph is not provided, and the LLM must generate it, as well as a final answer.

### IIRC
- The Incomplete Information Reading Comprehension (IIRC) dataset consists of information-seeking questions that require retrieving the necessary information missing from the original context.
- Each original context is a paragraph from the English Wikipedia, which comes with a set of links to other Wikipedia pages.
- Pre-processing: we used the dynamic prompt described in Section 3. For this, we automatically generated reasoning paragraphs for 10% of the IIRC training set (1340 questions) using GPT-3.
- In addition, we processed the context articles provided by the dataset to create a searchable index.
- Procedure: following the framework depicted in Figure 1, we first decomposed the questions from the IIRC test set. We performed document retrieval on a database of Wikipedia documents provided in the dataset.
- In the aggregation step, we applied four methods: 1) using GPT-3 without CoT and providing the links and the ground truth contexts, i.e., skipping the document retrieval step; 2) using the reasoning step with the links and ground truth contexts; 3) using reasoning step over the intersection of retrieved documents and the documents cited by the main context; and 4) reasoning over the documents retrieved from the entire Wikipedia subset provided by the dataset.

### Qasper
- Qasper is an information-seeking QA dataset over academic research papers
- The task consists of retrieving the most relevant evidence paragraph for each question and answering the question
- Procedure: we did not apply query decomposition because the questions in this dataset are closed-ended, i.e., they do not require decomposition as they are grounded in a single paper of interest
- For example, the question "How is the text segmented?" only makes sense concerning its grounded paper. Besides, we skipped the BM25 step for document retrieval as the monoT5 reranker can score each paragraph in the paper in a reasonable time
- The document retrieval step consists of reranking the paper's paragraphs based on the question and choosing the top five as context documents
- We did not notice any advantage in using a dynamic prompt in this dataset

### StrategyQA
- StrategyQA is a dataset of open-domain questions that require reasoning steps
- The dataset has three tasks: 1) question decomposition, measured using a metric called SARI, generally used to evaluate automatic text simplification systems; 2) evidence paragraph retrieval, measured as the recall of the top ten retrieved results; and 3) question answering, measured in terms of accuracy
- Pre-processing: we did not generate reasoning paragraphs for the training examples since the context comprises long paragraphs that exceed the model input size limit (4000 tokens)
- Procedure: we applied question decomposition and performed retrieval using the approach described in Section 3.

### Results
- Table 1: The results of the experiments.
- First, the results obtained in the IIRC dataset are shown.
- Second, the results obtained when searching for context in the links the dataset provides are shown.
- Third, the results obtained when searching for contexts in the entire dataset are shown.
- The LED-base model is the baseline [5].
- The SOTA model is the model proposed by Xiong et al. [33].
- Visconde outperformed the baseline but did not surpass the SOTA model.
- Xiong et al. [33]'s model is a long Transformer fine-tuned on the task.
- Regarding evidence F1, our system outperforms the baseline, but there is still a gap between our performance and human performance.
- Visconde had a high performance on the boolean questions but a low score on the abstractive ones.

## Conclusion
- Our system rivals state-of-the-art supervised models in three datasets
- Our results suggest that using GPT-3 as a reader is close to human-level performance
- Inducing the model to give explanations before answering a question improves effectiveness
