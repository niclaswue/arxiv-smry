---
title: "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference"
date: 2022-12-15T21:35:46.000Z
author: "Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, William Cohen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08153v1.webp" # image path/url
    alt: "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08153)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08153).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/fido-fusion-in-decoder-optimized-for-stronger).

# Abstract
- Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
- However, FiD suffers from very expensive inference.
- We show that the majority of inference time results from memory bandwidth constraints in the decoder, and propose two
- The faster decoder inference then allows for a much larger decoder.
- We denote FiD with the above modifications as FiDO, and show that it strongly improves performance

# Paper Content

## Introduction
- A large body of work has demonstrated that language model performance on downstream tasks can be improved by augmenting the model with relevant retrieved text
- In particular, the Fusion-in-Decoder (FiD) architecture (Izacard and Grave, 2021) stands out for strong performance, even outperforming much larger models on many knowledge-intensive tasks
- However, FiD is also expensive, and its high computational burden leads to challenges in many practical settings
- In particular, inferences with FiD are costly, making it uneconomical to use in the field
- It is usually possible to improve performance at the cost of increased computational resources, through scale (Brown et al., 2020;Chowdhery et al., 2022), sampling (Graves, 2012) or increased processing (Wang et al., 2022;Yu et al., 2022b)
- Therefore, performance and computational cost are two sides of the coin: the computational burden of FiD consumes resources that could be spent on other avenues to improve the model
- Progress is made by pushing out the frontier of the performance-cost tradeoff. That leads to the primary question investigated in this work: what are the factors that most affect FiD's inference cost, and how can we reduce that cost without impairing performance?
- In our analysis, we show that because the encoder is applied to a large number of retrieved passages, the encoder requires an order of magnitude more Floating Point Operations (FLOPs) than the decoder
- However, the majority of inference time is actually spent in the decoder, as has been observed in prior work (Hofstätter et al., 2022)
- This surprising result is shown in Figure 1
- Our analysis finds that for typical inference settings the FiD decoder is memory bandwidth bound (Williams et al., 2009) as a result of multi-head attention (Vaswani et al., 2017)  cost of cross-attention over retrieved passages by removing most cross-attention layers from the decoder.
- With these changes to the model the memory-bandwidth bottleneck is eliminated: decoder inference is now orders of magnitude faster and most inference time is spent in the encoder, consistent with the balance of FLOPs between components.
- Finally, we propose to take advantage of the faster decoder by scaling decoder size, using a smaller encoder to extract information from retrieved passages and a large decoder to assimilate the information and reason about the desired output. We refer to the resulting series of models as FiDO (Fusion in Decoder Optimized) and show that FiDO strongly outperforms vanilla and efficient FiD models on question-answering datasets Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and WebQuestions (Berant et al., 2013) for a wide range of inference budgets and settings.

## Analysis
- Retrieval-augmented models generally process many context tokens for each question or answer token, which consumes the bulk of operations.
- However, past work has shown that most inference time for Fusion-in-Decoder (FiD) is spent in the decoder (Hofstätter et al., 2022).
- Our own experiments yield the same conclusion (Figure 1).
- This section investigates expensive FiD decoder inference and finds the slower decoder speed to be the result of memory bandwidth constraints exacerbated by attention over retrieved documents.

### Fusion-in-Decoder
- The backbone of the Fusion-in-Decoder model is a T5 encoder-decoder architecture.
- The model is provided a question or other input, as well as a number of relevant retrieved text passages.
- The question is prepended to each retrieved passage, and then the encoder is applied to each passage separately.
- The resulting representations are concatenated.
- Finally, the decoder cross-attends to the large number of concatenated representations and assimilates the information from the different passages to generate an answer.

### FLOPs of FiD model
- Model speed is determined by the number of floating point operations (FLOPs) required relative to the speed at which computations are performed
- The decoder consumes roughly 1 6 the FLOPs of the encoder
- Figure 1 shows that actual measured training time closely mirrors this FLOPs approximation

### Effective computational throughput

### Operational intensity of FiD inference
- The operational intensity of the MLP layer (R MLP 1) during inference is high
- Memory bandwidth is a more severe bottleneck for attention inference, particularly cross-attention
- At each decoding step the model applies projections for a single token, and has to load all cached key and value projections from encoder tokens and prior decoder tokens into memory
- This leads to very low operational intensity
- Specifically, query/key/value/output projections for a single position take O(bd 2 ) operations
- As discussed earlier, we can ignore the attention computation itself. The model needs to load projection matrices (O(d 2 ) memory) and past keys and values (O(bnd) memory)
- Therefore, the inverse operational intensities for self-attention layers, R S-MHA and cross-attention layers R C-MHA are

## Method
- accounts for a negligible fraction of the total FLOPs
- The encoder is more expensive than the decoder
- The encoder is more expensive than the decoder even when the number of training examples is doubled
- The encoder is more expensive than the decoder even when the number of training examples is tripled
- The encoder is more expensive than the decoder even when the number of training examples is quadrupled
- The encoder is more expensive than the decoder even when the number of training examples is quintupled

### Layer-sparse cross-attention
- The decoder cross-attention layer is the primary bottleneck for inference
- FiD-Light (Hofstätter et al., 2022) improves the operational intensity by reducing the effective input length by a factor of K
- We instead propose to remove cross-attention from some decoder layers entirely, keeping cross-attention only in one out of every K decoder layers
- We call this layer-sparse cross-attention (LSA)
- Section 5 provides evidence that LSA achieves similar speedups without FiD-Light's drop in quality
- For FiDO we use LSA with sparsity K = 6, which means that a Large decoder has cross-attention at the 6th, 12th, 18th and 24th layer
- In principle these methods can be combined, but we find that in practice after applying layer-sparse cross-attention and multi-query attention the remaining cross-attention makes up a small proportion of decoder inference cost and further speedups from reducing cross-attention cost are modest

### Multi-query attention
- Shazeer proposes to increase the operational intensity of decoder attention layers by applying multi-query attention, in which keys and values share a single head each and only queries have multiple heads.
- With a single head, keys and values use a factor h less memory and are much faster to load.
- With multi-query attention, keys and values occupy O(bnd/h) memory, so that the inverse operational intensity of cross-attention becomes which has the problematic term ns d reduced by factor of h.
- Multi-query attention further reduces inference cost (Figure 2) and memory (Table 1) on top of layer-sparse cross-attention, though not training speed (Table 2).

### Asymmetric Decoder
- After applying layer-sparse cross-attention and multi-query attention, the decoder is vastly cheaper than the encoder for both training and inference.
- We propose to exploit this computational structure by massively scaling the decoder by up to 15x, while only modestly increasing inference cost.
- For example, Figure 2 shows that replacing the Basesized decoder with an XL-sized decoder increases the total inference time per sample by only 21%.
- Fine-tuning costs also increase only modestly (Table 2). However, pre-training costs increase more strongly (though still much less than the scaling factor of the decoder), as T5 pre-training uses a much smaller ratio of input length to output length.
- After reducing the decoder cross-attention memory costs scaling the decoder only mildly increases activation memory, so that FiDO can still fit much larger batch sizes than vanilla FiD (Table 1).

## Related Work
- Retrieval-augmented models exist that use a large body of approaches
- Some particularly well known models are REALM (Guu et al., 2020), RAG (Lewis et al., 2020), RETRO (Borgeaud et al., 2022) and Fusion-in-Decoder (Izacard and Grave, 2021)
- FiD in particular has achieved state-of-the-art performance on a wide variety of tasks (Izacard and Grave, 2021;Izacard et al., 2022;Yu et al., 2022b) and in this work we focus on improving the performanceefficiency trade-offs for FiD
- RETRO is another closely related retrieval-augmented model, as it uses a small encoder for retrieved context and a larger primary decoder like FiDO does.
- Unlike RETRO, FiDO's efficiency improvements allow it to tractably attend to many retrieved passages with a much larger decoder
- Efficient Transformers Our work builds heavily on existing insights into neural network and particularly Transformer speed.
- Previous work has found that data movement is often a constraining factor for computations on modern devices (Williams et al., 2009;Dao et al., 2022;Shazeer, 2019)
- Shazeer (2019) shows that autoregressive Transformers are particularly bandwidth bound during inference, and proposes multi-query attention as a partial solution.
- We find that this is exacerbated by the FiD setting, and adopt multi-query attention for FiDO to ameliorate the problem.
- Pope et al. (2022)  whereas we focus on performance/cost trade-offs for the retrieval-augmented setting.
- Another way to alleviate memory bandwidth constraints is to quantize model parameters and possibly activations (Dettmers et al. , 2022;Zeng et al., 2022).
- Quantizing models reduces data that needs to be sent to device registers, and also reduces overall memory usage which allows for larger, more efficient batch sizes.
- Finally, it is possible to distill (Hinton et al., 2015;Gou et al., 2021) models into a smaller student model, which is cheaper for inference. However, knowledge distillation requires labeling a very large number of samples with the larger model, so reducing the inference costs of larger models is highly valuable.
- Efficient retrieval-augmented models FiDO lies in a body of work that attempts to improve the efficiency of retrieval-augmented or long-input models.
- One direction focuses on reducing the cost of the attention mechanism.
- LongT5 (Guo et al., 2022) routes long-range attention through a small number of global tokens.
- FiD-Light (Hofstätter et al., 2022), the most closely related work to FiDO, employs a similar mechanism for FiD, as the decoder attends to only the first 1 K proportion of representations of each retrieved passage.
- We opt to introduce sparsity in attention layers as in ReadTwice (Zemlyanskiy et al., 2021) instead of attention patterns.
- FiDO applies cross-attention from the decoder to the encoder in one out of every K layers, which achieves a similar speedup to FiD-Light but with only minor performance penalty.
- FiDO also incorporates multi-query attention leading to a further order of magnitude reduction in decoder inference cost, and takes advantage of this to massively scale the decoder.

## Experiments

### Experiment Setup
- Pre-trained models are based on the T5.1.1 architecture (Raffel et al., 2020), pre-trained from scratch on C4 (Dodge et al., 2021) using JAX (Bradbury et al., 2018), FLAX (Heek et al., 2020), and T5X (Roberts et al., 2022).
- We employ the standard T5 training recipe except for a modified Adafactor (Shazeer and Stern, 2018) optimizer.
- Appendix A describes training in greater detail.
- Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and WebQuestions (Berant et al., 2013) are used as datasets.
- For all datasets, each sample is paired with a set of 100-word Wikipedia passages ranked by DPR (Karpukhin et al., 2020) score.
- The question is prepended to each retrieved passage, and then truncated to 256 tokens.
- The experiments in the paper use 40 retrieved passages to balance performance and computational cost, but our results hold across a wide range of retrieved passages.
- We fine-tune each model with batch size 64 and learning rate 0.001 with early stopping based on dev set performance.
- Inference setup is chosen for our main results and is performed on a single TPUv4.
- Predictions are generated with greedy decoding.
- Analysis in Section 5.4 investigates how trade-offs change with input and output length, low batch size and different sampling methods.

### Main results
- Layer-sparse cross-attention significantly reduces inference cost with modest performance degradation.
- Separately, Table 4 compares the inference speed and performance impact of layersparse cross-attention with the token-sparse crossattention from FiD-Light.
- Reducing cross-attention layers and inducing encoder output sparsity by the same factor lead to similar speedups, but layer-sparse cross-attention achieves the inference speedup with much lower performance penalty.
- Note that we find a much larger performance degradation from compressing the encoder output in our setting compared to the experiments in Hofstätter et al. (2022).
- Some exploratory experiments suggest that multi-task training fine-tuning on large amounts of data as done in FiD-Light may ameliorate the performance penalty from compressing encoder output; however even with such training Hofstätter et al. (2022) still report significant peformance degradation, in contrast to LSA.
- Layer-sparsity over a factor of 6 incurs greater performance penalties. However, as shown in Table 4, with layer-sparsity of 6 cross-attention already makes up a small proportion of the total inference cost of the decoder.
- Multi-query attention achieves a large cost reduction on top of layer-sparse cross-attention with minimal performance degradation, consistent with our analysis and findings from Shazeer (2019).
- NQ TQA WQ REALM (Guu et al., 2020) 40.4 -40.7 RAG (Lewis et al., 2020) 44.5 56.8 45.2 RETRO (Borgeaud et al., 2022) 45.5 --T5-XXL (Roberts et al., 2020) 35.2 51.9 42.8 ATLAS (Izacard et al., 2022) 60  improvement in performance at the cost of a modest increase in inference time.

### Other analysis
- Layer-sparse cross-attention and multi-query attention are critical across all settings
- For standard output length, the asymmetric decoder is cheap for any reasonable number of retrieved passages, becoming negligible as a fraction of total inference time as the number of retrievals increases
- As output length increases, the cost of the disproportionately large decoder rises, although it only becomes a substantial proportion of inference time for output length of 256-512 and above
- For tasks with long outputs, such as summarization, one may want to reduce the level of decoder asymmetry (e.g. Base-Large rather than Base-XL)
- Low batch size setting is not binding for most retrievalaugmented models one might want to use at scale: due to FiDO's memory efficiency we are able to fit larger batches even for the XL-XXL model, and if necessary model size can be further extended with quantization (Zeng et al., 2022) and parallelization (Roberts et al., 2022)
- For real-time serving latency can be a constraint, but in those settings it is common practice to use much smaller models which are distilled from larger teacher models (Gou et al., 2021)
- For rare cases where a lower batch size is required layer-sparse and multi-query attention are still important, but cannot fully eliminate the decoder as a bottleneck for inference
- Scaling the decoder is significantly more effective

## Conclusion
- The encoder requires more FLOPs but most time is spent in the decoder due to memory bandwidth constraints.
- We propose FiDO, an extension of FiD which removes most cross-attention layers and employs multi-query attention to vastly reduce the cost of the decoder.
- The resulting model spends most time in the encoder, consistent with compute analysis, which FiDO takes advantage of by strongly increasing the size of the decoder.
- We show that FiDO achieves much stronger performance given the same inference budget relative to existing standard and efficient FiD models.
- Figure 1: Shows the percentage of FLOPs in forward pass, training time and inference time for the encoder and decoder for a Fusion-in-Decoder model with 40 retrieved passages and batch size 24. The vast majority of FLOPs and training time originate from the encoder, but the decoder is much more expensive for inference.
- Figure 3: MAIN RESULT. FiDO achieves much higher performance for any given inference budget. Exact match on Natural Questions (NaturalQ), TriviaQA and WebQuestions (WebQ) test sets as a function of inference budget (log scale). Compares FiD Small, Base and Large models with FiDO Small-Large, Base-XL, Large-XXL and XL-XXL models.
- Cross-attention and total decoder inference time for FiDO Base-XL with varying factors of layersparse cross-attention. The main FiDO configuration uses LSA-6 which has cross-attention every 6 layers.
- Figure 5: Performance on Natural Questions dev set as a function of inference time for FiDO Small, Base and Large models with and without asymmetric decoder.
- Figure 6: Log time per sample (TPS) as a function of retrieved passages (left) or the number of generated tokens (right) for Base FiD variants and FiDO-Base-XL. over a large input sequence. Based on this analysis, we propose two architectural changes. We first propose to reduce the cost of the decoder by removing most cross-attention layers, and secondly to massively scale up the decoder by employing multi-query attention.
- Table 5 compares FiDO to published results. Time per sample (ms) and QA exact match for FiD, FiD-Light, and FiD Base-sized models with layer-sparse cross-attention.
