---
title: "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement"
date: 2023-01-08T12:20:46.000Z
author: "Yan Li, Xinjiang Lu, Yaqing Wang, Dejing Dou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03028v1.webp" # image path/url
    alt: "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03028)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03028).


# Abstract
- Time series forecasting is a widely explored task of great importance
- In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE
- To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting
- In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation

# Paper Content

## Introduction

### Coupled Diffusion Probabilistic Model
- The diffusion model is a family of latent variable models aiming to generate high-quality samples
- To equip the generative time series forecasting model with high expressiveness, a coupled forward process is developed to augment the input series and target series synchronously
- Besides, in the forecasting task, more tractable and accurate prediction is expected
- To achieve this, we propose a bidirectional variational auto-encoder (BVAE) to take the place of the reverse process in the diffusion model
- We present the technical details in the following two parts, respectively
- The forward diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the data
- To diffuse the input and output series, we propose a coupled diffusion process, which is demonstrated in Fig. 2
- Specifically, given the input X = X (0) ∼ q(X (0) ), the approximate posterior q(X (1:T ) |X (0) ) can be obtained as where a uniformly increasing variance schedule
- Furthermore, according to Proposition 1 we decompose X (0) as X r , X . Then, with Eq. (3), the diffused X (t) can be decomposed as follows: where δ X denotes the standard Gaussian noise of X.
- As α can be determined when the variance schedule β is known, the ideal part is also determined in the diffusion process.
- Let X then, according to Proposition 1 and Eq. ( 4), we have where Y denotes the generated noise of Y (t) .
- To relieve the effect of aleatoric uncertainty resulting from time series data, we further apply the diffusion process to the target series Y = Y (0) ∼ q(Y (0) ). In particular, a scale parameter ω ∈ (0, 1) is adopted, such that β t = ωβ t , α t = 1 − β t and ᾱ t = t s=1 α s . Then, according to Proposition 1, we can obtain the following decomposition (similar to Eq. ( 4)):
- Consequently, we have q(Y (t) ) = q( Y (t) Y ).
- Afterward, we can draw the following conclusions with Proposition 1 and Eqs. ( 5) and (6). The proofs can be found in Appendix B.

### Scaled Denoising Score Matching for Diffused Time Series Cleaning
- The time series data can be augmented with the aforementioned coupled diffusion probabilistic model
- The generative distribution p θ ( Y (t) ) tends to move toward the diffused target series Y (t) which has been corrupted
- To further "clean" the generated target series, we employ the Denoising Score Matching (DSM) to accelerate the de-uncertainty process without sacrificing the model flexibility
- DSM [90,65] was proposed to link Denoising Auto-Encoder (DAE) [91] to Score Matching (SM) [43]
- Let Y denote the generated target series, then we have the objective where p σ0 ( Y , Y ) is the joint density of pairs of corrupted and clean samples ( Y , Y ), ∇ Y log(q σ0 ( Y |Y )) is derivative of the log density of a single noise kernel, which is dedicated to replacing the Parzen density estimator: p σ0 ( Y ) = q σ0 ( Y |Y )p(Y )dY in score matching, and E( Y ; ζ) is the energy function
- In the generative time series forecasting setting, the generated samples will be tested without applying the diffusion process
- To further denoise the generated target series Y , we apply a single-step gradient denoising jump

### Disentangling Latent Variables for Interpretation
- The interpretability of the time series forecasting model is of great importance for many downstream tasks
- Through disentangling the latent variables of the generative model, not only the interpretability but also the reliability of the prediction can be further enhanced
- To disentangle the latent variables Z = {z 1 , • • • , z n }, we attempt to minimize the Total Correlation (TC)
- Lower TC generally means better disentanglement if the latent variables preserve useful information
- However, a very low TC can still be obtained when the latent variables carry no meaningful signals
- Through the bidirectional structure of BVAE, such issues can be tackled without too much effort
- As shown in Fig. 1, the signals are disseminated in both the encoder and decoder, such that rich semantics are aggregated into the latent variables
- Furthermore, to alleviate the effect of potential irregular values, we average the total correlations of z 1:n , then the loss w.r.t. the TC score of BVAE can be obtained

### Training and Forecasting
- The denoising network is proposed without sacrificing generalizability
- The latent variables of the generative model are disentangled by minimizing the TC of the latent variables
- The loss function is minimized to learn the generative model accordingly
- Algorithm 1 displays the complete training procedure of D3 VAE with the loss function in Eq. (14)
- For inference, as described in Algorithm 2, given the input series X, the target series can be generated directly from the distribution p θ which is conditioned on the latent states drawn from the distribution p φ .

### Experiment Settings

### Main Results
- Two different prediction lengths, 8 and 16, are evaluated
- The results of longer prediction lengths are available in Appendix D
- Toy Datasets are used to study the uncertainty of the yielded series
- The diffusion process can effectively augment the input or the target
- When the target is not diffused, the denoising network would be deficient.

### Model Analysis
- Variance schedule β affects the prediction performance
- The number of diffusion steps T affects the prediction performance

## Discussion
- Sampling for Generative Time Series Forecasting
- The Langevin dynamics has been widely applied to the sampling of energy-based models (EBMs)
- An illustrative showcase can be found in Appendix F.
- With K and ρ being properly configured, high-quality samples can be generated.
- The Langevin dynamics has been successfully applied to applications in computer vision [56,102], and natural language processing [20].
- We employ a single-step gradient denoising jump in this work to generate the target series.
- The experiments that were carried out demonstrate the effectiveness of such single-step sampling.
- We conduct an extra empirical study to investigate whether it is worth taking more sampling steps for further performance improvement of time series forecasting.
- We showcase the prediction results under different sampling strategies in Fig. 5.
- By omitting the additive noise in Langevin dynamics, we employ the multi-step denoising for D 3 VAE to generate the target series and plot the generated results in Fig. 5a.
- Then, with the standard Langevin dynamics, we can implement a generative procedure instead of denoising and compare the generated target series with different ρ (see Figs. 5b to 5d).
- We can observe that more sampling steps might not be helpful in improving prediction performance for generative time series forecasting (Fig. 5a).
- Besides, larger sampling steps would lead to high computational complexity.
- On the other hand, different configurations of Langevin dynamics (with varying ρ) cannot bring indispensable benefits for time series forecasting (Figs. 5b to 5d).
- With the coupled diffusion probabilistic model, although the aleatoric uncertainty of the time series can be reduced, a new bias is brought into the series to mimic the distribution of the input and target.
- However, as a common issue in VAEs that any introduced bias in the input will result in bias in the generated output [92], the diffusion steps and variance schedule need to be chosen cautiously, such that this model can be applied to different time series tasks smoothly.
- The proposed model is devised for general time series forecasting, it should be used properly to avoid the potential negative societal impacts, such as illegal applications.

## Conclusion
- The generative model has the bidirectional VAE as the backbone
- To further improve the generalizability, a coupled diffusion probabilistic model is developed
- Then a scaled denoising network is developed to guarantee the prediction accuracy
- The latent variables are further disentangled for better model interpretability
- Extensive experiments on synthetic data and real-world data validate that the proposed generative model achieves SOTA performance

## A Related Work

### A.1 Time Series Forecasting
- TSF methods are used to model complex temporal patterns
- Statistical models, such as ARIMA and Gaussian process regression, are well established and applied to many downstream tasks
- Recurrent neural network models are also introduced to model temporal dependencies for TSF in a sequence-to-sequence paradigm
- Besides, temporal attention and causal convolution are further explored to model the intrinsic temporal dependencies
- Many works employ a variational auto-encoder (VAE) to model the probabilistic distribution of sequential data
- To yield predictive distribution for multivariate TSF, TLAE is implemented
- Another line of generative methods for TSF focus on energy-based models (EBMs)

### A.2 Time Series Augmentation
- Traditional methods and deep learning methods can both deteriorate when limited time series data are encountered
- Generating synthetic time series is commonly adopted for augmenting short time series
- Transforming the original time series by cropping, flipping, and warping is another approach dedicated to TSF when the training data is limited
- Whereas the synthetic time series may not respect the original feature relationship across time, and the transformation methods do not change the distribution space, our probabilistic diffusion model for TSF differentiates our work from existing time series augmentation methods

### A.3 Uncertainty Estimation and Denoising for Time Series Forecasting
- There exist works estimating the uncertainty for time series forecasting
- The aleatoric uncertainty of time series is often ignored
- One line of studies focuses on detecting noise in time series data
- However, none of the existing works attempts to quantify the aleatoric uncertainty

### A.4 Interpretability of Time Series Forecasting
- Several works put effort into explaining the deep neural networks to make the prediction more interpretable
- For multivariate time series, the interpretability of the representations can be improved by mapping the time series into latent space
- Besides, recent works have been proposed to disentangle the latent variables to identify the independent factors of the data, which can further lead to improved interpretability of the representation and higher performance
- The disentangled VAE has been applied to time series to benefit the generated results
- However, the choice of the latent variables is crucial for the disentanglement of time series data. We devise a bidirectional VAE (BVAE) and take the dimensions of each latent variable as the factors to be disentangled.

## B Proofs of Lemma 1 and Lemma 2

## C Extra Implementation Details
- The embedding layer takes as input a sequence of words and outputs a vector of embeddings
- The RNN extracts the temporal dependencies between the words in the sequence
- The concatenation layer takes the embeddings and the time stamps of the words in the sequence and outputs a vector of sentences.
- The datasets used in the experiments are real-world datasets
- The embedding method is the embedding method introduced in [109]
- The RNN extracts the temporal dependencies between the words in the sequence
- The concatenation layer takes the embeddings and the time stamps of the words in the sequence and outputs a vector of sentences.

### C.2 Implementation Details of Baselines
- GP-copula is a method based on the Gaussian process
- DeepAR combines traditional auto-regressive models with RNNs by modeling a probabilistic distribution in an auto-encoder fashion
- TimeGrad is an auto-regressive model for multivariate probabilistic time series forecasting with the help of an energy-based model
- Vanilla VAE (VAE for short) is a classical statistical variational inference method on top of auto-encoder
- NVAE is a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization
- factor-VAE (f-VAE for short) disentangles the latent variables by encouraging the distribution of representations to be factorial and independent across dimensions
- β-TCVAE (15) learns the disentangled representations with total correlation variational auto-encoder algorithm
- To train DeepAR, TimeGrad, and GP-copula in accordance with their original settings, the batch is constructed without shuffling the samples.
- Besides, these three baselines employ the cumulative distribution function (CDF) for training, so the CDF needs to be reverted to the real distribution for testing.
- For f-VAE, β-TCVAE, and VAE, since the dimensionality of different time series varies, we design a preprocess block to map the original time series into a tensor with the fix-sized dimensionality, which can further suit the VAEs well.
- The preprocess block consists of three nonlinear layers with the sizes of the hidden states: {128, 64, 32}.
- For NVAE, we keep the original settings suggested in [89] and use Gaussian distribution as the prior.
- All the baselines are trained using early stopping, and the patience is set to 5.
- Longer-Term Time Series Forecasting. To further inspect the performance of our method, we additionally conduct more experiments for longer-term time series forecasting. In particular, by configuring the output length with 32 and 64, we compare D3 VAE to other baselines in terms of MSE and CRPS, and the results (including short-term and long-term) are reported in Table 6.
- We can conclude that D3 VAE also outperforms the competitive baselines consistently under the longer-term forecasting settings.

### D.4 Sensitivity Analysis of Trade-off Parameters in Reconstruction Loss L
- The trade-off hyperparameters in loss L affect the model's performance
- The relative value of MSE is plotted to ensure the difference is distinguishable
- The trade-off hyperparameters ψ, λ and γ affect the model's performance

### D.5 Scalability Analysis of Varying Time Series Length and Dataset Size
- z i = α + βX i
- where α and β are unknown parameters.
- Finally, the latent variable Z is estimated by minimizing the cost function
- where λ is a learning rate.
- Figure 13 shows the disentangling process of latent variable Z of time series.
- We first map input X into latent variable Z.
- Then, for every z i in Z, we decompose it as
- z i = α + βX i
- where α and β are unknown parameters.
- Finally, we estimate latent variable Z by minimizing the cost function
- where λ is a learning rate.

## E Disentanglement for Time Series Forecasting
- Algorithm 3 trains a discriminator for time series disentanglement
- The number of factors affects the prediction performance, as well as the disentanglement quality
- The mutual information between z i,j and a factor v k can be calculated by I d (z i,j , v k )
- The quality of disentanglement is evaluated by measuring the mutual information gap

## F Model Inspection: Coupled Diffusion Process
- The author demonstrates how a time series can be diffused under different settings in terms of variance schedule β and the max number of diffusion steps T
- It can be seen that when larger diffusion steps or a wider variance schedule is employed, the diffused series deviates far from the original data gradually, which may result in the loss of useful signals
- Therefore, it is important to choose a suitable variance schedule and diffusion steps to ensure that the distribution space is deviated enough without losing useful signals.

## G Necessity of Data Augmentation for Time Series Forecasting
- Limited data would result in overfitting and poor performance.
- To demonstrate the necessity of enlarging the size of data for time series forecasting when deep models are employed, we implement a two-layer RNN and evaluate how many time points are required to ensure the generalization ability.
- A synthetic dataset is adopted for this demonstration.
- According to [23], we generate a toy time series dataset with n time points in which each point is a d-dimension variable: Generative Time Series Forecasting Problem Formulation.
- Given an input multivariate time series X = {x 1 , x 2 , • • • , x n | x i ∈ R d } and the corresponding target time series Y = {y n+1 , y n+2 , • • • , y n+m | y j ∈ R d } (d ≤ d). ),
- without loss of generality, Y r can be fully captured by the model. That is, Y r − Y r −→ 0 where Y r is the ideal part of ground truth target series Y .
- In addition, Y can be decomposed as Y = Y r , Y ( Y denotes the noise of Y ). Therefore, the error between ground truth and prediction, i.e., Y − Y = Y − Y > 0, can be deemed as the combination of aleatoric uncertainty and epistemic uncertainty.
- First, the input and output series are augmented simultaneously with the coupled diffusion process.
- Then the diffused input series are fed into a proposed BVAE model for inference, which can be deemed a reverse process.
- A denoising score-matching mechanism is applied to make the estimated target move toward the true target series. Meanwhile, the latent states in BVAE are leveraged for disentangling such that the model interpretability and reliability can be improved.
- Figure 1: The framework overview of D 3 VAE.
- Figure 2: An illustration of the coupled diffusion process.
- Figure 3: Uncertainty estimation of the prediction of the last six dimensions in the Traffic dataset and the colored envelope denotes the estimated uncertainty.
- Figure 4: Comparisons of predictions with different β T and varying T on the Electricity dataset.
- Study of the Coupled Diffusion and Denoising Network.
- To evaluate the effectiveness of the coupled diffusion model (CDM), we compare the full versioned D 3 VAE with its three variants: i) D 3 VAE − Y , i.e. D 3 VAE without diffused Y , ii) D 3 VAE − X , i.e. D 3 VAE without diffused X, and iii) D 3 VAE −CDM , i.e. D 3 VAE without any diffusion.
- Besides, the performance of D 3 VAE without denoising score matching (DSM) is also reported when the target series is not diffused, which are denoted as D 3 VAE − Y −DSM and D 3 VAE −CDM−DSM .
- The ablation study is carried out on Traffic and Electricity datasets under input-16-predict-16 and input-32-predict-32.
- Figure 5: The prediction showcases in the Electricity dataset with different sampling strategies.
- Proof. According to Proposition 1, the noise of Y consists of the estimation noise Y and residual noise δ Y , i.e., Y = Y , δ Y where Y and δ Y are independent of each other, then q
- Figure 6: Forecasting process of DeepAR, TimeGrad, and GP-copula.
- Figure 7: The case study of forecasting results on the Traffic dataset under input-32-predict-32 settings.
- Figure 8: Case study of the forecasting results from the Electricity dataset (same settings as Fig. 7).
