---
title: "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement"
date: 2023-01-08T12:20:46.000Z
author: "Yan Li, Xinjiang Lu, Yaqing Wang, Dejing Dou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03028v1.webp" # image path/url
    alt: "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03028)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03028).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/generative-time-series-forecasting-with).

# Abstract
- Time series forecasting is a widely explored task of great importance
- In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE
- To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting
- In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation

# Paper Content

## Introduction
- Time series forecasting is important for risk-averse decision-making
- Traditional RNN-based methods capture temporal dependencies of the time series to predict the future
- LSTMs and GRUs introduce the gate functions into the cell structure to handle long-term dependencies effectively
- CNNs capture complex inner patterns of the time series through convolutional operations
- Recently, Transformer-based models have shown great performance in time series forecasting
- However, one big issue of neural networks in time series forecasting is the uncertainty
- The models based on VAR try to model the distribution of time series from hidden states
- Interpretable representation learning is another merit of time series forecasting
- Variational autoencoders (VAEs) have shown not only the superiority in modeling latent distributions of the data and reducing the gradient noise but also the interpretability of time series forecasting
- However, the interpretability of VAEs might be inferior due to the entangled latent variables
- There have been efforts to learn representation disentangling
- To this end, we address the time series forecasting problem with generative modeling. Specifically, we propose a bidirectional variational auto-encoder (BVAE)
- Besides, we develop a scaled denoising score-matching network for cleaning diffused target time series.
- In addition, we disentangle the latent variables of the time series by assuming that different disentangled dimensions of the latent variables correspond to different temporal patterns
- Our contributions can be summarized as follows:
- We propose a coupled diffusion probabilistic model aiming to reduce the aleatoric uncertainty of the time series and improve the generalization capability of the generative model
- We integrate the multiscale denoising score matching into the coupled diffusion process to improve the accuracy of generated results
- We disentangle the latent variables of the generative model to improve the interpretability for time series forecasting
- Extensive experiments on synthetic and real-world datasets demonstrate that D 3 VAE outperforms competitive baselines with satisfactory margins.

### Coupled Diffusion Probabilistic Model
- The diffusion probabilistic model is a family of latent variable models aiming to generate high-quality samples.
- To equip the generative time series forecasting model with high expressiveness, a coupled forward process is developed to augment the input series and target series synchronously.
- Besides, in the forecasting task, more tractable and accurate prediction is expected. To achieve this, we propose a bidirectional variational auto-encoder (BVAE) to take the place of the reverse process in the diffusion model.
- We present the technical details in the following two parts, respectively.
- The forward diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the data [85,41].
- To diffuse the input and output series, we propose a coupled diffusion process, which is demonstrated in Fig. 2.
- Specifically, given the input X = X (0) ∼ q(X (0) ), the approximate posterior q(X (1:T ) |X (0) ) can be obtained as where a uniformly increasing variance schedule
- Furthermore, according to Proposition 1 we decompose X (0) as X r , X . Then, with Eq. (3), the diffused X (t) can be decomposed as follows:
- where δ X denotes the standard Gaussian noise of X.
- As α can be determined when the variance schedule β is known, the ideal part is also determined in the diffusion process.
- Let X then, according to Proposition 1 and Eq. ( 4), we have where Y denotes the generated noise of Y (t) .
- To relieve the effect of aleatoric uncertainty resulting from time series data, we further apply the diffusion process to the target series Y = Y (0) ∼ q(Y (0) ).
- In particular, a scale parameter ω ∈ (0, 1) is adopted, such that β t = ωβ t , α t = 1 − β t and ᾱ t = t s=1 α s .
- Then, according to Proposition 1, we can obtain the following decomposition (similar to Eq. ( 4)):
- Consequently, we have q(Y (t) ) = q( Y (t) Y ).
- Afterward, we can draw the following conclusions with Proposition 1 and Eqs. ( 5) and (6).
- The proofs can be found in Appendix B.

### Scaled Denoising Score Matching for Diffused Time Series Cleaning
- The time series data can be augmented with the aforementioned coupled diffusion probabilistic model
- The generative distribution p θ ( Y (t) ) tends to move toward the diffused target series Y (t) which has been corrupted
- To further "clean" the generated target series, we employ the Denoising Score Matching (DSM) to accelerate the de-uncertainty process without sacrificing the model flexibility
- DSM [90,65] was proposed to link Denoising Auto-Encoder (DAE) [91] to Score Matching (SM) [43]
- Let Y denote the generated target series, then we have the objective where p σ0 ( Y , Y ) is the joint density of pairs of corrupted and clean samples ( Y , Y ), ∇ Y log(q σ0 ( Y |Y )) is derivative of the log density of a single noise kernel, which is dedicated to replacing the Parzen density estimator: p σ0 ( Y ) = q σ0 ( Y |Y )p(Y )dY
- In the generative time series forecasting setting, the generated samples will be tested without applying the diffusion process
- To further denoise the generated target series Y , we apply a single-step gradient denoising jump [78]

### Disentangling Latent Variables for Interpretation
- The interpretability of the time series forecasting model is of great importance for many downstream tasks [88,38,44].
- Through disentangling the latent variables of the generative model, not only the interpretability but also the reliability of the prediction can be further enhanced [64].
- To disentangle the latent variables Z = {z 1 , • • • , z n }, we attempt to minimize the Total Correlation (TC) [94,50], which is a popular metric to measure dependencies among multiple random variables.
- Lower TC generally means better disentanglement if the latent variables preserve useful information. However, a very low TC can still be obtained when the latent variables carry no meaningful signals.
- Through the bidirectional structure of BVAE, such issues can be tackled without too much effort.
- As shown in Fig. 1, the signals are disseminated in both the encoder and decoder, such that rich semantics are aggregated into the latent variables. Furthermore, to alleviate the effect of potential irregular values, we average the total correlations of z 1:n , then the loss w.r.t. the TC score of BVAE can be obtained: Algorithm 1 Training Procedure.
- 1: repeat
- 2: Randomly choose t ∈ {1, • • • , T } and with Eqs. ( 4) and ( 6),
- 4: Generate the latent variable Z with BVAE, Z ∼ p φ (Z|X (t) )
- 6: Sample Y (t) ∼ p θ ( Y (t) |Z) and calculate DKL(q(Y (t) )||p θ ( Y (t) ))
- 7: Calculate DSM loss with Eq. (10)
- 8: Calculate total correlation of Z with Eq. (13)
- 9: Construct the total loss L with Eq. ( 14)
- 10: θ, φ ← argmin(L)
- 11: until Convergence
- 2 Forecasting Procedure. Output: Yclean and the estimated uncertainty with Eq. ( 11)

### Training and Forecasting
- The coupled diffusion algorithm is proposed without sacrificing generalizability
- The denoising network is used to reduce uncertainty
- The latent variables are disentangled by minimizing the TC of the latent variables
- The loss function is used to learn the generative model
- Algorithm 1 displays the complete training procedure
- Algorithm 2 is used to generate the target series

### Experiment Settings

### Main Results
- Two different prediction lengths, i.e., l y ∈ {8, 16} (l x = l y ), are evaluated.
- The results of longer prediction lengths are available in Appendix D.
- Toy Datasets. In Table 1, we can observe that D Uncertainty Estimation.
- The uncertainty can be assessed by estimating the noise of the outcome series when doing the prediction (see Section 2.3).
- Through scale parameter ω, the generated distribution space can be adjusted accordingly (results on the effect of ω can be found in Appendix D.3).
- The showcases in Fig. 3 demonstrate the uncertainty estimation of the yielded series in the Traffic dataset, where the last six dimensions are treated as target variables.
- We can find that noise estimation can quantify the uncertainty effectively.
- For time series forecasting, it is difficult to label disentangled factors by hand, thus we take different dimensions of Z as the factors to be disentangled: We build a classifier to discriminate whether an instance z i,j belongs to class j such that the disentanglement quality can be assessed by evaluating the classification performance. Besides, we adopt the Mutual Information Gap (MIG) [15] as a metric to evaluate the disentanglement more straightforwardly.
- Due to the space limit, the evaluation of disentanglement with different factors can be found in Appendix E.

### Model Analysis
- The variance schedule β and the number of diffusion steps T are important for reducing the uncertainty while preserving the informative temporal patterns.
- If β and T are not set properly, the diffusion process could be out of control.

## Discussion
- Sampling for Generative Time Series Forecasting
- The Langevin dynamics has been widely applied to the sampling of energy-based models (EBMs) [101,21,103],
- An illustrative showcase can be found in Appendix F. where k ∈ {0, • • • , K}, K denotes the number of sampling steps, and ρ is a constant.
- With K and ρ being properly configured, high-quality samples can be generated.
- The Langevin dynamics has been successfully applied to applications in computer vision [56,102], and natural language processing [20].
- We employ a single-step gradient denoising jump in this work to generate the target series.
- The experiments that were carried out demonstrate the effectiveness of such single-step sampling.
- We conduct an extra empirical study to investigate whether it is worth taking more sampling steps for further performance improvement of time series forecasting.
- We showcase the prediction results under different sampling strategies in Fig. 5.
- By omitting the additive noise in Langevin dynamics, we employ the multi-step denoising for D 3 VAE to generate the target series and plot the generated results in Fig. 5a.
- Then, with the standard Langevin dynamics, we can implement a generative procedure instead of denoising and compare the generated target series with different ρ (see Figs. 5b to 5d).
- We can observe that more sampling steps might not be helpful in improving prediction performance for generative time series forecasting (Fig. 5a).
- Besides, larger sampling steps would lead to high computational complexity.
- On the other hand, different configurations of Langevin dynamics (with varying ρ) cannot bring indispensable benefits for time series forecasting (Figs. 5b to 5d).
- With the coupled diffusion probabilistic model, although the aleatoric uncertainty of the time series can be reduced, a new bias is brought into the series to mimic the distribution of the input and target.
- However, as a common issue in VAEs that any introduced bias in the input will result in bias in the generated output [92], the diffusion steps and variance schedule need to be chosen cautiously, such that this model can be applied to different time series tasks smoothly.
- The proposed model is devised for general time series forecasting, it should be used properly to avoid the potential negative societal impacts, such as illegal applications.

## Conclusion
- The bidirectional VAE is proposed as the backbone for generative TSF
- To further improve generalizability, a coupled diffusion probabilistic model is developed
- Then a scaled denoising network is developed to guarantee the prediction accuracy
- Afterward, the latent variables are further disentangled for better model interpretability
- Extensive experiments on synthetic data and real-world data validate that the proposed generative model achieves SOTA performance compared to existing competitive generative models
