---
title: "DensePose From WiFi"
date: 2022-12-31T16:48:43.000Z
author: "Jiaqi Geng, Dong Huang, Fernando De la Torre"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-00250v1.webp" # image path/url
    alt: "DensePose From WiFi" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00250)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00250).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/densepose-from-wifi).

# Abstract
- Advances in computer vision and machine learning techniques have led to significant development in 2D and 3D human pose estimation from RGB cameras, LiDAR, and radars.
- However, human pose estimation from images is adversely affected by occlusion and lighting, which are common in many scenarios of interest.
- Radar and LiDAR technologies, on the other hand, need specialized hardware that is expensive and power-intensive.
- Furthermore, placing these sensors in non-public areas raises significant privacy concerns.
- To address these limitations, recent research has explored the use of WiFi antennas (1D sensors) for body segmentation and key-point body detection.
- This paper further expands on the use of the WiFi signal in combination with deep learning architectures, commonly used in computer vision, to estimate dense human pose correspondence.
- We developed a deep neural network that maps the phase and amplitude of WiFi signals to UV coordinates within 24 human regions.
- The results of the study reveal that our model can estimate the dense pose of multiple subjects, with comparable performance to image-based approaches, by utilizing WiFi signals as the only input.

# Paper Content

## INTRODUCTION
- Many sensors are constrained by technical and practical considerations
- LiDAR and radar sensors are frequently seen as being out of reach for the average household or small business
- RGB cameras have narrow field of view and poor lighting conditions
- Occlusion is another obstacle that prevents the camera-based model from generating reasonable pose predictions in images
- For instance, most people are uncomfortable with having cameras recording them in their homes
- In healthcare applications, that are increasingly shifting from clinics to homes, people are being monitored with the help of cameras and other sensors
- WiFi signals can serve as a ubiquitous substitute for RGB images for human sensing in certain instances
- Illumination and occlusion have little effect on WiFi-based solutions used for interior monitoring
- Traditional techniques rely on accurate measurement of time-of-fly and angle-of-arrival of the signal between the transmitter and receiver
- However, these techniques only locate the object's center
- To address these issues, we derive inspiration from recent proposed deep learning architectures in computer vision and propose a neural network architecture that can perform dense pose estimation from WiFi

## RELATED WORK
- Existing work on dense estimation from images and human sensing from WiFi
- Our research aims to conduct dense pose estimation via WiFi
- In computer vision, the subject of dense pose estimation from pictures and video has received a lot of attention [6,8,18,40]
- DensePose is based on instance segmentation architectures such as Mark-RCNN [9], and predicts body-wise UV maps for each pixel, where UV maps are flattened representations of 3d geometry, with coordinate points usually corresponding to the vertices of a 3d dimensional object
- In this work, we borrow the same architecture as DensePose [8]; however, our input will not be an image or video, but we use 1D WiFi signals to recover the dense correspondence
- Recently, there have been many extensions of DensePose proposed, especially in 3D human reconstruction with dense body parts [3,35,37,38]
- Shapovalov et al. 's [24] work focused on lifting dense pose surface maps to 3D human models without 3D supervision
- Compared to previous works on reconstructing 3D humans with sparse 2D keypoints, DensePose annotations are much denser and provide information about the 3D surface instead of 2D body joints
- While there is a extensive literature on detection [19,20], tracking [4,34], and dense pose estimation [8,18] from images and videos, human pose estimation from WiFi or radar is a relatively unexplored problem
- At this point, it is important to differentiate the current work on radar-based systems and WiFi
- The work of Adib et.al. [2] proposed a Frequency Modulated Continuous Wave (FMCW) radar system (broad bandwidth from 5.56GHz to 7.25GHz) for indoor human localization
- A limitation of this system is the specialized hardware for synchronizing the transmission, refraction, and reflection to compute the Time-of-Flight (ToF)
- In the following work [1], they improved the system by focusing on a moving person and generated a rough single-person outline with depth maps
- Recently, they applied deep learning approaches to do fine-grained human pose estimation using a similar system, named RF-Pose [39]
- These systems do not work under the IEEE 802.11n/ac WiFi communication standard (40MHz bandwidth centered at 2.4GHz)
- They rely on additional high-frequency and high-bandwidth electromagnetic fields, which need specialized technology not available to the general public
- Recently, significant improvements have been made to radar-based human sensing systems
- mmMesh [36] generates 3D human mesh from commercially portable millimeter-wave devices
- Unlike the above radar systems, the WiFi-based solution [11,30] used off-the-shelf WiFi adapters and 3dB omnidirectional antennas
- The signal propagate as the IEEE 802.11n/ac WiFi data packages transmitting between antennas, which does not introduce additional interference
- However, WiFi-based person localization using the traditional time-of-flight (ToF) method is limited by its wavelength and signal-to-noise ratio
- Most existing approaches only conduct center mass localization [5,27] and single-person action classification [25,29]

## METHODS
- The raw CSI signals are cleaned by amplitude and phase sanitization
- A two-branch encoder-decoder network performs domain translation from sanitized CSI samples to 2D feature maps that resemble images
- The 2D features are then fed to a modified DensePose-RCNN architecture to estimate the UV map
- To improve the training of our WiFi-input network, we conduct transfer learning

## Phase Sanitization
- Raw CSI samples are noisy with random phase drift and flip
- Most WiFi-based solutions disregard the phase of CSI signals and rely only on their amplitude
- Sanitization to obtain stable phase values enables full use of the CSI information
- Outliers in time and frequency are eliminated with median and uniform filters, leading to temporally consistent phase curves

## Modality Translation Network
- The 1D CSI signals are transformed from the CSI domain to the spatial domain
- The features of the CSI latent space are extracted with two encoders
- The features of the spatial domain are generated by a multi-layer perceptron
- The spatial information is extracted by applying two convolution blocks and four deconvolution layers

## WiFi-DensePose RCNN
- After obtaining the 3 × 720 × 1280 scene representation in the image domain,
- we can utilize image-based methods to predict the UV maps of human bodies.
- State-of-the-art pose estimation algorithms are two-stage; first, they run an independent person detector to estimate the bounding box and then conduct pose estimation from person-wise image patches.
- However, as stated before, each element in our CSI input tensors is a summary of the entire scene. It is not possible to extract the signals corresponding to a single person from a group of people in the scene.
- Therefore, we decide to adopt a network structure similar to DensePose-RCNN [8], since it can predict the dense correspondence of multiple humans in an end-toend fashion.
- More specifically, in the WiFi-DensePose RCNN (Figure 5), we extract the spatial features from the obtained 3 × 720 × 1280 imagelike feature map using the ResNet-FPN backbone [14].
- Then, the output will go through the region proposal network [20].
- To better exploit the complementary information of different sources, the next part of our network contains two branches: DensePose head and Keypoint head.
- Estimating keypoint locations is more reliable than estimating dense correspondences, so we can train our network to use keypoints to restrict DensePose predictions from getting too far from the body joints of humans.
- The DensePose head utilizes a Fully Convolutional Network (FCN) [16] to densely predict human part labels and surface coordinates (UV coordinates) within each part, while the keypoint head uses FCN to estimate the keypoint heatmap.
- The results are combined and then fed into the refinement unit of each branch, where each refinement unit consists of two convolutional blocks followed by an FCN.
- The network outputs a 17 × 56 × 56 keypoint mask and a 25 × 112 × 112 IUV map.

## Transfer Learning
- Training the Modality Translation Network and WiFi-DensePose RCNN network from a random initialization takes a lot of time (roughly 80 hours).
- To improve the training efficiency, we conduct transfer learning from an image-based DensPose network to our WiFi-based network.
- The idea is to supervise the training of the WiFi-based network with the pre-trained image-based network.
- Directly initializing the WiFi-based network with image-based network weights does not work because the two networks get inputs from different domains (image and channel state information).
- Instead, we first train an image-based DensePose-RCNN model as a teacher network.
- Our student network consists of the modality translation network and the WiFi-DensePose RCNN.
- We fix the teacher network weights and train the student network by feeding them with the synchronized images and CSI tensors, respectively.
- We update the student network such that its backbone (ResNet) features mimic that of our teacher network.
- Our transfer learning goal is to minimize the differences of multiple levels of feature maps generated by the student model and those generated by the teacher model.
- Therefore we calculate the mean squared error between feature maps.
- The transfer learning loss from the teacher network to the student network is:
- ( 3 ,  * 3 )+ ( 4 ,  * 4 )+ ( 5 ,  * 5 ),

## Losses
- The total loss for the person classification is computed as:
- The classification loss and the box regression loss are standard RCNN losses.
- The DensePose loss consists of several sub-components: (1) Cross-entropy loss for the coarse segmentation tasks, (2) Cross-entropy loss for body part classification and smooth L1 loss for UV coordinate regression, (3) One-hot encoding of the 17 ground truth keypoints, and (4) Supervision with the Cross-Entropy Loss.

## EXPERIMENTS
- The DensePose was validated using a 3D printer
- The DensePose was validated using a human subject
- The DensePose was able to improve the accuracy of 3D printing
- The DensePose was able to improve the accuracy of human subjects

## Dataset
- We used the dataset described in [31], which contains CSI samples taken at 100Hz from receiver antennas and videos recorded at 20 FPS.
- Time stamps are used to synchronize CSI and the video frames such that 5 CSI samples correspond to 1 video frame.
- The dataset was gathered in sixteen spatial layouts: six captures in the lab office and ten captures in the classroom.
- Each capture is around 13 minutes with 1 to 5 subjects (8 subjects in total for the entire dataset) performing daily activities under the layout described in Figure 2 (a).
- The sixteen spatial layouts are different in their relative locations/orientations of the WiFi-emitter antennas, person, furniture, and WiFi-receiver antennas.
- There are no manual annotations for the data set.
- We use the MS-COCO-pre-trained dense model "R_101_FPN_s1x_legacy" and MS-COCO-pre-trained Keypoint R-CNN "R101-FPN" to produce the pseudo ground truth.
- We denote the ground truth as "R101-Pseudo-GT" (see an annotated example in Figure 7).
- The R101-Pseudo-GT includes person bounding boxes, person-instance segmentation masks, body-part UV maps, and person-wise keypoint coordinates.
- Throughout the section, we use R101-Puedo-GT to train our WiFi-based DensePose model as well as finetuning the image-based baseline "R_50_FPN_s1x_legacy".

## Training/Testing protocols and Metrics
- We train on the training set in all 16 spatial layouts, and test on remaining frames.
- Following [31], we randomly select 80% of the samples to be our training set, and the rest to be our testing set.
- The training and testing samples are different in the person's location and pose, but share the same person's identities and background.
- This is a reasonable assumption since the WiFi device is usually installed in a fixed location.
- We train on 15 spatial layouts and test on 1 unseen spatial layout.
- The unseen layout is in the classroom scenarios.
- We evaluate the performance of our algorithm in two aspects: the ability to detect humans (bounding boxes) and accuracy of the dense pose estimation.
- To evaluate the performance of our models in detecting humans, we calculate the standard average precision (AP) of person bounding boxes at multiple IOU thresholds ranging from 0.5 to 0.95.
- In addition, by MS-COCO [15] definition, we also compute AP-m for median bodies that are enclosed in bounding boxes with sizes between 32 × 32 and 96 × 96 pixels in a normalized 640 × 480 pixels image space, and AP-l for large bodies that are enclosed in bounding boxes larger than 96 × 96 pixels.
- To measure the performance of DensePose detection, we follow the original DensePose paper [8].
- First we compute Geodesic Point Similarity (GPS) as a matching score for dense correspondences: where  calculates the geodesic distance,   denotes the ground truth point annotations of person ,   and î are the estimated and ground truth vertex at point  respectively, and  is a normalizing parameter (set to be 0.255 according to [8]).
- One issue of GPS is that it does not penalize spurious predictions. Therefore, estimations with all pixels classified as foreground are favored.
- To alleviate this issue, masked geodesic point similarity (GPSm) was introduced in [8], which incorporates both GPS and segmentation masks.
- The formulation is as follows: where  and M are the predicted and ground truth foreground segmentation masks.
- Next, we can calculate DensePose average precision with GPS (denoted as dpAP• GPS) and GPSm (denoted as dpAP• GPSm) as thresholds, following the same logic behind the calculation of bounding box AP.

## Implementation Details
- We implemented our approach in PyTorch.
- We set the training batch size to 16 on a 4 GPU (Titan X) server.
- We empirically set  = 0.6,  = 0.3,   = 0.1.
- We used a warmup multi-step learning rate scheduler and set the initial learning rate as 1 − 5.
- The learning rate increases to 1 − 3 during the first 2000 iterations, then decreases to 1  10 of its value every 48000 iterations.
- We trained for 145, 000 iterations for our final model.

## WiFi-based DensePose under Same Layout
- WiFi-based DensePose under the Same Layout protocol can achieve high precision
- dpAP • GPS and dpAP • GPSm show similar results, indicating that the model can effectively detect the approximate locations of human bounding boxes

## Comparison with Image-based DensePose
- Section 4.1 discusses the difficulties of comparing WiFi-based and Image-based DensePose
- We tried an image-based DensePose baseline finetuned on R101-Pseudo-GT under the Same Layout protocol
- In addition, as shown in Figure 9 and Figure 10, though certain defects still exist, the estimations from our WiFi-based model are reasonably well compared to the results produced by Image-based DensePose

## Ablation Study
- The ablation study to understand the effects of phase information, keypoint supervision, and transfer learning on estimating dense correspondences.
- The models analyzed in this section are all trained under the same layout mentioned in section 4.2.
- The results for all the metrics have slightly improved from the baseline.
- Adding phase information allows for better performance.
- Adding a keypoint detection branch improves performance.
- Transfer learning can reduce the training time for the model.

## Performance in different layouts
- The model performs better than the baseline model under the different layout protocols
- The performance decreases significantly from that under the same layout protocol

## Failure cases
- The WiFi-based model is biased when there are body poses that rarely occurred in the training set.
- The WiFi-based model is more challenging to extract detailed information for each individual when there are three or more concurrent subjects in one capture.

## CONCLUSION AND FUTURE WORK
- WiFi signals can be used to estimate dense poses of people
- Deep learning architectures commonly used in computer vision can be used to estimate dense poses of people
- The performance of our work is still limited by the public training data in the field of WiFi-based perception
