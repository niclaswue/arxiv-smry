---
title: "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP"
date: 2022-12-28T18:52:44.000Z
author: "Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, Matei Zaharia"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14024v1.webp" # image path/url
    alt: "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14024)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14024).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/demonstrate-search-predict-composing).

# Abstract
- Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM).
- Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt.
- To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably.
- We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-200%, 8-40%, and 80-290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.

# Paper Content

## Introduction
- In-context learning adapts a frozen language model (LM) to tasks by conditioning the LM on a textual prompt including task instructions and a few demonstrating examples
- For knowledge-intensive tasks such as question answering, fact checking, and information-seeking dialogue, retrieval models (RM) are increasingly used to augment prompts with relevant information from a large corpus
- On its own, the LM often makes false assertions
- An increasingly popular retrieve-then-read pipeline fails when simple search can't find an answer
- In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response
- Texts edited for presentation. Recent work has shown such retrieval-augmented in-context learning to be effective in simple "retrieve-then-read" pipelines
- We argue that the fact that both LMs and RMs consume (and generate or retrieve) natural language texts creates an opportunity for much more sophisticated interactions between them
- Fully realizing this would be transformative: frozen LMs and RMs could serve as infrastructure across tasks, enabling ML-and domain-experts alike to rapidly build grounded AI systems at a high level of abstraction and with lower deployment overheads and annotation costs

### Datatypes and Control Flow
- We have implemented the DSP framework in Python.
- The present section introduces the core data types and composable functions provided by the framework.
- We use illustrative code snippets to ground the examples, and to convey the power that comes from being able to express complex interactions between the LM and RM in simple programs.
- This snippet contains two labeled examples, each with a multi-hop question (e.g., "In which city did Akeem Ellis play in 2017?") and its short answer ("Ellesmere Port"). Arbitrary keys and values are allowed within an Example, though typical values are strings or lists of strings.
- In this task, we are unlikely to find an individual passage that provides the answer to any question.
- For example, the first training example can probably be resolved only by first answering the question of who discovered Palomar ("Edwin Hubble") and then addressing the question of Hubble's birth date using different evidence passages.
- We typically assume that the human-labeled training data do not include labels for intermediate transformations (e.g., queries for individual hops) that would be useful for following these steps, and so it is the job of the DSP program to discover these strategies via in-context learning.
- The following code snippet is a complete program for resolving multi-hop questions like those in Figure 1, with help from train examples like those above.

## Evaluation
- We now consider how to implement DSP programs for three diverse knowledge-intensive NLP tasks: open-domain question answering (QA), multi-hop QA, and conversational QA.
- All of these tasks are "open-domain", in the sense that systems are given a short question or participate in a multi-turn conversation without being granted access to context that answers these questions.
- We build and evaluate intuitive compositions of the functions explored in ยง2 for each task.
- We show that, despite low development effort, the resulting DSP programs exhibit strong quality and deliver considerable empirical gains over vanilla in-context learning and a standard retrieve-then-read pipeline with in-context learning.

### Evaluation Methodology
- The open-domain version of SQuAD (Rajpurkar et al., 2016;Lee et al., 2019)
- The multi-hop HotPotQA (Yang et al., 2018) dataset in the opendomain "fullwiki" setting
- The conversational question answering QReCC (Anantha et al., 2020;Vakulenko et al., 2022) dataset

### Pretrained Modules
- We use ColBERTv2 (Santhanam et al., 2022b), a state-of-the-art retriever based on late interaction (Khattab & Zaharia, 2020).
- We choose ColBERTv2 for its highly effective zero-shot search quality and efficient search (Santhanam et al., 2022a).
- However, our DSP programs are agnostic to how the retriever represents examples or scores passages, so essentially any retriever can be used.
- In addition, by making retrieval a first-class construct, DSP allows us to change or update the search index over time.
- We simulate this in our experiments by aligning each of our datasets with the nearest Wikipedia corpus among the Dec 2016 Wikipedia dump from Chen et al. 2017, the Nov 2017 Wikipedia "abstracts" dump from Yang et al. 2018, and the Dec 2018 Wikipedia dump from Karpukhin et al. 2020.

### Baselines

### Proposed DSP Programs
- We build on transformations presented in ยง2.
- Our programs for all three tasks have the following structure, illustrated for open-domain QA.
- Unless otherwise stated, our programs default to greedy decoding during the DEMONSTRATE stage.
- For SEARCH, our open-domain QA program uses the question directly for retrieving k = 7 passages and concatenates these passages into our QA prompt with CoT.
- For PREDICT, it generates n = 20 reasoning chains and uses self-consistency (SC; Wang et al. 2022c) to select its final prediction.
- For DEMONSTRATE, our open-domain QA program uses the following approach, slightly simplified for presentation.
- In it, the parameter k = 3 passed to annotate requests annotating only three demonstrations, which will then be used in the prompts.
- Our multi-hop program adopts a very similar approach for DEMONSTRATE and PREDICT.
- For SEARCH, it uses the approach described in ยง2.4, with the following adjustments.
- It uses result fusion across n = 10 queries per hop and, among the n predictions, uses the summary corresponding to the largest average log-probability.
- It uses a fixed number of hops for HotPotQA, i.e., two hops.
- In each prompt (i.e., each hop and QA), it concatenates the summaries of all previous hops (i.e., hop 1 onwards) and a total of k = 5 passages divided between the hops (i.e., five passages from the first hop or two passages from the first and three from the second).

### Development Datasets & Results
- We conduct the open-domain version of SQuAD over the Wikipedia 2016 corpus from Chen et al. (2017), as processed by Khattab et al. (2021b).
- We use the same train/validation/test splits as in Karpukhin et al. (2020) and Khattab et al. (2021b).
- Table 1 reports the answer EM and F1.
- The task-aware DSP program achieves 36.6% EM, outperforming the vanilla LM baseline by 126% EM relative gains. This indicates the importance of grounding the LM's predictions in retrieval, and it shows that state-of-the-art retrievers like ColBERTv2 have the capacity to do so off-the-shelf.
- The proposed DSP program also achieves relative gains of 8% in EM and 6% in F1 over the retrieve-then-read pipeline, highlighting that nontrivial gains are possible by aggregating information across several retrieved passages as we do with self-consistency.
- These in-context learning results are competitive with a number of popular fine-tuned systems. For instance, on the Open-SQuAD test set, DPR achieves 29.8% EM, well below our 16-shot DSP program. On the Open-SQuAD dev set, the powerful Fusion-in-Decoder (Izacard & Grave, 2020) "base" approach achieves approximately 36% (i.e., very similar quality to our system) when invoked with five retrieved passages. Nonetheless, with the default setting of reading 100 passages, their system reaches 48% EM in this evaluation.
- This may indicate that similar gains are possible for our DSP program if the PREDICT stage is made to aggregate information across many more passages.
- For comparison, we also evaluate the self-ask pipeline, which achieves 9.3% EM, suggesting that its fixed pipeline is ineffective outside its default multi-hop setting.
- Studying a few examples of its errors reveals that it often decomposes questions in tangential ways and answers these questions instead.
- For reference, Table 1 also reports (as No-retrieval LM SoTA) the concurrent in-context learning results from Si et al. (2022) using code-davinci-002, who achieve 20.2% EM without retrieval and 34.0% EM with retrieval, albeit on a different sample and split of the SQuAD data. Overall, their approaches are very similar to the baselines we implement (vanilla LM and retrieve-then-read), though their retrieval-augmented approach retrieves (and concatenates into the prompt) 10 passages from a Wikipedia dump.
- We use the open-domain "fullwiki" setting of HotPotQA using its official Wikipedia 2017 "abstracts" corpus.
- The HotPotQA test set is hidden, so we reserve the official validation set for our testing.
- We sub-divide the training set into 90%/10% train/validation splits.
- In the training (and thus validation) split, we keep only examples marked as "hard" in the original dataset, which matches the designation of the official validation and test sets.
- We report the final answer EM and F1 in Table 1.
- On HotPotQA, the task-aware DSP program outperforms the baselines and existing work by very wide margins, exceeding the vanilla LM, the retrieve-then-read baseline, and the self-ask pipeline by 82%, 39%, and 80%, respectively, in EM. This highlights the effectiveness of building up more sophisticated programs that coordinate the LM and RM for the SEARCH step.

## Conclusion
- The dominant paradigm for building models in AI has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration.
- However, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.
- The promise of in-context learning is that we can build complex systems from pretrained components using only natural language as the medium for giving systems instructions and, as we argue for, allowing components to communicate with each other.
- In this new paradigm, the building blocks are pretrained models and the core operations are natural language instructions and operations on natural language texts.
- If we can realize this potential, then we can broaden participation in AI system development, rapidly prototype systems for new domains, and maximize the value of specialized pretrained components.
- In the current paper, we introduced the DEMONSTRATE-SEARCH-PREDICT (DSP) framework for retrieval augmented in-context learning.
- DSP consists of a number of simple, composable functions for implementing in-context learning systems as deliberate programs-instead of endtask prompts-for solving knowledge intensive tasks.
- We implemented DSP as a Python library and used it to write programs for Open-SQuAD, HotPotQA, and QReCC. These programs deliver substantial gains over previous in-context learning approaches.
- However, beyond any particular performance number, we argue that the central contribution of DSP is in helping to reveal a very large space of conceptual possibilities for in-context learning in general.
- Figure1. A comparison between three systems based on GPT-3.5 (text-davinci-002).
- On its own, the LM often makes false assertions.
- An increasingly popular retrieve-then-read pipeline fails when simple search can't find an answer.
- In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response.
- Texts edited for presentation. How many storeys are in the...
- Figure 2. A toy example of a DSP program for multi-hop question answering.
- Given an input question and a 2-shot training set, the DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.
- Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information over two retrieval hops.
- Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.
