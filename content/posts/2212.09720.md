---
title: "The case for 4-bit precision: k-bit Inference Scaling Laws"
date: 2022-12-19T18:48:33.000Z
author: "Tim Dettmers, Luke Zettlemoyer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09720v1.webp" # image path/url
    alt: "The case for 4-bit precision: k-bit Inference Scaling Laws" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09720)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09720).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/the-case-for-4-bit-precision-k-bit-inference).

# Abstract
- Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies.
- However, the final model size depends on both the number of parameters of the original model and the rate of compression.
- For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies.
- In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance.
- We run more than 35,000 zero-shot experiments with 16-bit inputs and k-bit parameters to examine which quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 66B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2.
- We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that 4-bit precision is almost universally optimal for total model bits and zero-shot accuracy.

# Paper Content

## Introduction
- Large Language Models (LLMs) are widely adopted for zero/few-shot inference (Zhang et al., 2022;Black et al., 2022;Zeng et al., 2022;Scao et al., 2022), but can be challenging to use both due to their large memory footprintsup to 352 GB of GPU memory for 175B models -and high latency.
- However, both the memory and latency are primarily determined by the total number of bits in the parameters.
- Therefore, if we reduce the model bits through quantization, we can expect the latency of the model to reduce proportionally, potentially at the expense of end task accuracy (Frantar et al., 2022;Park et al., 2022;Yao et al., 2022).
- Since we can quantize the parameters of a trained model to an arbitrary bit-precision, this raises the question of how many bits should be used to optimally trade off accuracy and total model bits, given our current base methods for model quantization.
- For example, if we have a 60B LLM in 4bit precision and a 30B LLM in 8-bit precision, which will achieve higher accuracy?
- To study such trade-offs, it is helpful to take the perspective of scaling laws (Kaplan et al., 2020;Henighan et al., 2020), which evaluate the underlying trends of variables to make generalizations beyond individual data points.
- In this paper, we study bit-level inference scaling laws to determine the precision that maximizes zero-shot accuracy given a certain number of total bits in the model.
- Our main finding is that 4-bit parameters yield optimal zeroshot accuracy for a fixed number of model bits across all model scales and model families tested.
- We study five different model families, OPT, Pythia/NeoX, GPT-2, BLOOM, and BLOOMZ (Zhang et al., 2022;Black et al., 2022;Radford et al., 2019;Scao et al., 2022), with 19M to 66B parameters, and for 3 to 16-bit precision.
- In addition, we run more than 35,000 zero-shot experiments to vary many of the recently studied advances in quantization precision, including the underlying data types, quantization block size, and distribution centering.
- We find that reducing the precision steadily from 16 to 4 bits increases the zeroshot performance for a fixed number of model bits, while at 3-bit, the zero-shot performance decreases.
- This relationship holds across all models studied and did not change when scaling from 19M to 66B parameters, making 4-bit quantization is universally optimal across all cases tested.
- Beyond this, we analyze which quantization methods improve and which degrade bit-level scaling.
- We find that none of the quantization methods we test improve scaling behavior for 6 to 8-bit precision.
- For 4-bit precision, data types and a small quantization block size are the best ways to enhance bit-level scaling trends.
- We find that quantile quantization and floating point data types are the most effective, while integer and dynamic exponent data types generally yield worse scaling trends.
- For most 4-bit models, a block size between 64 to 128 is optimal in our study.

## Background
- Reducing the number of bits in a model is directly related to inference latency for LLMs
- Quantization data types and methods are used to reduce the number of bits in a model

### Relationship between inference latency and total model bits
- The relationship between total model bits and inference latency is strong when the inference batch size is below 60
- As the inference batch size increases, caching becomes more effective and the relationship between total bits and inference latency becomes less strong

### Data types
- We study the effects of quantization techniques, including blocking/grouping, centering, and outlier quantization, and we also introduce four different quantization data types, as reviewed in this section.
- We find that data types are one of the few things that improve bit-level scaling laws, and as such, we provide a general view of quantization that unifies data types under a common formalism so that data types can be more easily compared.
- Blocking is another concept that improves bit-level scaling, and we introduce this quantization method here.
- Quantization as a mapping from integers to values. While there are many ways to define quantization, we provide a general definition that unifies data types.
- We define quantization as a mapping from k-bit integers I to floating point values F in the range [−1, 1]. This definition has the advantage that a data type is fully specified by the set of floating point values F alone and the number of bits k in the set I.
- More formally, we can describe k-bit quantization as a mapping from the set of k-bit integers I to the set F , that is,
- For example, the IEEE 32-bit floating point data type maps the indices 0...2 32 − 1 to the set S with domain [-3.4e38, +3.4e38]. Furthermore, to be able to compare data types more easily, we normalize the domain of the real values in set F to the range [−1, 1] by dividing by the absolute maximum value of the set of possible values F .
- We use the following notation: Q map k (i) = q i , for example Q uint 8 (83) = 83/255 = 0.3255 for an 8-bit unsigned integer data type.
- With this notation, the mapping index i represents the quantized value to be stored.
- To quantize an arbitrary input, we normalize the input into the range [−1, 1] 1 and then do a binary search in set F to find the closest real value to the input and its associated mapping index i. Once the closest value is found, we store the quantized value as the mapped index.
- Formally, this can be described as:
- where T is the normalized input tensor.
- To dequantize the tensor T back to a real-valued tensor T F , we perform a lookup: where c is the normalization constant that normalized T into the range [−1, 1].
- To do computation, we dequantize the weight in the cache and perform a 16-bit floating point multiplication with the 16-bit input.
- With this general definition, we can now define the following data types by defining their set of quantization values F , also called a codebook.
- Integer data types. Integer quantization, also known as linear or uniform quantization, maps the integers to itself with an offset of 128 for a signed integer
- Floating Point data types. Floating point data types are represented by a combination of exponent bits E (with base 2) and mantissa bits M (fraction). Since we use 3-8 bit precision in our work, we take the FP8 data type as a reference (Micikevicius et al., 2022).
- The only difference is that we do not allocate a value for NaN values. As such, the floating point data type we use is defined by the following equations:
- Subnormal numbers, if the exponent is zero:
- Normalized numbers, if the exponent is non-zero:
- where bias=2 k−1 + 1, signbit={0, 1}, and b[i] represents the binary mantissa bit value at index i in the bit-mask.
- To create the set F , we can...

### Blocking / grouping & distribution centering
- Quantization precision is increased by centering and blocking the data.
- Centering the data and using blocking helps to confine outliers to particular blocks.
- In distribution centering, we subtract the mean from the input tensor before quantization.
- We add the mean as the final operation to dequantize a distribution-centered value.

## Outlier-dependent quantization through proxy quantization
- While it is sufficient to use 16-bit inputs and 8-bit weights to avoid this disruption, it is unclear if outlier features can cause degradation if we use 16-bit inputs and weights below 8-bit
- To this end, we develop outlier-dependent quantization through proxy quantization, where we quantize weights to a higher precision for the corresponding outlier feature dimensions to test how much precision is needed in the weights
- A significant challenge is that each model has a different number of outlier features, and outliers partially depend on inputs that are different for each zero-shot task
- As such, we seek a model-independent metric that has a constant memory footprint across all models and tasks
- In initial experiments, we noted that the criterion developed by Dettmers et al. (2022a), which thresholds the hidden states to detect outlier features, is unreliable as it depends on the standard deviation of the hidden state
- This causes problems because, for models such as OPT, the standard deviation of the hidden states increases in later layers, which causes too many outliers to be detected
- By inspecting this anomaly in OPT models, we find that a better measure of detecting outlier dimensions is the standard deviation of the weights of each hidden unit of the previous layer
- The standard deviation of hidden unit weights that produce outliers are up to 20x larger than the standard deviation of other dimensions

## Experimental setup
- In our experiments, we use 16-bit inputs and k-bit quantized parameters for 3 ≥ k ≥ 8.
- Attention matrices are not quantized since they do not contain parameters.
- We also use a 16-bit baseline that does not use any quantization (16-bit floats).
- To measure inference performance for k-bit quantization methods, we use perplexity on the CommonCrawl subset of The Pile (Gao et al., 2020) and mean zero-shot performance on the EleutherAI LM Evaluation harness (Gao et al., 2021).
- In particular, for the zero-shot setting, we use the EleutherAI LM eval harness (Gao et al., 2021) in the GPT-2 setting on the tasks LAMBADA (Paperno et al., 2016), Winogrande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019), and PiQA (Bisk et al., 2020).
- The choice of these particular zero-shot tasks was mainly motivated by previous work (Dettmers et al., 2022a;Yao et al., 2022;Xiao et al., 2022).
- However, in our evaluation, we find that perplexity is a superior metric since its continuous value per sample leads to less noisy evaluations.
- This has also been noted by Frantar et al. (2022).
- For example, when using perplexity to evaluate data types, quantile quantization is the best data type.
- Still, when we use zeroshot accuracy as an evaluation metric, the float data type is sometimes better because zero-shot accuracy is noisier.
- Moreover, we find that across more than 35,000 zero-shot experiments, the Pearson correlation coefficient between The Pile Common Crawl perplexity and zero-shot performance is -0.94.
- This highlights that perplexity is sufficient and preferable for evaluation purposes.
- A serious drawback is that perplexity is challenging to interpret.
- As such, we use zero-shot accuracies in the main paper for clarity but encourage the reader to use perplexity measures found in the appendix for replication and comparison purposes.
- Since perplexity evaluations are so reliable, it is possible to replicate our work by evaluating a small number of samples using the perplexity metric, which makes the construction of scaling laws less computationally intensive.
- Scaling laws.
- We try to fit power laws to our data, but we find that bivariate power functions with respect to the number of parameters and the bit-precision provide a poor fit.
- However, when we fit linear interpolations to represent scaling curves, we find that different bit-precisions are almost parallel, indicating that the scaling trends of different precisions can be represented faithfully by a base function and an offset for each bit-precision.
- As such, we choose to use linear interpolations to represent scaling trends.
- 4-bit is best for all models and scales Figure 3.
- Bit-level scaling laws for mean zero-shot performance across Lambada, PiQA, Winogrande, and Hellaswag.
- 4-bit precision is optimal for almost all models at all scales, with few exceptions.
- While lowering the bit precision generally improves scaling, this trend stops across all models at 3-bit precision, where performance degrades.
- OPT and Pythia are unstable at 3-bit precision, while GPT-2 and BLOOM remain stable.
- Plots for all intermediate bit precisions can be found in the Appendix A.1.

## Results & Analysis

### Bit-level Inference Scaling Laws
- For a given zero-shot performance, 4-bit precision yields optimal scaling for almost all model families and model scales.
- Scaling curves are almost parallel, which indicates that bit-level scaling is mostly independent of scale.
- An exception to this is 3-bit quantization.
- Pythia and OPT are unstable for 3-bit inference where performance is close to random (35%) for the largest Pythia/OPT models.

### Improving Scaling Laws
- Quantization precision (quantization block size, data types, centering, etc.) usually improves quantization error at a small cost of additional bits
- For 4-bit Pythia models, a small block size adds only 0.25 bits per parameter but provides an improvement similar to going from 4-bit to 5-bit precision
- For 3 to 5-bit precision, we do see improvements in scaling by applying quantization methods
- For 5-bit models, the improvement of using small block sizes is minor but still significant
- Data types improve scaling
- From Figure 4, we see that data types improve scaling trends for 4-bit Pythia. In particular, the quantile quantization and float data types provide better scaling than integer and dynamic exponent quantization
- For 4-bit precision, outlier-dependent quantization has no scaling benefit, as shown in Figure 5 (right), which means 4-bit precision is optimal despite the considerable improvements for 3-bit precision when using proxy quantization

## Related work
- Our work studies some of the quantization concepts introduced by this literature and highlights promising areas for further study. Quantization methods. - Another aspect related to our work are quantization methods which can be grouped into specific categories. For example, there are methods associated with blocking and grouping (Park et al., 2022;Wu et al., 2020;Jain et al., 2020;Nagel et al., 2019;Krishnamoorthi, 2018;Rusci et al., 2020), centering (Krishnamoorthi, 2018;Jacob et al., 2017), learned data types that are found through clustering (Gong et al., 2014;Han et al., 2015;Choi et al., 2016;Park et al., 2017), or direct codebook optimization (Rastegari et al., 2016;Hou et al., 2016;Leng et al., 2018;Zhang et al., 2018).

## Recommendations & Future Work
- Default to 4-bit quantization for LLM inference
- 128 or lower block size for stability
- Use a floating point or quantile quantization data type
- If maximal zero-shot accuracy is desired, 5-bit precision and a 66B model is preferable

## Discussion & Limitations
- While we ran more than 35,000 experiments, a main limitation is that we did not consider certain classes of quantization methods.
- For example, there are quantization methods where a data type is optimized either through additional input data passed through the model (Rastegari et al., 2016) or from the weights of the model alone (Gong et al., 2014).
- Optimization from the weights alone is similar to quantile quantization which was the most effective data type in our study.
- As such, this hints that such quantization methods could improve scaling for inference and present a missed opportunity to be included in our study.
- However, our study is an essential step towards recognizing the importance of these methods for optimizing the model-bits-accuracy tradeoff -a perspective that did not exist before.
- Another limitation is the lack of optimized GPU implementations.
- While our quantization methods could be efficiently implemented on any GPU that has Tensor Cores, and while it has been demonstrated before that optimized code for Int3/Int8 matrix multiplication can yield a speedup of 4.46x/5.2x (Frantar et al., 2022;Yao et al., 2022), it is unclear if other data types that rely on lookup tables can achieve significant speedups or if non-Tensor-Core GPUs would be effective.
- While a 4-bit data types are small enough to be stored in fast registers, it is unclear if some of the data types that we studied, such as quantile and dynamic exponent quantization, are only memory-efficient or also latency efficient.
- However, we also show that the float data type is effective and does not require a lookup table or additional registers for the data type.
- While we only study the latency-optimal perspective indirectly through studying the model-bits-accuracy trade-off, a practical limitation of the latency-optimal perspective is that low-bit models with 16-bit inputs might be less latency efficient if such a model is deployed to be used by many users.
- Given a busy deployed API with thousands of requests per second, the inference batch size of about 60 to 200 samples would make 4-bit inference with 16-bit inputs compute-bound, which means bit-level scaling laws would be increasingly unrelated to inference latency beyond such batch sizes.
- For such high throughput systems, scaling laws that model both low-bit weights and low-bit inputs are required to study optimal inference latency scaling.

## Conclusion
- 4-bit quantization is almost universally optimal to reduce the model bits and maximize zero-shot accuracy.
- Scaling behavior is improved by using a smaller blocksize.
- For any bit-precision, the exponent bits should make up at least half the bits rounded up.
- While lowering the precision generally improves scaling, this trend stops across all models at 3-bit precision where performance degrades.
