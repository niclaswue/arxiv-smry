---
title: "Settling the Reward Hypothesis"
date: 2022-12-20T16:55:33.000Z
author: "Michael Bowling, John D. Martin, David Abel, Will Dabney"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10420v1.webp" # image path/url
    alt: "Settling the Reward Hypothesis" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10420)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10420).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/settling-the-reward-hypothesis).

# Abstract
- The reward hypothesis is that all goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).
- This hypothesis will not be settled with a simple affirmation or refutation, but will specify completely the implicit requirements on goals and purposes under which the hypothesis holds.

# Paper Content

## Introduction
- The reward hypothesis states that all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).
- This statement takes on considerable import if one also accepts McCarthy's claim that "Intelligence is the computational part of the ability to achieve goals in the world."
- Together these two statements offer a sort of sufficiency to the study of reinforcement learning (RL), whose agents learn to achieve goals through the maximization of expected future rewards.
- They imply that to succeed at building AI, it is sufficient to succeed at solving RL.

## The Reward Hypothesis
- The reward hypothesis is that there is a positive relationship between the amount of effort put in and the amount of reward received.
- The reward hypothesis is that the relationship between effort and reward is linear.
- The reward hypothesis is that the relationship between effort and reward is constant across different situations and tasks.
- The reward hypothesis is that the relationship between effort and reward is invariant across different individuals.
- The first step in settling the reward hypothesis is to formalize what it claims.
- The assumptions made for the reward hypothesis are that there is a positive relationship between effort put in and reward received, that the relationship between effort and reward is linear, and that the relationship between effort and reward is constant across different situations and tasks.

### Goals as Preferences
- We ground "all of what we mean by goals and purposes" with a binary preference relation expressing preference for one outcome over another.
- The core of agent interaction is the cycle of repeat-edly observing the environment and then taking action to affect the environment.
- Let O be a finite set of observations, and A a finite set of actions.
- A history is then a sequence  1 ,  1 ,  2 ,  2 , . . . with  as the empty history of zero length.
- We define the set of histories of length  ∈ ℕ ≥0 as In deterministic settings, preferences are expressed over histories. However, when the environment and agent behavior are stochastic, we will want to consider distributions over histories, Δ(H ).
- Given ,  ∈ Δ(H ) and  ∈ [0, 1], let  + (1 − ) ∈ Δ(H ) be the distribution that samples a history from  with probability  and  with (1 − ).
- For  ∈ Δ(H ) and  ∈ O × A, let  •  ∈ Δ(H ), be the distribution where  is prepended to a history sampled from .
- The reward hypothesis posits a "received scalar signal". This could mean that the posited scalar reward signal is present in the agent's received observation, or can be computed by the agent from it. Alternatively, it could mean that there is an additional scalar signal provided to the agent by an external observer.
- We call the first setting subjective goals, as the posited reward signal can be constructed from the agent's subjective observation, and the latter case correspondingly objective goals.
- We first develop our main result with the subjective goals setting, but will later broaden the result to objective goals.
- For ,  ∈ Δ(H ), we write   if  is weakly preferred to , meaning that either  is strictly preferred , or the two are indifferently preferred.
- Notice that our notion of "goals and purposes" make no reference to the environment. Goals are stated as desirable histories, whereas the environment will act to constrain what histories and distributions over histories are possible.
- An agent's behavior, along with the environment, then induces a distribution over histories. Formally, given an environment  : H → Δ(O) and policy  : H × O → Δ(A), let    be the distribution over H  induced by  and .
- While    depends on the environment , we do not parameterize it as such since  typically is fixed throughout.
- We assume preferences over agent policies are then consistent with the distributions over histories that they induce.
- When comparing policies, we write  1   2 to mean  1 is weakly preferred to  2 under the goal . for all  ≥ . This notion of eventually preferring one policy's history distribution to another allows us the generality of goals and purposes that can be achieved in a defined time frame as well as those of a continuing nature.

### Maximizing Cumulative Sums
- The "maximization of the cumulative sum of a received scalar signal (reward)" means that there is a reward function  : O × A → ℝ and transition-dependent discount function  : O × A → [0, 1], such that we weakly prefer  1   2 under our reward if and only if there exists  such that   1  ≥  2  for all  ≥ .
- The "maximization of the expected value of the cumulative sum of a received scalar signal (reward)" means that there is a reward function  : O × A → ℝ and transition-dependent discount function  : O × A → [0, 1], such that we weakly prefer  1   2 under our reward if and only if there exists  such that   1  ≥  2  for all  ≥ , where  is a function that weakly prefers  1   2 under our reward.
- If () = 1 for some ,  pair , the objective corresponds to a typical total reward or average reward setting (even if the sum of the reward is not bounded). If () =  < 1 is constant for all , then the objective corresponds to a discounted reward objective. If () = 0 infinitely often then the objective can correspond to an episodic setting.

## Rationality Axioms

## Objective Goals
- The results of Theorem 2 apply to arbitrary sequences that contain the designer's experiences
- In the objective goals setting, the designer experiences a stream of observations ō ∈ Ō. These form a distinct process which is potentially related the observation stream of the agent.

## History and Related Work
- We discuss relevant literature from across RL and economics
- We propose a model of RL agents that can be used to simulate economic outcomes
- The model is able to reproduce key features of economic data
- The model is able to generate new insights about economic phenomena

### Economics
- Economics has been studying the nature of rational behavior for centuries
- In the 1700s, Gabriel Cramer and Daniel Bernoulli independently formulated what is now called the Expected Utility Hypothesis (Machina, 1990) in response to the "St. Petersburg Paradox" (Martin, 2011) articulated by Daniel's cousin Nicholas Bernoulli (1738)
- The Expected Utility Hypothesis says, roughly, that individuals "might maximize the expectation of 'utility' rather than of monetary value"
- Centuries later, Ramsey (1926) provided the first formal axiomatic treatment of expected utility, which would later be refined by von Neumann and Morgenstern (1953) to form the now widely adopted foundations of decision theory
- Following this development, an expansive body of research has explored how to account for other aspects of rationality, including uncertainty (Kreps and Porteus, 1978), time (Koopmans, 1960;Koopmans et al., 1964), and computation (Lewis, 1985;Richter and Wong, 1999;Rustem and Velupillai, 1990)

### Utility Theory for Sequential Decision Making
- Pitis (2019) explored the relationship between the vNM axioms and the typical objectives of RL, with a focus on the discount factor,
- Pitis provides a normative account for why we should embrace a state-action dependent discount factor, as developed by White (2017): A fixed discount cannot capture all preferences we might consider rational.
- Second, Pitis presents three axioms on top of the vNM axioms that characterize the conditions under which a state-action dependent discount factor can be viewed as rational. These axioms bear some resemblance to aspects of our formalism: For instance, Pitis' countable transitivity axiom also addresses the issue of infinite experiences, like our Assumption 2.
- Shakerinava and Ravanbakhsh (2022) focus on utility theory for a variety of controlled MDPs. They formalize the notion of "goals and purposes" using rational preference relations.
- And using expected utility theory, they explicate the preconditions (i.e. axioms) under which a reward function is guaranteed to exist in each type of MDP.
- Our work takes inspiration from this approach to formalize "goals and purposes" in a general sequential decision making setting.
- Furthermore, we can prove that our new Temporal -Indifference axiom generalizes two of their axioms. Here we state their axioms using our notation.
- satisfies Axioms 1-5 with () = 1 if and only if Axioms 1-4 are satisfied as well as Additivity.
- In contrast to our work, Shakerinava and Ravanbakhsh (2022) are not predominately concerned with "settling" the reward hypothesis. Rather, they use the hypothesis as a template to frame existential questions about reward functions for various flavors of controlled MDPs.
- As previously mentioned, our work decouples these concepts by treating goals and purposes in isolation of the environment dynamics. Even in this more general setting, we show there exists an efficient algorithm to construct rewards using a preference relation that satisfies Axioms 1-5 (See Algorithm 1 in the Appendix).
- Separately, Sunehag andHutter (2011, 2015) study what constitutes a rational reinforcement learning agent. More concretely, in both works, Sunehag and Hutter suppose the environment and reward function are defined, and set out to characterize what kinds of agents might we consider rational.
- In their first work (Sunehag and Hutter, 2011), they provide a series of properties that characterize what it means for an RL agent to be rational. These properties bear some similarity to the vNM axioms, but are agent-side properties rather than implicit requirements on the structure of goals or rewards themselves.
- In follow up work Sunehag and Hutter (2015) focus specifically on the rationality of optimistic agents. That is, given a reward function and environment, they contrast agents that are strictly expected utility maximizers with those that act optimistically. They give a full characterization of rational agency that justifies the use of optimism for exploration, showing that expected utility maximization on its own can be strictly worse than optimistic behavior.
- Abel et al. (2021) study the expressivity of reward in Markovian environments. In particular, they suppose the environment is characterized by a finite state space and transition function, and assume that preferences over objects defined with respect to the environment are given.
- These preferences come in three forms: (1) A set of acceptable policies, (2) A partial ordering on policies, or (3) A partial ordering on fixed-length state-action trajectories.
- For example, in the case of (1), the policy space is defined as all deterministic mappings of the form  : S → A.

### The Limited Expressivity of Markov Reward
- The steady state counterexample in Figure 1a violates Assumption 2, because the preference asserted by the counterexample goes against the indifference expressed by the Axiom.
- The entailment counterexample in Figure 1b violates Assumption 5, because the preference asserted by the counterexample goes against the indifference expressed by the Axiom.

## Challenges to the Reward Hypothesis
- Challenges to the reward hypothesis:
- Can the reward hypothesis explain why some behaviors are more common than others?
- Can the reward hypothesis account for why some behaviors are more persistent than others?

### Human Irrationality
- Rational agents make decisions that are in their best interest, according to a clear and precise set of preferences
- Decades ago it was common to view humans as rational agents, but this is not needed to settle the reward hypothesis
- The reward hypothesis is about the expression of goals, not the behaviors that emerge in their pursuit

### Multiple Objectives
- It is difficult to reduce the nuance of tradeoffs down to a single scalar
- Multi-criteria decision making has been studied extensively in the literature
- Constrained MDPs are more expressive than single-reward MDPs
- Risk-sensitive objectives can cause the optimal policy to be non-Markovian

## Conclusion
- The reward hypothesis for Markov reward functions holds if and only if Axioms 1-5 are satisfied.
- Axiom 5 states that the reward function is a function that is unique up to a positive scale factor, and is the function for which Axiom 5 is satisfied.
- The axioms imply the existence of a Markov reward.
- By Theorem 1 and axioms 1-4, we can select  : Δ(H ) → ℝ, such that () = 0, leaving  unique up to a positive scale factor.
- Define () def = ().
- From Axiom 5 and choosing ℎ 1 = ℎ and ℎ 2 = , Multiplying by () + 1, we get,
- We now show that any Markov reward satisfies the axioms. Due to Theorem 1 we know Axioms 1-4 are satisfied. We also know, for all ℎ ∈ H , Rearranging we get, ( Dividing all terms by () + 1, Applying Theorem 1 (first consequence 2 then consequence 1), thus Axiom 5 is satisfied.
- By the Independence Axiom, the preference remains unchanged after mixing distributions with ( • ) by an amount  ∈ [0, 1]: If we take  = (1 − ), then we can form uniform compound distributions of (, •) and ( • ): The last line follows from Lemma 3, which takes Temporal Indifference as its precondition.
- Finally, we invoke the Independence axiom to remove ( • ) mixture: ⇐=
- For any ,  ∈ , and , , ,  ∈ Δ(H ), the Additivity Axiom states
- The last line follows from independence on . Finally, Theorem 3 established that the Memoryless Axiom holds if and only if the Temporal -Indifference Axiom holds. Therefore, the remainder of the proof follows from that.
- Lemma 3. If Temporal -Indifference holds when  = 1, then for any ,  ∈ O × A, and ,  ∈ Δ(H ),
- Proof. Take some  from O × A, and let () = 1. Temporal -Indifference states that for any distributions  and This applies to any , so it must apply to any other  from O × A with ( ) = 1: The vNM utility theorem guarantees these preferences have a utility representation, meaning: Regardless of whether a distribution involves  or  , the difference between utilities will be equal: The notion of the cumulative sum eventually being larger allows us to capture settings where the series is convergent (i.e., lim →∞    exists) as well as cases where it is not, such as average reward; if one policy has higher average reward, then its finite sum must eventually be larger.
- For this analysis we will assume without loss of generality that the transition-dependent discount () = 1 ∀ ∈ . This last point is made formal in the following proposition, where the average reward for policy  is   lim →∞ 1      .
- Proposition 1. For any policies   ,   whose average rewards   ,   exist, if   >   , then there exists an  such that    >    for all  ≥ .
- Proof. Assume   >   exist. According to the definition of a limit, for any   ,   > 0, there exists some   and   such that We can show a similar result for the specialized bias-optimal case of average reward. Choose   =   = 1 2 (  −   ) > 0 and let  = max{  ,  ...
