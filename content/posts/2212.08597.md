---
title: "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better"
date: 2022-12-16T17:24:49.000Z
author: "David Dale, Elena Voita, Loïc Barrault, Marta R. Costa-jussà"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08597v2.webp" # image path/url
    alt: "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08597)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08597).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/detecting-and-mitigating-hallucinations-in).

# Abstract
- While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little.
- Recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative.
- It means that characteristics internal to the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself?
- We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations "detached" from the source, hence they can be identified by low source contribution. This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models.
- Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results.

# Paper Content

## Introduction
- Hallucinations in machine translation are cases when the model generates output that is partially or fully unrelated to the source sentence.
- Hallucinations have a relatively low impact on corpus-level automatic metrics, but the impact on user experience can be rather dramatic.
- For the most part, hallucinations were defined as translations with low quality according to some metric such as e.g. adjusted BLEU or chrF (Lee et al., 2019;Raunak et al., 2021;Müller and Sennrich, 2021).
- It was not clear whether proposed methods indeed detected hallucinations and, if so, whether they transferred to more natural settings.
- Recently, when revisiting previous work in a relatively clean setting, Guerreiro et al. (2022) found that existing methods fall short and the standard sequence log-probability is the most informative.
- To show this, the authors gathered a large dataset with professional annotations of translations that, according to 10 previously proposed methods, are likely to be hallucinations.
- For the most part, the method only looked at highly artificial hallucinations (Voita et al., 2021;Ferrando et al., 2022).
- We propose to use ALTI+ by Ferrando et al. (2022), the method that aggregates layer-wise tokens attributions, for both hallucination detection and reranking in the "detect-then-rewrite" framework.
- For detection of the most severe hallucinations, it is twice more accurate than sequence logprobability.
- For reranking, it performs on par with the previous best COMET-QE.
- All in all, we show that we can improve the overall pipeline results by solely relying on internal model characteristics.

## Background and Setting
- The framework was proposed by Guerreiro et al. (2022)
- The framework consists of a large dataset of annotated translations
- To the best of our knowledge, this is the only released data that can be used to analyze hallucinations in a "clean" setting.

### Model
- The model is Transformer base (Vaswani et al., 2017) from fairseq (Ott et al., 2019) with the standard hyperparameters setting.
- It was trained on the WMT'18 German-English news translation data excluding Paracrawl (Bojar et al., 2018) -totalling 5.8M sentence pairs.
- Since Guerreiro et al. (2022) used randomly chosen 1/3 of the dataset as a held-out set for analysis, the model was trained on the remaining 2/3 of the dataset.
- We use the model released by Guerreiro et al. (2022): this is the model that generated hallucinations we analyze.

### Hallucination Dataset
- The Hallucination dataset released by Guerreiro et al. (2022) contains fine-grained manual annotations of 3415 German-to-English translations generated by the model above.
- These translations are chosen from a set of 1.8M translations of held-out data as the ones that are likely to be pathological.
- The criteria used to flag the translations include 10 methods ranging from previously proposed heuristics (Lee et al., 2019;Berard et al., 2019;Raunak et al., 2021) to quality estimation models (Rei et al., 2020b) and uncertainty detectors (Fomicheva et al., 2020;Zerva et al., 2021;Guerreiro et al., 2022).
- The taxonomy of translation pathologies in the dataset is shown in Figure 1.
- Here, hallucinations are defined as severe translation errors that are detached from the source.
- These can be either oscillatory (i.e. contain erroneous repetitions of words and phrases) or largely fluent.
- The latter is also split by severity of an error into fully detached (the whole content is not supported by the source) and strongly, but not fully, detached (significant proportion of output is not supported by the source).
- Other than hallucinations, the annotated data also contains translation errors that are deemed to be not detached from the source (see Figure 1).
- Overall, 323 examples are judged to be hallucinations, 1044 as less severe translation errors and the rest as correct translations.
- Note that so far, there is no "canonical" hallucination taxonomy and previous work used various, mostly overlapping, definitions (Lee et al., 2019;Raunak et al., 2021;Zhou et al., 2021;Ji et al., 2022;Raunak et al., 2022;Guerreiro et al., 2022).
- We follow the taxonomy by Guerreiro et al. (2022) for two reasons. Firstly, for consistency with the dataset and the evaluation framework we use. Secondly, this taxonomy is rather general, considers some hallucination types overlooked in previous work (e.g. strongly detached hallucinations), and was shown to be reasonable: under these definitions, properties of hallucinations differ from those of translation errors (Guerreiro et al., 2022).

## Hallucination Detection Methods
- Methods for handling hallucinations can be either internal or external
- There are "oracles" that rely on reference translation

### Reference-Based Oracles
- Uses chrF to measure the quality of a translation
- Uses COMET to measure the quality of a translation

### Internal Measures

### Main results
- ALTI outperforms Seq-Logprob for all hallucinations, but performs worse for fully detached hallucinations
- LaBSE and XNLI substantially outperform previous best detector: for both all and fully detached hallucinations, their precision at 90% recall is roughly twice better than that of Seq-Logprob
- LASER2, the cross-lingual part of the objective uses cosine similarity between sentence encodings, performs worse than LaBSE

### Analysing Distributions of the Scores
- Methods: internal methods (partial hallucinations are bimodal, ALTI and Seq-Logprob show similar behavior: errors are distributed similarly to correct translations, and the scores for partial (strongly detached) hallucinations have bimodal distribution)
- At a high level, for the model, some partial hallucinations "look" more like full hallucinations, and some -more like errors
- What is also expected, is that compared to reference-free COMET-QE, the overlap between the scores for correct and incorrect translations is much lower for reference-based COMET
- ChrF exhibits a behavior similar to COMET
- LaBSE: ranks hallucination severity best
- LASER: no middle ground; modes for the three distributions are also ordered; unfortunately, the distributions themselves overlap significantly which makes LASER not a good hallucination detector
- XNLI: no middle ground

### Detected Pathology Types
- Translation is "detected" if it is contained in a fraction (e.g. 10%) of the hallucination dataset corresponding to the lowest scores.
- Then we look at the distribution of pathology types contained among detected examples (Figure 3);
- Recall for different translation types with respect to the whole dataset (Figure 4).
- The three best methods are similar. Here, the types are presented in a multilabel manner, i.e. one translation may contribute to multiple axes.

## Mitigating Hallucinations at Test Time
- The "detectthen-rewrite" pipeline includes flagging a translation as likely to be hallucinated and generating several alternative translations.
- For generating hypotheses, simply applying MC dropout (as done in Guerreiro et al. ( 2022)) outperforms more involved methods such as diverse beam search.
- For reranking, we can match COMET-QE with internal ALTI and improve the hallucination rate by using LaBSE.

### Evaluation methodology
- The setup for the experiments with automatic evaluation
- The setup for manual annotation
- The impact of the method on translations
- Simulating a practical application

### Generation Strategies
- Guerreiro et al. ( 2022) use Monte Carlo dropout (Gal and Ghahramani, 2016) to generate alternative hypotheses
- This means they leave standard beam search inference intact and achieve variability in translations via activating model dropout at inference
- A natural question is whether using other generation strategies can give better results
- For example, if we use e.g. beam search specifically designed to produce diverse translations, can we get better hypotheses?
- To test this, we use the following methods:
- DEFAULT: standard decoding without reranking, i.e. beam search with size 5, where we pick only the top 1 candidate;
- BEAM SEARCH: beam search with size n;
- sampling from the predicted distribution:
- SAMPLING: from the whole distribution;
- SAMPLING P=80: from the top p = 80% of the distribution, i.e. nucleus sampling (Holtzman et al., 2020);
- diverse beam search:  search with dropout, each with size 10.
- Unless stated otherwise, n = 10 in all experiments.
- The results are shown in Figure 5.
- We see that the scores increase with more hypotheses and do not saturate at 10. This implies that in cases when the quality of a translation is much more important than its computational cost, one can potentially improve the quality by generating more candidate hypotheses.

### Reranking Approaches
- Apart from detecting hallucinations, the methods we propose can be applied as rerankers in the "detect-than-rewrite" pipeline.
- Cor. Avg. Reranker F. No reranking -1.23 -0.97 -0.59 0.27 -0.63 Baseline COMET-QE -0.21 -0.13 -0.14 0.35 -0.03 Ours ALTI -0.17 -0.24 -0.39 0.25 -0.14 LASER -0.11 -0.23 -0.35 0.27 -0.11 LaBSE -0.07 -0.12 -0.26 0.39 -0.01 XNLI -0.12 -0.18 -0.28 0.30 -0.07 ).
- In the appendix, we also show XNLI scores.
- Figure 5 shows that, regardless of the generation method, LaBSE is the best reranker and it performs notably better than the strong COMET-QE baseline.
- Apart from the average results, Table 2 also shows COMET scores for each pathology type. We can see that reranking with any method is better then no reranking for all groups of original translations.
- Compared to the COMET-QE baseline, LABSE improves the scores for hallucinations and correct translations, but drops quality for other pathologies.
- The only internal method ALTI performs better than COMET-QE for fully detached hallucinations, but is inferior when looking at other translation types. It means that ALTI is very sensitive to the most severe pathology, but not capable to rank relatively good translations.
- Data. To confirm the results of automatic evaluation, we perform manual annotation. For each of the methods, we their translations of the same 200 source sentences. These sentences were randomly sampled from the hallucination dataset with the distribution of pathologies roughly mimicking outputs of the best detectors (Figure 3).
- Overall, for 55% of the sentences their original translations are labeled as hallucinations, 25% as errors and 20% as correct translations.
- 7 e compare the original translations and three reranking methods: the baseline COMET-QE used in Guerreiro et al. (2022), the best overall reranker LaBSE, and the only internal method ALTI.
- Annotation. For each source sentence, the four translations were deduplicated and shuffled to avoid annotator bias. The resulting sentence pairs were given to 3 annotators who labeled them with three categories: Correct, Error, and Hallucination. The labels were aggregated by majority vote; in case of ties (20 out of the 602 sentence pairs that were left after deduplication) we pessimistically assumed a hallucination.
- Further details on the annotation guidelines and inter-annotation agreement are reported in Appendix B.
- We evaluate the statistical significance of the pairwise differences in the proportions of correct and hallucinated translations using two-sided Student test for two related samples with 5% confidence level.
- Results. Annotation results are shown in Figure 7. All reranking methods reduce hallucinatory rate by a factor of 2.5-3. Interestingly, when looking at hallucinations, internal ALTI performs on par with COMET-QE: the differences between these two methods are not statistically significant. COMET-QE, however, has less errors. This is expected: it was trained to distinguish correct translations from errors.
- Coming to LaBSE, we find that it produces slightly less hallucinations that other reranking methods, and more correct translations than ALTI; these differences are significant at 5% confidence level.
- Overall, by using sentence similarity from LaBSE, we improve both halucinations detection and alleviating hallucinations at test time.

## Conclusions
- We started this work by asking how far we can go at detecting and mitigating hallucinations if we use nothing but the translation model itself.
- Turns out, we can improve the results of the overall "detect- For hallucinations, all the differences are significant, except the one between ALTI vs COMET-QE.
- For correct translations, the difference between LaBSE and ALTI is statistically significant.
- then-rewrite" pipeline by evaluating the percentage of source contribution to generated translation: translations with low source contributions are likely to be "detached" from the source, i.e. hallucinations.
- For detecting the most severe type of hallucinations, this method improves previous results twice; for mitigating hallucinations at test time, it matches the hallucination reduction rate of the previous method based on COMET-QE.
- We believe this can motivate future work on model understanding and help practitioners: using nothing but model inner working can allow mitigating hallucinations for language pairs where models such as COMET-QE are not available.
- When allowing external models, we propose to expand the methods for handling hallucinations from models specialized for quality estimation to a broader set of objectives, e.g. sentence similarity from cross-lingual embeddings.
- Apart from showing that e.g. LaBSE improves previous results significantly, we also find that models so far overlooked in the context of machine translation (such as natural language inference) can be beneficial.
- We hope future work will build on this idea.
