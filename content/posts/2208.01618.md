---
title: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"
date: 2022-08-02T17:50:36.000Z
author: "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2208-01618v1.webp" # image path/url
    alt: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2208.01618)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2208.01618).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text).

# Abstract
- Text-to-image models offer unprecedented freedom to guide creation through natural language
- Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes
- In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy?
- Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way.
- Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.

# Paper Content

## Introduction
- In a famous scene from the motion picture "Titanic", Rose makes a request of Jack: "...draw me like one of your French girls".
- This request contains a wealth of information. It indicates that Jack should produce a drawing; It suggests that its style and composition should match those of a subset of Jack's prior work; Finally, through a single word, "me", Rose indicates that this drawing should portray a specific, unique subject: Rose herself.
- In making her request, Rose relies on Jack's ability to reason over these conceptsboth broad and specific -and bring them to life in a new creation.
- Introducing new concepts into large scale models is often difficult. Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022;Li et al., 2022).
- More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021;Gao et al., 2021;Skantze & Willemsen, 2022).
- However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts (Kumar et al., 2022;Cohen et al., 2022).
- We propose to overcome these challenges by finding new words in the textual embedding space of pre-trained text-to-image models.
- We consider the first stage of the text encoding process (Figure 2). Here, an input string is first converted to a set of tokens. Each token is then replaced with its own embedding vector, and these vectors are fed through the downstream model.
- Our goal is to find new embedding vectors that represent new, specific concepts.
- We represent a new embedding vector with a new pseudo-word (Rathvon, 2004) which we denote by S * . This pseudo-word is then treated like any other word, and can be used to compose novel textual queries for the generative models.
- One can therefore ask for "a photograph of S * on the beach", "an oil painting of a S * hanging on the wall", or even compose two concepts, such as "a drawing of S 1 â€¢
- We analyze the embedding space in light of GAN-inspired inversion techniques and demonstrate that it also exhibits a tradeoff between distortion and editability.
- We show that our approach resides on an appealing point on the tradeoff curve.
- We evaluate our method against images generated using user-provided captions of the concepts and demonstrate that our embeddings provide higher visual fidelity, and also enable more robust editing.

## Related work
- Text-guided synthesis has been widely studied in the context of GANs
- Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets
- Leveraging attention mechanisms or cross-modal contrastive approaches
- More recently, impressive visual results were achieved by leveraging large scale auto-regressive or diffusion models
- Rather than training conditional models, several approaches employ test-time optimization to explore the latent spaces of a pre-trained generator
- These models typically guide the optimization to minimize a text-to-image similarity score derived from an auxiliary model such as CLIP
- Moving beyond pure image generation, a large body of work explores the use of text-based interfaces for image editing, generator domain adaptation, video manipulation, motion synthesis, style transfer and texture synthesis for 3D objects
- Our approach builds on the open-ended, conditional synthesis models
- Rather than training a new model from scratch, we show that we can expand a frozen model's vocabulary and introduce new pseudo-words that describe specific concepts
- Manipulating images with generative networks often requires one to find a corresponding latent representation of the given image, a process referred to as inversion
- In the GAN literature, this inversion is done through either an optimization-based technique or by using an encoder
- Optimization methods directly optimize a latent vector, such that feeding it through the GAN will re-create a target image
- Encoders leverage a large image set to train a network that maps images to their latent representations
- In our work, we follow the optimization approach, as it can better adapt to unseen concepts
- Encoders face harsher generalization requirements, and would likely need to be trained on web-scale data to offer the same freedom
- We further analyze our embedding space in light of the GAN-inversion literature, outlining the core principles that remain and those that do not
- Diffusion-based inversion
- Personalization

## Method
- Latent Diffusion Models are a type of Denoising Diffusion Probabilistic Model that operate in the latent space of an autoencoder
- LDMs consist of two core components: an autoencoder and a diffusion model
- The LDM loss is minimized over images sampled from a small set of images that depict the target concept across multiple settings
- To condition the generation, we randomly sample neutral context texts that contain prompts of the form "A photo of S * ", "A rendition of S * ", etc.

## Qualitative comparisons and applications
- Textual Inversions enable a wide range of applications, including machine translation, captioning, and summarization
- The state-of-the-art and human-captioning baselines provide different levels of accuracy and accessibility
- Textual Inversions provide a more accurate and accessible alternative to the state-of-the-art

### Image variations
- Our method is better than human captioning at capturing the most prominent features of an object
- Our method is better than DALLE-2 at capturing well-known details like color patterns
- Our method can successfully capture finer details of personalized objects
- Our method can synthesize new scenes by incorporating learned pseudo-words into new conditioning texts

### Style transfer
- A typical use-case for text-guided synthesis is in artistic circles, where users aim to draw upon the unique style of a specific artist and apply it to new creations.
- We show that our model can also find pseudowords representing a specific, unknown style.
- To find such pseudo-words, we simply provide the model with a small set of images with a shared style, and replace the training texts with prompts of the form: "A painting in the style of S * ".
- Results are shown in Figure 6.

### Concept compositions
- The model can concurrently reason over multiple novel pseudo-words at the same time
- The model struggles with relations between them (e.g. it fails to place two concepts side-by-side)

### Bias reduction
- The DALLE-2 system card (Mishkin et al., 2022) reports that their baseline model tends to produce images of people that are whitepassing and male-passing when provided with the prompt "A CEO".
- Similarly, results for "wedding", tend to assume Western wedding traditions, and default to heterosexual couples.
- Here, we demonstrate that we can utilize a small, curated dataset in order to learn a new "fairer" word for a biased concept, which can then be used in place of the original to drive a more inclusive generation.
- Figure 7: Compositional generation using two learned pseudo-words. The model is able to combine the semantics of two concepts when using a prompt that combines them both. It is limited in its ability to reason over more complex relational prompts, such as placing two concepts side-by-side.
- Image credits: @QinniArt (left), @Leslie Manlapig (right). Reproductions authorized for non-commercial / non-print use respectively.
- A common limitation of text-to-image models is that they inherit the biases found in the internet-scale data used to train them.
- These biases then manifest in the generated samples.
- For example, the DALLE-2 system card (Mishkin et al., 2022) reports that their baseline model tends to produce images of people that are whitepassing and male-passing when provided with the prompt "A CEO".
- Similarly, results for "wedding", tend to assume Western wedding traditions, and default to heterosexual couples.
- Here, we demonstrate that we can utilize a small, curated dataset in order to learn a new "fairer" word for a biased concept, which can then be used in place of the original to drive a more inclusive generation.
- Figure 8: Bias Reduction. Uncurated samples synthesized with pretrained biased embeddings (left) and our debiased embeddings (right). Our approach can be used to reduce bias by learning new pseudo-words for known concepts. These can be optimized using small datasets, which can be carefully curated for diversity. Specifically, in Figure 8 we highlight the bias encoded in the word "Doctor", and show that this bias can be reduced (i.e. we increase perceived gender and ethnic diversity) by learning a new embedding from a small, more diverse set.

### Downstream applications
- Pseudo-words can be used in downstream models
- This localized synthesis process can also be conditioned on our learned pseudo-words

### Image curation
- We generated 16 candidates for each prompt
- We manually selected the best result
- We note that similar curation processes with larger batches are typically employed in text-conditioned generation works

## Quantitative analysis
- Inversion into an uncharted latent space provides a wide range of possible design choices.
- Many of the solutions typically used in GAN inversion fail to generalize to this space.

### Evaluation metrics
- To analyze the quality of latent space embeddings, we consider two fronts: reconstruction and editability.
- First, we wish to gauge our ability to replicate the target concept. As our method produces variations on the concept and not a specific image, we measure similarity by considering semantic CLIP-space distances. Specifically, for each concept, we generate a 64 of images using the prompt: "A photo of S * ". Our reconstruction score is then the average pair-wise CLIP-space cosine-similarity between the generated images and the images of the concept-specific training set.
- Second, we want to evaluate our ability to modify the concepts using textual prompts. To this end, we produce a set of images using prompts of varying difficulty and settings. These range from background modifications ("A photo of S * on the moon"), to style changes ("An oil painting of S * "), and a compositional prompt ("Elmo holding a S * ").
- For each prompt, we synthesize 64 samples using 50 DDIM steps, calculate the average CLIP-space embedding of the samples, and compute their cosine similarity with the CLIP-space embedding of the textual prompts, where we omit the placeholder S * (i.e. "A photo of on the moon").
- Here, a higher score indicates better editing capability and more faithfulness to the prompt itself.

### Evaluation setups
- We evaluate the embedding space using a set of experimental setups inspired by GAN inversion:
- Following Abdal et al. (2019), we consider an extended, multi-vector latent space. In this space, S * is embedded into multiple learned embeddings, an approach that is equivalent to describing the concept through multiple learned pseudo-words.
- We consider an extension to two and three pseudo-words (denoted 2 âˆ’ word and 3 âˆ’ word, respectively). This setup aims to alleviate the potential bottleneck of a single embedding vector to enable more accurate reconstructions.
- We follow Tov et al. (2021) and consider a progressive multi-vector setup. Here, we begin training with a single embedding vector, introduce a second vector following 2, 000 training steps, and a third vector after 4, 000 steps. In this scenario, we expect the network to focus on the core details first, and then leverage the additional pseudo-words to capture finer details.
- We introduce a regularization term that aims to keep the learned embedding close to existing words. In practice, we minimize the L2 distance of the learned embedding to the embedding of a coarse descriptor of the object (e.g. "sculpture" and "cat" for the images in Figure 1).
- Finally, we consider our own setup, as outlined in Section 3. We further evaluate our model with an increased learning rate (2e âˆ’ 2, "High-LR") and a decreased learning rate (1e âˆ’ 4, "Low-LR").

### Results
- The semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.
- The single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines.
- By using long captions, we increase the chance of the model ignoring our desired setting, focusing only on the object description itself.
- Our model, however, uses only a single token and thus minimizes this risk.

### Human evaluations
- We evaluated our models using a user study.
- The user study showed that the models produced similar results to the CLIP-based evaluation.
- The user study also showed that the models have a reconstruction-editability tradeoff.

## Limitations
- The method offers increased freedom
- It may still struggle with learning precise shapes
- For artistic creations, this is often enough
- In the future, we hope to achieve better control over the accuracy of the reconstructed concepts
- Another limitation of our approach is in the lengthy optimization times

## Social impact
- Text-to-image models can be used to generate misleading content and promote disinformation.
- Personalized creation could allow a user to forge more convincing images of non-public individuals.
- However, our model does not currently preserve identity to the extent where this is a concern.
- These models are further susceptible to the biases found in the training data.
- Examples include gender biases when portraying "doctors" and "nurses", racial biases when requesting images of scientists, and more subtle biases such as an over-representation of heterosexual couples and western traditions when prompting for a "wedding" (Mishkin et al., 2022).
- As we build on such models, our own work may similarly exhibit biases.
- However, as demonstrated in Figure 8, our ability to more precisely describe specific concepts can also serve as a means for reducing these biases.
- Finally, the ability to learn artistic styles may be misused for copyright infringement.

## Conclusions
- We introduced the task of personalized, language-guided generation, where a text-to-image model is leveraged to create images of specific concepts in novel settings and scenes.
- Our approach, "Textual Inversions", operates by inverting the concepts into new pseudo-words within the textual embedding space of a pre-trained text-to-image model.
- These pseudo-words can be injected into new scenes using simple natural language descriptions, allowing for simple and intuitive modifications.
- In a sense, our method allows a user to leverage multi-modal information -using a text-driven interface for ease of editing, but providing visual cues when approaching the limits of natural language.
- Our approach was implemented over LDM (Rombach et al., 2021), the largest publicly available text-toimage model. However, it does not rely on any architectural details unique to their approach. As such, we believe Textual Inversions to be easily applicable to additional, larger-scale text-to-image models.
- There, text-to-image alignment, shape preservation, and image generation fidelity may be further improved.
- We hope our approach paves the way for future personalized generation works. These could be core to a multitude of downstream applications, from providing artistic inspiration to product design.
