---
title: "Diffusion models as plug-and-play priors"
date: 2022-06-17T21:11:36.000Z
author: "Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2206-09012v3.webp" # image path/url
    alt: "Diffusion models as plug-and-play priors" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.09012)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.09012).


# Abstract
- Inference of high-dimensional data is a problem that can be solved by using a diffusion model
- The diffusion model can be used to adapt models to new domains and tasks
- The diffusion model can be used to approximate inference by iterating differentiation

# Paper Content

## Introduction
- Deep generative models, such as denoising diffusion probabilistic models [DDPMs], can capture the details of very complex distributions over high-dimensional continuous data.
- The immense effective depth of DDPMs, sometimes with thousands of deep network evaluations in the generation process, is an apparent limitation on their use as off-the-shelf modules in hierarchical generative models, where models can be mixed and one model may serve as a prior for another conditional model.
- In this paper, we show that DDPMs trained on image data can be directly used as priors in systems that involve other differentiable constraints.
- One obvious application of plug-and-play priors is conditional image generation.

### Related work
- Conditioning DDPMs.
- With few exceptions -such as [3], which uses a pretrained DDPM as a feature extractor -these algorithms assume access to paired data and conditioning information during training of the DDPM model.
- In [7], a classifier p(y | x t ) that guides the denoising model towards the desired subset of images with the attribute y is trained in parallel with the denoiser.
- In [5], generation is conditioned on an auxiliary image by guiding the denoising process through correction steps that match the low-frequency components of the generated and conditioning images.
- In contrast, we aim to build models that combine an independently trained DDPM with an auxiliary constraint.
- Our approach is also related to work on adversarial examples.
- Adversarial samples are produced by optimizing an image x to satisfy a desired constraint c -a classifier p(y|x) -without reference to the prior over data.
- As supervised learning algorithms can ignore the structure in data x, focusing only on the conditional distribution, it is possible to optimize for input x that provides the desired classification in various surprising ways [41].
- In [31], a diffusion model is used to defend from adversarial samples by making images more likely under a DDPM p(x).
- We are instead interested in inference, where we seek samples x that satisfy both the classifier and the prior.
- Latent vectors in DDPMs.
- Modeling the latent prior distribution in VAE-like models using a DDPM has been studied in [38,43].
- On the other hand, in §5, we perform inference in the low-dimensional latent space under a pretrained DDPM on a high-dimensional data space.
- Our approach to semantic segmentation ( §4) is also related to [34], where a prior p(z) over latents is used to tune a posterior network q(z|x).

## Method

### Problem setting
- The free energy is minimized when the approximate posterior is closest to the true posterior
- When the approximate posterior is Dirac delta, it is optimal to concentrate q at the mode of p(x|y)
- When the prior involves latent variables, the free energy is minimized by hybridizing a DDPM and a decoder

### Denoising diffusion probabilistic models as priors
- DDPMs are a deep learning model that reverses a noise process
- The model starts with a sample from a unit Gaussian and successively transforms it with a nonlinear network
- After T steps, the sample is obtained
- In general, using such a model as a prior over x would require an intractable integration over latent variables
- However, DDPMs are trained under the assumption that the posterior q(x t |x t−1 ) is a simple diffusion process that successively adds Gaussian noise according to a predefined schedule
- Therefore, if p(x) is the likelihood (5) of x under a DDPM, then in the first expectation of (2) we should use q(h = {x T , ..., x 1 })
- The simplest approximation to the posterior over x = x 0 is a point estimate: where by δ we denote the Dirac delta function. Thus, we can sample x t at any arbitrary time step using the forward noising process
- Analogously to [13], we can also extract a conditional Gaussian q(x t−1 | x t , η) and express the first expectation in (2) as
- After reparametrization [13] leads to
- The weighting w t (β) is generally a function of the noise schedule, but in most pretrained diffusion models it is set to 1. Thus, the free energy in (2) reduces to
- To perform inference under an already trained model θ , we instead minimize F with respect to η through sampling in the summands over t.
- A similar derivation applies if a Gaussian approximation to the posterior q(x) is used
- Such an approximation allows to model not only a mode of the posterior, but the uncertainty in its vicinity.
- We summarize the algorithm for a point estimate q(x) as Algorithm 1.
- Variations on this algorithm are possible. Depending on how close to a good mode we can initialize η, this optimization may involve summing only over t ≤ t max < T ; different time step schedules can be considered depending on the desired diversity in the estimated x.
- Note that optimization is stochastic and each time it is run it can produce different point estimates of x which are are both likely under the diffusion prior and satisfy the constraint as much as possible.
- We observed that optimizing simultaneously for all t makes it difficult to guide the sample towards a mode in image generation applications; therefore, we anneal t from high to low values.
- Intuitively, the first few iterations of gradient descent should coarsely explore the search space, while later iterations gradually reduce the temperature to steadily reach a nearby local maximum of p(x|y).
- Examples of annealing schedules designed for the tasks demonstrated in §3, 4, 5 are presented in the Appendix (Fig. A).

### Simple illustration on MNIST
- We explore the idea of generating conditional samples from an unconditional diffusion model on MNIST
- We train the DDPM model of [7] on MNIST digits and experiment with different sets of  constraints log c(x, y) to generate samples with specific attributes
- The examples in Fig. 1 showcase such generated samples
- For the digit in (a) we set the constraint log c to be the unnormalized score of 'thin' digits, computed as negative of the average image intensity, whereas in (b) we invert that and generate a 'thick' digit with high mean intensity. Similarly, in (c) and (d) we hand-craft a score that penalizes the vertical and horizontal symmetry respectively, by computing the L 2 distance between the two folds (vertical/horizontal) of the digit x, which leads to the generation of skewed, non-symmetric samples
- We also showcase how the auxiliary constraint c(x, y) can be modeled by a different, independently trained network
- The digit in Fig. 1 (e) is generated by constraining the DDPM with a classifier network that is separately trained to distinguish between the digit class y = 3 and all other digits
- Finally, for (f) we multiply horizontal symmetry and digit classifier constraints, prompting the inference procedure to generate a perfectly centered and symmetric digit.

### Using off-the-shelf components for conditional generation of faces
- We consider the generation of natural images with a pretrained DDPM prior and a learned constraint.
- We utilize the pretrained DDPM network on FFHQ-256 [19] from [3] and a pretrained ResNet-18 face attribute classifier on CelebA [25].
- The attribute classifier computes the likelihood of presence of various facial features y in a given image x, as they are defined by the CelebA dataset.
- To generate a conditional sample from the unconditional DDPM network we select a subset of these and enforce their presence or absence using the classifier predicted likelihoods as our constraint c.
- If y is a set of attributes we wish to be present, the constraint log c(x, y) can be expressed as
- We only strictly enforce a small subset of facial attributes and therefore x is allowed to converge towards different modes that correspond to samples that exhibit, in varying levels, the desired features.
- In Fig. 2 we demonstrate our ability to infer conditional samples x with desired attributes y, using only the unconditional diffusion model and the classifier p(y | x).
- In Fig. 3 we showcase the intermediate steps of the optimization process for inference with the conditions blond hair+smiling+not male, thus solving a problem like that studied in [8] using only independently trained attribute classifiers and an unconditional generative model of faces.
- The sample x is initialized with Gaussian noise N (0, I), and as we perform gradient steps with decreasing values of t, we observe facial features being added in a coarse-to-fine manner.
- Second row: denoising as in [31] to remove artifacts that appear when optimizing with a classifier network enforcing the constraint.
- The last step performs denoising as in [31] to remove artifacts that appear when training on a classifier as a constraint.

## Experiments: Semantic image segmentation
- We use the EnviroAtlas dataset to infer land cover labels in four other US cities
- We use Algorithm 1 to perform an inference procedure that does not directly take imagery as input, but uses constraints derived from unsupervised color clustering
- We use only cluster indices in inference, making the algorithm dependent on image structure, but not color
- Local cluster indices as a representation have a promise of extreme domain transferability, but they require a form of a combinatorial search which matches local cluster indices to semantic labels so that the created shapes resemble previously observed land cover, as captured by a denoising diffusion model of semantic segmentations
- We train a DDPM model on the 1 4 -resolution one-hot representations of the land cover labels, using the U-Net diffusion model architecture from [7]
- To convert the one-hot diffusion samples to probabilities we follow [15] and assume that for any pixel i in the inferred sample x, the distribution over the label is, p( i ) ∝ 1.5 0.5 N (x i | η i , σ), where σ is user-defined a parameter
- We choose this approach for its simplicity and ease to apply in our inference setting of Algorithm 1
- Alternatively, we could use diffusion models for categorical data [14] with the appropriate modifications to our inference procedure
- Samples drawn from the learned distribution are presented in Fig 4
- Inferring semantic segmentations
- We directly apply Algorithm 1 with a hand-crafted constraint c which provides structural and label guidance
- To construct c, we first compute a local color clustering z of input the image ( §B.3 in the Appendix)
- In addition, we utilize the available weak labels weak and  force the predicted segments' distribution to match the weak label distribution when averaged in non-overlapping blocks
- We combine the two objectives in a single constraint c(x, z, weak ) by (i) computing the mutual information between the color clustering z and the predicted labels x , transformed into a valid probability distribution from the inferred one-hot vectors, in overlapping image patches and (ii) computing the negative KL divergence between the average predicted distribution and the distribution given by the weak labels in non-overlapping blocks
- Empirically, we find that we can reduce the number of optimization steps needed to perform inference by initializing the sample x with the weak labels weak instead of random noise, allowing us to start from a smaller t i .
- Examples of images and their inferred segmentations are shown in Fig. 5
- Domain transfer with inferred samples
- The above inference procedure is agnostic to colors by design, and we expect it to have a greater ability to perform in new areas than the approach in [34], which still finetunes networks that take raw images as input.

## Experiments: Continuous relaxation of combinatorial problems
- Inference under a DDPM prior and a differentiable constraint c(x, y)
- Inference under a DDPM prior and a 'hard' constraint where y is a latent vector deterministically encoded in an image x (x = f (y))
- Algorithm 1 is used to obtain a point estimate of the distribution over y, p(y) ∝ p DDPM (f (y))
- The algorithm mimics the convolutional inductive bias of the visual system
- Encoding function. Fix a set of points v 1 , . . . , v N ∈ [0, 1] × [0, 1]
- We encode an symmetric N × N matrix with 0 diagonal A as a 64 × 64 greyscale image f (A) by superimposing: (i) raster images of line segments from v i to v j with intensity value A ij for every pair (i, j), and (ii) raster images Extracted + 2-opt Oracle
- The procedure for solving the Euclidean TSP with a DDPM: Gradient descent is performed on a latent adjacency matrix A to minimize a stochastic denoising loss on an image representation f (A) with steadily decreasing amounts of noise (here, 256 steps). In the process, pieces of the tour are 'burned in' and later recombined in creative ways. Finally, a tour is extracted from the inferred adjacency matrix and refined by uncrossing moves.

## Conclusion
- We have shown how inference in denoising diffusion models can be performed under constraints in a variety of settings.
- Imposing constraints that arise from pretrained classifiers enables conditional generation, while common-sense conditions, such as mutual information with a clustering or divergence from weak labels, can lead to models that are less sensitive to domain shift in the distribution of conditioning data.
- A notable limitation of DDPMs, which is inherited by our algorithms, is the high cost of inference, requiring a large number of passes through the denoising network to generate a sample.
- We expect that with further research on DDPMs for which inference procedures converge in fewer steps [37,45], plug-and-play use of DDPMs will become more appealing in various applications.
- Finally, our results on the traveling salesman problem illustrate the ability of DDPMs to reason over uncertain hypotheses in a manner that can mimic human 'puzzle-solving' behavior. These results open the door to future research on using DDPMs to efficiently generate candidates in combinatorial search problems.
