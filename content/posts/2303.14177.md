---
title: "Scaling Expert Language Models with Unsupervised Domain Discovery"
date: 2023-03-24T17:38:58.000Z
author: "Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff and 2 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-14177v1.webp" # image path/url
    alt: "Scaling Expert Language Models with Unsupervised Domain Discovery" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.14177)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.14177).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/scaling-expert-language-models-with).

# Abstract
- Large language models are typically trained densely.
- We introduce a method to asynchronously train large, sparse language models.
- Our method clusters a corpus into sets of related documents and trains a separate expert language model on each cluster.
- Our technique outperforms dense baselines on multiple corpora and few-shot tasks.

# Paper Content

## Introduction
- Language models are trained on large amounts of text to improve performance on tasks.
- This requires thousands of GPUs to update parameters at each step.
- Branch-Train-Merge (BTM) alleviates this cost by dividing the compute among smaller expert language models.
- C-BTM is a new technique to asynchronously scale expert LMs.
- C-BTM splits a corpus into clusters, trains an expert LM on each cluster, and creates a sparse ensemble during inference.
- C-BTM uses unsupervised clustering to discover domains in a corpus.
- At inference time, C-BTM sparsely activates a subset of the trained ELMs.
- C-BTM ensembles ELMs by weighting their outputs with the distances between an embedding of the current context and each expert's cluster center.
- C-BTM enables scaling to arbitrary numbers of domains and compute budgets on any corpus.

### Training
- Employ k-means clustering to segment corpus
- Formulate expectation step as balanced linear assignment problem
- Use tf-idf embedding function for efficient and interpretable clusters
- Use single shard of corpus to train clustering model
- Use OPT LM as seed for ELM training

### Inference
- We use a sparse ensemble of the outputs of ELMs for incoming test contexts.
- We introduce a domain variable D alongside each sequence.
- We set ensemble weights based on Euclidean distance and a temperature parameter.
- We find that the performance of our models is robust to even top-2 or top-4 experts.

### Comparing to dense training
- Dense LMs are typically trained using hundreds or thousands of GPUs.
- C-BTM reduces communication overhead and improves training efficiency.
- C-BTM improves resiliency of distributed training.
- C-BTM makes training large LMs more feasible on shared GPU clusters.

### Comparing to mixture-of-experts (moe)
- C-BTM allows for efficient scaling of large LMs while keeping inference costs manageable.
- C-BTM routes sequences (instead of tokens) using offline balanced clustering (instead of online load balancing).
- C-BTM eliminates complexities associated with balancing expert utilization, avoids expensive all-to-all operations between experts, and leads to interpretable expert specialization.

## Experimental setup
- Train MoE with sparse upcycling on C4, starting from OPT-1.3B
- Follow settings from Komatsuzaki et al. (2022) and Lepikhin et al. (2021)
- MoE outperforms C-BTM models with 16 experts at small budgets, but fails at larger budgets
- Could be due to distribution shifts after pretraining
- Use 32 experts, capacity factor of 2, peak learning rate of 2e-5
- Compare 32-expert MoE LM to 1-cluster and 16-cluster C-BTM models
- Release code for sparse upcycling publicly

### Data
- C4 is a publicly available dataset of a Common Crawl snapshot
- S2ORC is a publicly available corpus of full-text academic papers
- C4 consists of 393M documents and 220B BPE tokens
- S2ORC consists of 16M documents and 87B BPE tokens
- Experiments report language modeling perplexity on 200 randomly-sampled held out documents

### Making fair model comparisons
- Report results with multiple cost metrics
- Primarily concerned with true monetary cost of model training
- Model inference comparisons have two main considerations: monetary cost and latency
- Do not compare or match number of model parameters
- Compare FLOP-matched models with same training data budget
- Compare FLOPs via inference parameters
- FLOPs not ideal metric for inference latency

## Language modeling results
- C-BTM provides substantial benefits over dense training
- Performance improves as total compute grows
- Optimal cluster count increases with compute budget
- 16 clusters gets best performance at highest budget

### Controlling for total training tokens
- Training on more than one cluster always outperforms training with a single cluster
- As the amount of training data grows, the gap between models and the dense one widens
- There exists an optimal cluster count for each token budget
- Using more clusters than optimal for the highest token budget still outperforms the dense model
- The optimal number of clusters is likely dataset specific

### Comparing training time
- Training times of models with same effective batch size are compared
- Update speed increases with more clusters and training data size
- C-BTM models can be exposed to more data for same amount of time as dense models
- Allocating many smaller jobs is more efficient than single, large synchronous job
- C-BTM experts are trained independently, if a node fails only corresponding expert needs to be restarted

### Controlling for inference costs via parameter count
- Comparing models with just training budgets ignores the fact that C-BTM inference GPU-time costs grow with the number of clusters.
- Using the top-k function can mitigate these costs.
- Results show that sparsifying can outperform a densely trained baseline.
- Sparsifying even larger expert models is still highly effective.

### Comparing to a larger dense model
- C-BTM model has lower inference cost than 6.7B parameter dense model
- C-BTM model with 16 clusters and top-4 inference has 5.2B inference parameters
- C-BTM model with 168B tokens achieves same perplexity as 6.7B dense model with 3.5x fewer FLOPs

### Summary
- C-BTM performance is not only due to ensembling, but also due to other components of the training method.
- Representation of domains in a corpus can be improved using pretrained representations or clustering algorithms.
- Heatmap shows % overlap between a cluster and a metadata label.
- Documents with certain labels are primarily assigned to a single cluster, while documents with other labels are distributed across multiple clusters.

## Downstream task results
- C-BTM models outperform their dense counterparts in downstream text classification tasks.
- 8-shot evaluations were performed, with accuracy calculated by counting the proportion of test examples where the gold label had the highest probability.
- Baselines included 1- and 16-cluster C-BTM models, a random baseline, the original OPT-1.3B and 6.7B models, and a 6.7B parameter 1-cluster model.

### Results
- MoE expends more FLOPs than other models due to additional layers
- MoE underperforms C-BTM with 16 clusters as compute budget grows
- MoE underperforms dense LM with enough compute
- MoE becomes slower as more GPUs are used
- MoE requires synchronous compute to train experts

## Comparing to mixture-of-experts
- C-BTM is compared to an alternative sparse LM, a mixture-of-experts (MoE)
- C-BTM is simpler than MoE

### Sparse upcycling
- Initialize MoE with dense checkpoint
- Use sparse upcycling technique from Komatsuzaki et al. (2022)
- Initialize shared and expert parameters from dense checkpoint
- Train model as MoE with top-2, token-level routing (Lepikhin et al., 2021)

## Analysis
- C-BTM outperforms other models
- C-BTM performance is studied in detail

### Is clustering important?
- C-BTM performance is improved when documents are assigned to learned clusters, rather than random clusters.
- Cluster specialization is important for C-BTM performance.
- Results from Li et al. (2022) and Jang et al. (2023) confirm the importance of cluster specialization.

### Is it important to balance the clusters?
- Applying a balancing constraint in k-means improves cluster sizes
- Increasing the number of clusters makes the long tail in cluster sizes more consistent
- Balancing becomes more important as the number of clusters increases
- Top terms across clusters become more specific and varied as the number of clusters increases
- Experts specialize to their cluster and perform best on their own cluster

### How do clusters and metadata domains compare?
- C-BTM removes reliance on metadata to delineate domains for ELMs
- S2ORC used to study overlap between metadata and clusters
- Partial alignment between metadata and clusters in S2ORC
- Experiments using Pile to compare performance of experts trained with metadata and clusters

## Related work
- C-BTM is related to sparse models which activate only a subset of parameters
- C-BTM is inspired by MoE and is simpler and more efficient to train
- BTM is partially inspired by prior work on variations of MoE models
- Study contributes to research into communication-efficient algorithms for training large models

## Conclusion
- Introduce C-BTM, a technique to efficiently train sparse LMs
- C-BTM splits a corpus into k clusters, trains an expert LM on each cluster, and creates a sparse ensemble during inference
- Optimal number of clusters for C-BTM increases with amount of data
- Training can be aggressively parallelized to scale into massive datasets
- Future work could investigate C-BTM in multitask or multilingual settings
- Experiments use OPT-1.3B and C4
- Increasing cluster count in C-BTM improves language modeling performance for a fixed compute budget
- Train large, then sparsify
- Training with C-BTM is substantially more efficient than training a larger dense LM
- Random clusters underperform
- Experts specialize to their cluster
- Clusters and metadata do not perfectly align
- Cluster balancing narrows the range and increases the median size of clusters
