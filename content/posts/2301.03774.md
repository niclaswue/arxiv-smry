---
title: "How Data Scientists Review the Scholarly Literature"
date: 2023-01-10T03:53:05.000Z
author: "Sheshera Mysore, Mahmood Jasim, Haoru Song, Sarah Akbar, Andre Kenneth Chase Randall, Narges Mahyar"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03774v1.webp" # image path/url
    alt: "How Data Scientists Review the Scholarly Literature" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03774)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03774).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/how-data-scientists-review-the-scholarly).

# Abstract
- Keeping up with the research literature plays an important role in the workflow of scientists
- allowing them to understand a field, formulate the problems they focus on, and develop the solutions that they contribute, which in turn shape the nature of the discipline
- In this paper, we examine the literature review practices of data scientists
- Data science represents a field seeing an exponential rise in papers, and increasingly drawing on and being applied in numerous diverse disciplines
- Recent efforts have seen the development of several tools intended to help data scientists cope with a deluge of research and coordinated efforts to develop AI tools intended to uncover the research frontier
- Despite these trends indicative of the information overload faced by data scientists, no prior work has examined the specific practices and challenges faced by these scientists in an interdisciplinary field with evolving scholarly norms
- In this paper, we close this gap through a set of semi-structured interviews and think-aloud protocols of industry and academic data scientists (N = 20)
- Our results while corroborating other knowledge workers' practices uncover several novel findings: individuals are challenged in seeking and sensemaking of papers beyond their disciplinary bubbles, struggle to understand papers in the face of missing details and mathematical content, grapple with the deluge by leveraging the knowledge context in code, blogs, and talks, and lean on their peers online and in-person. Furthermore, we outline future directions likely to help data scientists cope with the burgeoning research literature.

# Paper Content

## INTRODUCTION
- Literature reviews play an important role in the workflow of various scientists
- A challenge in this space is presented by an increasing volume of papers with studies estimating that the past few decades have seen a doubling of published research every 9 years
- To the best of our knowledge, no prior work has examined the practices or challenges of data scientists as they review the research literature
- Our goal is to address this gap.

## Search
- There are many different types of sources that can be used for information
- Some sources are more reliable than others
- It is important to select a source that is reliable and will provide accurate information
- It is important to use multiple sources to get a complete picture of the information
- It is also important to use sources that are relevant to the topic of the paper
- It is important to be aware of the bias of the sources
- It is also important to be aware of the credibility of the sources
- It is important to be aware of the bias of the sources when selecting information
- It is important to be aware of the credibility of the sources when selecting information

## Interacting with sources

## Synthesis & presentation
- The literature review practices of data scientists are examined
- The study examines search and discovery, selection of literature, skimming and reading, and formation of a synthesized understanding of a collection of papers
- The findings suggest that data scientists struggle in seeking literature, skimming, and establishing the credibility of literature outside their domains of expertise, and that they face an onslaught of seemingly similar papers and seek to understand them through their differences
- The study highlights under-explored areas providing a rich canvas for future work.

## RELATED WORK
- Information seeking systems and practices for various knowledge workers
- Sensemaking systems and practices for the scholarly literature
- Practices of knowledge workers grounded in prior work
- Data scientists' practices
- Search as learning
- Effects of tasks on learning-oriented goals
- Recent work on data scientists practices

## Info-seeking & Sensemaking -Practices
- A large body of work has studied the information-seeking practices of a range of different knowledge workers, ranging from medical researchers conducting systematic literature reviews [63], their use of search interfaces [76], design students' use of search for ideation [95], social science and education researchers practices [51], astrophysicists practices [112], and engineering professors perceptions of institutional library services [29].
- Niu et al. [91] and Alhoori et al. [4] present extensive reviews of early work in this area.
- Given our focus on data scientists, we review studies that have examined information-seeking practices of populations in related disciplines [4,7,53,81,118].
- Athukorala et al. [7] examine computer scientists for their goals in conducting literature searches and their tools. Their primary findings indicate that respondents used search most frequently to stay up to date on topics and find the exploration of unknown areas most challenging. They find that the most common form of navigating papers involves following citations to and from a known seed paper, often obtained via an initial keyword search.
- Alhoori et al. [4] examine STEM researchers' information-seeking behavior in conjunction with social media use and reference management tools. Hoeber et al. [46] examine the differences in practices of STEM researchers across levels of seniority. Finally, recent work of Soufan et al. [118] presents a large-scale survey of STEM researchers intended to reveal the alignment between theoretical models of exploratory search and actual practices followed by researchers.
- While a sizable body of work has examined information-seeking behaviors, a smaller body of work has examined practices in sensemaking tasks such as managing gathered literature, reading, and note-taking.
- Nosheen et al. [92] examines the challenges of personal information management in engineering researchers, finding fragmentation of data across different systems and difficulty determining the future value of information to be a challenge. Similarly, Inie et al. [52] explore researchers' tool use patterns in managing ideas, finding interoperability between tools to be important. Finally, Morabito and Chan [85] studies researchers' use of tools to support the capture of context and metadata for documents to facilitate the effective reuse of found information.
- In contrast with this body of work which has examined informationseeking and sensemaking tasks in isolation, our work examines the two in conjunction through a series of interviews and think-aloud observations. This combined examination more faithfully examines the iterative processes of information seeking and sensemaking [110,133], especially in the early stages of sensemaking. This allows our work to shed light on the practices and challenges at the intersection of these activities informing the development of systems for both activities.

## Info-seeking & Sensemaking -Systems
- Systems exploring more specialized querying methods are useful for information seeking
- Systems for sensemaking from collections of documents also contain components to aid information seeking
- Note-taking aids for individual papers have explored aids for skimming through highlights over salient text, reading, and organizing notes
- Systems for a literature review, spanning search and discovery, organization, synthesis, and composition are also explored

## Search as Learning
- The body of work on search as learning has studied several aspects of how learning occurs in the context of the search process
- Learning has most generally been considered the formation of mental models of knowledge, their retention over time, and their application
- Viewing search as a series of tasks in a constructive process -necessitating support for search as well as the use of search results

## User characteristics.
- Domain knowledge is beneficial for searching
- Domain knowledge influences searching only for users with knowledge of the search system

## System characteristics .
- System characteristics examined include search features [18,25,104] and features relating to reading and note-taking [33,109,123].
- Findings suggest that transparent and controllable search systems lead to better learning outcomes than the widely used PubMed, while web-search interfaces lead to better knowledge gain compared to conversational search interfaces.

## Search behaviors.
- Work examining search behaviors has contributed several findings.
- Vakkari et al. [128] find psychology students to use more specific queries as their vocabulary improved though their search strategies employed remained consistent.
- Dosso et al. [26] find domain knowledge to not influence the number or length of queries across medicine and computer science students, though they find medicine students to use more domain-specific vocabulary in queries.
- Vakkari and Huuskonen [127] find increased effort in examining documents to be associated with improved essays even in the face of lower precision search results, finding students to compensate for bad search results -a result also mirrored elsewhere [76,116].
- Work of Moraes et al. [86] finds search paired with instructor lectures to have improved learning outcomes than the lectures alone.
- Finally, Urgo and Arguello [125] abstract away from specific interactions and characterizes the "pathway" toward learning objectives in terms of learning-oriented sub-goals for tasks varying in cognitive complexity.
- This also ties to a line of work examining task complexity and learning outcomes.
- In our work, rather than focusing on specific learning outcomes and implementing interventions to facilitate learning, we contribute an exploratory needs-finding study for highlighting the practices and challenges throughout the process of search and the use of search results by data scientists in a learning-oriented task.

## Practices of Data Scientists
- A sizable body of recent work has examined the broader practices of data scientists
- This work helps paint a picture of who data scientists are by conducting a review of prior work
- A few papers have also examined the dataset-related information-seeking needs of data scientists
- Kross and Guo [68] interview individuals engaged in training data scientists in industry and academia and identify that instructors find searching for pedagogically-relevant datasets to be one of their challenges
- Koesten et al. [64] investigate data scientists' challenges in searching for structured data repositories
- A range of work also examines data science workflows via codebooks [79,120], their documentation practices [130], and how they arrive at analysis decisions [62]
- Other recent work examines the challenges and needs of data scientists in developing fair machine learning systems [48], and their adoption of explainable machine learning systems [67]

## STUDY METHODS

## Study Design
- Participants self-described their research areas in a semi-structured interview
- Research Areas were probed in a think-aloud observation
- Participants conducted a literature review in response to one of three open-ended task prompts
- Design and content of the study were developed in a pilot study

## Study Participants
- We invited participants for our study using social media posts on Twitter and LinkedIn, and email invitations sent to university mailing lists.
- In recruitment forms, a broad definition of "data scientist" was displayed and respondents were asked if they identified as data scientists.
- Further, participants were also asked to submit links/titles of 3 research papers they found useful or enjoyable to read in the past year.
- Of the respondents, 20 were selected as participants for our study (P1-P20) on a first-come-first-serve basis while ensuring that they identified as data scientists and had read papers in the past year.
- Participants were compensated for their time with a $25 gift card and all study procedures and materials were approved by the university IRB.
- Appendix A includes our recruitment form.
- Of 20 participants 11 noted gender pronouns he/him, 9 noted she/hers, and 2 noted they/them, with some noting multiple pronouns.
- 14 participants were enrolled in or had completed Ph.D. degrees and 6 had completed master's degrees.
- 13 participants worked in universities and 7 worked in non-profit or for-profit industry labs.
- On average participants had published 4 research papers.

## Study Procedure
- Each study session lasted about 1 hour
- The whole session was conducted over Zoom
- The study began by explaining the study procedure, obtaining participants' consent for participation, and having them fill out a short demographic survey
- This was followed by a semi-structured interview
- Here participants were asked about their research focus and their goals, practices, and challenges for conducting literature reviews
- Participants were encouraged to think of literature reviews as broader than a single directed search and discuss all interactions with the scientific literature
- This was followed by a think-aloud where participants conducted a literature review over Zoom screenshare
- Participants were shown 3 broad task scenarios and selected one as a prompt for their think-aloud
- One prompt asked them to recall and demonstrate a previous literature review, another asked them to enhance their knowledge on a known topic, and the final asked them to research the literature for a future project of their interest, an area they had lesser experience in
- The latter two scenarios were drawn from the work of Hoeber et al. [46]

## Data Collection and Analysis
- All our analyses used transcribed audio of the interview with the video examined for our think-aloud session.
- Automatic transcription of the audio was obtained from Zoom and was corrected substantially for errors.
- This was followed by 3 rounds of coding by 3 authors of this paper and a thematic analysis of the interview and think-aloud transcripts -this process was conducted from Aug-October 2022.
- In our first round of coding 2 authors independently examined 5 transcripts and generated a set of codes using open and axial coding, then, these codes were consolidated into a unified set of codes.
- In consolidation, both authors examined each other's codes independently, then met to discuss any differences and consolidated them into a unified set of codes.
- This was followed by application of the unified codes to the 5 transcripts by both coders.
- We observed an agreement of 0.92 in terms of nominal Krippendorff's alpha.
- This unified set contained 167 codes of which 51 noted names of tools used by participants or logistic aspects of the study.
- This was followed by discussion and resolution of disagreements in codes and application of the resultant codes to the remaining 15 transcripts while also adding any new codes to our initial codebook.
- This process added 59 new codes of which 32 noted names of tools or logistic aspects.
- The coded transcripts were then used for thematic analysis -which resulted in themes corresponding in part with the stages of the Information Search Process [126, Sec 3], we present our themes next.
- Our codes and extended quotes per theme may be examined online.

## RESULTS
- Participants focus on understanding problems and solutions reflect prior understanding across scientific and creative problem-solving disciplines with individuals often thinking in terms of "problem" and "solution"
- A focus on evaluations and metrics also matches understanding of a large focus on quantitative performance in data science
- Search for creative brainstorming has been noted elsewhere
- Seeking solutions for application matches findings from citation analysis indicating that solutions either assume the role of vetted methods which are widely adopted or ones on the frontier of research knowledge and seeing current development, necessitating comparisons
- Challenges in formulating queries in corroborated in prior work noting literature searches to be exploratory with evolving, ill-defined, and open-ended searches
- While many systems have been developed for exploratory search (see §2.2), large-scale systems for literature search lack support for it
- In the face of not knowing where to begin a search, participants also noted soliciting recommendations from expert peers

## How

## 4.2.2
- Participants found literature through search, social media, and automated methods
- Tools for discovery often trapped participants in a disciplinary bubble
- Recommendations from peers were often received

## How do data scientists select papers?
- Participants found literature overwhelming and difficult to differentiate between similar papers
- Participants turned to surveys or reviews of literature to find salient papers
- Some chose to focus on a handful of papers, minimizing the overlap between papers they examined, and pairing the examination of a few papers with citation chaining
- Others also leveraged repeated references to specific concepts or papers as a sign of having found the papers worthy of examination

## Establishing the credibility of papers.
- Participants relied on expected indicators like authors, affiliations, publication venues, and citation counts to determine credibility
- Challenges with credibility arose from papers with exaggerated or re-branded claims, and needing to sift through many similar papers
- These challenges warrant deeper examination in data science and other rapidly expanding disciplines.

## Everyone skims papers.
- Participants often skimmed individual papers to make quick decisions of correctness or glean contributions of a paper
- Skimming is often interspersed with information seeking with frequent context changes between the two, however, few prior lines of work have examined close integration of information seeking and skimming

## What challenges do data scientists face in reading papers?
- P1: Participants noted the challenge of missing details in papers.
- P2: Papers are written to be accepted, with little incentive to include the details which make a solution effective.
- P5: Some noted that including a lot of detail would interfere with readers hoping to get a high-level idea of a paper.
- P9: In order to cope with missing details, participants noted the value of augmentations provided by code.
- P16: The use of code to augment reading experiences for scientific papers presents a future venue for improvement.

## 4.4.2
- Participants noted the importance of understanding the specific "delta" that papers offered in comparison to other work that was influential or known papers
- To understand the increments participants noted the importance of gaining familiarity with the norms of a sub-discipline and the challenge in its absence
- However, it was a challenge to establish if a paper was poorly written or if the participants were missing necessary context

## How do data scientists lean on social ties?
- Participants used their social ties to find papers.
- Participants also relied on their peers for information.

## Collaboratively brainstorming and making sense of papers.
- All participants noted leveraging social ties in making sense of the literature (20/20).
- Here they noted the value of group discussions centered on papers to keep up with the literature, help brainstorm ideas, or spark new research directions (12/20): "In independent research at the very least, I talk about my idea with someone else just to see if I'm in the right direction. On the other hand, we have bi-weekly brain-storming sessions in which we discuss at least one paper that is very relevant to our work and so we get a lot of inputs ... this might be useful, this might be a limitation, it might not work. "
- Others noted the value of discussions with collaborators to understand the details of specific important papers (11/20):"If I'm having one on one meetings then we dig into why people made certain decisions in their paper, and if we should be following the same"
- Finally, a few participants (5/20) noted the value of sharing notes and literature with collaborators to establish the provenance of ideas: I usually share these notes with collaborators to give them a sense of where I am getting this idea from or where this hypothesis is coming from -P19 or correctness of information: It also helps with collaborators double checking my writing.
- Besides close peers, participants also sought weaker social ties and online discussions (14/20). Here, they noted many of the same benefits that interaction with peers provided -seeking recommendations from experts on forums: You will find a Quora or Yahoo answer which is dominated by exceptional mathematicians. Someone will have asked a similar question [to yours] and someone will have pointed them to some relevant work -P17, establishing the credibility of papers as in §4.
- A body of work has examined collaborative information seeking [87], with some examining social reading [97,138], and sensemaking with collaborative annotations [114,143]. However, examinations of collaborative reading and sensemaking in specific task contexts are understudied avenues that are likely to present specific challenges, as noted in cases of information re-use in software teams [75] or collaboratively verifying the credibility of claims [41].
- Leveraging authors. While peers were useful in-person and online, participants also found value in interacting with authors (11/20).
- While some contacted them actively (5/20), others noted more passively seeking them in recorded talks and forums such as Twitter or Reddit (6/20).
- Direct communication ensured a fuller understanding of work, helped develop ties with other researchers, and sometimes turned into fruitful collaborations (2/20): A little while ago a new paper got released that was very similar to my work I emailed the authors. This is something I do in this type of situation to create a conversation and also connect with researchers who are doing similar work to me.
- Others also found these useful to broach under-discussed aspects: I send them a message congratulating them about the work they've released and understand some of the secrets behind their work -P4.
- Here, prior work has explored recommending expert peers [102] and "people recommendation" more broadly [37]. However, important challenges remain in how incentive structures must be set up to facilitate these interactions e.g. Breitinger et al. [15] note a trade-off in the discovery of ongoing research and keeping it confidential.
- While an active engagement was important for a deep understanding, passively interactions with authors through talks or forum posts made work easier to understand than their papers.
- Participants noted the value of visual communication and an incentive for authors to communicate their idea for understanding: Sometimes getting a very good intuition of what their motivation helps understanding. It might be not that clear in the paper but when they explain it to you in the video it's much more exciting.
- But [use of talks] isn't always applicable because it's only the more famous people's projects that get this much coverage.
- Examination of authors' engagement through social media has only recently begun to be studied [35,65] and incorporation of this into aids for sensemaking remains under-explored.

## DISCUSSION
- Data scientists are reviewing scientific literature
- Anchoring results along the formulation of an information need, query formulation and search
- Assessment of search results through skimming and reading
- Synthesis of a collection of documents
- Leveraging social ties to accomplish tasks

## Support cross-disciplinary access
- Data science is seeing exponential growth in the number of papers
- As fields develop several subdisciplines emerge around specific problems and methods each with their own disciplinary vocabulary and norms
- Individual scientists are now tasked with conducting work in increasingly fragmented disciplines only some of which are familiar to them while knowing that significant innovation and progress is to be had from drawing across multiple disciplines
- The challenges stemming from fragmented knowledge are echoed in our findings at each examined stage of the information search process
- Future work necessary to support information seeking and sensemaking of cross-disciplinary research can take several forms
- To tackle the challenge of search in unknown disciplines aids such as query recommendation have been explored ( §4.2.1), however querying strategies intended to allow richer specification of user context remain under-explored
- These may take the form of verbose queries [2,89], conversational and interactive searches [5,39] paired with mechanisms for cross-domain retrieval -Kang et al. [60] present a preliminary example
- Further, the exploration of papers in unfamiliar disciplines is likely to benefit from the presentation of "explanations" to aid in understanding their credibility and relevance
- While explanations have been extensively studied in recommender systems [141], examination of explanations for cross-disciplinary exploration remains an open question
- Aids may also be developed for skimming. Since skimming relies on knowing norms of writing in a discipline, these aids may take the form of adaptive document layouts [54] or automatically generated "FAQs" for a paper [8], personalized to the discipline of a reader. Similarly, reading aids may take the form of paraphrasing text for different disciplinary audiences or presenting "pre-requisite" concepts or papers necessary to understand a given paper [73] -thereby aiding readers to judge the quality of a paper independent from their own knowledge gaps
- However, since scientists see better learning outcomes in tasks perceived as challenging the trade-offs involved in easing this process remains to be seen

## Facilitate reliance on close peers
- Participants relied on close peers -teammates, lab members, collaborators, mentees, and mentors.
- From peers, they received recommendations, engaged in brainstorming, established the credibility of papers, and understood the details of papers.
- The collaborative practices of data scientists have been examined in the context of code and data work [137] we extend this to include the creation and understanding of ideas.
- Careful understanding and support for these practices are likely to be fruitful -especially with the move toward remote work [135].
- The dominant line of work in information-seeking has taken an egocentric perspective and exploration of methods and systems to leverage communities has been examined largely in early work in collaborative search [87] and group recommendations [55].
- This line of work may fruitfully be re-explored in our present circumstance -evidence for this is provided in recent work of Piao et al. [99] who find bringing "friends-into-the-loop" of recommender systems to result in more accurate and diverse recommendations.
- Re-incarnations of early work melding sensemaking, reading, and discovery in a collaborative feed-reader [3] also promise to leverage the trust scientists have in their peers for selection and consumption of research.
- A different line of work is suggested by Morris [87] where users preferred to interleave egocentric search with lightweight communication.
- Templates for this work are provided in aids to summarize group chats [136] or collaborative conversational agents [9] -as aids in brainstorming.
- Implementing these tools requires careful design, however -Brucks and Levav [16] note virtual communication through video conferences to curb creative ideation.
- Similarly, the work of Inie et al. [52] notes users' preferences for their own scholarly tool chains, making interoperability of new tools important for uptake.

## Leverage the knowledge context of papers
- While a close community of expert peers was important this was not always available to participants
- In this scenario, a variety of other resources were sought to augment papers -forum and social media discussions, recorded talks and videos, blogs, and code.
- These resources provided starting points for exploration, helped establish the credibility of papers, and aided in understanding missing details and math in papers.
- Traditionally this information (e.g. entity cards, videos) referred to as the knowledge context has seen use in web search engine result pages (SERP) and aids users in making information literate decisions.
- Smith and Rieh [117] advocate for greater use of this knowledge context instead of their reduction and search engines "getting out of the way" of users.
- Here, we echo this push and observe its values for data scientists searching the literature.
- Furthermore, we also note its value in augmenting reading and skimming aids for found documents.
- In the presentation of the knowledge context alongside academic search numerous questions remain.
- Prior work in the TREC Blog Track has examined blog retrieval with an emphasis on their subjective contents -similar efforts are necessary for the retrieval of knowledge contexts for academic publications [93].
- The presentation of this information is also likely to require further investigation, e.g. Levi et al. [72] report that only some queries benefit from the presentation of a cluster of results from a Community Question Answering site.
- Besides use in information seeking, this knowledge context is likely to benefit skimming and reading aids.
- Recent work of Rachatasumrit et al. [105] presents an early example and places discussions from follow-on work into the margins of a paper.
- There is room however for more complex augmentations -This may take the form of using blogs, and videos for aiding math understanding, using code to infer missing details, or use of social media and blogs to aid skimming.

## FUTURE WORK AND LIMITATIONS
- The study was conducted over a single session
- The study did not probe longer-term activities such as longer-term synthesis and composition
- A more varied participant pool is likely to result in different findings
- Challenges arising from incentive structures surrounding disseminating research were under-examined

## CONCLUSIONS
- An exponential rise in the number of scholarly publications, its expanding influence, a large number of new entrants, and its likely impact on data science drove us to examine information seeking and sensemaking practices of data scientists reviewing the literature.
- In our examination, we ran an exploratory interview study with 20 data scientists recruited from both industry and academic institutions.
- Here, we established their goals for accessing the literature, then we examined their practices and challenges in search and discovery, selection of search results, skimming and reading, and their reliance on peers in a number of these tasks.
- A number of our results corroborate those seen in prior work -here, our work offers a synthesis of disparate prior work. Next, our work also uncovers specific results stemming from our focus on data scientists and our joint examination of information-seeking and sensemaking. In our findings, we highlighted challenges arising from fragmented scientific disciplines influencing all stages of the information search process -we believe this represents a special challenge of data science given its inter-disciplinary nature. Besides, we highlighted the challenges of missing detail and mathematical content in reading papers -likely, a feature of computational disciplines such as data science.
- Our joint examination of information seeking and sensemaking revealed the nature of information sought at the intersection of these two activities: a desire to establish the credibility of papers owing to exaggerated/re-branded claims or unfamiliarity with the discipline, and a desire to understand the incremental differences between many seemingly similar papersthis likely represents a consequence of crowded and incentivized sub-disciplines of data-science.
- To cope with these challenges, we found our participants to leverage the knowledge context surrounding scientific papers in the form of code, blogs, talks, and forums and by leaning on their peers -these practices were also examined in our work. Finally, examination of our participants' challenges and coping mechanisms lead us to carefully speculate on future work likely to present meaningful solutions to these challenges while also presenting meaningful scientific questions for future work in the IR, NLP, HCI, and CSCW communities.
