---
title: "Rethinking with Retrieval: Faithful Large Language Model Inference"
date: 2022-12-31T22:35:34.000Z
author: "Hangfeng He, Hongming Zhang, Dan Roth"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-00303v1.webp" # image path/url
    alt: "Rethinking with Retrieval: Faithful Large Language Model Inference" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00303)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00303).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/rethinking-with-retrieval-faithful-large).

# Abstract
- Large language models (LLMs) are successful in various NLP tasks, but their stored knowledge may be incomplete, out-of-date, or incorrect
- To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting
- RR does not require additional training or fine-tuning and is not limited by the input length of LLMs
- We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.

# Paper Content

## Introduction

## Related Work
- Retrievalenhanced LMs have received significant attention as a means of improving performance through the incorporation of external knowledge.
- For example, the k-most similar training contexts can be retrieved to improve the estimation of the next word distribution in both the training stage (Borgeaud et al., 2021) and the inference stage (Khandelwal et al., 2020).
- Other approaches have utilized retrieved documents as the additional context in generation tasks (Joshi et al., 2020;Guu et al., 2020;Lewis et al., 2020).
- Nakano et al. (2021) instead use human feedback in a text-based web-browsing environment.
- Among these previous works, Khandelwal et al. (2020) is most closely related to our approach. However, they focus on improving local inference by using the nearest neighbor datastore constructed from training data, whereas we focus on conducting faithful inference using external knowledge.
- In contrast to other aforementioned approaches, which require training or fine-tuning to incorporate retrieved knowledge, we propose a post-processing method for leveraging retrieved knowledge without additional training or fine-tuning.
- Incorporating external knowledge into LMs.
- Significant effort has been devoted to leveraging external knowledge to improve the reasoning ability of LMs.
- Previous work has incorporated external knowledge sources such as WordNet (Miller, 1995) and ConceptNet (Speer et al., 2017) to enhance LMs for tabular reasoning tasks (Neeraja et al., 2021;Varun et al., 2022).
- Explicit rules have also been added to inputs to improve reasoning ability over implicit knowledge (Talmor et al., 2020).
- In addition, explicit knowledge from Wikidata (Vrandečić and Krötzsch, 2014) and implicit knowledge in LLMs have been integrated into a transformer (Vaswani et al., 2017) for visual question answering (Gui et al., 2021).
- Nye et al. (2021) instead introduces a symbolic reasoning module to improve coherence and consistency in LLMs.
- Among these previous works, Nye et al. (2021) is the most relevant to our approach.
- Still, they focus on incorporating logical constraints to improve coherence and consistency, whereas we aim to improve the faithfulness of explanations through the use of external knowledge.
- In contrast to other aforementioned approaches that incorporate external knowledge before generation and require additional training or fine-tuning, our proposal leverages external knowledge in a postprocessing manner to enhance LMs without additional training or fine-tuning.
- There has been a line of work exploring the knowledge hidden within LLMs for reasoning. This has included the use of careful prompting to encourage LLMs to generate explanations in the reasoning process, such as through chain of thought prompting in few-shot (Wei et al., 2022) or zero-shot (Kojima et al., 2022) learning, or through the use of scratchpads for intermediate computation (Nye et al., 2022).
- Zelikman et al. (2022) instead iteratively bootstrap the ability of LLMs to generate high-quality rationales from a few initial examples.
- Liu et al. (2022) further propose generating knowledge from LLMs, which is then used as additional input to improve commonsense reasoning.
- In contrast to this line of work, our proposal focuses on leveraging external knowledge to enhance LLMs, while they aim to explore the knowledge hidden within LLMs.

### Baselines
- Zero-shot/few-shot prompting: GPT-3 with standard zeroshot/few-shot prompting as baselines
- Chain-of-thought prompting: GPT-3 with CoT prompting proposed in (Wei et al., 2022) as a baseline
- Self-consistency: GPT-3 with sampling a diverse set of reasoning paths and selecting the most consistent answer by marginalizing the sampled paths

### Commonsense Reasoning

### Temporal Reasoning

### Tabular Reasoning

### Evaluation
- Experimental settings
- In all experiments, we utilize GPT-3 text-davinci-002 unless otherwise stated
- The maximum number of tokens for generation during completion is set to 256
- For zero-shot, few-shot, and chain-of-thought prompting, the temperature is fixed at 0
- For selfconsistency and rethinking with retrieval, we randomly sample 10 outputs6 with temperature 0.7
- We evaluate the performance of different methods on commonsense and tabular reasoning using accuracy, and on temporal reasoning using the exact match metric as defined in Rajpurkar et al. (2016)

## Analysis
- RR is a data structure that is used to store data in a sorted manner
- RR can be used to solve certain problems that are difficult to solve with other data structures
- RR is efficient when compared to other data structures
- RR can be used to solve certain problems that are difficult to solve with other data structures
- RR is efficient when compared to other data structures

### Limitations of LLMs in Reasoning
- GPT-3 can provide reasonable explanations and correct predictions for a number of questions
- For example, when given the question "Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?", GPT-3 produced the following output: The Albany in New York has a population of about 98,000. The Albany in Georgia has a population of about 77,000. Thus, the Albany in New York is more populous than the Albany in Georgia. So the answer is no.
- However, GPT-3 may occasionally produce incorrect supporting facts for its explanations or make incorrect inferences for its predictions, despite generally being able to identify suitable perspectives.

### Ablation Study

### Variations of the Proposed Approach
- Weight outputs as individual units and use a voting mechanism to select the best-supported prediction
- Fact selection: select facts from the outputs of LLMs based on external knowledge
- Fact generation: generate facts based on both the outputs of LLMs and external knowledge
- Inference with supporting facts: use LLMs or an off-the-shelf model for inference with supporting facts

### Impact of the Size of LMs
- Our proposed method (Variant II) consistently outperforms CoT prompting in terms of both prediction accuracy and the faithfulness of explanations, even when using smaller LMs.
- The size of LMs has a significant impact on the performance of our proposed method.

## Conclusion
- RR is a promising solution for utilizing external knowledge to assist LLMs
- RR does not require additional training or fine-tuning, making it a lightweight and feasible option for LLMs
- Through extensive experiments on three reasoning tasks using GPT-3, we have shown that RR is able to produce more faithful explanations and improve the performance of LLMs
- In the future, we plan to investigate various variations of RR to enhance its effectiveness and efficiency in augmenting LLMs with external knowledge
