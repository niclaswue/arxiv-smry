---
title: "'Real Attackers Don't Compute Gradients': Bridging the Gap Between Adversarial ML Research and Practice"
date: 2022-12-29T14:11:07.000Z
author: "Giovanni Apruzzese, Hyrum S. Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, Kevin A. Roundy"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14315v1.webp" # image path/url
    alt: "Real Attackers Don't Compute Gradients: Bridging the Gap Between Adversarial ML Research and Practice" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14315)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14315).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/real-attackers-don-t-compute-gradients).

# Abstract
- Recent years have seen a proliferation of research on adversarial machine learning
- Numerous papers demonstrate powerful algorithmic attacks against a
- However, abundant real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems
- As a result security practitioners have not prioritized adversarial ML defenses
- This position paper aims to bridge the two domains
- We first present three real-world case studies from which we can glean practical insights unknown or neglected in research
- Next we analyze all adversarial ML papers recently published in top security conferences, highlighting positive trends and blind spots
- Finally, we state positions on precise and cost-driven threat modeling, collaboration between industry and academia, and reproducible research
- Recent years have seen a proliferation of research on adversarial machine learning
- Numerous papers demonstrate powerful algorithmic attacks against a
- However, abundant real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems
- As a result security practitioners have not prioritized adversarial ML defenses
- This position paper aims to bridge the two domains
- We first present three real-world case studies from which we can glean practical insights unknown or neglected in research
- Next we analyze all adversarial ML papers recently published in top security conferences, highlighting positive trends and blind spots
- Finally, we state positions on precise and cost-driven threat modeling, collaboration between industry and academia, and reproducible research

# Paper Content

## I. INTRODUCTION
- Several recent surveys indicate that protecting machine learning (ML) models is not a leading security concern for practitioners [1]- [5].
- According to the few recorded accounts of security failures "in the wild," ML systems can be broken by naïve attackers that are not systematically exploiting the vulnerabilities of ML, but rather are developing attacks by guessing-either indiscriminately or by some coarse heuristic [6], [7].
- Red-team exercises on ML systems often take advantage of security gaps that are agnostic to the existence of an ML model, and subsequent defensive recommendations are likewise more broad than, e.g., adversarial training [8], [9].
- Additionally, the ML models deployed in productiongrade ML systems are often not directly observable (and are sometimes even unreachable) by most attackers [10].
- On the other hand, researchers assert that real ML implementations should not follow the principle of "security by obscurity" [11], and that security evaluations of ML models should assume worst-case scenarios [12]- [14].
- Indeed, no one can exclude the possibility of future powerful adversaries turning their attention to the ML models embedded in real systems.
- Fueled by such "what ifs," thousands of papers [15] have showcased successful security violations of ML models by means of sophisticated strategies such as gradient-based algorithms [16].
- These few anecdotes suggest that the field of ML security suffers from a mismatch between the priorities of practitioners and the focus of researchers.
- In this position paper we argue that the gap between adversarial ML research and practice is real and identify several underlying causes.
- We aim to reduce this gap by proposing guidelines for future endeavours that reflect our observations of aspects overlooked by, or inconsistent within, existing literature.
- To reach our goal, we first revisit the fundamentals of ML security ( §II). We argue that real deployed ML models are part of complex ML systems whose architectures are unknown to researchers, and that cybersecurity is rooted in economic considerations for both attackers and defenders.
- To assist researchers in understanding crucial facets of operational ML security, we then present three original case studies from the real-world ( §III) that elucidate: (a) the architecture of a complex ML system deployed in an online social network; (b) the lack of evidence of adversarial examples against a commercial ML system for phishing webpage detection; and (c) the role of time and domain expertise in devising sophisticated attacks during an acclaimed ML evasion competition.
- Next we turn our attention to the research domain and take a snapshot of the current landscape of adversarial ML as portrayed in scientific papers ( §IV).
- After surveying the proceedings of the "Top-4" security conferences from 2019 to 2021, we systematically analyze all 88 papers that consider attacks against ML or corresponding defenses.
- Of these papers, 89% only evaluate algorithms based on neural networks, 63% focus on computer vision, and 80% perform their experiments on "benchmarks".
- We discover several inconsistencies in the terminology adopted in reputable prior work. We also identify several positive trends, such as an increasing amount of papers envisioning attackers who "ignore" the ML model and reach their goal by targeting other elements of the ML system.
- Finally, we coalesce all our observations by stating four positions that researchers and practitioners in ML security can adopt to close the gap between these domains ( §V). We encourage the community to (a) adapt threat models to ML systems, (b) integrate threat models with cost-driven assessments, (c) build collaborations between industry and academia, and (d) embrace a "just culture" for source-code disclosure.

## arXiv:2212.14315v1 [cs.CR] 29 Dec 2022
- Presents three real-world case studies
- Analyzes all recent adversarial ML papers in top security venues
- States four positions that, if adopted, will help bridge the gap between research and practice in ML security

## II. REVISITING MACHINE LEARNING SECURITY
- Machine learning systems are based on a number of foundations, including learning algorithms, data structures, and probabilistic models.
- Machine learning systems can be used for a number of purposes, including security, and can be vulnerable to attacks.
- Security of machine learning systems is important, as attacks can result in data theft or unauthorized actions.
- Practical cybersecurity is important, as attacks can have serious consequences.

## A. Overview of Machine Learning (Systems)
- Machine learning is the process of creating "machines that can make decisions by learning from data"
- The fundamental goal of machine learning is to generalize to new data
- Effectiveness of an ML model is measured during a validation phase
- If an ML model is determined to be effective, it can be integrated into a machine learning system

## Input
- Preprocessing the ML model involves transforming the data into a form that the ML model can understand
- The preprocessing steps include cleaning up the data, transforming the data into a feature space, and reducing the dimensionality of the data
- The goal of preprocessing is to make the ML model work better with the data
- Preprocessing can improve the accuracy of the ML model
- Preprocessing can also improve the speed of the ML model
- There are a variety of preprocessing techniques available, and each has its own benefits and drawbacks
- Preprocessing is an important step in building an ML model

## ... Output

## Machine Learning System
- Preprocessing steps can be complex. They can entail, e.g., transformations, filtering, normalization, feature extraction, and/or noise reduction.
- ML systems can be OPEN or CLOSED. OPEN ML systems are publicly available, while CLOSED ML systems are distributed to their end users and can be inspected neither the underlying source code nor the components enclosed in the ML system.
- An ML system is merely a single component within a machine learning system. Types of ML systems. Many IT infrastructures already rely on ML systems at either small or large scales.

## B. Security of Machine Learning
- Thousands of papers have addressed the security of machine learning, a research field typically denoted as "adversarial machine learning"
- Adversarial ML became popular when Szegedy et al showed that deep learning is vulnerable to adversarial examples
- Adversarial examples can certainly be used in malicious ways: for instance, an attacker can craft an adversarial example that evades an ML model powering a security system, e.g., a malware classifier
- The terminology above has a limitation: it focuses exclusively on the ML model (the inner, dotted box in Fig. 1), which is just a single component of the much more complex ML system (the outer box in Fig. 1)
- We find it surprising that the viewpoint of practitioners has barely changed, even in spite of warnings from sources beyond the research domain.

## C. (Economics of) Cybersecurity
- It is well known that "there is no such thing as a foolproof system"
- Given a sufficient amount of resources, any attacker can succeed in their goal
- Indeed, the purpose of security mechanisms is to raise the cost sustained by the attacker
- However, countermeasures also have costs (e.g., implementation efforts, periodic evaluations, maintenance, and computational resources) that must be factored into any decision about whether to build or deploy them.
- Cybersecurity in practice is rooted in economics-both for attackers and defenders.

## D. Related Work
- Many papers share our goal of improving the security of real-world ML systems.
- The authors of [14] and [64] provide practical guidelines on ML for security-whereas we focus on the security of ML.
- Some related papers (e.g., [2]- [5]) focus exclusively on the industry perspective; while most literature surveys (e.g., [39], [41], [65]) focus just on the research perspective.
- We consider both perspectives, because we aim to bridge the existing gap between these two sides.
- Perhaps the closest effort to our paper is the (unpublished) work by Gilmer et al. [66], which attempts to contextualize the implications of adversarial ML in the real-world.
- Despite sharing our goal, [66] has two limitations: first, it mostly focuses on computer vision and deep learning-which, as we will show, represent only a subset of the conceivable applications of ML in reality; second, the remarks made in [66] relate to papers published before 2018-i.e., almost five years ago, when ML deployments were not as popular as today.
- We briefly summarize two orthogonal research areas to our paper. (i) The performance of ML tends to degrade over time [67]- [68].
- ML can also be used as an attack vector (e.g., "deepfakes" [69], or "attribute inference" [70]).
- This research area (i.e., "offensive ML") is outside our scope, because we focus on the protection of ML systems.

## III. CASE STUDIES (REAL EXPERIENCE)
- The first case study presents the architecture of a real INVISIBLE ML system deployed at Facebook.
- The second case study describes the triaging of phishing webpages that confused a security company's ML detector.
- The third case study sheds light on the role of time and domain expertise in devising queryefficient attacks during an ML evasion challenge organized by renowned tech companies.
- In Appendix A-D, we provide a fourth case study showing that even "adversarially aware" malware detectors can easily be fooled with expert knowledge.

## A. "The four A's of abuse fighting": ML Systems in an OSN
- Facebook uses machine learning to detect spam on their platform
- The ML system is composed of 4 layers: AUTOMATION, ACCESS, ACTIVITY, and APPLICATION.
- The AUTOMATION layer detects if the action is due to automation. If it is, the ACCESS layer is checked to see if the user has the correct permissions to do the action. If not, the ACTIVITY layer is checked. If that fails, the APPLICATION layer is checked.

## B. Are there Adversarial Examples in the Wild (Web)?
- The AI Incident Database contains over 1 600 reports of AI failures.
- To assess the prevalence and nature of adversarial examples "in the wild", we first consulted the AI Incident Database.
- Unfortunately, querying the database with the keywords "evasion" and "adversarial machine learning" yielded only 2 and 6 results, respectively.
- Apparently, only a limited number of (reported) incidents involve the types of attacks typically portrayed in adversarial ML papers.
- To gain further insight, we asked a leading cybersecurity company whether they encounter adversarial examples when manually triaging security incidents.
- This case study provides their answer.
- Context.
- We consider the problem of phishing webpage detection.
- Similarly to the previous case study, the ML system is a commercial-grade detector, composed of diverse modules, all with the underlying goal of "catching phish."
- The ML system leverages the principle of active learning: For each input, the output is a "phishing confidence" score that is used (i) to make an automated dection (i.e., block/allow) and (ii) to improve the ML system by triaging to security analysts inputs for which the ML system is "uncertain."
- Method.
- We searched for adversarial examples in the ML system's usage logs, restricting ourselves to the 40 domains that (according to the cybersecurity company) were most commonly targeted by phishing campaigns in July 2022.
- During this month the ML system analyzed hundreds of thousands of inputs, and among these 9 174 were flagged as "uncertain" by the ML system and required manual triage.
- We used this set as a starting point, and we began to review all such samples (i.e., screenshots).
- To make our in-depth analysis humanly feasible, as we visually inspected each sample we asked ourselves "can this be an adversarial example?" and decided to stop once we obtained a positive answer for 100 samples.
- We reached this number after going through 4 600 samples (half of our initial population).
- The discarded samples had nothing in common with adversarial examples, due to being "out of distribution" [80]-i.e., they were significantly different from any sample included in the training data of the ML system (e.g., a domain using a new logo, or shifting backgrounds-typical of, e.g., Netflix).
- We then used the 100 positive samples as basis for our in-depth analysis.
- Our aim was to determine the causes of the ML system's "uncertain" response and, in particular, if any cause could be traced back to the gradient-based techniques typical of adversarial examples.
- This entire process (first inspection and in-depth analysis) was conducted by two authors who worked independently and had regular meetings to resolve issues and arrive at a consensus.
- Results.
- Our analysis suggests that attackers rely on an arsenal of relatively simple, yet often effective, strategies.
- We quantify the prevalence of these strategies in Table II and show their effects on real webpages ("in the wild") in Fig. 4.
- The most prevalent attacks include cropping, masking, stretching, and/or blurring techniques, all of which induce optical character recognition (OCR) algorithms to incorrectly extract text from the page.
- Such "anti-OCR" techniques have been employed for decades by phishers [81] and can (still) ably fool ML systems without relying on gradient computations.
- These methods typically target the company name when it is part of a logo, mostly by manipulating the logo's visual representation.
- We also found samples with a different logo altogether, as well as "company name...

## C. In-Depth Analysis of an ML Evasion Competition
- The "anti-phishing evasion" competition organized by industry practitioners in ML security [84]
- The rules of the competition resembled a constrained and likely scenario simulating a real attack
- Four teams crafted variants of all ten phishing webpages that evaded all seven detectors, thereby achieving a perfect score of 70 points
- In order of ranking, these teams made 320, 343, 608, and 9 982 queries [85]
- The 1st-place (two scientists from Kaspersky) and 3rd-place (a single individual) teams published their methodologies [86], [87]
- We can reasonably assume that the strategy of the 4th-place team involved some automation that resulted in thousands of queries
- Using query count as a cost metric (as researchers often do, e.g., [34]), leads to the conclusion that the winning solution had the lowest cost: only 320 queries, i.e., an improvement of 47% over the 3rd-place (608 queries) and of 97% over the 4th-place (9 982 queries)
- However, an in-depth analysis shows that this viewpoint is quite misleading
- Further Considerations. We provide further analysis in Appendix A-C, and in particular we attempt to derive the human effort entailed in this competition.

## IV. SNAPSHOT OF ADVERSARIAL ML RESEARCH

## A. Overview
- 72% of papers focus on attacks
- 52% envision "evasion" attacks
- 89% consider only deep learning algorithms
- 27% do not make any mention of economics
- 51% publicly release their source code
- 63% carry out their evaluations on image data
- 20% reproduce a pipeline by building an ML model only
- 20% experiment on real ML systems

## B. Positive findings in Threat Models of Attack Papers
- Attacks against ML systems for automated speech recognition (ASR) often consider adversaries that are ignorant of the existence of FlowLens itself (which are unsuccessful), and then proceed to increase the knowledge/capability of the attacker.
- Some papers propose evasion strategies that have very little in common with established techniques for traditional adversarial examples (e.g., those using gradients).
- We highlight two such papers, both of which consider attacks against ML systems for automated speech recognition.
- Broader goals are being explored, such as attacks that succeed despite the ML model's correct response.

## C. Positive Findings in Threat Models of Defense Papers
- Most defense papers put a stronger focus on the cost of the countermeasure, typically by measuring its overhead (i.e., baseline performance degradation).
- In some cases, such knowledge can be easily acquired, especially if the targeted ML system is OPEN (see, e.g., the attacks by Sato et al. [28]).
- Notably, some works have shown that attackers with limited knowledge may be as dangerous as omniscient ones: Song and Mittal [99] show that "the gap between white-box attack accuracy and black-box attack accuracy is much smaller than previous estimates [100]."

## D. Inconsistencies that May Harm Future Research
- The definitions of the knowledge and capabilities of the envisioned attackers are inconsistent
- The box-based terminology used in the papers is not clear
- The output received by the attacker in query-based strategies is not specified
- Adapting threat models to ML systems is recommended

## A. Adapting Threat Models to ML Systems
- The threat model must precisely define the viewpoint of the attacker on every component of the ML system.
- The terms GOAL, KNOWLEDGE, CAPABILITY, and STRATEGY must be extended to cover the entire ML system.
- BOX, ACCESS, EVASION, and ADVERSARIAL are all subject to misinterpretation and should be used sparingly.

## B. Cost-driven Security Assessments
- Threat models must include cost-driven assessments for both the attacker and the defender
- The best way to approach cybersecurity is through proactive defense: the system designer should first model the adversary, simulate the attack, and then develop a countermeasure "if the attack has a relevant impact."
- From a research perspective, it is appropriate to assume (and evaluate) attackers with perfect knowledge who break any ML system. From a practical perspective, however, security evaluations are costly, and it is reasonable that real companies prioritize more common threats

## Context. Biggio and Roli
- The proactive security lifecycle should include economic considerations for both the attacker and the defender.
- When establishing a given threat model, the system designer (i.e., the researcher) weighs the potential benefit to the attacker and the corresponding cost to reap such benefit.
- This preliminary analysis yields the likelihood of the attack.
- The designer can then simulate the attack and gauge the corresponding damage, yielding the risk of the attack.
- Finally, the designer can conceive of any given countermeasure, estimate its costs (e.g., implementation, overhead, upkeep), and-depending on the risk of the corresponding attack-assess whether each countermeasure should be deployed or not.

## C. Collaborations between Industry and Academia
- Only 20% of papers consider real ML systems, and 90% of papers only consider deep learning algorithms-
- This gap between research and practice would diminish if researchers had easier access to production-grade ML systems-
- Most real ML systems are INVISIBLE or CLOSED, preventing researchers from truly understanding how they work-
- For example, a researcher can reasonably assume that an OSN uses an (INVISIBLE) ML system for spam detection-
- However, without knowing its internal architecture, they could hardly develop a "realistic" ML system with equivalent functionality-
- Similar issues exist also for CLOSED ML systems.

## D. Just Culture and Reproducible Research
- In ML security, source-code disclosure must be promoted with a just culture
- A "just culture" denotes an approach in which mistakes are assumed to occur and derive from organizational issues
- Mistakes are avoided by understanding their root causes and using them as constructive learning experiences
- This definition is in contrast to a blame culture
- We believe that our community should avoid the latter and embrace the former
- In the ML security context, some attacks (or defenses) may be incorrectly implemented or evaluated, as was the case with DeepSec
- Some of these flaws, however, were spotted because the implementation of [132] was publicly accessible
- Fear of criticism could have induced its authors [132] to not disclose the details of their platform-but they rightly released their code, and our community learned from such mistakes, turning a negative result into a positive outcome

## VI. CONCLUSIONS
- As our positions are adopted, we hope to see more reproducible research that describes threat models for entire ML systems using precise terminology, makes costdriven assessments factoring in human effort, and fosters active collaboration with practitioners from industry.
- In related literature, the adversarial perturbation ε used to craft an adversarial example is typically very "small" (i.e., imperceptible by humans). However, as pointed out by several works, this constraint does not hold universally. Some attackers may want adversarial examples to be noticed by humans, while in other domains (e.g., malware) humans do not inspect inputs to the ML model, meaning that perturbations can be of higher magnitude.

## B. Researchers vs Practitioners (Twitter thread in §II-C)
- B and J discuss a paper that B wrote
- B and J disagree on some points
- K intervenes and tries to mediate the disagreement
- K and B have a friendly discussion

## J1) Why robustness to adversarial examples isn't a first-priority
- Concern on the Sophos AI team
- Researchers have a 'simplistic' vision of a ML malware detector by researchers, compared with the 'complex' pipeline of a real ML malware detector.
- The goal of all these defenses in the end is about raising the bar for the attacker, and this work in my opinion is useful in that direction.
- It depends. The transformations used are perfectly legit (e.g. adding sections).
- Despite this gap (which our paper attempts to close), we believe that B3's point is valid.

## C. Further Considerations on our third case study ( §III-C)
- Queries and submissions were clarified
- On day 0 all teams had a common objective, to use as few queries as possible
- On day 5, 3rd made his first submission
- On day 12, 3rd made his last submission, thereby completing his attack and leading the ranking with 608 queries. This information was publicly available: after this moment, all teams teams knew that, to win, they had to use fewer than 608 queries.
- From day 17 to day 42, 4th made various submissions and reached a perfect evasion by making 9 982 queries.
- On day 31, both 1st and 2nd made their first submission. (This does not mean that they did nothing before day 31.)
- On day 41, 2nd made their last submission, producing a perfect evasion with 343 queries. This information was publicly available.
- On day 42, 7 hours after 2nd's last submission, 1st made their second submission, which included 9 webpages (out of 10), all of which evaded all the ML detectors.
- Also on day 42, 1st made their last submission, producing a perfect evasion with 320 queries.
- At the end of the challenge, 1st stated [86]: "At first, we thought of attempting a classic model replication attack [...] but as we started working on the competition, we noted that the leader had already achieved the highest possible score using just 343 API calls"
- 3rd required less time than 2nd to accomplish the attack

## D. Case Study: Malware Competition (2020)
- The Malware Competition organized in the 2020 edition of MLSEC had two phases, each with a specific challenge: the first focused on defense, the second on attack.
- In the first phase, participants were asked to submit ML systems for malware detection. Submitted solutions had to meet some performance metrics: at most 1% false positive rate, at least 90% true positive rate, and at most 5 seconds response time-all of which were measured on an unknown test set. Winners were determined depending on how well their solutions performed against the attacks launched in the second phase.
- In the second phase, participants (which could be different from those of the first phase) were given 50 portable executable (PE) malware samples and allowed to manipulate them in any way that preserved the original malicious functionality. The manipulated PE had to evade as many ML-based detectors as possible-including those accepted in the previous defense phase.
- These "attackers" were given API access to the detectors, which resembled CLOSED ML systems: attackers could only submit an input and observe the output (i.e., the probability that the sample was malicious). Winners were determined on the basis of how each of the 50 manipulated malware samples performed against all the considered detectors.

## E. Example: Gains and Losses is not a zero-sum game
- The gain of an attacker may not correspond to the loss of a defender.
- Consider, for example, a company D whose main business is the development of ML-based malware detectors and who sells their products to a customer C.
- Now consider an attacker A that successfully evades one of the detectors developed by D and manages to inject some ransomware to the systems owned by customer C, asking for a ransom in the amount of R.
- Two scenarios can occur: • Pay. If C pays the ransom, then C loses R while A gains R. However, D (the developer of the 'evaded' detector) loses very little.
- In most cases, the contract between D and C protects D in case of "faults" in their systems.
- In contrast, if the contract determines that D is liable for the losses incurred by C, then D will pay R to C; however, in this case, D is typically insured by other companies, who will reimburse R to D.
- Hence, if the ransom is paid to A, then A gains R, and D loses nothing (assuming that C regains control of their systems).
- If C does not pay the ransom, then A gains nothing (actually, A may lose something, e.g., the time spent to carry out the attack).
- However, C will incur a loss, because their systems are compromised.
- Two cases can follow, depending on the contract terms: -If D is liable, then they must help C to recover their data, and D themselves may either incur a loss (e.g., human labour), or not (e.g., if they are insured).
- If D is not liable, then D may gain something, for example if C asks (and pays) D to assist in the recovery of their systems.

## F. Unreproducible Papers
- The ideal of a publication that releases all its source code and uses public datasets containing attacks from the real world is difficult to meet in practice
- The most common and understandable constraint is that of user privacy: real-world data is tied to consumers or employees, and custodians of this data must respect its confidentiality
- When constraints of this nature do not exist, researchers and practitioners must provide both descriptive details and technical artifacts that maximize reproducibility
- What can be done
- Publications that face privacy constraints and/or cannot release their source code can, however, still be valuable so long as they work within those constraints to make reproducibility a priority
- To this purpose, we propose the following actionable principles, which can be adopted by research papers to improve their scientific value:
- ML systems and models should be described with sufficient clarity that a reader can re-create the model with a high degree of accuracy after reading the paper
- Features used by the model should be described in detail
- Papers should evaluate their models on public datasets whenever data that is sufficiently similar to the private data is available
- When constraints of this nature do not exist, researchers and practitioners must provide both descriptive details and technical artifacts that maximize reproducibility
- If attacks that appear in private datasets are not releasable, they should be described with sufficient detail to enable researchers to reproduce similar attacks in their own studies and to understand attacker threats that appear in real systems.

## G. Discussion (with the Reviewers)
- Spammers and regular users: according to the description in §III-A, the malicious user wishing to upload a problematic image seems to perform a very similar action to millions of users uploading any sort of images every day on any OSN.
- Defenses vs. limited-knowledge attackers: it is true that perfect-knowledge attacks are rare, but we should be very careful to draw a distinction between "defenses that are robust in a query-only setting" (a reasonable threat model) and "defenses that only work because the attacker doesn't know what the defender is doing" (an unreasonable threat model).
- Domain Expertise: in §III-C, the 1st-3rd place teams all reached a perfect evasion by exploiting their domain knowledge.
- "Adversarial" in practice: can you clarify what you meant by "everything is adversarial" in §V-A?

## B. Research Questions
- The paper discusses 88 papers that were looked at, and asks 12 research questions.
- The questions asked relate to the attack and defense paradigms, the costs taken into account, the data types considered, and whether the source code has been released.
- The paper also considers the weakest attacker and how they would be able to attack the model.
- The paper provides the answers to all the research questions in Table IV.
