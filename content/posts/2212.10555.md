---
title: "PairReranker: Pairwise Reranking for Natural Language Generation"
date: 2022-12-20T18:56:57.000Z
author: "Dongfu Jiang, Bill Yuchen Lin, Xiang Ren"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10555v1.webp" # image path/url
    alt: "PairReranker: Pairwise Reranking for Natural Language Generation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10555)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10555).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/pairreranker-pairwise-reranking-for-natural).

# Abstract
- Pre-trained language models have been successful in natural language generation tasks
- Various decoding methods have been employed, but they often produce suboptimal results
- We first present an empirical analysis of three NLG tasks: summarization, machine translation, and constrained text generation
- We found that selecting the best output from the results of multiple decoding methods can significantly improve performance
- To further improve reranking for NLG tasks, we proposed a novel method, \textsc{PairReranker}, which uses a single encoder and a pairwise loss function to jointly encode a source input and a pair of candidates and compare them
- Experiments on three NLG tasks demonstrated the effectiveness and flexibility of \textsc{PairReranker}
- In addition, our \textsc{PairReranker} can generalize to significantly improve GPT-3 (text-davinci-003) results (e.g., 24.55% on CommonGen and 11.35% on WMT18 zh-en), even though our rerankers are not trained with any GPT-3 candidates.

# Paper Content

## Introduction
- Pre-trained encoder-decoder language models (LMs) have been found to be effective in various NLG tasks
- BART (Lewis et al., 2020) employs a denoising autoencoder architecture, in which the model is trained to reconstruct the original sentence from corruptions introduced during the training process
- T5 (Raffel et al., 2020) conducted a comprehensive usc.edu/PairReranker. study of alternative pre-training methods
- In order to effectively utilize encoder-decoder LMs for NLG tasks, it is crucial to employ an appropriate decoding method
- Various decoding approaches, including beam search, diverse beam search (Vijayakumar et al., 2016), top-k sampling, and top-p sampling (Holtzman et al., 2019), have been widely employed during inference
- However, previous studies (Cohen and Beck, 2019;Meister et al., 2020;Ravaut et al., 2022) have found that the top beam search results are frequently inferior to the oracle selections from the candidates generated by these methods
- The oracle performance can be significantly improved by selecting the best output from the results of multiple decoding methods
- In Table 1, we present an empirical analysis of three typical NLG tasks: summarization (CNN/DM), machine translation (WMT18), and constrained text generation (CommonGen)
- For example, using the PEAGUS model (Zhang et al., 2020) on the CNNDM dataset (Hermann et al., 2015), we find that the Rouge-2 score for the top beam search generation is only 21.48. However, by selecting the best output from the top 15 beam search results based on the maximum Rouge-2 score, the upper-bound performance can be increased to 27.74. Further improvement can be achieved by combining the results from four different decoding methods and selecting the best output for evaluation, resulting in a performance of 33.75, a 57% increase over the top beam search results. Similarly, the T5-large model on the CommonGen dataset (Lin et al., 2020) can achieve a 93% gain in CIDEr score; the Opus-MT model (Tiedemann and Thottingal, 2020) can achieve a 79.7% gain in BLEU on the WMT18 (zh-en) translation task (Bojar et al., 2018).
- These oracle performances highlight the importance of re-ranking candidate outputs in order to further improve the performance of LMs in NLG tasks
- Re-ranking candidates after decoding is a simple yet effective way to mitigate this gap, and there have been several recent efforts in this direction
- For instance, SimCLS (Liu and Liu, 2021) trains a re-ranker using a simple contrastive training objective, which encodes the source text and each candidate output using the same encoder and scores each candidate based on the cosine similarity between the embeddings
- Another successful approach is SummaReranker (SR) (Ravaut et al., 2022), which uses mixture-ofexperts training to improve the re-ranker for multiple metrics simultaneously
- The encoding module in SR is based on a cross-encoder, which takes the concatenation of a source and a candidate target as a single input sequence and produces a score for the candidate. This cross-encoder design leverages the attention layers in the Transformer architecture (Vaswani et al., 2017) and further enhances the ability to score candidates in the context of the source inputs.
- Prior reranking approaches, such as SimCLS and SummaReranker, have primarily focused on individually scoring each candidate output based on a given input. However, this approach is limited as it does not directly learn to differentiate between candidate outputs. This is a particularly pertinent issue when the candidates for reranking are...

## Problem Formulation

## Methods
- Previous baselines have limitations
- Proposed pair-based reranker is better than previous baselines
- Improvements over previous baselines

### Baseline methods
- The reranking is treated as a traditional learning-to-rank problem
- First, the language encoder is used to encode a given text x and each generated candidate c i ∈ C ∈ C with the same encoder, denoted as h(x) and h(c i ).
- Then, the cosine similarity between the source and candidate is used as the predicted score.
- The overall training objective is to find the predicted score s i that gives the best performance.
- The predicted score s i is sorted according to the metric score µ(c i , y).
- The reranker is trained to tell the best candidates from the other candidates.
- Besides, a mixtureof-expert (MoE) layer is added to jointly optimize the reranker with multiple metrics.

### Pairwise Reranking
- The candidates are highly homogeneous, making it difficult for the model to learn their difference.
- Different from traditional document retrieval where the document corpus is rich and heterogeneous, the search space of our problem only contains dozens of generated candidates from a normal language model.
- Therefore, how to train a reranker that could capture the subtle nuance among the candidates is what we will focus on.
- Similar to the baselines, our method also follows the paradigm of two-stage training.
- In contrast, our goal is to train a reranker f with the parameter φ that correctly identifies the better one from a given pair of candidates in a direct comparison manner.
- We want our reranker to learn to focus on the differences between two candidates and select the better one.
- We give a formal description of our method.
- As stated in Sec. 2, let D = {(x, y), ..., (x, y)} be the set of source-target text pairs for a specific language generation task.
- Given a data point (x, y), let C = {c, ..., c} be a set of corresponding generated candidates for source x.
- For each candidate, there are some specific metrics M that indicate their quality (e.g., BLEU score).
- For each metric µ ∈ M, the score of candidate c i is denoted as µ(c, y).
- For a metric µ, our model is expected to output two scores: s, s= f(s, c, c) respectively for a certain pair of candidates (c, c).
- We use s= σ(s−s,j−sj) to denote the model's confidence of cis better than c, where σ is the sigmoid function.
- Then we consider the learning problem as a multitask binary classification problem: where y).
- For all the metrics in M, we take the average as the final loss:
- To better capture the difference among the highly homogeneous candidate groups, we propose to conduct in-context attention among each candidate pair.
- Before concatenating each segment, we truncate each segment to specific lengths in case the total length after concatenating exceeds the model's capacity.
- After truncating, we add special tokens <source>, <candidate1>, and <candidate2> to the start of corresponding segments.
- By separating with default separator token </s>, we get the concatenated input form: "<s><source>x</s><candidate1>c i </s> <candidate2> c j </s>", where x is the text of a source input and c i and c j are the text of two output candidates.
- We then feed them into the language model encoder and get the final hidden states.
- Among these hidden states, we use the embeddings of special tokens <source>, <candidate1>, and <candidate2> as their representations.
- To compute the scores of the two candidates, we concatenate embeddings of <source> with <candidate1>, and <source> with <candidate2> and feed them into a single-head layer that computes scores for each metric.
- Since there are O(N) unique pair combinations, it is thus necessary for us to apply an effective subsampling strategy during both the training and the inference time for improving efficiency.
- During the training state, the selected pairs are supposed to be distinct enough to represent the characteristics of this small search space. Therefore, we select the best k candidates and the worst k candidates to form k pairs for the training.
- In practice, we found that let k = 1 is enough to get a decent result, which means for every source, we only use one pair of candidates for training. Besides, due to the position embeddings of the language model, the order of the candidates in a pair (s, c, c) matters. That is, the comparison result of (s, c, c) and (s, c, c) might not be consistent. Therefore, we also shuffle the order of candidates within each training pair so that the model learns to be consistent with itself.
- In the inference stage, it would be too costly for us to compute the results of every possible pair and give the best candidate. Therefore, we instead do a single bubble run of comparisons to select the best candidate, which reduces the inference time complexity from O(N) to O(N...

### Base models
- The PEGASUSlarge and BART-large datasets are famous for their ability to abstractively summarize data.
- We use the public fine-tuned checkpoint from Hugging face 56 for the generative commonsense reasoning task on the CommonGen dataset.
- For the Chinese-English translation task on the WMT2018 dataset, we use the public pre-trained opus-mt checkpoint 7.

### Evaluation setups
- The training dataset for the reranker needs to be constructed so that the original model used to generate candidates on the training dataset should never have seen these candidates.
- To construct the training dataset for the reranker, we first fine-tune the original non-finetuned pretrained model on half of the training dataset, which gives us 2 half-finetuned base models that each of them has only seen their own half of the training dataset.
- Then we use them to generate candidates on their un-seen half of the training dataset using the decoding method talked about before. These generated candidates together form a whole training dataset with generated candidates that resemble the quality during the inference stage.
- During the inference stage, we directly adopt the public checkpoints that have been finetuned on the whole training dataset.
- We generate candidate candidates on the validation and testing dataset with this public checkpoint, which constitutes the validation and testing dataset on which our reranker does inference.
- We use two decoding methods, beam search, and diverse beam search, in the experiments, following the prior work of SummaReranker.
- We train our reranker for 5 epochs.
- We use the Adafactor optimizer (Shazeer and Stern, 2018), with maximum learning rate being 1e-5.
- The warmup ratio is 5% with a linear learning rate scheduler.

### Main results
- Overall performance in summarization is improved by 6.12% with finetuned PEGASUS-large as the base model
- Task generalization is tested on CommonGen and WMT2018 and is improved by 2.90% and 3.87% respectively
- Transferring re-rankers to GPT-3 improves the quality of the generated candidates by a large margin

## Related Work
- How to exploit the encoder-decoder LMs for NLG tasks is one of the main focuses in natural language generation (NLG) research
- Decoding methods like beam search and diverse beam search are widely used due to their famous ability of decoding candidates of high quality
- However, previous studies have found that the top-beam candidate is usually not the best one
- Reranking methods have long been used to improve the quality of natural language generation
- Shen et al. (2004) propose an effective ranking method for machine translation
- Kratzwald and Feuerriegel (2018); Nogueira and Cho (2019) use passage reranking as the first stage of the question-answering system
- (Krishna et al., 2022) use reranking for Generative common sense generation task
- For the summarization task, Liu et al. ( 2021) first provided a unified view of text summarization and summaries combination
- After that, Liu and Liu (2021) proposed a contrastive learning framework to effectively rerank the summarization candidates
- Ravaut et al. ( 2022) then introduce multi-task learning in the reranking by applying a mixture-of-experts layer to the head of the language model encoder
- As one of the most traditional and effective ranking methods, the pairwise ranking has shown its brilliant performance on a wide range of NLP tasks
- Ranknet (Burges et al., 2005) has been proposed as an effective method for a lot of ranking problems
- The later improved LambdaRank (Burges, 2010) continues to show the great potential of pairwise ranking
- However, though training in a pairwise way, they still compute the score for each item separately, which is why the pairwise method is outperformed by the listwise method
- However, our method focuses on capturing the difference between a pair of data through the attention mechanism, which brings better performance
- Following this, Shen et al. (2022) creatively trains the original language model and reranker jointly by applying the self-critic algorithm
- That also points out a brand-new perspective on how to exploit the potential of the language model

## Conclusion
- Pre-trained encoder-decoder language models (LMs) are effective for natural language generation (NLG) tasks
- However, the performance of these models can be improved by using an appropriate decoding method during inference
- Decoding approaches like beam search and top-k sampling often produce suboptimal results, and selecting the best output from the results of multiple decoding methods can significantly improve performance
- Re-ranking candidate outputs after decoding can also help mitigate the gap between oracle selections and top-ranked outputs
- However, these approaches have limitations, including their inability to directly differentiate between candidates and their reliance on the quality of the initial rankings
- In order to address these limitations, we propose a novel reranking method called PAIRRERANKER
- PAIRRERANKER outperforms baseline methods by a consistent margin
- Interestingly, trained rerankers can also be transferred and used with large language models such as GPT-3, which also produces a significant improvement
