---
title: "A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation"
date: 2022-03-18T12:07:36.000Z
author: "Rachel M. Bittner, Juan José Bosch, David Rubinstein, Gabriel Meseguer-Brocal, Sebastian Ewert"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2203-09893v2.webp" # image path/url
    alt: "A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.09893)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.09893).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-lightweight-instrument-agnostic-model-for).

# Abstract
- Automatic Music Transcription (AMT) has been recognized as a key enabling technology with a wide range of applications
- Best results have typically been reported for systems focusing on specific settings, e.g. instrument-specific systems tend to yield improved results over instrument-agnostic methods
- Higher accuracy can be obtained when only estimating frame-wise $f_0$ values and neglecting the harder note event detection
- Despite their high accuracy, such specialized systems often cannot be deployed in the real-world
- Storage and network constraints prohibit the use of multiple specialized models, while memory and run-time constraints limit their complexity
- In this paper, we propose a lightweight neural network for musical instrument transcription, which supports polyphonic outputs and generalizes to a wide variety of instruments (including vocals)
- Our model is trained to jointly predict frame-wise onsets, multipitch and note activations, and we experimentally show that this multi-output structure improves the resulting frame-level note accuracy.

# Paper Content

## INTRODUCTION
- Automatic transcription of music has been studied for more than four decades
- Various intrinsic challenges remain, such as a lack of an objective ground truth
- AMT systems are usually designed with a limited scope, and focus on a sub-task
- There are a number of common sub-tasks in AMT which branch along three dimensions: (1) the degree of output polyphony (monophonic, polyphonic) (2) the types of output to be estimated (notes, f0), and (3) the type of input audio (pop songs, solo piano, solo guitar, jazz ensembles, etc.)
- For example, specializing for a specific instrument class allows models to exploit instrument-specific characteristics to increase the transcription accuracy
- The proposed model is a lightweight neural network which runs efficiently on low-end devices
- Unless otherwise noted, we deal with polyphonic recordings of a single instrument class (e.g. solo piano, an ensemble of violins, solo vocals, a choir, etc.), but do not restrict which classes we consider
- We study the ability of the proposed model to transcribe a variety of instruments and vocals without retraining, and compare with a recent baseline model for instrument-agnostic polyphonic note estimation
- Further, we evaluate the contribution of components of the proposed model with an ablation study.

## BACKGROUND AND RELATED WORK
- There is a huge body of work on AMT.
- Due to space constraints we refer to [1,11] for a more comprehensive overview.
- AMT outputs are typically either frame-level multipitch estimation (MPE) or note-level estimation, which transcribe polyphonic music at different levels of granularity [1].
- Both are useful depending on the application: MPE provides lower-level expressive performance information (such as vibrato, glissando), whereas note-level estimation gives information closer to the musical score.
- MPE methods predict the fundamental frequencies (f0s) which are active at a given time-frame (note that even if not strictly equivalent, we use pitch and f0 interchangeably following the literature in the field [1]).
- They commonly first estimate a pitch posteriorgram [12,13], where each time-frequency bin is assigned an estimate of the likelihood of that fundamental frequency being active at a given time. Such matrices typically contain multiple bins per semitone, which allows estimation of small ("continuous") variations of pitch.
- Various methods aim at estimating and subsequently grouping MPE outputs from polyphonic recordings into note events [14][15][16][17][18], or attempt to group pitches into contours [11,12,19].
- Note estimation (or note tracking) methods aim at estimating notes events (defined as: pitch, onset time, offset time).
- Notes cannot be trivially estimated from the output of an MPE system, because MPE information does not encode onsets/offsets, and preserves fluctuations in pitch which should not always be quantized to the nearest semitone.
- In particular, note estimation is difficult for singing voice, which may have a high degree of fluctuation around a center pitch [9] compared to instruments such as the piano.
- Multiple methods have been proposed for estimating notes from pitch posteriorgrams e.g. using median filtering [11], Hidden Markov Models [16] or neural networks [20,21].
- While most approaches consider each semitone independently, some approaches attempt to model the interactions between notes, using spectral likelihood models [1,18], or music language models [3,17].
- Transformers have recently been applied to AMT, directly predicting MIDI-like note events from spectrograms in piano music [5].
- A few AMT models perform both note and pitch estimation [14,18,22], and most work with monophonic data.
- Regarding input audio characteristics, traditional AMT methods based on signal-processing have been more generalizable to multiple instruments than more recent approaches, as well as being simpler and faster [1,12].
- However, the best performing systems often come at the expense of higher computational requirements and a focus on instrument-specific systems [4].

## MODEL
- The model generalizes across a set of polyphonic (or monophonic) instruments without retraining
- The model is lightweight enough to run in low-resource settings
- The number of parameters of a model does not necessarily correlate with its memory usage
- Harmonic Stacking
- The architecture is structured in order to exploit the differing properties of the three outputs
- Class-balanced cross entropy loss is used to counterbalance the heavy class imbalance in Yo

## EXPERIMENTS
- The proposed method, "Notes and Multipitch" (NMP), is evaluated on the note estimation task
- AMT methods have commonly been evaluated using a set of metrics proposed for MIREX3 evaluation tasks
- In this work we report the note-level Fmeasure (F), where notes are considered correct if the pitch is within a quarter tone, the onset is within 50 ms, and the offset is within 20% of the note's duration, the note-level F-measure-no-offset (Fno) with the same criterion as F-measure, but ignoring offsets, and the frame-level note accuracy (Acc), which is computed for frames with a hop size of 10 ms
- We use Fno as the main measure of overall note estimation accuracy since the definition of offsets is less objective than onsets (e.g. due to reverberation, sustain pedal, annotation procedure)

## Note Transcription Baseline Comparison
- NMP outperforms MI-AMT on all test datasets
- NMP performs well without needing to be instrument-specific

## Ablation Experiments
- Harmonic Stacking is effective at reducing the number of required convolutional kernels
- The supervised bottleneck layer Yp improves performance on all datasets
- Open-source instrument-specific models outperform the proposed model on solo, monophonic data

## Comparison with instrument-specific approaches
- NMP outperforms TENT for all metrics
- For vocals (Molina), Vocano outperforms NMP in Fno and F, but the frame-level pitch accuracy (Acc) is comparable to NMP, suggesting that Fno could increase with improved onset detection
- The largest difference in performance between NMP and an instrument-specific method is in the MAESTRO dataset compared with OF, which was specifically trained for piano transcription, and achieves 95.2% Fno, in comparison to 70.9% of our method (which is notably still a reasonably high score for this task)

## MPE Baseline
- NMP outperforms deep salience for the Bach10 dataset with a frame-level accuracy of 72.5±3.8
- NMP achieves better results on Su 43.6 ± 7.9

## Efficiency
- Both methods are comparable in estimated overhead
- NMP uses 490 MB peak memory and takes 7 s
- MI-AMT uses 561 MB and takes 10 s
- NMP substantially outperforms MI-AMT, using only 951 MB peak memory and taking 24 s

## CONCLUSIONS
- NMP outperforms a recent strong baseline note estimation model across five different datasets
- NMP is a "one-size-fits-all" solution, and has much lower computational requirements
- We hope to encourage further research into low-resource, multi-purpose AMT systems and believe that the proposed solution can be a valuable baseline
