---
title: "RangeAugment: Efficient Online Augmentation with Range Learning"
date: 2022-12-20T18:55:54.000Z
author: "Sachin Mehta, Saeid Naderiparizi, Fartash Faghri, Maxwell Horton, Lailin Chen and 3 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10553v1.webp" # image path/url
    alt: "RangeAugment: Efficient Online Augmentation with Range Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10553)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10553).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/rangeaugment-efficient-online-augmentation).

# Abstract
- Automatic augmentation methods use a large set of operations to diversify training data.
- Magnitude ranges of operations are continuous, so methods use fixed and manually-defined ranges.
- RangeAugment allows efficient learning of range of magnitudes for individual and composite operations.
- RangeAugment uses an auxiliary loss based on image similarity to control range of magnitudes.
- RangeAugment achieves competitive performance with fewer operations.

# Paper Content

## Abstract
- Automatic augmentation methods use fixed and manually-defined magnitude ranges for each operation.
- RangeAugment allows us to efficiently learn the range of magnitudes for individual and composite augmentation operations.
- RangeAugment uses an auxiliary loss based on image similarity to control the range of magnitudes of augmentation operations.
- RangeAugment achieves competitive performance to state-of-the-art automatic augmentation methods with fewer augmentation operations.

## Introduction
- Data augmentation is a regularization method used to train deep neural networks
- Automatic augmentation methods have been developed to search for augmentation policies that maximize validation performance
- Most augmentation operations are parameterized by two variables: probability of applying them and their range of magnitudes
- RangeAugment is a method to learn the range of magnitudes for each augmentation operation
- RangeAugment takes a target image similarity value as an input and searches for it via linear search
- RangeAugment is able to achieve competitive performance with only three simple operations
- RangeAugment can be trained end-to-end with any downstream model to learn model-and task-specific policies
- RangeAugment is the first automatic augmentation method that learns model-and task-specific magnitude range for each augmentation operation at scale

## Related work
- Data augmentation combines different operations to create additional training data
- Traditional augmentation methods rely on expert knowledge and trial-and-error
- Recent methods focus on finding optimal augmentation policies automatically
- Search space is large and computationally intractable, so policies found for smaller datasets are transferred to larger datasets
- RangeAugment focuses on learning the magnitude range of each augmentation operation

## Ra n g eau g m e n t
- Existing augmentation methods use manually-defined ranges of magnitudes.
- This paper introduces RangeAugment, a method for learning a range of magnitudes for each augmentation operation.
- RangeAugment uses image similarity to learn the range of magnitudes.

## Problem formulation
- T is a set of N differentiable augmentation operations.
- Each augmentation operation is parameterized by a scalar magnitude parameter.
- RangeAugment learns the range of magnitudes for each augmentation operation.
- The goal of automatic augmentation is to find the augmentation policy that diversifies training data and helps improve model's generalization ability.

## Policy learning
- RangeAugment introduces an auxiliary loss to enable learning model-specific range of magnitudes for each augmentation operation in an end-to-end fashion.
- RangeAugment minimizes the distance between the expected value of an image similarity function and a target image similarity value.
- RangeAugment reduces to searching a single scalar parameter that maximizes downstream model's validation performance.

## Implementation details
- PSNR is used as image similarity function
- Training overhead of 0.5%-3% over baseline
- Magnitude range is clipped to prevent objects from being unrecognizable
- Augmentation operations used with uniform probability
- Smooth L1 loss used as augmentation loss function
- RangeAugment can achieve competitive performance with fewer operations
- Performance of ResNet-50 on ImageNet dataset shown when data diversity is increased
- Gap between training and validation accuracy indicates better generalization

## Evaluating ra n g eau g m e n t on image classification
- RangeAugment can learn model-specific augmentation policies
- Evaluate importance of single and composite augmentation operations using ResNet-50 on ImageNet dataset
- Study model-level generalization of RangeAugment

## Experimental set-up
- ImageNet dataset used with 1.28M training and 50k validation images
- Baseline models include MobileNetv1, MobileNetv2, MobileNetv3, MobileViT, ResNet-50, ResNet-101, EfficientNet, and SwinTransformer
- Single augmentation operation with wider magnitude range improves generalization capability
- Composite augmentation operations reduce training accuracy while having similar validation performance
- Curriculum matters, annealing target PSNR from high to low helps identify useful ranges for each augmentation operation
- RangeAugment achieves comparable validation accuracy and smaller generalization gap compared to existing methods

## Model-level generalization of ra n g eau g m e n t
- RangeAugment can be applied to other vision models
- Data regularization decreases performance of mobile models, but improves performance of non-mobile models
- RangeAugment can learn model-specific magnitude ranges for each augmentation operation

## Does rangeaugment increase variance on model performance?
- Training models with RangeAugment leads to a stable model performance
- Standard deviation of model's validation accuracy is between 0.01 and 0.2

## Comparison with existing methods
- ResNet-50 is used as a baseline model for automatic augmentation methods.
- RangeAugment achieves similar performance as previous methods, but with fewer augmentation operations.

## Comparison with other models
- RangeAugment is compared to AutoAugment and RandAugment
- RangeAugment delivers competitive performance to existing methods with fewer augmentation operations
- RangeAugment is recommended to train non-mobile models by annealing ∆ from high to low similarity
- RangeAugment is competitive to previous methods and is highlighted in bold when performance is within ImageNet's standard deviation range

## Task-level generalization of ra n g eau g m e n t
- RangeAugment is effective on different downstream models on the ImageNet dataset.
- RangeAugment is evaluated for tasks other than image classification, such as semantic segmentation, object detection, foundation models, and knowledge distillation.

## Semantic segmentation with deeplabv3 on ade20k
- Uses ADE20k dataset with 20k training and 2k validation images
- Reports segmentation performance in terms of mean intersection over union (mIoU)
- Compares with 3 baseline augmentation methods
- RangeAugment improves performance of different models
- Gaussian noise is important for semantic segmentation
- RangeAugment improves performance of DeepLabv3 significantly
- RangeAugment's curriculum is transferable to other segmentation datasets

## Object detection with mask r-cnn on ms-coco
- Used MS COCO dataset with 118k training and 5k validation images
- Reported instance detection and segmentation performance in terms of COCO mean average precision (mAP)
- Integrated mobile and non-mobile backbones with Mask R-CNN
- Finetuned model for 230k iterations with multi-scale sampler

## Baseline augmentation methods
- Mask R-CNN is studied with two augmentation methods
- Best performance is achieved when annealing ∆ from 40 to 10
- RangeAugment improves detection performance across backbones

## Learning augmentation for foundation models at scale
- Common perception is that data augmentation is not needed for large datasets
- This paper challenges this perception and shows that RangeAugment improves accuracy and data efficiency of large scale models
- CLIP model is trained with two different dataset sizes and evaluated on ImageNet's validation set
- RangeAugment is used from scratch and ViT-B/16 and transformer are used as image and text encoders
- Multi-scale sampler is used to make CLIP more robust to input scale changes
- Target PSNR is annealed from 40 to 20

## Learning augmentation for student model during distillation
- Knowledge distillation is a popular approach for training accurate mobile models.
- Augmentation strength is less for mobile models due to their low complexity.
- RangeAugment can be used to find an optimal range of magnitudes for student models.

## Dataset and teacher details
- Distilled knowledge from ResNet-101 to MobileNetv1 on ImageNet dataset
- Best performance when annealing ∆ from 40 to 20
- RangeAugment improves performance of different models
- Range of magnitudes for each augmentation operation varies across models and tasks
