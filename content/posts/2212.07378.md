---
title: "3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation"
date: 2022-12-14T17:59:03.000Z
author: "Zhuoqian Yang, Shikai Li, Wayne Wu, Bo Dai"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07378v1.webp" # image path/url
    alt: "3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07378)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07378).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/3dhumangan-towards-photo-realistic-3d-aware).

# Abstract
- We present 3DHumanGAN, a 3D-aware generative adversarial network (GAN) that synthesizes images of full-body humans with consistent appearances under different view-angles and body-poses.
- To tackle the representational and computational challenges in synthesizing the articulated structure of human bodies, we propose a novel generator architecture in which a 2D convolutional backbone is modulated by a 3D pose mapping network. The 3D pose mapping network is formulated as a renderable implicit function conditioned on a posed 3D human mesh. This design has several merits: i) it allows us to harness the power of 2D GANs to generate photo-realistic images; ii) it generates consistent images under varying view-angles and specifiable poses; iii) the model can benefit from the 3D human prior.

# Paper Content

## Introduction
- Human image generation is a long-standing topic in computer vision and graphics with countless applications across multiple areas of interest including movie production, social networking and e-commerce.
- Compared to physically-based methods, data-driven approaches are preferred due to the photo-realistcness of their results, versatility and ease of use.
- In this work, we are interested in synthesizing fullbody human images with a 3D-aware generative adversarial network (GAN) that produces appearance-consistent images under different view-angles and body-poses.
- Rapid developments have been seen in using 3D-aware GANs to generate view-consistent images of human faces [4,5,13,21,44,54,66]. However, these methods have limited capacity when dealt with complex and articulated objects such as human bodies.
- To begin with, methods based solely on neural volume rendering [5,44,54] are too memory inefficient. Rendering human bodies requires a volumetric representation that is much more dense than that of faces, which makes it computationally infeasible.
- A line of work improves the computational efficiency and rendering quality of 3D-aware GANs by upsampling the rendered result with a convolutional neural network [4,13,21,66]. We argue that this feed-forward approach is not well-suited for generating full-body human images, since the 3D representation is tasked with simultaneously modeling the articulated geometry and the appearance of human bodies, which poses great challenges on representational capacity, while the capacity of the 2D-network is not fully utilized.
- As depicted in Figure 2a, our work seeks to solve these problems by introducing a novel generator architecture in which a 2D convolutional backbone is modulated by a 3D pose mapping network. This design is motivated by the observation that in a StyleGAN2 [27] model trained on human images certain layers of styles correlate strongly with the pose of the generated human [10] while others correlate more apparently with the appearance.
- The 3D Pose Mapping Network is formulated as a renderable implicit function conditioned on a posed 3D human mesh derived with a parametric model [37]. In this way, the 3D representation handles the simplified task of parsing a geometric prior while the network no longer has to learn the articulated body geometry by itself.
- As an additional benefit of explicitly conditioning on posed human mesh, the pose of the generated human can be specified.
- The output of the 3D pose mapping network is used to render a 2D low-resolution style map through ray integration [42]. The style map is passed into the first few layers of our backbone network. Then, we introduce the Appearance Mapping Network to control the appearance of the generated human. The appearance mapping network is formulated as an MLP, following a common practice of style-based generators [26,27].
- For the network to learn to parse the 3D geometric prior, we use a segmentation-based GAN loss [57] calculated using a U-Net [50] discriminator. This design enables the network to establish one-to-many mapping from 3D geometry to synthesized 2D textures using only collections of single-view 2D photographs without manual annotations.
- Although capable of producing photo-realistic results, CNN-based generator networks often sacrifice consistency under geometric transformations [4], i.e. the appearance of the generated human may change with varying pose and view-angle.
- To preserve consistency, we propose a network design with two key aspects: 1) Our backbone network is built entirely with 1 × 1 convolutions. This helps eliminate positional reference and promotes equalvariance [25]. 2) Modulation from the pose mapping network is passed into the backbone by means of spatial adaptive batch normalization instead of the commonly used instance normalization [27,45], so that underlying structure from the geometric information parsed by our pose mapping network is not impaired.

## Related Works
- 3D-aware image generation is possible with generative adversarial networks
- Earlier efforts devised different network structures to process meshes, voxels, or block-based representations
- Neural implicit representations rendered via ray integration have been found to be an effective representation for data-driven 3D synthesis
- Progress was gained in generating objects with simple structure such as human faces, but these approaches are burdened by heavy computational cost and limited to generating low-resolution images
- Several works propose to remedy this by upsampling the rendered results with a 2D convolutional network and achieved impressive quality
- However, human image generation is still not ready for the challenging 3d-aware task
- Inspired by the success of StyleGAN on unconditional generation, several works explore the capacity of the model for full-body synthesis from different aspects

## Generator Architecture
- We build a generative adversarial network that synthesizes 3D-aware full-body human images with specified pose and view-angle.
- Pose is specified as a set of coordinates of the vertices in a posed body mesh derived using a parametric human body model.
- The appearance of the generated human is randomly sampled, represented as a vector z ∈ R Nz , but required to stay consistent when pose and view-angle vary.
- To this end, we design a 2D-3D-hybrid generator architecture and a training strategy to achieve partially conditioning on pose.
- Architecture. An overview of our generator is presented in Figure 2a. We use a style-based generator but with a major difference from prior works: different layers of the convolutional backbone are separately modulated by two mapping networks, i.e., a pose mapping network that handles 3D human-body geometry and an appearance mapping network which resembles that of other style-based gener-ators.
- This design is motivated by the observation that in a StyleGAN2 model trained on human images. The low-level styles correlate strongly with the pose and orientation of the generated human [10] while others correlate more apparently with the appearance, as shown in Figure 3.
- This inspires us to inject the human geometric prior by calculating the low-level styles from the conditioning posed mesh.
- 3D Pose Mapping Network. To parse the 3D geometric information, the pose mapping network is formulated as a renderable 3D implicit function f p (ψ(x|p, θ), d) = (σ, s x ) where x ∈ R 3 is a coordinate in the 3D camera space; d ∈ R 3 denotes the orientation of the camera ray; p ∈ R 6890×3 denotes the coordinates of the body vertices in camera space; ψ(x|p, θ) transforms the coordinate x from the camera space to a canonical space, added to facilitate learning.
- Following literature in 3D human body reconstruction [41,47,65], ψ is defined as the inverse process of linear blend skinning, x ω k (x, p) returns the blend weights corresponding to the vertex nearest to the querying point x; G k (θ) ∈ SE(3) denotes the cumulative linear transformation at the k th skeleton joint.
- The outputs of the implicit function are the opacity σ ∈ R and a style vector s ∈ R Ns , which are used to render a 2D low-resolution pose style map w s ∈ R Hs×Ws×Ns via ray integration [42].
- The implicit function is parameterized as an MLP with periodic activation [56] to handle low-dimensional coordinate input.

## Training
- Our training pipeline is illustrated in Figure 2b
- Inspired by [53], we use a U-Net [50] discriminator architecture together with a segmentation-based GAN loss to establish a one-to-many mapping between geometry and textures
- Specifically, the U-Net discriminator classifies each pixel as fake, background or one of 25 semantic classes (e.g. head, torso ...)
- The segmentation-based GAN loss is sufficient for training the model to generate consistent images with desired pose and view-angle.

## Preserving Consistency
- CNN-based generator networks often sacrifice consistency under geometric transformations
- Our network is carefully designed to preserve the consistency of appearance against varying pose and viewangle
- Pixel-wise independent convolutional backbone
- Our backbone network is based entirely on 1 × 1 convolutions and no upsampling is used
- The purpose of this is to eliminate unwanted positional references so that the network is equal-variant to geometric transformations
- Spatial-adaptive batch normalization is used to pass the style maps into the backbone in a way that does not impair underlying structure

## Experiments

### Comparisons
- Quantitative results are presented in Table 1
- Qualitative results are shown in Figure 6
- Our method demonstrates the best overall performance, with an FID comparable to the 2D state-of-the-art method StyleGAN-Human and the best consistency
- With GAN inversion techniques applied, the unconditional StyleGAN2, StyleNeRF and EG3D are able to generate images where the target pose is close to the condition but with some gaps
- This suggests that the representations learned by these methods are entangled in appearance and pose
- Style mixing is not enough to achieve clean separation in the two domains
- Our method is able to generate images that accurately matches the conditioning pose
- Besides, all of the compared baselines show different extents of inconsistencies under view and pose variation
- This is an indicator of entanglement of appearance and pose in the learned latent spaces of the baselines

### Ablation Study
- To assess the effectiveness of each designed module in our 3d-aware generator, we evaluate a set of ablated models where the modules to be evaluated are replaced with approaches used by previous works.
- Specifically, to examine the effects of passing the 3D geometric prior into the 2D backbone as styles, i.e. the 2D-3D hybrid generator, we compare with the feed-forward approach used by [4,13,66], denoted as (Feed Forward).
- To examine the contribution of the segmentation-based GAN loss, we train a model in which this loss is replaced with the traditional GAN loss with binary discrimination. This results in a model trained under VAE-style supervision as in [21], denoted as VAE.
- To examine the effectiveness of the pixel-independent backbone design, we compare with a variant that uses traditional upsampling convolution network like in [27,53], which is denoted as Upsampling.
- To examine the effectiveness of spatial adaptive batch normalization, we compare with a variant that uses spatial adaptive instance normalization instead, denoted as Instance Norm.
- All ablation models are trained and evaluate in the resolution of 256 × 128.
- Quantitative results are presented in Table 2 and qualitative results are presented in the appendix.
- Our full model performs best in fidelity and view-consistency.
- The Feed-Forward approach slightly outperforms the full model in consistency, but with no small sacrifice in image quality.
- In other words, passing the geometric prior in the form of low-level styles has a small toll on pose-accuracy and consistency, but completely within the acceptable range. Meanwhile, the image quality is greatly improved.
- By examining the results of the VAE setting, we can see that segmentation-based GAN loss is crucial to pose accuracy. It also improves fidelity, possibly because this loss helps the model to make better use of the 3D Human Prior.
- The Upsampling and Instance Norm configurations show the best accuracy of generated pose but with impaired image consistency in different extents. These two configurations show that the pixel-wise independent backbone and spatial adaptive batch normalization indeed preserves consistency.

### Internal Representation
- Figure 7 visualizes typical internal representations from our networks.
- In the model that uses batch normalization, we are able to find certain feature activations that are consistent across different poses, view-angles and appearances.
- In contrast, activations of such kind cannot be found in the model that uses instance normalization.
- One explanation is that the feature activations in the instance normalization model mainly controls the presence of image features rather the location, which is similarly reported in [25].

### Limitations and Discussion
- The network still generates artifacts when rotated by a large angle
- There are small inconsistencies in the appearance when the view-angle changes

## Conclusion
- We build one of the first generative adversarial networks, 3DHumanGAN, that achieves 3D-aware synthesis of fullbody human images.
- Our 2D-3D hybrid generator effectively and efficiently uses 3D geometric prior of the human body and achieves consistent appearance under varying poses and view-angles with competitive image quality.
- Our segmentation-based GAN loss plays an irreplaceable role in teaching the generator to parse and condition on the 3D human body prior.
- By examining ablated models and visualizing internal representation, we find that our pixelindependent backbone together with the spatial adaptive batch normalization technique preserve consistency by keeping the underlying structures inferred from the 3D geometric priors intact.
