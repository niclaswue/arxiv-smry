---
title: "We are Going to the Space -- Part 1: Which device to deploy in a satellite?"
date: 2023-01-12T11:51:11.000Z
author: "Robert Bayer, Julian Priest, Pınar Tözün"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04954v1.webp" # image path/url
    alt: "We are Going to the Space -- Part 1: Which device to deploy in a satellite?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04954)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04954).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/we-are-going-to-the-space-part-1-which-device).

# Abstract
- Shrinkage in sizes of components that make up satellites led to wider and low cost availability of satellites.
- As a result, there has been an advent of smaller organizations having the ability to deploy satellites with a variety of data-intensive applications.
- One popular application is image analysis to detect, for example, land, ice, clouds, etc.
- However, the resource-constrained nature of the devices deployed in satellites creates additional challenges for this resource-intensive application.
- In this paper, we investigate the performance of a variety of edge devices for deep-learning-based image processing in space.
- Our goal is to determine the devices that satisfy the latency and power constraints of satellites while achieving reasonably accurate results.
- Our results demonstrate that hardware accelerators (TPUs, GPUs) are necessary to reach the latency requirements.
- On the other hand, state-of-the-art edge devices with GPUs could have a high power draw, making them unsuitable for deployment on a satellite.

# Paper Content

## INTRODUCTION
- In the last century, most innovation in real-world satellite applications were only available to the largest countries such as USA and Russia.
- These innovations led to significant reductions in the size of the components that make up a satellite and in the cost of the manufacturing and deployment process of a satellite.
- This, in turn, introduced a new CubeSat class of miniature satellites.
- Their format is based on a 10cm cube, with the possibility of combining multiple modules to create a larger satellite.
- This standardization makes a batch deployment of satellites easier as the format affords tight configuration.
- The reduction in costs that came as a result of this new deployment method led to the advent of satellites owned by small or private organizations.
- However, the size of the CubeSat class satellites poses new complex challenges such as the power and thermal constraints as well as the physical dimensions of components.
- These CubeSats often perform resource-intensive tasks, which clashes with the resourceconstrained nature of their format.
- Image processing and analysis is one class of possible satellite workloads.
- Satellites with this type of workload take large-scale images, which they have to store and send back to a ground station.
- The link between the satellite and a ground station is of limited bandwidth and short-lived.
- Images must be highly compressed to send all of them or filtered by quality and areas of interest.
- The images already have a resolution of tens or hundreds of meters per pixel, and lossy compression would lead to even more loss of detail.
- Filtering preserves the details of the images, but is more resource-intensive.
- This paper is a step toward understanding the requirements and limitations of deep-learning-based image filtering systems on small satellites.
- More specifically, we characterize the requirements and constraints of an image processing unit (IPU) deployed on a satellite and outline possible use case scenarios of such a system, each introducing a different degree of resource constraints and intensity.
- We then further analyze the performance of multiple edge devices on these different scenarios and their suitability for possible deployment on one of the Danish Student CubeSat Project's (DISCO) [7] satellites.
- These devices are based on different hardware architectures, ranging from a microcontroller to a tensor processing unit (TPU).

## BACKGROUND
- DISCO project is a research project that focuses on data processing in satellites.
- Survey related work specifically targets data processing in satellites.

## DISCO
- The Danish Student Cubesat Program (DISCO) offers Danish university students the opportunity to design and operate a small satellite and to gain space flight experience.
- DISCO is a collaboration between four universities in Denmark, which will initially launch three student Cubesats into Low Earth Orbit with the first launch scheduled for 2023.
- Initially focused on a long term marine systems and cryosphere monitoring projects, the satellite will supplement ground based observations with remote sensing data, providing both good polar coverage and on demand availability for the projects.
- Students will be able to carry out conventional on-satellite image processing in addition to running machine learning applications for classification, discrimination, and feature identification.

## Related Work
- Neural networks are used to filter data
- Several works have tested the suitability of SoC devices for this purpose
- TPUs have not been explored for this purpose before
- DISCO project would be the first satellite to use them

## REQUIREMENTS
- The requirements and constraints used in this study are based on the DISCO2 Arctic imaging mission use case.
- The power and mass constraints of the edge device to be deployed on the satellite are based on the power and mass budgets developed in the DISCO2 engineering design.
- The satellite design is a modular 3U Cubesat with off-the-shelf modules for attitude control, power, communications, and flight control.
- The power budget values are typical for a 3U earth imaging Cubesat of this type.
- The resulting payload capacity is 1U (10x10x10cm) and 1.3kg and this must include the cameras, module enclosure, optics. and image processing unit (IPU).
- The full list of constraints of the IPU is shown in Table 1.
- The planned orbit of DISCO2 is a solar synchronous polar orbit at 550 km altitude.
- The camera sensor is based on an Alvium 1800 C-2040 and the lens focal length is the maximum that could be considered for the mission. Assuming a 50% overlap between images, i.e., capturing the same land area, this provides a minimum time of 4.42s between consecutive images (  ), which was derived as follows: , where  (ground sample distance) is the spatial resolution of the image,   is the width of image in pixels, ∩  is the overlap between images and  is the orbital velocity of the satellite.
- Taking this latency value as a baseline, we now describe three image processing scenarios with different levels of difficulty based on this Arctic mission's goals.
- Scenario 1: Real-time imaging. Images are taken with a period of 4.42s, as derived above. The inference has to be performed and completed before the next image is captured. This would allow for the results of an inference to be used in decision making for the next image capture.
- Scenario 2: Arctic region imaging. In this scenario, images are taken only while the satellite is over the polar regions relaxing the latency requirements. Images are buffered and inference is performed in the remainder of the orbit, with the IPU available for inference before the subsequent orbit. The capture-inference cycle completes in 5739s.
- Scenario 3: Greenland imaging. The images are taken only while over Greenland further relaxing the latency requirements. As the orbit is solar synchronous with a period of one day. This results in subsequent bursts but only when the swathe passes over Greenland. Images are again buffered and the capture-inference cycle completes in 21600s.

## METHODOLOGY AND SETUP
- We use a modern low-power hardware setup to characterize the performance of the IPU of the DISCO2 satellite.
- Our goal is to characterize the performance of modern low-power hardware for deployment as the IPU of the DISCO2 satellite.

## Devices under Test
- Based on three different hardware architectures, ARM Cortex-M Mirocontroller, based on the STM32H745 chip, has the potential of the lowest power draw, while the least performant of the choices.
- To use this device as an on-board IPU, we can use Tensor-Flow Lite for Microcontrollers [5], a framework designed to allow neural network inference with only 16 KB of memory overhead (core runtime) and without the need for an operating system.
- In addition, the framework also provides support for the use of specialized neural network kernels, such as CMSIS-NN [12] kernels designed specifically for Cortex-M processor cores.
- The X-CUBE-AI [17] framework makes the deployment of the neural networks with TensorFlow Lite for microcontrollers even more accessible. It provides an intuitive GUI for the libraries or code samples based on the provided and trained TensorFlow Lite [2] model.
- Even though this device has support for floating point operations, a quantized model was used, in order to achieve better latency and, more importantly, lower memory footprint, which is a large constraint of this device.
- To simulate the exact performance and power characteristics of a flight computer already available for a potential deployment in DISCO project, OBC-P3, we used only the Cortex-M7 core of the STM32H745 and scaled its clock from 480MHz down to 300MHz.
- Parameters of the OBC-P3 system can be found in the NVIDIA Jetson Nano is a portable SoC composed of powerefficient ARM CPU and NVIDIA GPU designed for embedded systems that require GPU-friendly computations, e.g., image processing, video encoding/decoding, and machine learning tasks.
- It is the only device in our experiments that supports TensorFlow Core. Therefore, the deployment process to this device is equivalent to that of servers or desktops.
- In addition, both of the devices run Linux.

## Metrics
- The metrics to evaluate the suitability of the devices as the onsatellite IPUs are based on the requirements Section 3 outlined.
- Latency is reported in seconds per inference of a sample, where a sample is a whole image of size 4512 x 4512 pixels or 400 tiles of size 224 x 224 pixels.
- We measure the latency of the neural network inference once the data is already in the device's memory.
- Nominal power draw is the average power draw in mW multiplied by the duty cycle, which is the ratio between achieved and required latency for each scenario (outlined in Section 3).
- We use an external appliance for CoralAI TPU and the ARM Cortex-M7 and tegrastats for Jetson Nano to measure this metric.
- Peak power draw is the maximum power draw over the period of inference in mW.
- It is measured using the same tools as in nominal power draw.
- Power consumption is reported per inference of a sample.
- This metric shows the power efficiency of the device and will guide the choice of a more suitable device if multiple devices fulfill the requirements of a particular scenario.

## Workload
- To simulate the imaging scenarios of interest, we use an image classification workload consisting of a 5-class classification problem.
- For this workload, an off-the-shelf MobileNetV1 model [11] pre-trained on the ImageNet dataset [6] was chosen for its strong predictive power and availability in multiple scaling factors, affecting the number of hidden layers in the model.
- The availability of multiple scaling factors is an important factor as the memory of some of the devices is highly limited and can therefore fit only the smallest of the variants.
- We further fine-tuned the model before deployment on the devices using the Flowers dataset [15], containing 3670 color images of size 224 x 224 pixels belonging to 5 different classes.
- While the weights in the model do not affect the inference latency or power required to perform the inference, the model was fine-tuned using this dataset because it mimics one of the possible use cases closely (in number of classes as well as size of the images) and allows us to quantify the effects of the size and precision of the model.
- Since the models running on the Cortex-M7 and CoralAI TPU were quantized, rescaling of images was not needed before inference.
- For Jetson Nano, the pixel values must be rescaled to the range of [0, 1), or, in other words, divided by 255.
- An additional layer was prepended to the model for this process in order to take advantage of the GPU parallelism on Jetson Nano.

## RESULTS
- The results are split into three parts
- Analysis of latency and power draw results with respect to the three scenarios outlined in Section 3
- Impact of quantization on the accuracy of models with various scaling factors
- Impact of batch size on the performance of NVIDIA Jetson Nano

## Scenarios
- Table 5: Latency achieved and power consumption of devices using various scaling factors
- Tables 6 and 7: Nominal and peak power draw of different configurations
- Sections 5.1.1-5.1.3: Suitability of devices for different scenarios based on the results on these tables

## Scenario 1:
- Real-time imaging is the most constrained scenario.
- Therefore, a high degree of specialization is necessary to fulfill the requirements.
- Latency. The pipeline has to achieve a 4.42s latency, which is the spacing between images with 50% overlap.
- There are no passive periods. Therefore the latency of the pipeline has to be lower than that of the imaging for the pipeline not to get backed up.
- There are multiple configurations that achieved the required latency (Table 5).
- These are all the configurations utilizing a TPU and the smallest model running on Jetson Nano with batch size of 64.
- Power draw. Out of the configurations that satisfy the latency requirements for this scenario, only of the TPU configurations fit into the nominal power budget of the satellite (Table 6).
- Even though the Jetson Nano with batch size 64 achieves a latency comparable to the TPUs with a scaling factor of 0.25, the power consumption per inference is more than twice as high.
- Furthermore, the Raspberry Pi with external TPU module does not fit into the power budget when using the largest model, due to exceeding the 5W requirements for peak power draw (Table 7).
- NVIDIA Jetson Nano using the largest model with batch size of 64 also exceeds this peak power requirement.
- Since the peak power draw does not depend on neither the use case scenario nor the duty cycle, this conclusion about peak power draw holds for the rest of the scenarios as well.
- The peak power is in bold when exceeding the power budget.
- Scenario 2: Arctic region imaging.
- By relaxing the constraints and performing the workload equivalent to taking images of only the areas above the arctic polar circle, we see more configurations passing the requirements necessary to perform such a workload.
- Latency. Since there would be passive imaging periods performing the workload for this scenario, the pipeline does not need to be lower than the latency of imaging. It can instead buffer the images and perform the inference at higher latency, given that the pipeline can finish inference of one burst before the next burst of images comes, which corresponds to a total latency of 5739 seconds or 71.74 seconds per image.
- All the configurations except the ones with ARM Cortex-M7 achieve this latency (Table 5).
- Power draw. Because of the large margin between the required and achieved latencies for all configurations passing the latency requirements, the active duty cycle is relatively low. Therefore, the corresponding nominal power draw is well below the required one in all cases (Table 6).
- Latency. The maximum total latency for a burst of images is 21600 seconds, corresponding to 270 seconds per single image. This requirement is easily passed by all of other configurations.
- Power draw. Due to the low active duty cycle, each configuration passes the nominal power draw requirement (Table 6). There is, however, a large gap between the nominal power draw of TPU and the rest of the devices. TPU performs the inference much more efficiently, and its power draw scales more favourably with increasing scaling factors in comparison to the other devices.

## Effect of quantization and scaling factor
- Table 8 shows the accuracy of the model with the various scaling factors before and after quantization to an 8-bit integer
- The model with the smallest scaling factor has a 3% drop in accuracy
- However, this difference is acceptable in our case and is outweighed by the benefits of lower memory footprint and latency, and higher overall efficiency.

## Batch size impact on NVIDIA Jetson Nano
- NVIDIA Jetson Nano is the only device in our evaluation that allows doing inference with a batch size higher than 1.
- The impact of increasing the batch size on latency and power consumption of the inference tapers off after batch size 8.
- Maximizing the batch size can lead to 45.3-74.6% lower latency and 41.7-64.8% lower power consumption.

## DISCUSSION
- TPU shows the most promising results as it is the only device to fulfill the requirements for real-time imaging, which is the most constrained scenario.
- The systolic array architecture is purpose-built to perform fast multiply-accumulate operations in a highly parallel fashion, which allows performing matrix multiplication without the need to load/store intermediate values.
- On the other end of the spectrum was the ARM Cortex-M7 microcontroller. Due to its lack of parallelism, this device could only fulfill the requirements of the least constrained scenario. Furthermore, the microcontroller has a very low amount of memory available even after heavy optimizations, such as quantization of the model and a stripped-down version of the TensorFlow Core.
- The low amount of memory means that the device cannot hold the full-size image in memory. Therefore, in-camera tiling is used, and the results of the operations had to be buffered to storage before the device could perform inference on the saved tiles.
- NVIDIA Jetson Nano can fulfill the requirements of the realtime imaging scenario. However, its nominal power draw exceeds the power budget and therefore buffering, similar to the case of microcontroller, is used to be able to work at latencies higher than those of the imaging.
- Even though the Jetson Nano could match the latency of the devices utilizing a TPU, when doing inference on large batches of image tiles using the smallest model, it only could do so at the cost of a higher power draw.
- Even with a batch size of 64, the Jetson Nano performs inference with power consumption 112-437% higher compared to the devices with TPU.

## CONCLUSION
- Devices that are possible candidates to be deployed on a satellite for different image analysis scenarios have different performance requirements
- The only device that fulfilled these requirements was CoralAI TPU
- ARM Cortex-M7 could only fulfill the requirements of the least constrained scenario due to the low degree of parallelism and limited memory it has
- Even though it provided the lowest peak power draw, the high latency of inference using this device led to nominal power draw higher than any of the other devices
