---
title: "GIT: A Generative Image-to-text Transformer for Vision and Language"
date: 2022-05-27T17:03:38.000Z
author: "Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2205-14100v5.webp" # image path/url
    alt: "GIT: A Generative Image-to-text Transformer for Vision and Language" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.14100)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.14100).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer).

# Abstract
- Design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering
- Simplify the architecture as one image encoder and one text decoder under a single language modeling task
- Scale up the pre-training data and the model size to boost the model performance
- Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin

# Paper Content

## Introduction
- Table 1: Comparison with prior SOTA on image/video captioning and question answering (QA) tasks.
- CIDEr scores are reported for Captioning tasks.
- Image A delta plane is parked on the tarmac at an airport.
- A white marble taj mahal is reflected in a pool.
- A star wars movie poster with a Darth Vader helmet.
- A Marilyn Monroe photo with a black background.
- A person holding a five dollar bill with a picture of abraham lincoln on the front.
- A bowl of chinese food called mapo tofu.
- A poster for the national administrative professionals day is shown.
- A chevron competitive profile matrix is shown on a table.
- A red apple with a green label that says fuji 94131.
- A bag of purina friskies frango adultos cat food.
- curved text table money bill food logo landmark celebrity product tag/photo long text
- A paper that says shakespeare was the son of john shakespeare, an alderman and a successful glover originally from snitterfield in warwickshire, and mary arden.
- character
- A book by o. henry titled el regalo de los reyes magos.
- A pie chart with 28% over 3. 75 on it.
- chart
- A person holding a bottle of bacon bits.
- A baseball player with the blue jays on his jersey is about to hit a ball.
- blurry text occluded text
- Tremendous advances have been made in recent years on vision-language (VL) pre-training, especially based on the large-scale data of image-text pairs, e.g., CLIP (Radford et al., 2021), Florence (Yuan et al., 2021), and SimVLM (Wang et al., 2021b).
- The learned representation greatly boosts the performance on various downstream tasks, such as image captioning (Lin et al., 2014), visual question answering (VQA) (Goyal et al., 2017), and image-text retrieval.
- During pre-training, Masked Language Modeling (MLM) and Image-Text Matching (ITM) tasks have been widely used (Wang et al., 2020;Fang et al., 2021c;Li et al., 2020b;Zhang et al., 2021a;Chen et al., 2020b;Dou et al., 2021;Wang et al., 2021a;Kim et al., 2021).
- However, these losses are different from the downstream tasks, and task-specific adaptation has to be made.
- For example, ITM is removed for image captioning (Wang et al., 2021a;Li et al., 2020b), and an extra randomly initialized multi-layer perceptron is added for VQA (Wang et al., 2021b;Li et al., 2020b).
- To reduce this discrepancy, recent approaches (Cho et al., 2021;Wang et al., 2021b;Yang et al., 2021b;Wang et al., 2022b;Wang et al., 2021a) have attempted to design unified generative models for pre-training, as most VL tasks can be cast as generation problems.
- These approaches typically leverage a multi-modal encoder and a text decoder with careful design on the text input and the text target.
- To further push the frontier of this direction, we present a simple Generative Image-to-text Transformer, named GIT, which consists only of one image encoder and one text decoder.
- The pre-training task is just to map the input image to the entire associated text description with the language modeling objective.
- Despite its simplicity, GIT achieves new state of the arts across numerous challenging benchmarks with a large margin, as summarized in Table 1.

### Network Architecture
- The image encoder is based on the contrastive pre-trained model (Yuan et al., 2021).
- The input is the raw image and the output is a compact 2D feature map, which is flattened into a list of features.
- With an extra linear layer and a layernorm layer, the image features are projected into D dimensions, which are the input to the text decoder.
- We use the image encoder pre-trained with contrastive tasks because recent studies show superior performance with such image encoder, e.g. Yuan et al. (2021); Dou et al. (2021); Alayrac et al. (2022).
- In Sec 4.6 and supplementary materials, we also observe the VL performance boosts significantly with a stronger image encoder.
- This is consistent with the observation in object detection-based approaches, e.g. in Wang et al. (2020); Zhang et al. (2021a).
- The concurrent work of CoCa (Yu et al., 2022) unifies the contrastive task and the generation task. as one pre-training phase.
- Our approach is equivalent to separating the two tasks sequentially: (i) using the contrastive task to pre-train the image encoder followed by (ii) using the generation task to pre-train both the image encoder and text decoder.
- The text decoder is a transformer module to predict the text description.
- The transformer module consists of multiple transformer blocks, each of which is composed of one self-attention layer and one feed-forward layer.
- The text is tokenized and embedded into D dimensions, followed by an addition of the positional encoding and a layernorm layer.
- The image features are concatenated with the text embeddings as the input to the transformer module.
- The text begins with the [BOS] token, and is decoded in an auto-regressive way until the [EOS] token or reaching the maximum steps.
- The seq2seq attention mask as in Fig. 3 is applied such that the text token only depends on the preceding tokens and all image tokens, and image tokens can attend to each other.
- This is different from a unidirectional attention mask, where not every image token can rely on all other image tokens.
- Instead of well initializing the image encoder, we randomly initialize the text decoder.
- This design choice is highly motivated from the experiment studies of Wang et al. (2020), in which the random initialization shows similar performance, compared with the BERT initialization.
- This could be because the BERT initialization cannot understand the image signal, which is critical for VL tasks.
- Without dependency of the initialization, we can easily explore different design choices.

### Pre-training
- For each image-text pair, let I be the image, y i , i ∈ {1, • • • , N } be the text tokens, y 0 be the [BOS] token and y N +1 be the [EOS] token.
- We apply the language modeling (LM) loss to train the model.
- That is, where CE is the cross-entropy loss with label smoothing of 0.1.
- An alternative choice is MLM, which predicts typically 15% of input tokens in each iteration.
- To predict all tokens, we have to run at least 1/0.15 = 6.7 epochs.
- For LM, each iteration can predict all tokens, which is more efficient for large-scale pre-training data.
- In Hu et al. (2021a), the ablation studies also show that LM can achieve better performance with limited epochs.
- In our large-scale training, the number of epoch is only 2 due to computational resource limitation, and thus we choose LM.
- Meanwhile, most of the recent large-scale language models are also based on LM, e.g. Brown et al. (2020); Chowdhery et al. (2022).

### Fine-tuning
- For the image captioning task, as the training data format is the same as that in pre-training, we apply the same LM task to fine-tune our GIT.
- For visual question answering, the question and the ground-truth answer are concatenated as a new special caption during the fine-tuning, but the LM loss is only applied on the answer and the [EOS] tokens.
- During inference, the question is interpreted as the caption prefix and the completed part is the prediction.
- Compared with the existing approaches (Wang et al., 2021a;b;Zhang et al., 2021a;Li et al., 2022b) for VQAv2 (Goyal et al., 2017), our model is generative without pre-defining the candidate answers, even in inference.
- This imposes more challenges as the model has to predict at least two correct tokens: one for the answer and another for [EOS].
- In contrast, the existing work pre-collects the answer candidate, recasts the problem as a classification problem, and only needs to predict once.
- However, considering the benefit of the free-form answer, we choose the generative approach.
- Due to difficulty of the generative model, we observe slightly worse performance on VQAv2 than the discriminative existing work.
- For the scene-text related VQA tasks, existing approaches (Yang et al., 2021c;Hu et al., 2020) typically leverages the OCR engine to generate the scene text and use dynamic pointer network to decide the current output token should be OCR or the general text. Here, our approach depends on no OCR engine, and thus no dynamic pointer network.
- Empirically, we find the model gradually learns how to read the scene text with large-scale pre-training, and our model achieves new SoTA performance on these tasks.
- Our model is not specifically designed for the video domain, but we find our model can also achieve competitive or even new SOTA performance with a simple architecture change.

## Experiments

## Setting
- We collect 0.8B image-text pairs for pre-training
- The image encoder is initialized from the pre-trained contrastive model
- The hidden dimension (D) is 768
- The text decoder consists of 6 randomly-initialized transformer blocks
- The total number of model parameters is 0.7 billion
- The learning rates of the image encoder and the decoder are 1e −5 and 5e −5 , respectively
- The total number of epochs is 2
- During inference, the beam size is 4 and the length penalty (Wu et al., 2016) is 0.6 by default

### Results on Image Captioning and Question Answering
- Our model achieves the new SOTA performance on all these metrics except on COCO Karpathy test.
- On nocaps, compared with CoCa (Yu et al., 2022), our model is much smaller in the model size (0.7B vs 2.1B), but achieves higher performance (123.0 vs 120.6 in CIDEr).
- On Textcaps, our solution outperforms the previous SOTA (TAP Yang et al. (2021c)) by a breakthrough margin (28.5 points in CIDEr), and also surpasses the human performance for the first time.
- For zero/few-shot evaluation as shown in Table 3, our model can significantly benefit from more shots. With 32-shots, our approach is also better than Flamingo.
- On VQA, the evaluation benchmarks include VQAv2 (Goyal et al., 2017), TextVQA (Singh et al., 2019), VizWiz-VQA (Gurari et al., 2018). ST-VQA (Biten et al., 2019), and OCR-VQA (Mishra et al., 2019).
- Before fine-tuning the model, we run an intermediate fine-tuning on the combination of the training data of VQAv2, TextVQA, ST-VQA, OCR-VQA, VizWiz-VQA, Visual Genome QA (Krishna et al., 2016), GQA (Hudson &  Manning, 2019), and OK-VQA (Marino et al., 2019).

## Results on Video Captioning and Question Answering
- The performance is evaluated on MSVD (Chen & Dolan, 2011) with the widely-used splits from Venugopalan et al. (2014), MSRVTT (Xu et al., 2016), YouCook2 (Zhou et al., 2018) (results in supplementary materials.)
- VATEX (Wang et al., 2019b), and TVC (Lei et al., 2020) (results in supplementary materials.)
- On VATEX, the performance is evaluated on both the public test and private test (evaluated on the server).
- Video QA is evaluated on MSVD-QA (Xu et al., 2017;Chen & Dolan, 2011), MSRVTT-QA (Xu et al., 2017;2016), and TGIF-Frame (Jang et al., 2017), which are all open-ended tasks.
- The results are shown in Table 5 and Table 6 for captioning and QA, respectively.
- Although our model is not  (Tang et al., 2021), which relies on model ensemble and additional subtitle input. This is also better than Flamingo (Alayrac et al., 2022) (84.2) with 80B parameters.

## Results on Image Classification
- We fine-tune GIT on ImageNet-1k
- Each category is mapped to a unique class name, and the prediction is correct only if it is exactly matched with the ground-truth label subject to more or fewer whitespaces
- As shown in Table 7, our approach can achieve descent accuracy without pre-defining the vocabulary
- Compared with Florence (Yuan et al., 2021) (same image encoder), our approach is worse in about 1.2 points
- The reason might be similar to the case on VQAv2. That is, the generative approach needs to predict more tokens correctly to make one correct prediction, which increases the difficulty.
- Zero-shot/Few-shot. The result is shown in Table 9.
- With no knowledge of the vocabulary, the pretrained GIT cannot infer the expected vocabulary, and thus the exactly-match accuracy is only 1.93% (in the column of equal). However, if we relax the requirement and take it correct if the prediction contains the ground-truth, the accuracy is 40.88% (in the column of in), which shows the predicted caption can well identify the image content.
- If we have the vocabulary as a prior and limit the output tokens to be within the vocabulary, the accuracy drops to 33.48% (in the column of voc-prior).
- This may suggest the network is less natural to directly predict the category name.
- By fine-tuning the model with only 1 shot or 5 shots per category, we observe that the accuracy is significantly improved. This demonstrates our model can be easily adapted to downstream tasks even with a few training samples.
- With the shot increased from 1 to 5, the gap between voc-prior and the other two columns (equal and in) becomes smaller. This is expected as more shots can be better to guide the network to predict in-vocabulary output.
- Compared with Flamingo, our GIT achieves higher accuracy.

## Results on Scene Text Recognition
- The task aims to read scene text directly from the image
- We evaluate our model in two settings: one is the GIT fine-tuned on TextCaps datasets, and the prediction is considered correct if the output is the exact match to the ground-truth.
- Following the established setup, we evaluate on six standard benchmarks, including ICDAR 2013 (IC13) (Karatzas et al., 2013), ICDAR 2015 (IC15) (Karatzas et al., 2015), IIIT 5K-Words (IIIT) (Mishra et al., 2012), Street View Text (SVT) (Wang et al., 2011), Street View Text-Perspective (SVTP) (Phan et al., 2013), and CUTE80 (CUTE) (Risnumawan et al., 2014). The average accuracy is reported in Table 8.

## Analysis
- To study the trending with data scales, we construct two smaller pre-training datasets: one is the combination of COCO, SBU, CC3M and VG, leading to 4M images or 10M image-text pairs; the other is to further combine CC12M, leading to about 14M images or 20M image-text pairs.
- When pre-training on small-scale datasets, we use 30 epochs rather than 2 epochs as on the 0.8B data.
- For the network structure, we name our model as Huge and replace the image encoder with ViT-B/16 and ViT-L/14 from CLIP Radford et al. (2021) as Base and Large, respectively.
- Fig. 4 shows the results on COCO, TextCaps, and VizWiz-QA.
- On COCO, the base model benefits from 4M to 14M, but the performance drops with 0.8B data.
- The 14M data are more similar to COCO than the majority of the noisy 0.8B data.
- Meanwhile, the Base model with limited capacity may not be able to benefit effectively from large-scale data.
- Similar observations are also reported in Kolesnikov et al. (2020) for ImageNet-1k classification.
- On TextCaps and VizWiz-QA, all model variants benefit significantly from more pre-training data.
- Also, a larger backbone improves more especially with 0.8B data.
- Here, we scale the image encoder. Empirically, we find it is difficult to effectively scale up the text decoder. Preliminary results are shown in Table 10, which shows a larger decoder shows no improvement.
- The reason might be that it is difficult to effectively train with limited amount of text by LM.
- Another plausible reason is that the image encoder is responsible for object recognition, and the decoder is responsible for organizing the object terms in a natural language way.
- The latter task might be easy since most of the descriptions follow similar patterns, e.g. object + verb + subject, and thus a small decoder is enough during end-to-end training.
- Larger decoders increase the learning difficulty, which might degrade the performance.

### Scene text in pre-training data.
- Examines the pre-training dataset to study how many image-text pairs contain scene text
- Estimates that 15% of CC12M and 31% of the downloaded images contain scene text descriptions
- Learns to read the scene text as the training task is to predict the texts

## Conclusion
- GIT is a simple generative model that is trained to map input images to associated text descriptions
- On image/video captioning and question answering tasks, GIT achieves new state-of-the-art performance across numerous benchmarks and surpasses the human performance on TextCaps for the first time
- For the image classification, GIT is applied to predict the label name directly
- Limitations include that the model is pre-trained on large-scale data and that the data are not guaranteed to contain no toxic language
- Although we observe few such instances qualitatively, special care should be taken to deploy the model in practice and more research exploration is required to control the output
- The supplementary materials provide more details on the experiments
