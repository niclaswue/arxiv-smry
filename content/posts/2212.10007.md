---
title: "CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"
date: 2022-12-20T05:48:09.000Z
author: "Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, Bing Xiang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10007v1.webp" # image path/url
    alt: "CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10007)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10007).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/cocomic-code-completion-by-jointly-modeling).

# Abstract
- Pre-trained code models (LM) for code completion are limited in their ability to understand the semantics of other files in the same project
- In this work, we develop a cross-file context finder tool, CCFINDER, that effectively locates and retrieves the most relevant cross-file context
- We propose CoCoMIC, a framework that incorporates cross-file context to learn the in-file and cross-file context jointly on top of pretrained code LMs. CoCoMIC successfully improves the existing code LM with a 19.30% relative increase in exact match and a 15.41% relative increase in identifier matching for code completion when the cross-file context is provided.

# Paper Content

## Introduction

## Preliminaries
- Project entities are code components that constitute the skeleton of software projects; developers frequently import and reuse these entities as cross-file contexts.
- We focus on four types of code components: files, global variables, classes, and functions.
- Further define several types of relations.
- The locale is the entity's source code location within the software project.
- For example, the locale of class entities is defined as file_name.class_name.
- The locale is assigned a unique name according to the specific location of a project entity, so we maintain the one-to-one mapping between each entity and its locale.
- The locale will benefit COCOMIC in two ways: (1) when we construct cross-file context, the locale could efficiently map the relative path of a code snippet to its project entity CCFINDER builds ( §3.2), and (2) it indicates hierarchical relations among project entities and can be used to help model with code completion ( §6.2).
- In-file & Cross-file Context: For an incomplete source file S, we define two types of context: infile and cross-file.
- In-file context represents code snippets included in the current file, i.e., code tokens before the predicting position.
- Cross-file context C represents the relevant code information (i.e., classes, functions) from the same project but not the current file and is imported by the current file.
- Concretely, in-file context refers to the list of code tokens within the source file S until the predicting location, and cross-file context refers to a collection of relevant project entities that might assist with the missing code prediction but are not in S.
- Cross-file Context Finder: CCFINDER
- Our goal is to locate and retrieve the most relevant code snippets within the current project that will assist with the code completion but are not in the current program.
- However, the structure of the software project is inherently complex (Parnas et al., 1985) since the project typically maintains a complicated dependency structure, and distinct components interact with others based on the dependencies.
- We need to design and implement a tool with two main requirements. First, the tool should be able to show a bird's-eye view of the project structure among modules so that, given an incomplete program, it could immediately derive its file-level dependencies. Second, the tool should be able to identify the relevance among concrete code components and efficiently retrieve the detailed code as cross-file context for better code completion.
- Off-the-shelf tools could not completely serve our needs. For example, module dependency analysis tools2,3 can only provide the module interactions while missing the hierarchical details inside each module and cannot directly output the concrete code.
- Therefore, we develop a new tool, CCFINDER, to aggregate cross-file context.

### Project-level Context Graph
- Extracts entities and relations in a given project
- Parses project structure and source files to identify project entities and entity relations
- Builds context graph with a top-down style, starting with a root node for the project and connecting all file nodes
- Nodes link to others within the file-level sub-graph based on the dependencies or scope

### Cross-file Context Retrieval
- Given an incomplete code, we build the context graph of the current project or query the built graph for project details.
- As the project-level context graph already represents the project hierarchies and interactions among code components well, we can safely assume that the closer a graph neighbor to a root node, the more relevant that neighbor should be.
- For example, if the incomplete file imports a class, then the most useful information regarding this class, such as its member functions and the global variables it depends on, should be only 1 or 2 hops away.
- Thus, given the incomplete program, we are able to retrieve a set of project entities from the context graph as the cross-file context.
- The detailed workflow is described in Algorithm 1.
- ReorderN ode(ctx_nodes, P) 12: end for 13: return ctx_nodes
- Specifically, we extract the import statements from the incomplete code file that only imports code snippets within the same project (i.e., local imports).
- We iteratively analyze each import to generate the imported content's relative project path(s), which could be the whole file, global variables, classes, or functions.
- Then, we locate these imported entities in the project-level context graph.
- With the maintained direct mapping between the code snippets and their locales ( §2), we can locate the node given its relative path with O(1) time complexity.
- We use such a node as the root node and retrieve its neighbors within k hops using the depth-first graph search.
- k is a configurable hyperparameter in which increasing k will retrieve a broader context but introduce more redundant information.
- We set k = 2 for our main experiments.
- Finally, once we collect the k-hop neighbors for all import statements, we re-order the nodes, ensuring the nodes from the same source file follow the original code order, to maintain the naturalness (Hindle et al., 2012) of human-written code.

### Input Representation
- The model includes a source code sample and its cross-file context.
- The source code consists of a sequence of tokens, and the cross-file context is a list of entities.
- Each entity is a short piece of code sequence describing the details of that entity, and each locale is the locale of that entity.
- When completing code, the model attends to the representations of the [SUM] tokens for cross-file context.

### Encoding Cross-file Context
- The computational cost of the Transformer model increases exponentially w.r.t. the input length
- For each entity c i , the model f θ will encode its code sequence into one representation h c i ∈ R d h , where d h is the hidden dimension
- COCOMIC takes the hidden state of the last token, [SUM], as the node representation
- Finally, the model will output a list of entity embeddings, H C , representing the retrieved cross-file context

### Modeling In-file and Cross-file Context for Code Completion
- After getting representations of cross-file context, COCOMIC continues to encode the in-file context and train the model to learn both contexts jointly.
- In-file Context COCOMIC utilizes the causal language model setting to support the code completion task, where each token will consider its former texts as in-file context.
- Specifically, the infile context of source code S, at time step t, will be s t = (x 1 , ..., x t−1 ).
- We pass these tokens through the model and get the embeddings of each token to construct the representation of the in-file context.
- Different layers of a Transformer model have been shown to capture different language components (e.g., lower layers learn language syntax or grammar while upper layers capture language semantics).
- We hypothesize that both in-file and cross-file context contribute to forming the understanding of language components.
- Therefore, we fuse the infile and cross-file context at each Transformer layer such that generating the hidden state of the next token will always depend on both contexts.
- At each time step t, for the l-th layer, we first compute the keys and values for cross-file and in-file contexts, using their (l − 1)-th hidden states.
- Then, we concatenate the keys and values from both contexts so that, at time step t, the generating token can jointly attend to them.
- With fused attention at each layer, we will get the comprehensive representation from the model to predict x t .
- We further feed the representation to a classification layer and project it to the most likely token in the vocabulary.
- Finally, we train the model to predict the next code token conditioned jointly on both in-file and cross-file context and compute the negative loglikelihood as the training loss.

### Implementation Details
- Cross-file Context Finder (CCFINDER) uses tree-sitter4 to parse source files.
- Tree-sitter is a widely used source code parser that generates the abstract syntax tree (AST) given a program.
- Each tree-sitter node is composed of type and the corresponding code span in AST.
- CCFINDER will traverse the AST to extract information as described in §3.
- To build the project-level context graph, we analyze the import statements to capture the cross-file dependencies, and our analyzer is built on top of import-dep5 that matches the import statements with the file locations within the same project.
- We include at max 128 project entities as the cross-file context, with the order of prioritizing the most relevant ones, and each entity contains up to 128 tokens.
- The decision of the thresholds is data-driven to ensure the model input covers most of the relevant cross-file context.

### Baselines & Evaluation Metrics
- We consider two variations of the vanilla CodeGen model with in-file context only: zero-shot and finetuned.
- For zero-shot, we directly load the pre-trained CodeGen model and evaluate it on our test dataset.
- For finetuned, we finetune CodeGen on our dataset as an apple-to-apple comparison with our models.
- We also consider a prompting baseline where we prepend the cross-file context to the input sequence and finetune.
- Similar to the configuration of our proposed model, we reserve the first 128 positions of the input for the code tokens from the cross-file context and use the rest positions for the in-file context.
- We measure the identifier match to evaluate whether cross-file context improves the model's ability to predict the right APIs.
- To this end, we extract the identifiers from the model prediction and the ground truth, resulting in two ordered lists of identifiers.
- Then, we compare the predicted identifiers with the ground-truth identifiers and report the results in exact match, precision, and recall.
- Besides, we evaluate perplexity on all the tokens on the test set to study whether adding cross-file context degrades performance when the cross-file context is not explicitly required.

## Results and Analysis
- COCOMIC outperforms all baselines on all metrics with a clear margin
- COCOMIC proposes to encode the code sequence of a node into one single token, enabling the model to incorporate more cross-file contexts while saving the input length
- Adding locales consistently improves COCOMIC across all metrics
- [SUM] token is important for a better representation of cross-file context

## Related Work
- Transformer language models (LMs) are pretrained using unlabeled source code
- Among these efforts, the development of code generation models is noteworthy
- Codex (Chen et al., 2021), PolyCoder (Xu et al., 2022), GPT-J-6B (Wang and Komatsuzaki, 2021), GPT-Neo (Black et al., 2021), GPT-NeoX-20B (Black et al., 2022), CodeGen (Nijkamp et al., 2022), Incoder (Fried et al., 2022), AlphaCode (Li et al., 2022) are some of the significant ones
- Most of these are decoder-only Transformer models that learn to generate code from left to right in an autoregressive manner
- In a concurrent work, Zhou et al. (2022) proposed to retrieve API documentation given a natural language (NL) intent and generate code based on the NL intent and the retrieved documentation
- Our work has the same spirit as we propose to retrieve cross-file context (user-defined classes, functions from other project files) given a source code
- Earlier works (Henninger, 1991;Rosson and Carroll, 1996;Michail, 2001;Ye et al., 2000;Ye and Fischer, 2002;Cubranic and Murphy, 2003;Inoue et al., 2003;Hill and Rideout, 2004;Holmes and Murphy, 2005) in software engineering literature focused on developing tools to extract information from software repositories to help developers complete code fragments
- On the other hand, recent works focus on modeling cross-file information in neural approaches

## Conclusion
- Language models for code completion have achieved great success in recent years
- In this work, we propose COCOMIC, a framework that incorporates both in-file and cross-file context for code completion based on any autoregressive code language models
- For this purpose, we build CCFINDER, a static code analysis tool that builds project-level context graph, and find the most relevant cross-file context based on import statements
- Empirical results show that CCFINDER successfully retrieves 37.11% more relevant contexts that are not in the current file for code completion
- Our best model achieves up to 19.30% improvement over the in-file only baseline
- We further study the effectiveness of various components in COCOMIC
- Our work focuses on Python language as it is widely used and due to the availability of software projects through PyPI
- However, the main concept introduced in our work should be extensible to other languages
- In addition, we focus on project-level context in this work, and a potential extension is to incorporate third party packages
- We leave these as future work.
