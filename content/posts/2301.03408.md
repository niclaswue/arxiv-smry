---
title: "Dynamic Functional Connectivity"
date: 2023-01-09T15:04:12.000Z
author: "Christine Ahrends, Diego Vidaurre"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-03408v1_e8Il2L3Pr.jpg" # image path/url
    alt: "Dynamic Functional Connectivity" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03408)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03408).


# Abstract
- Dynamic functional connectivity refers to the non-instantaneous couplings across timeseries from a set of brain areas
- In this chapter, we provide a hands-on description of a non-exhaustive selection of different methods used to estimate dynamic FC
- We explain, using practical examples, how data should be prepared for dynamic FC analyses and how models of dynamic FC can be evaluated
- We also discuss current developments in the dynamic FC research field, including challenges of reliability and reproducibility

# Paper Content

## Unnamed Section 1
- Time-varying instantaneous FC reflects within-session modulations of the covariance matrix across regions.
- Static FC, as per this definition, only captures instantaneous relationships; that is, within a scanning session, if we permute the time points within a session such that the permutation is the same for all voxels or areas, the static FC estimate would not change.
- But there is other information that would be lost by permuting; this information exists above and beyond the static FC, and that is what we refer to as dynamic FC here.
- Under the umbrella of this definition, there are two different aspects of dynamic FC: time-varying instantaneous FC, and FC in the context of a linear dynamical system that models non-instantaneous aspects of FC.
- For now, we will discuss them conceptually. Later, when we cover the different approaches to model dynamic FC, we will specify which type of information each approach is aiming to model.
- Time-varying instantaneous FC, commonly referred to as just time-varying FC, reflects within-session modulations of the covariance matrix across regions.
- After standardising the signal, the mean is zero and the variance is one across the entire session, but this is not necessarily the case for shorter periods of the signal, where the signal, for example, could transiently have a higher variance.
- This means that covariance and correlation are not exactly equivalent anymore, opening different possibilities in how we model and interpret the data.
- These practical aspects and what they mean conceptually will also be discussed below.
- For example, if the signal contains both types of information, but we use an analysis approach that is focussed on only one, information of the other aspect will necessarily leak into the estimation.
- Different approaches to dynamic FC allow us to address different research questions in complementary ways; see Calhoun et al. (2014); Hutchison et al. (2013); Lurie et al. (2019); Preti et al. (2017) for some examples of applications.

## Preparing data for dynamic FC analysis
- Follow preprocessing guidelines for resting state fMRI, such as the Human Connectome Project's (HCP) resting state preprocessing pipeline (Glasser et al., 2013;Smith, Beckmann, et al., 2013).
- There are a few considerations specific to dynamic FC approaches, which we will outline here.
- For further reference, the issue of preprocessing for dynamic FC analyses has also been tested and discussed in Lydon-Staley et al. (2019) and Vergara et al. (2017).

## Temporal preprocessing
- Most importantly, dynamic approaches are more heavily affected by temporal noise than time-averaged types of analyses.
- With temporal noise, we here refer to any type of artefact that varies over time, e.g., head motion, other physiological artefacts such as cardiac or respiratory artefacts, etc.
- While these artefacts may almost disappear when averaging over timepoints, such as in time-averaged or trial-averaged approaches, they can drastically influence the estimation of dynamic FC (Nalci et al., 2019).
- For instance, if the dominant temporal fluctuations in an fMRI timeseries are due to head motion, a dynamic FC model may use its explanatory power to describe these movement-related variations rather than the more subtle signal fluctuations stemming from neural activity.
- In a state-based model, this may result in one or more states being actually motion states and not "brain" states, while in a continuously-varying FC estimation, each FC estimate may to some extent be biased by motion.
- On the other hand, dynamic FC analyses also suffer from too aggressive clean-up in the time domain, as some of the meaningful temporal variability can be removed along with the temporal artefacts.
- This may be the case, for instance, by applying preprocessing approaches that average over or censor time points, such as motion scrubbing (Power et al., 2012), or when regressing out temporal noise components using a full variance clean-up approach.
- Also global signal regression can affect temporal variability, both positively and negatively, and so its use in preprocessing data for dynamic FC analysis is controversial (Murphy & Fox, 2017).
- The goal in terms of temporal variability when preprocessing fMRI data for dynamic FC analyses should therefore be to remove temporal artefacts while retaining non-artefactual temporal variability as much as possible.
- This may be achieved by non-aggressive temporal preprocessing strategies such as independent component analysis (ICA) in combination with unique variance clean-up of noise-related components (Griffanti et al., 2014).
- An additional consideration as regards temporal variability is temporal filtering. While a relatively lenient high-pass filter can be useful in removing ultra-slow fluctuations, such as scanner drifts, a very narrow filter will restrict the timescale on which dynamic changes in FC can be detected by a model.
- In general, since temporal noise and the meaningful aspect of the signal are not perfectly separable, achieving the right balance can be a difficult task.
- A sensible approach may be to start with a relatively lenient temporal clean-up and to test post-hoc whether the dynamic FC model was affected by known temporal artefacts, such as head motion, and decide whether more aggressive temporal clean-up is necessary (Ciric et al., 2017;Parkes et al., 2018).
- The question of between-subject variability in FC is discussed in more detail in Bijsterbosch et al. (2017).

## Parcellations and timecourse extraction
- Parcelation and method of timecourse extraction greatly affect the estimation of dynamic FC
- In a binary parcellation, parcel timecourses can be extracted as the mean over voxels or the first principal component (PC).
- In a weighted parcellation, timecourses can be extracted using a dual regression approach to obtain individual spatial maps and individual timecourses of parcels.

## Practical Approaches to dynamic FC
- There are many different methods for evaluating dynamic fc, but we focus on methods that are based on second-order statistics.
- This excludes methods based on co-activation maps (CAPs), which are based on first-order information.
- We prioritise a practical understanding of the methods over breadth, so only a subset of representative methods will be discussed and analyzed.
- For each approach, there are several software packages available.
- We present methods and software to estimate single-subject, single-session dynamic fc.

## Continuously-varying estimators
- Methods that produce an estimate of FC per time point or window, such that the parameters that define such estimations vary smoothly in the time axis are referred to as continuously-varying estimators.
- Two kinds of continuously-varying estimators are based on sliding windows and all-data estimators.
- Sliding windows are simpler and much more common.

## Sliding windows
- The most basic approach to evaluate dynamic FC is sliding windows.
- For computational efficiency, the window can be slid by more than one time point.
- A family of variations of this scheme is related to the shape of the window.

## Figure 4 Static and time-varying FC. We refer to second-order statistics of fMRI timeseries, such as correlations between regions of interest (ROIs), as functional connectivity (FC). A) Time-averaged (static) FC can be computed, for instance, by correlating all pairs of ROIs over the entire timeseries. The resulting FC can be illustrated as (ROI x ROI) FC matrix or as FC map in the brain. B) Continuously varying dynamic FC, such as sliding window approaches, estimate FC separately on portions (windows) of the timeseries. Each window has an associated FC matrix that can also be projected into the brain. C) State-based FC models estimate recurring patterns of FC over the timeseries. Each state has an associated FC matrix that can also be projected into the brain. (Abbreviations: ROI -region of interest; FC -functional connectivity; TR -repetition time).
- Inverse covariance matrices (here, inv_mat) are also referred to as precision matrices.
- If the number of regions is comparatively large, we need to regularise the matrix inversion to avoid badly scaled results.
- This is what we did here by adding the identity matrix multiplied by the regularisation constant lambda.
- Other types of regularisation that impose sparsity, by driving some partial correlations to exactly zero, are also possible.
- In this case, the resulting precision matrices can be mapped to a graph where non-zero coefficients relate to edges between two nodes (voxels, areas, or components), and zero coefficients relate to conditional independence between node i and j, given all the rest of the nodes (Cai et al., 2018;Friedman et al., 2008).
- Sliding-window analyses can be very dependent on the choice of the window length, in the sense that too short windows will render very unstable estimates and too long windows will over-smooth the estimation and miss relatively fast changes.
- Although there are data-driven approaches to adaptively optimise the window length, the resulting window length is itself based on assumptions and subject to estimation noise.
- A critical weakness of the sliding window approach is that, except perhaps for very long windows, a large part of the variability observed across windows will inevitably be due to estimation noise.
- This is illustrated in the left panel of Figure 5A, where we show, for a pair of signals, the results of performing a sliding window analysis on data generated as per the above code; the dotted and solid grey lines correspond, respectively, to the ground-truth correlation (i.e. true_C(1,2)) and empirical static correlation (i.e. corr(X(:,1),X(:,2))) -note the large difference between the two due to smoothing, even when using the entire length of the signals.
- For comparison, we generated a second data set where there are two different covariance matrices underlying the generation of the data; the middle section of the time series was generated using one, and the beginning and end of the time series was generated using the other:

## Figure 5 Result of performing a sliding-window (AB) or all-data, flexible time squares (CD) analysis on synthetic data whose ground truth generative process either does not contain a time-varying component (AC), or does contain it (CD).
- Dynamic FC has been put on the spot of criticism
- Different approaches to generate surrogate data and statistical tests have been proposed
- These tend to agree that sliding-window approaches struggle to find genuine time-varying FC in most data

## All-data continuous estimators
- Methods that estimate one correlation coefficient per time point by considering all the data set at once are called "flexible least squares" or "dynamic least squares".
- These methods do not use a predefined window, but the estimate at a given time point still depends on neighbouring time points in an adaptive fashion.
- One such method is flexible least squares (Kalaba & Tesfatsion, 1989), which is implemented for example in the Dynamic BC toolbox (Liao et al., 2014).
- Multivariate approaches that estimate continuously-varying partial correlation matrices also exist based on the same idea (Monti et al., 2014).
- Focussing on just two signals, Figure 5C shows the behaviour of the flexible least squares method on the synthetic data generated above, where there was no actual fluctuation of FC in the underlying generative model of the data.
- As observed, choices with a low amount of regularisation (redder lines) result also in largely varying estimates (even above the 1.0 boundary), while higher amounts of regularisation converge to the empirical, non-varying correlation.
- When there is actual variability in FC, Figure 5D shows that for some choices of the regularisation parameter, the method is able to capture the modulation in the middle section of the time series above and beyond the noise.

## State-based estimators
- state-based estimators assume that the data can be reasonably described using a discrete set of states
- exclusivity: only one state can be active at a given time
- state time courses: together with each state's FC information
- estimation noise: at different levels

## Clustering approaches
- The most common clustering method builds upon sliding window estimates.
- Assuming we have a pool of FC matrices (one per window), the simplest procedure would be to vectorise the off-diagonal elements of the FC matrices, constructing a (no. of windows by pairs of regions) matrix, on which we will then apply a clustering technique such as k-means (Allen et al., 2014) or hierarchical clustering (Yang et al., 2014).
- By doing this, we would assign each window to a different state, which constitutes a categorical state time course.
- Probabilistic alternatives to k-means, such as a mixture of distributions, are also possible, in which case the state time courses will be made of probabilities (that is, per window and state).
- Together with the estimated state time courses, the state FC estimations would correspond to the centre of the clusters.
- The hope of this method is that, even though the individual sliding window estimates are quite noisy, the state FC matrices are made of averaging across many windows and therefore will be less noisy.
- A mathematically more principled version of this approach is based on the use of Riemannian distances between the window FC matrices (instead of Euclidean, as results from vectorising the FC matrices) (Pervaiz et al., 2020).
- For simplicity, we will here illustrate only the simplest (and most common) approach with code.
- Assuming we have already computed sliding-window estimates on data generated using the code above, the following code implements the most basic clustering approach:
- N_windows = total_session_duration -window_length + 1;
- C_unwrapped = zeros(N_windows,p*(p-1)/2);
- for j = 1:N_windows
- Cj = C(:,:,j);
- Cj = Cj(triu(true(p),1));
- C_unwrapped(j,:) = Cj(:);
- end
- K = 4;
- idx = kmeans(C_unwrapped,K);
- state_time_courses = zeros(length(T),K);
- for k = 1:K
- state_time_courses(idx==k,k) = 1;
- end
- We can now get the fractional occupancies (i.e. the proportion of time taken by each state), and display the state time courses as mean(state_time_courses) area(state_time_courses)
- Another clustering approach that does not necessitate the specification of a window length hyperparameter is Leading Eigenvector Dynamics Analysis (LEiDA) (Cabral et al., 2017).
- In short, LEiDA computes functional connectivity estimates at each time point by first computing the signals' instantaneous phase (by means of the Hilbert transform) and then calculating the cosine similarity between each pair of signals.
- Note that even though phase is instantaneous, it still needs information from neighbouring time points. This yields a relatively noisy estimation of functional connectivity per time point.
- To reduce the amount of noise, the first eigenvector of each FC matrix is extracted using a singular value decomposition, producing a (no. of time points by no. of regions) matrix containing FC information.
- We can then apply a k-means algorithm as usual.
- For illustration, this procedure is implemented in the code below:
- Here, eigen_centroids define the states. Each state essentially projects the brain areas into a one-dimensional axis, such that areas close to each other in one extreme of the axis (i.e., having the same sign in eigen_centroids) are in-phase, and areas that belong to opposite extremes of the axis (i.e., having opposite signs) are anti-phase.
- Note that the sign in eigen_centroids is arbitrary, and only the sign relationships between areas are meaningful.
- Overall, clustering approaches are generally simple to implement but they are not generative models, i.e., we do not have a compact set of parameters to sample data or perform statistical testing.
- The hidden Markov model (HMM), discussed in the next section, is a generative model.

## Generative models: Hidden Markov Models (HMMs)
- Generative models are those that specify a full mathematical structure from which we can sample new data sets
- The HMM is a generative model specifically designed to deal with temporal (and other sequential) data
- The HMM is parametrised by a mean vector and a covariance matrix containing FC information
- Having a shared covariance matrix for all states and one mean vector per state; this configuration however does not model changes in FC and will not be discussed here.
- Second, we can have one mean vector and covariance matrix per state, which is the most common approach.
- Third, in order to focus on FC, we can pin the state means to zero and have only a covariance matrix per state (which is equivalent to having a Wishart state distribution)
- The remaining parameters of the HMM are the initial probabilities, i.e., the probability of the trials to commence with a given state; and the transition probability matrix, with elements (j,k) encoding the probability of transitioning from state j to state k.
- The initial probabilities, as well as each of the rows of the transition probability matrix, are modelled as Dirichlet distributions.
- Apart from state exclusivity, and the fact that we use a discrete number of states, the use of a transition probability matrix implies that the HMM builds upon a third assumption: Markovianity.
- In practice, this means that which state is active in the present time point depends on which state was active in the previous time point; or in more rigorous terms, that the state at time point t is conditional independent to all the rest of the time points given t-1 and t+1.
- The HMM can be ported to different data sets, but the state time courses are specific to the data set on which they were estimated.
- To illustrate the use of the HMM and its inference, we will use the HMM- For interpretation purposes, it is important to note that, unlike the sliding window estimates (which are typically correlations matrices), here a FC matrix is a covariance matrix and therefore also conveys information about the variance of the signal.

## Multivariate autoregressive models
- The multivariate autoregressive model is a linear dynamical system that models non-instantaneous aspects of FC
- The autoregressive coefficients share with the precision matrix the property that, for a very large amount of data, the coefficient(s) between variable i and variable j will be exactly zero if they do not hold a direct linear (in this case lagged) relationship
- The MAR model can be used to infer effective connectivity

## ùë°ùë°
- And let us assume we sample data from this system with the following code:
- p = 5;
- total_session_duration = 1000;
- true_A = 0.5*eye(p);
- for j = 1:p-1 true_A(j,j+1) = 0.25;
- true_A(j+1,j) = 0.25;
- end
- X = randn(total_session_duration,p);
- for t = 2:total_session_duration X(t,:) = X(t,:) + X(t-1,:) * true_A;
- end
- Now, if we sample many data sets like this and fit autoregressive models with the code above, we would obtain distributions of estimated parameters as in Figure 6.
- If instead of being able to sample many data sets, we had a limited number of subjects, we could still perform a bootstrapped estimation (where we create pseudo data sets by randomly sampling subjects with repetitions).

## Evaluating dynamic FC models
- Discuss how to validate FC models
- Validate FC models by comparing predictions to actual data
- Use different validation methods
- Use different types of data
- Use different models
- Discuss how to validate FC models
- Validate FC models by comparing predictions to actual data
- Use different validation methods
- Use different types of data
- Use different models

## Testing against null models
- There is a discussion on whether FC fluctuates or is stable over time
- It has been argued that real fMRI data fluctuates over time, but it is difficult to reject the statistical hypothesis that FC is stationary
- Simulation studies also address whether static FC is being driven by transient (dynamic) events or whether the presence of transient events in FC may be driven by static FC

## Comparing models with simulated data
- The basic clustering approach is compared to the HMM and it is found to be less accurate
- The accuracy of the basic clustering approach is correlated with the accuracy of the HMM
- The accuracy of the HMM is correlated with the accuracy of the ground-truth state time courses

## Challenges & perspectives
- Dynamic FC is important for predicting behavioural and clinical traits
- Dynamic FC is reliable and reproducible
- Dynamic FC is a challenge to develop

## Reliability & reproducibility
- Reliability: low test-retest reliability
- Reproducibility: inconsistent canonical patterns across datasets
- Robustness: handles noise well

## Predicting behaviour and individual traits from dynamic FC
- Dynamic FC measures are highly individual and can be used for "fingerprinting" an individual
- Studies using features derived from dynamic FC were able to predict various phenotypes and behavioural measures at moderate to fair accuracy
- One promising avenue are kernel methods which allow working with highdimensional features such as dynamic FC features in a computationally efficient way

## Conclusions
- There is a variety of methods to estimate dynamic FC
- Each with unique advantages but also shortcomings
- Recent years have seen an explosion of dynamic FC studies
- However, methodological rigour is important

## Unnamed Section 2
- The authors use sin(T) to calculate the time courses for voxels in clusters A and B.
- The authors calculate the voxel weights for clusters A and B and show them in Figure 1, top row.

## Unnamed Section 3
- Parcels are divided into voxel clusters based on their timecourse
- Each voxel cluster has a unique timecourse
- The timecourse of each voxel is extracted and used to generate a graph
- The graph is used to determine the parcellation of the data
- Parcels are divided into voxel clusters based on their timecourse
- Each voxel cluster has a unique timecourse
- The timecourse of each voxel is extracted and used to generate a graph
- The graph is used to determine the parcellation of the data

## Unnamed Section 4
- Parcels are divided into voxel clusters based on their timecourse.
- Heterogeneous timecourses in voxel clusters lead to different parcellation results.
- Different parcellation results can be achieved by varying the number of voxel clusters.

## Unnamed Section 5
- The function calculates the sine of a given angle from 0 to 50.5 degrees
- The function calculates the sine of a given angle from 71 to 100 degrees

## Unnamed Section 6
- Mean of yA and yB over time
- Parcel mean over time
- PCA of yA and yB
- ICA with two components: ica_mdl = rica([yA;yB],2)
- Parcel z with voxel weights for each component
- Estimating dynamic FC between these two components more closely resembles the empirical dynamic FC between the functional clusters

## Unnamed Section 7
- Figure 3 shows the dynamic FC between two cluster timecourses extracted from binary parcels with suboptimal parcel boundaries (row 2 and 3) vs. extracted from weighted, data-driven parcels (row 4).
- The dynamic FC between two cluster timecourses extracted from binary parcels with suboptimal parcel boundaries (row 2 and 3) is worse than extracted from weighted, data-driven parcels (row 4).

## Unnamed Section 8
- Continuous estimators (like sliding windows) don't pool information across subjects
- State-based estimators can have states that are shared across subjects

## Unnamed Section 9
- d] is the eigenvalue of the matrix C
- ~,i] is the maximum eigenvalue of the diagonal matrix diag(d)
- Eigenvectors(t,:) is the eigenvector corresponding to the ith eigenvalue
- idx,eigen_centroids] is the index of the ith eigenvector in Eigenvectors
- kmeans() finds the centroids of the k-means clustering algorithm

## Unnamed Section 10
- The toolbox can be added to the Matlab path
- The toolbox can be used to estimate an HMM model
- The HMM model can be used to predict the behavior of a data set
- The HMM model can be used to predict the behavior of a subject

## Unnamed Section 11
- The autoregressive coefficients of a linear dynamical system can be estimated over 1000 data sets.
- The autoregressive coefficients are a function of the past data.
- The autoregressive coefficients can be used to predict future data.

## Unnamed Section 12
- Figure 7 shows real fMRI timecourses and their corresponding sliding window correlations.
- Figure 7 shows that FC is actually dynamic, not temporally changing estimates of FC.

## Unnamed Section 13
- beta(1,1) = beta(1,2) = beta(1,3)
- weights = beta(1,4:n_areas+1)
- tc_real_flip = tc_real
- tc_arr = zeros(T, n_areas)
- tc_arr(1,:) = tc_real_flip(:,randi(T-1))
- rand_t = randperm(T-1)
- res_mu = mean(residuals,2)
- res_Sigma = cov(residuals)
- noise = (mvnrnd(res_mu,res_Sigma,T-1))
- for i = 2:T
- tc_arr(i,:) = c' + (weights*tc_arr(i-1,:)') + ... noise(:,rand_t(i-1))
- end
- Finally, we again compute the sliding window correlations in the surrogate timeseries and compare them to the sliding window correlations of the real fMRI data we computed earlier:
- window_length = 200
- arr_dFC_tmp = zeros(n_areas, n_areas, T -window_length + 1)
- for t = 1:T -window_length + 1
- arr_dFC_tmp(:,:,t) = corr(tc_arr(t:t + window_length-1,:));
- arr_dFC(1,t) = arr_dFC_tmp(1,2,t);
- end

## Unnamed Section 14
- When generating surrogate data from a model that preserves the autocorrelation structure of the real fMRI data, the null hypothesis cannot easily be rejected.
- The surrogate data generated from a 1-lag autoregressive randomisation model is similar to the real data.
- The autocorrelation structure of the real fMRI data affects the quality of the surrogate data.

## Unnamed Section 15
- The standard clustering approach is not as accurate as the HMM when estimating the state transitions sampled from a state-based generative model.
- The HMM is faster at estimating the state transitions sampled from a state-based generative model.

## Unnamed Section 16
- tested this question across datasets from different scanning sites
- using two different state-based approaches, they could show that the same basic connectivity patterns emerge in all datasets

## Unnamed Section 17
