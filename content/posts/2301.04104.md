---
title: "Mastering Diverse Domains through World Models"
date: 2023-01-10T18:12:16.000Z
author: "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04104v1.webp" # image path/url
    alt: "Mastering Diverse Domains through World Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04104)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04104).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/mastering-diverse-domains-through-world).

# Abstract
- General intelligence requires solving tasks across many domains
- DreamerV3 is a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters
- Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula

# Paper Content

## Introduction
- Reinforcement learning has enabled computers to solve individual tasks through interaction, such as surpassing humans in the games of Go and Dota 1,2
- Applying algorithms to new application domains, for example from board games to video games or robotics tasks, requires expert knowledge and computational resources for tuning the algorithms 3
- This brittleness also hinders scaling to large models that are expensive to tune
- Different domains pose unique learning challenges that have prompted specialized algorithms, such as for continuous control 4,5 , sparse rewards 6,7 , image inputs 8,9 , and spatial environments 10,11
- Creating a general algorithm that learns to master new domains out of the box-without tuning-would overcome the barrier of expert knowledge and open up reinforcement learning to a wide range of practical applications
- We present DreamerV3, a general and scalable algorithm that masters a wide range of domains with fixed hyperparameters, outperforming specialized algorithms
- DreamerV3 learns a world model 12,13,14 from experience for rich perception and imagination training
- The algorithm consists of 3 neural networks: the world model predicts future outcomes of potential actions, the critic judges the value of each situation, and the actor learns to reach valuable situations
- We enable learning across domains with fixed hyperparameters by transforming signal magnitudes and through robust normalization techniques
- To provide practical guidelines for solving new challenges, we investigate the scaling behavior of DreamerV3. Notably, we demonstrate that increasing the model size of DreamerV3 monotonically improves both its final performance and data-efficiency.

## DreamerV3
- The DreamerV3 algorithm consists of 3 neural networks-the world model, the critic, and the actor-that are trained concurrently from replayed experience without sharing gradients
- To succeed across domains, these components need to accommodate different signal magnitudes and robustly balance terms in their objectives
- This is challenging as we are not only targeting similar tasks within the same domain but aim to learn across different domains with fixed hyperparameters
- We find that combining KL balancing and free bits enables the world model to learn without tuning, and scaling down large returns without amplifying small returns allows a fixed policy entropy regularizer

### Symlog Predictions
- Reconstructing inputs and predicting rewards and values can be challenging because their scale can vary across domains.
- Predicting large targets using a squared loss can lead to divergence whereas absolute and Huber losses 18 stagnate learning.
- On the other hand, normalizing targets based on running statistics 19 introduces non-stationarity into the optimization.
- We suggest symlog predictions as a simple solution to this dilemma.
- For this, a neural network f (x, θ) with inputs x and parameters θ learns to predict a transformed version of its targets y.
- To read out predictions ŷ of the network, we apply the inverse transformation: As shown in Figure 4, using the logarithm as transformation would not allow us to predict targets that take on negative values.
- Therefore, we choose a function from the bi-symmetric logarithmic family 20 that we name symlog as the transformation with the symexp function as its inverse: The symlog function compresses the magnitudes of both large positive and negative values.
- Unlike the logarithm, it is symmetric around the origin while preserving the input sign.
- This allows the optimization process to quickly move the network predictions to large values when needed.
- Symlog approximates the identity around the origin so that it does not affect learning of targets that are already small enough.
- For critic learning, a more involved transformation has previously been proposed 21 , which we found less effective on average across domains.
- DreamerV3 uses symlog predictions in the decoder, the reward predictor, and the critic.
- It also squashes inputs to the encoder using the symlog function.
- Despite its simplicity, this approach robustly and quickly learns across a diverse range of environments.

### World Model Learning
- The world model learns compact representations of sensory inputs through autoencoding
- The world model predicts future representations and rewards for potential actions
- We implement the world model as a Recurrent State-Space Model (RSSM)
- First, an encoder maps sensory inputs x t to stochastic representations z t .
- Then, a sequence model with recurrent state h t predicts the sequence of these representations given past actions a t−1 .
- The concatenation of h t and z t forms the model state from which we predict rewards r t and episode continuation flags c t ∈ {0, 1} and reconstruct the inputs to ensure informative representations: Sequence model:
- Figure 5 visualizes long-term video predictions of the world world.
- The encoder and decoder use convolutional neural networks (CNN) for visual inputs and multi-layer perceptrons (MLPs) for low-dimensional inputs.
- The dynamics, reward, and continue predictors are also MLPs.
- Given a sequence batch of inputs x 1:T , actions a 1:T , rewards r 1:T , and continuation flags c 1:T , the world model parameters φ are optimized end-to-end to minimize the prediction loss L pred , the dynamics loss L dyn , and the representation loss L rep with corresponding loss weights β pred = 1, β dyn = 0.5, β rep = 0.1.

### Actor Critic Learning
- The actor and critic neural networks learn behaviors purely from abstract sequences predicted by the world model
- During environment interaction, we select actions by sampling from the actor network without lookahead planning
- The actor aims to maximize the expected return R t . = ∞ τ =0 γ τ r t+τ with a discount factor γ = 0.997 for each model state
- To consider rewards beyond the prediction horizon T = 16, the critic learns to predict the return of each state under the current actor behavior
- Actor: Starting from representations of replayed inputs, the dynamics predictor and actor produce a sequence of imagined model states s 1:T , actions a 1:T , rewards r 1:T , and continuation flags c 1:T
- To train the critic, we symlog transform the targets R λ t and then twohot encode them into a soft label for the softmax distribution produced by the critic
- Twohot encoding is a generalization of onehot encoding to continuous values
- It produces a vector of length |B| where all elements are 0 except for the two entries closest to the encoded continuous number, at positions k and k + 1. These two entries sum up to 1, with more weight given to the entry that is closer to the encoded number
- Given twohot encoded targets y t = sg(twohot(symlog(R λ t ))), where sg(•) stops the gradient, the critic minimizes the categorical cross entropy loss for classification with soft targets
- We found discrete regression for the critic to accelerate learning especially in environments with sparse rewards, likely because of their bimodal reward and return distributions
- We use the same discrete regression approach for the reward predictor of the world model
- Because the critic regresses targets that depend on its own predictions, we stabilize learning by regularizing the critic towards predicting the outputs of an exponentially moving average of its own parameters
- We further noticed that the randomly initialized reward predictor and critic networks at the start of training can result in large predicted rewards that can delay the onset of learning
- We initialize the output weights of the reward predictor and critic to zeros, which effectively alleviates the problem and accelerates early learning

## Results
- DreamerV3 outperforms all previous algorithms on 4 of the 7 domains it was evaluated on
- DreamerV3 scales well to larger models
- DreamerV3 is able to learn successful behaviors quickly and efficiently

## Previous Work
- PPO 19 is a widely used algorithm and requires relatively little tuning
- SAC 38 is a popular choice for continuous control and leverages experience replay for higher data-efficiency
- MuZero 34 plans using a value prediction model and has achieved high performance at the cost of complex algorithmic components
- Gato 62 fits one large model to expert demonstrations of multiple tasks, but is only applicable to tasks where expert data is available
- Minecraft has been a focus of recent reinforcement learning research
- With MALMO 63, Microsoft released a free version of the popular game for research purposes
- MineRL 15 offers several competition environments, which we rely on as the basis for our experiments
- MineDojo 64 provides a large catalog of tasks with sparse rewards and language descriptions
- The yearly MineRL competition supports agents in exploring and learning meaningful skills through a diverse human dataset

## Conclusion
- DreamerV3 is a general and scalable reinforcement learning algorithm that masters a wide range of domains with fixed hyperparameters
- To achieve this, we systematically address varying signal magnitudes and instabilities in all of its components
- DreamerV3 succeeds across 7 benchmarks and establishes a new state-of-the-art on continuous control from states and images, on BSuite, and on Crafter
- Moreover, DreamerV3 learns successfully in 3D environments that require spatial and temporal reasoning, outperforming IMPALA in DMLab tasks using 130 times fewer interactions and being the first algorithm to obtain diamonds in Minecraft end-to-end from sparse rewards
- Finally, we demonstrate that the final performance and data-efficiency of DreamerV3 improve monotonically as a function of model size
