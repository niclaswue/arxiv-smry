---
title: "Mastering Diverse Domains through World Models"
date: 2023-01-10T18:12:16.000Z
author: "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-04104v1.webp" # image path/url
    alt: "Mastering Diverse Domains through World Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04104)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04104).


# Abstract
- General intelligence requires solving tasks across many domains
- DreamerV3 is a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters
- Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula

# Paper Content

## Introduction
- Reinforcement learning has enabled computers to solve individual tasks through interaction, such as surpassing humans in the games of Go and Dota 1,2
- Applying algorithms to new application domains, for example from board games to video games or robotics tasks, requires expert knowledge and computational resources for tuning the algorithms 3
- This brittleness also hinders scaling to large models that are expensive to tune
- Different domains pose unique learning challenges that have prompted specialized algorithms, such as for continuous control 4,5 , sparse rewards 6,7 , image inputs 8,9 , and spatial environments 10,11
- Creating a general algorithm that learns to master new domains out of the box-without tuning-would overcome the barrier of expert knowledge and open up reinforcement learning to a wide range of practical applications
- We present DreamerV3, a general and scalable algorithm that masters a wide range of domains with fixed hyperparameters, outperforming specialized algorithms
- DreamerV3 learns a world model 12,13,14 from experience for rich perception and imagination training
- The algorithm consists of 3 neural networks: the world model predicts future outcomes of potential actions, the critic judges the value of each situation, and the actor learns to reach valuable situations
- We enable learning across domains with fixed hyperparameters by transforming signal magnitudes and through robust normalization techniques
- To provide practical guidelines for solving new challenges, we investigate the scaling behavior of DreamerV3. Notably, we demonstrate that increasing the model size of DreamerV3 monotonically improves both its final performance and data-efficiency.

## DreamerV3
- The DreamerV3 algorithm consists of three neural networks-the world model, the critic, and the actor-that are trained concurrently from replayed experience without sharing gradients
- This is challenging as we are not only targeting similar tasks within the same domain but aim to learn across different domains with fixed hyperparameters
- We find that combining KL balancing and free bits enables the world model to learn without tuning, and scaling down large returns without amplifying small returns allows a fixed policy entropy regularizer

### Symlog Predictions
- Reconstructing inputs and predicting rewards and values can be challenging because their scale can vary across domains
- Predicting large targets using a squared loss can lead to divergence whereas absolute and Huber losses 18 stagnate learning
- On the other hand, normalizing targets based on running statistics 19 introduces non-stationarity into the optimization
- We suggest symlog predictions as a simple solution to this dilemma
- For this, a neural network f (x, θ) with inputs x and parameters θ learns to predict a transformed version of its targets y. To read out predictions ŷ of the network, we apply the inverse transformation: As shown in Figure 4, using the logarithm as transformation would not allow us to predict targets that take on negative values. Therefore, we choose a function from the bi-symmetric logarithmic family 20 that we name symlog as the transformation with the symexp function as its inverse
- The symlog function compresses the magnitudes of both large positive and negative values
- Unlike the logarithm, it is symmetric around the origin while preserving the input sign. This allows the optimization process to quickly move the network predictions to large values when needed
- Symlog approximates the identity around the origin so that it does not affect learning of targets that are already small enough
- For critic learning, a more involved transformation has previously been proposed 21 , which we found less effective on average across domains
- DreamerV3 uses symlog predictions in the decoder, the reward predictor, and the critic. It also squashes inputs to the encoder using the symlog function
- Despite its simplicity, this approach robustly and quickly learns across a diverse range of environments

### World Model Learning
- The world model learns compact representations of sensory inputs through autoencoding
- The world model predicts future representations and rewards for potential actions
- We implement the world model as a Recurrent State-Space Model (RSSM)
- First, an encoder maps sensory inputs x t to stochastic representations z t
- Then, a sequence model with recurrent state h t predicts the sequence of these representations given past actions a t−1
- The concatenation of h t and z t forms the model state from which we predict rewards r t and episode continuation flags c t ∈ {0, 1}
- Reconstruct the inputs to ensure informative representations: Sequence model: Figure 5 visualizes long-term video predictions of the world
- The encoder and decoder use convolutional neural networks (CNN) for visual inputs and multi-layer perceptrons (MLPs) for low-dimensional inputs
- The dynamics, reward, and continue predictors are also MLPs
- The representations are sampled from a vector of softmax distributions and we take straight-through  gradients through the sampling step
- Given a sequence batch of inputs x 1:T , actions a 1:T , rewards r 1:T , and continuation flags c 1:T , the world model parameters φ are optimized end-to-end to minimize the prediction loss L pred , the dynamics loss L dyn , and the representation loss L rep with corresponding loss weights β pred = 1, β dyn = 0.5, β rep = 0.1
- The prediction loss trains the decoder and reward predictor via the symlog loss and the continue predictor via binary classification loss
- The dynamics loss trains the sequence model to predict the next representation by minimizing the KL divergence between the predictor p φ (z t | h t ) and the next stochastic representation q φ (z t | h t , x t ).
- The representation loss trains the representations to become more predictable if the dynamics cannot predict their distribution, allowing us to use a factorized dynamics predictor for fast sampling when training the actor critic.
- Further model details and hyperparameters are summarized in Table W.1

### Actor Critic Learning
- The actor and critic neural networks learn behaviors purely from abstract sequences predicted by the world model
- During environment interaction, we select actions by sampling from the actor network without lookahead planning
- The actor and critic operate on model states s t . = {h t , z t } and thus benefit from the Markovian representations learned by the world model
- To consider rewards beyond the prediction horizon T = 16, the critic learns to predict the return of each state under the current actor behavior
- Actor: Starting from representations of replayed inputs, the dynamics predictor and actor produce a sequence of imagined model states s 1:T , actions a 1:T , rewards r 1:T , and continuation flags c 1:T . To estimate returns that consider rewards beyond the prediction horizon, we compute bootstrapped λ-returns that integrate the predicted rewards and values
- Critic Learning A simple choice for the critic loss function would be to regress the λ-returns via squared error or symlog predictions. However, the critic predicts the expected value of a potentially widespread return distribution, which can slow down learning. We choose a discrete regression approach for learning the critic based on twohot encoded targets
- For this, we transform returns using the symlog function and discretize the resulting range into a sequence B of K = 255 equally spaced buckets b i . The critic network outputs a softmax distribution p ψ (b i | s t ) over the buckets and its output is formed as the expected bucket value under this distribution. Importantly, the critic can predict any continuous value in the interval because its expected bucket value can fall between the buckets
- To train the critic, we symlog transform the targets R λ t and then twohot encode them into a soft label for the softmax distribution produced by the critic. Twohot encoding is a generalization of onehot encoding to continuous values. It produces a vector of length |B| where all elements are 0 except for the two entries closest to the encoded continuous number, at positions k and k + 1. These two entries sum up to 1, with more weight given to the entry that is closer to the encoded number
- Given twohot encoded targets y t = sg(twohot(symlog(R λ t ))), where sg(•) stops the gradient, the critic minimizes the categorical cross entropy loss for classification with soft targets
- We found discrete regression for the critic to accelerate learning especially in environments with sparse rewards, likely because of their bimodal reward and return distributions. We use the same discrete regression approach for the reward predictor of the world model. Because the critic regresses targets that depend on its own predictions, we stabilize learning by regularizing the critic towards predicting the outputs of an exponentially moving average of its own parameters. This is similar to target networks used previously in reinforcement learning
- We further noticed that the randomly initialized reward predictor and critic networks at the start of training can result in large predicted rewards that can delay the onset of learning. We initialize the output weights of the reward predictor and critic to zeros, which effectively alleviates the problem and accelerates early learning.
- The actor network learns to choose actions that maximize returns while ensuring sufficient exploration through an entropy regularizer
- Ideally, we would like the policy to explore quickly in the absence of nearby returns without sacrificing final performance under dense returns. To stabilize the scale of returns, we normalize them using moving statistics.
- For tasks with dense rewards, one can simply divide returns by their standard deviation, similar to previous work. However, when rewards are sparse, the return standard deviation is generally small and this approach would amplify the noise contained in near-zero returns, resulting in an overly deterministic policy that fails to explore. Therefore, we propose propose to scale down large returns without scaling up small returns. We implement this idea by dividing returns by their scale S, for which we discuss multiple choices below, but only if they exceed a minimum threshold of 1. This simple change is the key to allowing a single entropy scale η = 3 • 10 −4 across dense and sparse rewards

## Results
- DreamerV3 outperforms all previous algorithms on 4 of the 7 domains
- DreamerV3 sets a new state-of-the-art on the Atari 100k benchmark
- DreamerV3 outperforms the top model-free algorithms on the Atari 200M benchmark
- DreamerV3 scales well to larger models

## Previous Work
- PPO 19 is a widely used algorithm
- SAC 38 is a popular choice for continuous control
- MuZero 34 plans using a value prediction model
- Gato 62 fits one large model to expert demonstrations of multiple tasks
- Minecraft has been a focus of recent reinforcement learning research
- MineRL 15 offers several competition environments
- MineDojo 64 provides a large catalog of tasks with sparse rewards
- VPT 16 trained an agent to play Minecraft through behavioral cloning of expert data collected by contractors
- DreamerV3 learns to collect diamonds in 17 GPU days from sparse rewards and without human data.

## Conclusion
- DreamerV3 is a general and scalable reinforcement learning algorithm that masters a wide range of domains with fixed hyperparameters
- To achieve this, we systematically address varying signal magnitudes and instabilities in all of its components
- DreamerV3 succeeds across 7 benchmarks and establishes a new state-of-the-art on continuous control from states and images, on BSuite, and on Crafter
- Moreover, DreamerV3 learns successfully in 3D environments that require spatial and temporal reasoning, outperforming IMPALA in DMLab tasks using 130 times fewer interactions and being the first algorithm to obtain diamonds in Minecraft end-to-end from sparse rewards
- Finally, we demonstrate that the final performance and data-efficiency of DreamerV3 improve monotonically as a function of model size

## Appendices

## C Summary of Differences
- DreamerV3 builds on the DreamerV2 algorithm
- The main changes that were applied were symlog predictions, world model regularization and policy regularization
- DreamerV3 was tuned to perform well across both the visual control suite and Atari 200M
- Hyperparameters were not adjusted further

### Target Regularizers
- We experimented with constrained optimization for the world model and policy objectives
- We found combining this approach with limits on the allowed regularizer scales to perform well for the world model
- For the actor, choosing a target randomness of 40%-where 0% corresponds to the most deterministic and 100% to the most random policy-learns robustly across domains but prevents the policy from converging to top scores in tasks that require speed or precision and slows down exploration under sparse rewards.

## D Ablation Curves

### World Model Ablations
- NoFreeBits: Use KL balancing but no free bits
- NoKLBalance: Use free bits but no KL balancing
- NoObsSymlog: This ablation removes the symlog encoding of inputs to the world model
- TargetKL: Target a KL value of 3.5 nats on average over the replay buffer
- KLScale: Limit the KL scale to the range [10 −3 , 1.0]

### Critic Ablations
- RewardNorm instead of normalizing rewards
- Normalize rewards by dividing them by a running standard deviation and clipping them beyond a magnitude of 10
- ContRegression using MSE symlog predictions for the reward and value heads
- SqrtTransform using two-hot discrete regression with the asymmetric square root transformation introduced by R2D2 21 and used in MuZero 34
- SlowTarget instead of using the fast critic for computing returns and training it towards the slow critic

### Actor Ablations
- NoDenomMax normalizes returns by the range between 5-95%
- AdvantageStd normalizes returns by the standard deviation
- ReturnStd normalizes returns by their standard deviation and their return variance
- TargetEntropy adjusts the regularizer strength to target a policy randomness of 40% on average across imagined states
- Multiplicatively, instead of additively, adjusting the regularizer strength allows the scale to quickly move across orders of magnitude

## F Minecraft Environment
- Minecraft is a popular video game with over 100 million monthly active users
- Minecraft features a procedurally generated 3D world of different biomes
- The world consists of 1 meter sized blocks that the player and break and place
- There are about 30 different creatures that the player can interact and fight with
- From gathered resources, the player can use 379 recipes to craft new items and progress through the technology tree
- Environment: We built the Minecraft Diamond environment on top of MineRL to define a flat categorical action space and fix issues we discovered with the original environments via human play testing
- Rewards: We follow the same sparse reward structure of the MineRL competition environment that rewards 12 milestones leading up to the diamond, namely collecting the items log, plank, stick, crafting table, wooden pickaxe, cobblestone, stone pickaxe, iron ore, furnace, iron ingot, iron pickaxe, and diamond
- Inputs: The sensory inputs include the 64 × 64 × 3 RGB first-person camera image, the inventory counts as a vector with one entry for each of the game's over 400 items, the vector of maximum inventory counts since episode begin to tell the agent which milestones it has already achieved, a one-hot vector indicating the equipped item, and scalar inputs for the health, hunger, and breath levels
- MineRL environment provides a dictionary action space and delegates choosing a simple action space to the user
- Break Speed Multiplier: We follow Kanitscheider et al. in allowing the agent to break blocks in fewer time steps

## G Minecraft Item Rates
