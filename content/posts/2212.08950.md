---
title: "Beyond the C: Retargetable Decompilation using Neural Machine Translation"
date: 2022-12-17T20:45:59.000Z
author: "Iman Hosseini, Brendan Dolan-Gavitt"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08950v1.webp" # image path/url
    alt: "Beyond the C: Retargetable Decompilation using Neural Machine Translation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08950)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08950).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/beyond-the-c-retargetable-decompilation-using).

# Abstract
- The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software.
- Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation.
- Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code.
- In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages.
- We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages.
- We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation.

# Paper Content

## I. INTRODUCTION
- Decompilation is the problem of reversing the compilation process.
- Decompilers created in this way are expensive to develop and, for this reason, all binary decompilers we are aware of target the C programming language.
- Neural machine translation (NMT) is a new approach to decompilation that aims to leverage neural networks to mitigate some of the drawbacks of traditional methods, by treating the decompilation problem as a translation task and training on a large number of assembly/source pairs.
- Although these methods offer the promise of cheaply generating decompilers for diverse languages automatically, to date this promise has gone unrealized: the neural decompilers referenced above all target C.
- We argue that this is, in part, because although at first they appear to be language agnostic, they in fact rely on domain knowledge and tools that may be difficult to come by for arbitrary languages: integration with the compiler (e.g., a custom compiler pass or plugin), parsing/lexing the high level source code or the assembly (dependence on language or CPU architecture), requiring awareness of assembly instruction semantics or types (dependence on CPU architecture), lifting the binary to an intermediate language (dependence on lifter and that intermediate language), working with Abstract Syntax Trees (dependence on that language/AST format).

## II. BACKGROUND

## A. Challenges of Decompilation
- Decompilation of binary code is a difficult problem
- First, even basic problems like distinguishing code from data and recovering a control flow graph from a binary executable are hard
- The problem of accurately disassembling a binary executable is sometimes known as "reassembleable disassembly"
- We will not dwell too much on the problem of low-level disassembly in this paper, however, and will assume that we can recover assembly code in a form similar to what the compiler produces (i.e., with labels intact and data distinguished from code)
- Because we are primarily interested in source language retargetability, we consider in this paper only the familiar (64-bit) x86 architecture, but we expect our techniques to work equally well on other architectures
- x86 is also the best-supported architecture for traditional decompilers, allowing more direct comparisons.
- Beyond the challenge of disassembly, decompilation from assembly is still hard
- Assembly code is much lower level than any modern programming language, and a great deal of information is discarded during the compilation process
- For example, variable names and high-level types are lost, and optimizing compilers can entirely omit code if it can be proved to be unreachable (e.g., guarded by an always-false conditional)
- Compilers are also free to reorder and transform code significantly as long as semantics are preserved, and so can apply transformations like changing integer division into an equivalent multiplication
- Such transformations can be very difficult to invert without resorting to hand-crafted patterns specific to each compiler
- Finally, compilers may add significant amounts of code to the program that do not correspond to anything the programmer wrote. This code typically is used to implement runtime support needed by higher-level language features, such as exceptions, bounds checks, etc.
- If the decompiler is not aware of such automatically generated code, it may include it in the decompiled output, even though it is not helpful to human analysts.
- These shortcomings can be seen in Figure 1, which shows (from left to right) a small function in Go, a portion of its compiled assembly, and the result of decompilation with Ghidra.

## B. Neural Machine Translation
- Neural networks are used to translate text
- Transformer architecture is a type of neural network used in NMT
- The encoder and decoder are connected in a model that is trained using a loss function
- Gradient descent is used to minimize the loss function

## C. Neural Decompilation
- Researchers have been studying the application of NMT methods to decompilation
- These works have focused on the C language and used Recurrent Neural Network (RNN) architectures as their sequence to sequence model
- There is more recent work [13] which uses transformer [35] models, a more recent sequence to sequence model which has enjoyed great success in the field of Natural Language Processing (NLP)

## D. Classical vs. Neural Decompilation
- Traditional decompilers apply classic program analyses such as control flow recovery, data flow analysis, etc.
- Neural decompilation treats the problem as one of neural machine translation, and uses deep neural networks to learn a mapping between assembly and source code from many training samples.
- Because the neural networks on which they are based are not interpretable, neural decompilers are opaque and offer no guarantees about the correctness of the output.
- This means that when they make mistakes, it can be difficult (or impossible) to diagnose what went wrong or to fix them.
- In addition, whereas a traditional decompiler can refuse to provide a decompilation when it encounters some construct it cannot handle well, neural decompilers will always provide some output, which may be nonsensical when the input is very different from code in its training set.
- These disadvantages are balanced by some unique benefits of NMT-based decompilation.
- First, because it needs only training data (which can be easily obtained with a compiler) and GPU time, neural decompilation holds the promise of removing the need for handcrafted patterns designed by experts and refined through great effort and significant amounts of developer time (e.g., the Hex-Rays decompiler has been under active development for 18 years).
- In principle, this also means that neural decompilation can easily support new architectures or source languages as long as a compiler and programs in the source language are available; by contrast, Hex-Rays only supports four main CPU architectures (x86, ARM, MIPS, and PowerPC) and a single source language (C).
- Second, because they are trained on source code written by humans, neural decompilers will typically generate code that looks more natural and is free of artifacts like GOTO statements that plague traditional decompilers [39].
- Finally, particularly on modern GPUs, decompilation using NMT techniques can be much faster than a traditional decompiler.

## E. Computer Languages vs. Human Languages
- The parameter η measures the difference in the number of tokens between two languages
- The parameter η varies greatly between human language translation and decompilation
- There is also a large difference in the standard deviation for each, suggesting that decompilation is a distinctively different domain compared to NMT for human languages

## III. DESIGN
- High-level design of decompiler, BTC (Beyond the C): how we prepare the training data, what model architecture we use, and how we train the model.
- Formulates the problem of decompilation as a text-to-text translation, looking at both the source and machine code as a sequence of tokens and just that, with no source parsing, CPU instruction awareness or indication to the transformer about what a token means.

## A. Model and Data
- To train the model, we need pairs of source code and assembly code snippets.
- At function level, functions are a natural decomposition of a program, which mitigates the attribution problem.
- Extracting pairs of source and assembly code snippets at function level using CFI directives is a straightforward process.

## B. Preprocessing
- Once we have generated assembly files from the source files, we go over the assembly files and extract the functions from the assembly file.
- In each function there are directives specifying source files and lines in that source file which correspond to the function.
- Before emitting the pair of assembly and source text, for each of the source lines corresponding to that function, we do some minimal preprocessing:
- Normalize whitespace (tabs, spaces),
- Remove comments,
- Replace string literals with "STR".
- Even this small amount of preprocessing is a departure from our purpose of having no language dependence, since these constructs (whitespace, comments, and string literals) may have slighting different syntax depending on the source language.
- We make this compromise because transformer models have quadratic (n 2 ) complexity with respect to the input sequence, so preprocessing allows us to include larger functions in our training data.
- Luckily, the implementation for this preprocessing is quite simple (compared to, say, building an AST) and handles the four languages in our evaluation in only 166 lines of Python.

## C. Tokenization
- Extract and preprocess function pairs
- Tokenize snippets
- Use byte pair encoding to tokenize
- Use high-level and low-level tokenizers

## IV. IMPLEMENTATION A. Training Data
- C-Small: collected competitive programming and interview style problems (e.g., leetcode2, Project Euler3, and UVa4)
- Go-Small: collected general algorithm implementations and relatively small utility programs
- Fortran-Small: collected realworld programs solving computational problems (a Couette flow solver, ODE solvers, etc.)
- OCaml-Small: collected 119 packages of various sizes
- Large Datasets: gathered two large datasets for C and OCaml.
- Compiling a large number of C programs is more challenging, because there is no standardized build system used by all C projects. To solve this, we use packages included in Debian Linux, which can be automatically built in a uniform way.

## B. Training the Transformer
- For model training, Facebook's fairseq toolkit is used with its standard transformer encoder/decoder architecture
- The model parameters (encoder layers, attention heads, etc.) can be seen in Table II
- The only per-language parameters are the maximum sequence lengths, which were set based on the maximum observed in the dataset for each language, and the vocabulary sizes, which can be seen in Table I
- To batch the data, a fairseq option is used to specify a maximum number of tokens in a batch; fairseq then batches samples so that the number of tokens in each batch is close to that maximum value
- This helped us keep GPU utilization high given that we had samples covering a wide range of token lengths
- To take advantage of the larger amount of data for C and OCaml, two larger models are trained, with twice as many encoder and decoder layers and a larger embedding dimension; however, to fit within the memory limit of the GPU, the number of attention heads is reduced from 16 to 8

## C. BTC VSCode extension
- The extension consists of 146 lines of TypeScript code that extract the functions in an open ".s" file, run the decompilation model using an external Python script, and then render the results in a side panel within VSCode
- The AED for samples in our held-out test set for each model are shown in Table III
- For comparison, Katz et al. achieved an average edit distance of 0.70 over snippets which were an order of magnitude shorter than our samples.

## B. Effect of Dataset and Model Size
- The model improved its performance when trained on a larger dataset
- There are limits to how good the accuracy gets with a bigger model
- The model improved its performance with the 2M C dataset, but not with the 700K OCaml dataset

## C. Translation Performance
- The average translation speed is 10 functions per second on a V100 GPU
- There is room to pair BTC with more costly techniques that could fix up our candidate sketches

## D. Tokenization
- The BPE tokenization is less efficient than character-level tokenization, so the model was limited to functions less than 6,000 tokens.
- The BPE model is the Fortran-S model trained on the full Fortran dataset.
- We note that the reduced test set, which consists of smaller functions, is also likely to be somewhat easier to decompile, so the small improvement seen here is not likely to be significant.

## E. Qualitative Observations
- The decompilers produced by the four different models were able to produce a general structure of the original code.
- Major features like loops and conditionals correspond fairly well to the original code.
- However, the models often make small but semantically meaningful mistakes.
- In the C example, BTC is penalized for using printf instead of puts, for not storing the string in a separate variable (p), and for checking for EOF instead of 1 as the return value from scanf.

## F. Quality of Training Data
- Many issues can adversely affect the quality of training data
- Our function extractor uses ".loc" directives in assembly to attribute source lines to assembly procedures
- In a language with a simple structure like Fortran, each function will be represented in the source code in a contiguous set of lines, and so the loc directives can be used to match to that region
- In more complex languages, however, this mapping may not be straightforward
- For example, we may we have anonymous functions which perforate the otherwise connected regions, the effect of such problems with matching source code and assembly also depend on how frequent these constructs happen
- OCaml turned out to be the most problematic with lowest quality of training data; upon inspection, many of the training examples are not actually complete functions (or even syntactically valid due to mistakes in extraction)
- These problems are also likely to carry over to other functional languages like Haskell, in which functions are first class citizens and lambdas are used frequently
- However, even C is prone to such issues, as features like preprocessor macros can break the simple mapping between source and assembly represented in the debug information
- Neural Decompilation has not been previously attempted with any language except C, and it is clear that adapting it to tackle new languages will be more difficult for some languages more than others

## VI. RELATED WORK
- Research on reverse engineering binary code has been conducted
- There are different methods for decompiling binary code, including a formal specification
- Neural decompilation has been used in the past to decompile C code, but there is still work to be done
- There is a need for a decompiler that can be used on real-world code

## VII. LIMITATIONS AND FUTURE WORK
- Traditional decompilers operate with a completely different approach which consists of multiple stages of parsing and usually intermediate-languages like GHIDRA's pcode.
- They rely on hardcoded idioms, and data-flow and type analysis tailored for C.
- Although they can generate hard-to-understand code, traditional decompilers have the advantage that unlike neural networks, they are interpretable.
- Interpretability is a major drawback in deep learning methods and has been an active field of research [21].
- Given the black-box nature of deep networks, these methods cannot guarantee semantic correctness and suffer from lower semantic accuracy.
- However, there are techniques that can enhance the accuracy of neural decompilers.
- Schulte et al [29] use evolutionary algorithms to mutate a code sketch and compile it until the generated object code is byte-equivalent with the input.
- Our approach can be used for the sketch generation of their method.
- Another approach is to delegate some error handling to the human agent, as we suggest in the Evaluation section: many of the errors are the network getting a constant or operator wrong, which can be readily resolved by a human agent with access to the assembly code or other tools.
- Advanced data normalization, like the canonicalization scheme described by Katz et al. [18] can also enhance accuracy if it can be automated.
- A limitation specific to BTC is dependence on the debug info, the CFI directives emitted by the compiler to let the assembler generate DWARF information.
- The quality of implementation of these information and DWARF support varies across different compilers and may even be buggy.
- As we note in Section V-F, progress in neural decompilation may depend on improving the quality of the source to assembly mapping provided by compilers.
- Finally, all NMT methods suffer from limitations on length of input sequences supported.
- Previous methods based on RNNs were severely limited and could only handle small snippets of code, this might be one of the reasons that no work had tackled any language besides C until now, as Figure 2 shows.
- Other languages produce more verbose assembly code that would exacerbate this problem, shorter snippets also suffer more from the attribution problem and would make it harder for the model to learn larger constructs.
- We predict that this limitation will be alleviated as NMT methods in general and learning hardware gets more advanced.

## VIII. CONCLUSION
- Neural decompilation can provide a path to retargetable decompilers for different programming languages
- BTC does not rely on compiler customization, complex parsing, or even specifying keywords or operators of the programming language
- Despite the lack of any explicit knowledge about the source languages or CPU architectures baked in, we saw that the model can replicate many of the features of the language
