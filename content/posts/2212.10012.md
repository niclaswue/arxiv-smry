---
title: "Language Modeling with Latent Situations"
date: 2022-12-20T05:59:42.000Z
author: "Belinda Z. Li, Maxwell Nye, Jacob Andreas"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10012v1.webp" # image path/url
    alt: "Language Modeling with Latent Situations" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10012)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10012).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/language-modeling-with-latent-situations).

# Abstract
- Language models often generate incoherent outputs
- We introduce SituationSupervision, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states
- SituationSupervision has two components: an auxiliary situation modeling task that trains models to predict state representations in context, and a latent state inference procedure that imputes these states from partially annotated training data
- SituationSupervision can be applied to both fine-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text)
- In both cases, SituationSupervision requires only a small number of state annotations to produce major coherence improvements (between 4-11%), showing that standard LMs can be sample-efficiently trained to model not just language but the situations it describes.

# Paper Content

## Introduction
- Recent years have seen dramatic improvements in the quality of text generated by neural language models (LMs).
- Nevertheless, even the best LMs still suffer from failures of semantic coherence.
- Samples from LMs refer to entities that have not yet been mentioned, assert contradictory facts, or describe impossible sequences of events (Marcus and Davis, 2020).
- This paper introduces SITUATIONSUPER-VISION, a family of methods for efficiently mitigating incoherent language generation by adapting pre-trained LMs to explicitly model the situations they describe by tracking the properties and relations of entities in generated text.
- The core of the Sam unzipped the suitcase.
- The suitcase is open.
- She threw it away.
- The keyboard is broken.
- The proposed approach is an auxiliary situation modeling task that trains LMs to predict textual representations of entity state jointly with target text.
- However, for most generation tasks, state information must be manually annotated and is costly to collect.
- To make this auxiliary situation modeling task practical, we additionally introduce a semisupervised procedure for inferring entity states in unannotated text, making it possible to perform auxiliary situation modeling with very small numbers of initial state annotations.
- Modern LMs can be specialized to new tasks in a variety of ways, including fine-tuning their parameters and modifying their prompts.
- We develop versions of SITUATIONSUPERVISION suitable for both adaptation methods.
- For fine-tuned models, we introduce an auxiliary state prediction loss that encourages models' hidden representations to encode state variables.
- For prompted models, we introduce a scratchpad approach that instructs models to generate explicit textual descriptions of world states prior to generating output text.
- Both approaches ultimately yield ordinary LMs, compatible with standard pre-training and decoding procedures.
- We evaluate SITUATIONSUPERVISION on two challenging text generation tasks: the TextWorld (TW) task of generating acceptable next actions in a text-adventure game (Côté et al., 2018), and the TRIP task of evaluating commonsense physical plausibility of short (5-sentence) stories (Storks et al., 2021).
- In experiments on fine-tuned BART LMs (Lewis et al., 2020), applying SITUATION-SUPERVISION with 500 seed state annotations reduces coherence errors by 4% on TW and 7% on TRIP.
- In experiments on prompted GPT-3 models (Brown et al., 2020), 12 seed state annotations reduce coherence errors by 8.2% on TW and 20 seed state annotations reduce errors by 7.6% on TRIP.
- In both cases, it is far more sample-efficient to provide state annotations for existing training samples than to augment training data with additional text-only samples: in fine-tuned models, SIT-UATIONSUPERVISION with 500 state annotations can perform comparably to training on 9000 more text-only sentences, while in prompted models, devoting a fixed token budget to state annotations rather than additional text samples yields a coherence improvement of up to 10 points.
- Additional experiments characterize the ingredients of a good state representation, showing that training LMs to predict causally relevant state variables is essential for good performance.
- Because the latent state inference objective favors state representations that improve LM predictions, SIT-UATIONSUPERVISION discovers these variables automatically, sometimes improving on human-designed state representations.

## Background and Related Work
- A language model (LM) encodes a distribution p(T | T ) over texts T given contexts T
- Today, most LMs are implemented as deep neural networks trained on massive text corpora (Brown et al., 2020)
- Sampling from them produces naturalistic text that often resembles human-generated language
- However, LM generation is prone to several failure modes, including generation of text that is incoherent, untruthful, or unreliable (Zhou et al., 2021;Maynez et al., 2020;Martindale et al., 2019)
- Past work has shown that some of these behaviors stem from models' failure to build good representations, both of entities' default properties (Onoe et al., 2021) and state changes in context (Zellers et al., 2021)
- Humans' ability to avoid these failure modes, and to generate truthful and coherent text, is generally understood to rest upon explicit mental representations of the situations that language communicates
- The nature and structure of these representations remains an ongoing topic of research in linguistics and cognitive science, but existing theories broadly agree that language users maintain explicit beliefs about the properties of and relations among entities mentioned in a discourse, updating these beliefs in response to new observations or new information conveyed in language (e.g. Kratzer, 2007;Zwaan and Pecher, 2012)
- These representational theories suggest that language models p(T | T ) might also benefit from explicit modeling of situation state
- Given an input text T , such a model would begin by representing the situation S described by T . Following Barwise and Perry (1981), the situations we consider in this paper are not complete descriptions of possible worlds, but instead just sets of facts that are known or inferable about salient entities in a discourse. Examples, with facts expressed as sentences in natural language, are shown in Fig. 1 and Fig. 2
- Having inferred S from T , a language model may condition on it when sampling T from p(T | S, T ). Past work has proposed a number of language generation models that explicitly model the state of the world, primarily by developing specialized prediction architectures that maintain internal state representations (Henaff et al., 2016;Gupta and Durrett, 2019) or interact with outside simulation engines (Liu et al., 2022)
- While effective, these approaches come at a cost-requiring complex training data (Mishra et al., 2018), limiting models to narrow, pre-defined domains, and generally precluding the large-scale (text-only) pretraining responsible for many of the greatest successes of current LMs
- The main question this paper seeks to answer is whether the benefits of explicit world modeling may be obtained entirely within the language modeling paradigm itself, without the need for specialized model architectures or large amounts of specialized supervision
- We do so by adapting pre-trained LMs to better represent situations S. There are two standard frameworks for LM adaptation. In smaller models, which are generally adapted by fine-tuning of model parameters, we develop auxiliary loss functions that encourage models' hidden states to contain the information required to generate textual descriptions of state. In larger models, which can also be prompted by pre-pending a task description or set of examples to the input context, we develop prompts that induce models to generate textual state descriptions in LM output itself
- Our work builds on a large body of work that uses auxiliary prediction tasks to shape model representations, notably work using "scaffold" decoders to shape model representations of syntax (Swayamdipta et al., 2018;Wilcox et al., 2019), and and "scratchpad" or "chain-of-thought" approaches to perform intermediate computations in models' output spaces (Camburu et al., 2018;Nye et al., 2021;Wei et al., 2022)
- In §3, we show how to adapt both techniques for a new class of open-ended text generation problems.
- Adapting LMs with auxiliary prediction tasks requires a source of data for auxiliary supervision. This kind of supervision is uniquely difficult to obtain for open-ended generation tasks. But the probabilistic framing described above makes it natural to formulate language modeling with explicit situations as a latent variable problem. At training time, we may use context T and targets T to guide inference of the unknown S from which T was predicted. Once inferred, these states supervise the representation-building model that predicts S from T alone.

## Auxiliary Situation Modeling
- We assume access to a pre-trained LM and two sources of supervision: a dataset X U of text examples of the form (T, T ), and a smaller dataset X A of examples (T, S, T ) annotated with textual situation descriptions S.
- Our full training data X is thus X U ∪ X A .
- As depicted in Fig. 2, we take these state descriptions to consist of declarative sentences about the properties and relations of entities that are relevant to the text being generated.
- In this section, we describe two auxiliary prediction schemes that use these annotations to improve the LM's ability to model the conditional text distribution p(T | T ).

### Situation Modeling for Fine-tuning
- an auxiliary loss D S|T (distinct from the original decoder) which is trained to predict state representations S from the encoded context E(T );
- a loss function L(S, E(T )) which is used to optimize the parameters of D S|T ;
- a loss function L(S, E(T )) which is used to optimize the parameters of D ;
- a loss function L(S, E(T )) which is used to optimize the parameters of E ;
- a loss function L(S, E(T )) which is used to optimize the parameters of D S|T .
- an auxiliary loss D S|T (distinct from the original decoder) which is trained to predict state representations S from the encoded context E(T );- a loss function L(S, E(T )) which is used to optimize the parameters of D ;- a loss function L(S, E(T )) which is used to optimize the parameters of E ;- a loss function L(S, E(T )) which is used to optimize the parameters of D S|T .
- an auxiliary loss D S|T (distinct from the original decoder) which is trained to predict state representations S from the encoded context E(T );
- a loss function L(S, E(T )) which is used to optimize the parameters of D ;
- a loss function L(S, E(T )) which is used to optimize the parameters of E ;
- a loss function L(S, E(T )) which is used to optimize the parameters of D S|T .

### Prompting
- In TW, we used 25 sentences (3 stories) in P.
- In TRIP, we used 80 sentences (16 stories) in P.
- When evaluating latent supervision, we held out state annotations on 13 sentences (2 stories) in TW, and 60 sentences (12 stories) in TRIP.
- Due to budget restrictions, we were only able to run each prompting experiment once.
- uation modeling where all passages are fully annotated with state (rows 4,8) dramatically improves performance compared to a text-only baseline (rows 1,5) in both domains.
- In TW, we see a ∼ 6.5% improvement in generation coherence,4 while in TRIP, we see a ∼ 11% improvement to accuracy of coherence judgments.
- Next, we examine the setting where certain state annotations are missing from the prompt, comparing SITUATIONSUPERVISION with latent situation prediction (rows 3,7) against SITUATIONSUPERVI-SION with only auxiliary situation modeling (rows 2,6).
- We find hat incorporating generated latent states into the prompt helps performance on both TW and TRIP, by 7.1% and 8.9% respectively.

### Situation Prediction for Prompting
- The approach described above is general.
- In LMs with very large numbers of parameters, it might be costly to apply (or we may risk over-fitting if the fine-tuning dataset is too small).
- Thus, the second approach we describe is based on prompting models to build better state representations.
- We build on the observation in recent work that prompts can induce models to build better task representations by writing contents of these representations to output: generating, then conditioning on, textual representations of useful intermediate variables.
- To induce language models to output textual state descriptions, we construct prompts with three components: a task description D, a set of task demonstrations ("training set") X , and an input context T pred .
- The training set can include both unannotated and annotated examples: unannotated examples are sequences T i , T i , while annotated examples are sequences T i , S i , T i .
- Formally, we construct a prompt string: where: with • denoting string concatenation.
- When predicting (or scor-ing) a sentence T pred in context, we first prompt the model to generate a state representation S pred , then score T pred conditional on T pred , S pred , and the entire preceding context.
- The bottom portion of Fig. 2 shows a concrete example from the TRIP domain.

## Latent State Inference
- §3 describes methods for state supervision, which require ground-truth state annotations
- §4 describes how to obtain state annotations automatically, without the need for large amounts of annotation
- The techniques described in §4 can be used to train scaffold decoders or to design intermediate state representations for multi-step text generation

### Latent State Inference for Fine-Tuning
- Intuitively, a good state representation is one that is both predictable from context, and useful for predicting subsequent text.
- To guide inference of states for auxiliary prediction, we introduce another encoder-decoder into the model of §3.1: one which attempts to predict T from S.
- This model now has two pathways for predicting T : one that uses encoder representations to predict it directly from T , and another which generates textual state descriptions S from decoder representations, then uses these to predict T .
- We train this model's parameters and infer missing states that maximize probability of next sentences under both pathways, using information from both T and T to infer situations S, then using these to directly supervise the encoder.
- Formally, we optimize the complete likelihood: Eq. ( 5) extends auxiliary fine-tuning by concurrently training an encoder-decoder M T |S,T to model p(T | S, T ).
- To optimize this objective, we initialize θ E , θ D , θ D S|T using Eq. ( 3), and θ T |S by fine-tuning to convergence on X A .
- We then perform coordinate ascent ("hard EM") by alternating between:
- 1. E-step: Set Ŝ ≈ arg max S p(S | T )p(T | S) for X U by sampling from p(S | T ), then reranking according to p(S | T )p(T | S).
- 2. M-step: Using the new Ŝ, train Θ to maximize Eq. ( 5).
- Rather than training to convergence, we perform SGD on Eq. ( 5) for five epochs.
- As in auxiliary fine-tuning, E is shared the p(T | T ) and p(S | T ).

### Latent State Inference for Prompting
- Work on few-shot prompting consistently finds benefits from adding extra examples to prompts (Brown et al., 2020).
- As in §4.1, we produce extra examples for a seed prompt by finding situation descriptions S that are predictable from T and improve prediction of T on unannotated examples.
- We may do so using a very similar procedure to the one in §4.1: now we choose prompts (but not model parameters) to maximize: then add these newly annotated examples to the prompt (which we may do during both training and evaluation).
- Algorithmically, we iterate incrementally over unannotated examples X A : for each context-sentence pair (T, T ) in X U by prompting the LM with [D • P A • T ], then reranking the candidate states according to Once all examples in X U have been annotated and added to P A , we prompt with auxiliary supervision for each context in the evaluation set using
- The algorithm produces a set of auxiliary examples that improve prediction of the target task on unannotated examples.
- The auxiliary examples are added to the prompt and used to provide supplementary supervision during the evaluation phase.

## Experimental Setup

## Experiments

## Analysis
- The choice of state is important
- Because it is procedurally generated, the TW environment is able to provide detailed groundtruth state annotations for every entity that might be mentioned in text.
- All experiments described above use situation representations that include only a subset of entities and properties: namely (1) only those that are already known (i.e. those have been seen before), (2) only those that are causally relevant (i.e. those that, if changed, would induce a different distribution over next sentences), and (3) only those that are causally relevant to the currently accessible items (in this case, the old key, but not the chest).
- In this section, we explore the consequences of including various components of the state. Specifically, we train with auxiliary supervision using the three different choices of state: the full state, the known state (facts that satisfy condition (1)), and the relevant known state (facts that satisfy both conditions (1) and ( 2)).
- Results are shown in Table 3. We find that the training with the full state is not significantly better than simply training without state reranking examples.
- By using our handcrafted states, we were able to achieve a much higher accuracy than using the original states.

## Conclusion
- Effective generation of coherent text requires reasoning about the world that text describes.
- In this work, we use entity states as auxiliary supervision to improve LMs ability to perform this reasoning under both fine-tuning and prompting.
- We find that when either annotation budget (for fine-tuning) or context window size (for prompting) are limited, it is more sample-and token-efficient to increase the amount of state supervision rather than text-only supervision.
- However, since state annotations are harder to collect, we introduce latent supervision algorithms for sample-efficiently improving LM generation coherence, and demonstrate improvements in two domains.
- Our results point to a potentially broad role for semantic supervision in LM training and prompting-even small amounts can yield large coherence improvements.
