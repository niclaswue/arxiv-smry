---
title: "Inseq: An Interpretability Toolkit for Sequence Generation Models"
date: 2023-02-27T16:45:50.000Z
author: "Gabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van der Wal"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2302-13942v1.webp" # image path/url
    alt: "Inseq: An Interpretability Toolkit for Sequence Generation Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2302.13942)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2302.13942).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/inseq-an-interpretability-toolkit-for).

# Abstract
- Past work in natural language processing interpretability focused mainly on classification tasks and overlooked generation settings.
- Inseq is a Python library to make interpretability analyses of sequence generation models easier.
- Inseq can be used to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2.
- Inseq's extensible interface supports cutting-edge techniques and can drive advances in explainable natural language generation.

# Paper Content

## Introduction
- Recent years have seen an increase in studies and tools to improve understanding of neural language models.
- Feature attribution methods are used to quantify the importance of input tokens in relation to models' inner processing and final predictions.
- Feature attribution methods have mainly been applied to classification settings.
- There is relatively little interest in the more convoluted mechanisms underlying generation.
- Inseq is a Python library to democratize access to interpretability analyses of generative language models.

## Related work
- Feature Attribution for Sequence Generation has mainly focused on Machine Translation
- Bahdanau et al. (2015) showed how attention weights of neural MT models encode interpretable alignment patterns
- Tools for NLP Interpretability exist, such as LIT and Ecco

## Design
- Combines sequence models from Transformers and attribution methods from Captum
- Currently only supports text-based tasks, but modular design would enable other frameworks and modalities
- Optional dependencies include Datasets and Rich

### Guiding principles
- Inseq should support interpretability analyses of a broad range of sequence generation models.
- Inseq should prioritize the inclusion of new, cutting-edge methods.
- Inseq should provide an optimized interface for a wide range of use cases.

### Feature attribution and post-processing
- Inseq provides a simple interface to apply feature attribution techniques for sequence generation tasks
- Categorizes methods into three groups: gradient-based, internals-based and perturbation-based
- Supports attention weight attribution and Discretized Integrated Gradients method
- Allows for importance attribution of custom intermediate model layers
- Source and target-side attribution available
- Post-processing of attribution outputs available

### Customizing generation and attribution
- Inseq generates target tokens using Transformers and attributes them step by step
- Users can create and register custom step functions
- Step functions can be used as attribution targets
- Allows users to answer questions like "What inputs drive the prediction of y rather than ŷ?"

### Usability features
- Library provides batching capabilities to attribute entire datasets
- Allows specifying start and end position for attribution process
- API to attribute single examples or entire datasets from command line
- Attribution outputs can be saved and loaded in JSON format
- Attributions can be visualized and exported as HTML files
- Supports attribution of large models using quantization

### Gender bias in machine translation
- Investigated gender bias in MT models
- Used Inseq to study social biases embedded in models
- MT study could benefit from explainability techniques
- Used Turkish to English MarianMT model
- 49 Turkish occupation terms verified by native speaker
- Used Integrated Gradients, Gradients, and Input x Gradient to compute different metrics
- Correlated metrics with gender distribution
- Found strong gender bias, with "she" chosen only 5 times
- Low correlation between pronoun probability and gender stereotype
- Weaker correlation for Integrated Gradients
- Attribution scores for occupation tokens showed significant correlations with labor statistics
- Used contrastive approach to uncover gender bias

### Identifying factual knowledge in gpt-2 layers with contrastive attribution
- Used layer attribution method to locate factual knowledge in GPT-2 1.5B
- Results closely match those of original authors, providing evidence of how attribution methods can be used to identify salient network components and guide model editing

## Conclusion
- Introduced Inseq, a toolkit for interpreting sequence generation models
- Aimed at tasks such as machine translation, code synthesis and dialogue generation
- Goal of identifying unwanted biases and interesting phenomena in model predictions
- Plan to provide continued support and explore further developments
- Not suitable for high-risk and user-facing contexts
- Encourage users to critically approach results and validate effectiveness
- Assumed a simplified concept of binary gender for evaluation
- Encourage consideration of non-binary gender and different marginalized groups
- Measuring bias in language models is complex
- Demonstrate how attribution methods can be used for exploring social biases
- Hierarchy of models and attribution methods
- Example of gender translation pair
- Correlated ranking of occupations with US labor statistics
- Plan to expand core functionality by adding support for wider range of attribution methods
- Feature importance and next-step probability extraction and visualization
- Computing and visualizing source and targetside attributions
- Estimating causal importance of GPT-2 XL layers
- Average GPT-2 XL Gradient × Layer Activation scores
- Contrastive attribution of true vs false answer
- Aggregation and plotting of results
- Example code to contrastively attribute factual statements
- Visualization of contrastive attribution scores
