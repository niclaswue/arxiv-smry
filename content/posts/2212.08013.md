---
title: "FlexiViT: One Model for All Patch Sizes"
date: 2022-12-15T18:18:38.000Z
author: "Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08013v1.webp" # image path/url
    alt: "FlexiViT: One Model for All Patch Sizes" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08013)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08013).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/flexivit-one-model-for-all-patch-sizes).

# Abstract
- Vision Transformers convert images to sequences by slicing them into patches
- The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model.
- In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time.
- We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup.

# Paper Content

## Introduction
- Vision Transformers (ViTs) cut images into nonoverlapping patches and perform all computations on tokens created from these patches.
- Patchification has unlocked new capabilities, such as (random) dropping of image patch tokens [10,20,44,53,61], adding specialized tokens for new tasks [54,56] or mixing image tokens with tokens from other modalities [1,38,64].
- Despite the importance of patchification for ViT models, the role of the patch size has received little attention.
- While the original ViT paper [15] works with three patch sizes (32×32, 16×16, and 14×14 pixels), many follow-up works fix the patch size at 16×16 pixels [54,55,65].
- In this work, we show that the patch size provides a simple and effective lever to change the compute and predictive performance of a model, without changing model parametrization.
- For example, a ViT-B/8 model achieves 85.6% top-1 accuracy on ImageNet1k with 156 GFLOPs and 85 M parameters, while a ViT-B/32 model achieves only 79.1% accuracy with 8.6 GFLOPs and 87 M parameters.
- Despite the major difference in performance and compute, these models have essentially the same parametrization. However, standard ViT models perform well only at the patch size that they have been trained at.
- Tuning the patch size therefore requires complete re-training of the model.
- To overcome this limitation, we propose FlexiViT, a flexible ViT which matches or outperforms standard fixed-patch ViTs across a wide range of patch sizes with no added cost.
- To train FlexiViT, we randomize the patch size during training, and resize the positional and patch embedding parameters adaptively for each patch size, as shown in Figure 1.
- These simple modifications are already sufficient for strong performance, but we also propose a optimized resizing operation and a training procedure based on knowledge distillation which achieves even better results.
- We demonstrate the efficiency of FlexiViT models in many downstream tasks, such as image classification, transfer learning, panoptic and semantic segmentation, imagetext retrieval and open-world recognition, and provide a general recipe for flexifying existing ViT-based training setups.
- Furthermore, we show that flexibility of the backbone, i.e. strong performance across patch sizes, is often preserved even after fine-tuning with a fixed patch size.
- We leverage this observation to perform resource-efficient transfer learning: we finetune the model cheaply with a large patch size, but then deploy it with a small patch size for strong downstream performance.
- We further show that flexible patch size can be used to accelerate pre-training.

## Related work
- Several recent works explore improving ViT's efficiency by exploiting patchification.
- Some suggest removing tokens, either in randomized [20] or structured [10] fashion throughout training. Others aim to quantify a token's importance and remove the least important ones, during [44,61] or after [53] training.
- Trained a cascade of Transformers using increasing number of tokens to allow early exiting during inference.
- Conversely, we always keep all tokens and do not discard any information.

## Making ViT flexible
- Standard ViT models are not flexible
- The FlexiViT model is more flexible
- The FlexiViT model can be trained using a different training procedure than the standard ViT model

### Background and notation
- The Vision Transformer (ViT) architecture is a flexible machine learning model that works with any patch size
- The ViT architecture is based on the patchification procedure which tokenizes an input image into a sequence of patches
- The position embedding π i is learned using a variational autoencoder

### Standard ViTs are not flexible
- We first show that standard pre-trained ViT models perform poorly when evaluated at different patch sizes
- To improve performance, we resize the patch embedding weights and position embeddings with bilinear interpolation

### Training flexible ViTs
- The model needs to define an underlying parameter shape for ω and π.
- The learnable parameters are of that shape, and resized on-the-fly as part of the model's forward pass.
- We show in Appendix B that the exact shape of these underlying learnable parameters does not matter much, and we use an underlying size of 32 × 32 for patches and 7 × 7 for position embeddings in all experiments.
- Second, to have a large variety of patch sizes that perfectly tile the image, we use an image resolution of 240² px, which allows for patch sizes p ∈ {240, 120, 60,48,40,30,24,20,16,15,12,10,8,6,5,4, 2, 1}, of which we use all between 48 and 8, inclusive.
- At each iteration we sample p from the uniform distribution P over these patch sizes.

### How to resize patch embeddings
- Consider a patch x ∈ R p×p of the input image, and the patch embedding weights ω ∈ R p×p
- If we resize both the patch and the embedding weights with bilinear interpolation, the magnitude of the resulting tokens will differ greatly; for example x, ω ≈ 1 4
- We hypothesize that this dramatic change in token norm is part of the reason of ViT's inflexibility, and an inductive bias that hinders learning of a single FlexiViT.
- Ideally, as long as there is no loss of information during resizing, the patch embeddings e i = x, ω after resizing both the input x and the embedding ω should remain the same.
- One way to achieve this equality is to normalize the tokens right after their embedding, either explicitly or by using a LayerNorm [2] module. However, this approach requires changing the model architecture and is not compatible with existing pre-trained ViTs.
- Further, it does not exactly preserve the patch embeddings.
- As we will show, there is a more principled way of achieving this goal, which is compatible with existing pre-trained models and does not require any architectural change.
- First, we note that the linear resize operation introduced in Section 3.2 can be represented by a linear transformation: where o ∈ R p×p is any input, and B p * p ∈ R p * 2 ×p 2 .
- We resize channels of multi-channel inputs o independently.
- Intuitively, we would like to find a new set of patchembedding weights ω such that the tokens of the resized patch match the tokens of the original patch.
- Formally, we want to solve the optimization problem: where B = B p * p and X is some distribution over the patches.
- In case when we are increasing the patch size, i.e. p * ≥ p, we can use ω = P ω where P = B(B T B) −1 = (B T ) + is the pseudoinverse of B T :
- This way we match the patch embeddings exactly for all x.
- In the case of downsampling, i.e. when p * < p, the solution to the optimization problem in Eq. ( 2) will in general depend on the patch distribution X .
- In Appendix A.2, we show that for X = N (0, I), we recover the pseudoinverse ω = P ω = (B T ) + ω as the optimal solution.
- To sum up, we define PI-resize (pseudoinverse resize) as: where is the matrix corresponding to the PI-resize transformation.
- The PI-resize operation resizes the patch embedding weights, serving as an inverse of the bilinear resize operation.
- To experimentally validate the effectiveness of PI-resize and compare it to several alternative heuristics, including standard linear resize, we load a pre-trained ViT-B/8 model from [50] and evaluate it after resizing both the image and the model, thus preserving its sequence length s = (224/8) 2 = 784.
- The results, shown in Figure 4, demonstrate that PI-resize maintains nearly constant performance when upsampled, and degrades gracefully when downsampling.
- None of the heuristics works as well as thoughtful PI-resize across the board.

### Connection to knowledge distillation

### FlexiViT's internal representation
- Does FlexiViT process inputs with different patch sizes in similar ways? We investigate this by analyzing the model's internal representations.
- We apply minibatch centered kernel alignment (CKA) [14,28,39], a widely-used approach for comparing representations within and across neural networks.
- For visualization purposes, we apply an arccosine transform to transform CKA/cosine similarity to proper metrics [58] and then perform t-SNE. Results are shown in Figure 6.
- Feature map representations are similar across grid sizes from the first layer until the MLP sublayer of block 6. At the MLP sublayer of block 6, layer representations diverge, before converging again at the final block.
- By contrast, CLS token representations remain aligned across grid sizes. Thus, although internal representations of a substantial portion of FlexiViT differ by grid size, output representations are generally aligned.

## Using pre-trained FlexiViTs
- We have shown that ViTs can be trained flexibly without significant loss of upstream performance.
- Next, we verify that pre-trained FlexiViTs are still comparable to individual fixed patch-size ViTs when transferred to other tasks.
- We check this by transferring the single pre-trained FlexiViT with its patch size fixed to either 16 2 or to 30 2 during transfer.
- We compare FlexiViT to ViT-B/16 and a ViT-B/30 models that were pre-trained using the same distillation setup as FlexiViT (Section 3.5), but with a fixed patch size.
- We perform this transfer on the following set of diverse tasks. For each task, we provide more details along with many more results, all with the same take-away, in Appendix E.

### Results
- A single FlexiViT model roughly matches the two fixed ViT models
- There is no significant downside in using a pre-trained FlexiViT

### Resource-efficient transfer via flexibility
- FlexiViT enables a new way of making transfer learning more resource efficient
- This is possible because, surprisingly, flexibility is Input grid size

## Flexifying existing training setups
- Existing pre-trained models can be flexified during transfer to downstream tasks
- Different training setups can be flexified to improve performance

### Transfer learning
- We use the same set of 6 transfer datasets from Section 4
- We show the results for SUN397 in Figure 9 and all other datasets in Appendix E
- The difference is that we now also randomize the patch size during transfer, and evaluate the single resulting model at different patch sizes (x-axis, three groups of bars)
- Flexible transfer of FlexiViT works best, but flexifying a fixed model during transfer also works surprisingly well, considering the very short training and low learning rate used for transfer.

### Multimodal image-text training
- Next, we discuss two ways to flexify multimodal imagetext training: FlexiLiT and FlexiCLIP.
- In FlexiLiT, we train a text tower to produce text embeddings that align well with visual embeddings from various patch sizes (B/flexi).
- LiT baselines with direct use of either FlexiViT models at fixed resolutions, or ViT models are provided.
- Figure 10 shows zero-shot image to text retrieval results on the Flickr30k [42] dataset.
- FlexiLiT-B/flexi performs the best on average, while LiT with FlexiViT-B/30 and FlexiViT-B/16 both get very close results.
- Flexification additionally provides the possibility of fast transfer as discussed in Section 4.2.

### Open-vocabulary detection
- OWL-ViT can be flexified to improve performance
- The optimal patch size is not necessarily the smallest

### Training times and flexification
- Flexible model
- Pre-training fixed ViTs faster
- Can use FlexiViT's machinery to design curricula
- Leads to better performance per compute budget than standard training

## Analyzing FlexiViTs
- We find that decreasing the patch size results in attention relevance to concentrate into a larger number of smaller areas throughout the image.
- In Figure 13 (top) we observe that attention can significantly change at different scales.
- Relation of token representations across scales As we decrease FlexiViT's patch size, each token "splits" into multiple tokens.
- A natural question is how token representations at larger patch sizes relate to token representations at smaller patch size.
- To answer this question, we measure cosine similarity between the representation of a "seed" token at the center of a feature map at one patch size and representations of other tokens at the same and different patch sizes.
- As shown in Figure 13 (bottom), we are indeed able to find correspondences between tokens across scales.
- We explored whether it is possible to improve prediction accuracy by ensembling the predictions of the same FlexiViT at multiple scales.
- We find that, in terms of total compute spent, it is nearly always better to run a single FlexiViT at that compute budget than to ensemble multiple smaller ones.

## Discussion of alternatives
- Changing the input patch size is not the only way to trade off sequence length and compute in ViTs
- We explore two alternatives in our core setup: distillation on ImageNet-21k. Varying patch embedding stride
- One alternative is to fix the patch size and change its sampling stride, i.e. extract overlapping patches to increase sequence length. Intuitively, the advantage of this approach is that the intrinsic patch size is fixed and we avoid any special care when computing patch embeddings. Results in Figure 14 suggest varying the stride works almost as well, only slightly lagging behind our baseline.
- Varying model depth Another alternative is adding flexibility in terms of depth, i.e. number of layers. Depth pruning has been explored in the context of NLP [16,47] and more recently also for ViTs [62]. Depth pruning differs fundamentally from FlexiViT: it scales linearly in depth, uses a subset of parameters, and allows progresively refining a prediction. We randomize the depth by attaching the shared head to various intermediate layers. We also tried separate heads, which worked worse. The results in Figure 14 show that FlexiViT provides a significantly better computeaccuracy tradeoff than depth pruning.

## Conclusion
- FlexiViT is a simple and efficient way of trading off compute and predictive performance with a single model, enabled by the unique patch embedding strategy of ViTs.
- FlexiViT can be used to significantly reduce pre-training costs by only training a single model for all scales at once, and performs well at a variety of downstream tasks.
- There are many exciting directions for future work, and we hope that our results inspire the community to explore additional creative applications of patchification.
- Besides providing more details and results on various sections as mentioned in the main paper, we also provide full numerical (tabular) results of all figures in Appendix P in order to facilitate reproduction/comparison in future work.
- Finally, more details about the alternative ways of flexibility discussed in Section 7 are provided in Appendix O.
