---
title: "Lorentz Group Equivariant Autoencoders"
date: 2022-12-14T17:19:46.000Z
author: "Zichun Hao, Raghav Kansal, Javier Duarte, Nadezda Chernyavskaya"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07347v1.webp" # image path/url
    alt: "Lorentz Group Equivariant Autoencoders" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07347)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07347).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/lorentz-group-equivariant-autoencoders).

# Abstract
- Machine learning models are being adapted from those designed for datasets in computer vision or natural language processing
- Inductive biases suited to HEP data can make the model more performant and interpretable
- To that end, we develop the Lorentz group autoencoder (LGAE), an autoencoder model equivariant with respect to the proper, orthochronous Lorentz group $\mathrm{SO}^+(3,1)$, with a latent space living in the representations of the group

# Paper Content

## Introduction
- Machine learning and deep neural networks are becoming powerful and ubiquitous tools for the analysis of particle collisions and their products
- Lorentz symmetry has been successfully exploited recently in HEP for jet classification
- In this paper, we develop a Lorentz-group-equivariant autoencoder and explore its performance and interpretability
- We compare the LGAE's performance in reconstruction and anomaly detection to alternative architectures that exhibit different symmetry properties

## Related Work
- Equivariance is built into many successful DNN architectures
- Recently, equivariance in DNNs has been extended to a broader set of symmetries
- Broadly, equivariance to a group has been achieved either by extending the translation-equivariant convolutions in CNNs to more general symmetries with appropriately defined learnable filters
- Alternatively, by operating in the Fourier space of  (or a combination thereof)
- Lorentz symmetry, or invariance to transformations defined by the Lorentz group, is a fundamental symmetry of the data collected out of high-energy particle collisions
- There have been some recent advances in incorporating this symmetry into NNs
- The Lorentz group network (LGN) [34] was the first DNN architecture developed to be covariant to the SO + (3, 1) group, based on a GNN architecture but operating entirely in Fourier space on objects in irreps of the Lorentz group
- More recently, LorentzNet [2,35] uses a similar GNN framework for equivariance, with additional edge features -Minkowski inner products between connected node features -but restricting itself to features living only in scalar and vector representations of the group
- Both networks have been successful to jet classification, with LorentzNet achieving SOTA results in top quark and quark versus gluon classification, further demonstrating the benefit of incorporating physical inductive biases into network architectures

## LGAE Architecture
- The LGAE is built out of Lorentz group-equivariant message passing (LMP) layers, described in Section 3.1, which constitute the two encoder and decoder networks, described in Section 3.2 and Section 3.3 respectively.
- The LMP layer and overall model architecture are depicted in Fig. 1.
- Each LMP layer is defined by its features, all transforming under a corresponding irrep of the Lorentz group in the canonical basis [68], including at least one 4-vector (transforming under the (1/2, 1/2) representation) representing its 4-momentum.
- The ( + 1)-th MP layer operation consists of messagepassing between each pair of nodes, with a message  ()    to node  from node  (where  ≠ ) and a self-interaction term   defined as
- The encoder takes as input a point cloud of  particles, termed a "particle cloud", each associated with a 4-momentum vector and an arbitrary number of scalars representing physical features such as mass, charge, and spin.
- Each isotypic component is then mixed via learned weights, to a chosen multiplicity of  (0) (,) E . The resultant graph is then processed through  E MP LMP layers, specified by a sequence of multiplicities , where  ()   (,) E is the multiplicity of the (, ) representation at the -th layer.
- Weights are shared across the nodes in a layer to ensure permutation equivariance.
- After the final MP layer, node features are aggregated to the latent space by a component-wise minimum (min), maximum (max), or mean.
- The min and max operations are performed on the respective Lorentz invariants.
- We also find, empirically, interesting performance by simply concatenating isotypic components across each particle and linearly "mixing" them via a learned matrix as in Eq. ( 5), which thereby breaks the permutation symmetry.

## Experiments
- The LGAE is a model that is used to reconstruct and detect anomalies in high-momentum jets.
- The LGAE is better than a baseline GNN autoencoder model composed of fully-connected MPNNs adapted from Ref. [21].
- The LGAE is equivariant to Lorentz boosts and rotations up to numerical error.

### Dataset
- The model is trained to reconstruct 30-particle high transverse momentum jets from the J N [69] dataset
- For particle features, we use the respective 3-momenta in absolute coordinates
- In the processing step, each 3-momentum is converted to a 4-momentum:   = (|p|, p), where we consider the mass of each particle to be negligible
- We use a 60%/20%/20% training/testing/validation splitting for the total 177,000 jets
- For evaluating performance in anomaly detection, we consider jets from J N produced by top quarks,  bosons, and  bosons as our anomalous signals

### Reconstruction
- The LGAE and GNNAE models are evaluated on the reconstruction of the particle and jet features of QCD jets.
- Relative transverse momentum rel T =  particle T jet T and relative angular coordinates rel =  particle −  jet and  rel =  particle −  jet (mod 2) are considered as each particle's features, and total jet mass,  T and  are considered as jet features.
- Figure 2 shows random samples of jets, represented as discrete images in the angular-coordinate-plane, reconstructed by the models with similar levels of compression in comparison to the true jets, while Figure 3 shows histograms of the reconstructed features compared to the true distributions.
- The LGAE-Mix and GNNAE-PL models have the best performance overall, significantly outperforming both GNN models at similar compression rates.

### Anomaly Detection
- We test the performance of unsupervised anomaly detection algorithms by pre-training them solely on QCD and then using a reconstruction error for the QCD and new signal jets as a discriminating variable.
- We consider top quark, W boson, and Z boson jets as potential signals and QCD as the "background".
- We test the Chamfer distance, energy mover's distance [73], and MSE between input and output jets as reconstruction errors, and find the Chamfer distance most performant for all models.
- In general, LGAE models have significantly higher signal efficiencies than GNNAEs for all signals when rejecting > 90% of the background.
- LGAE-Mix consistently performs better than LGAE-Min-Max.

### Latent Space Interpretation
- The outputs of the LGAE encoder are irreducible representations of the Lorentz groups
- This implies a significantly more interpretable latent representation of the jets than traditional autoencoders
- For example, we learn how the LGAE-Mix model with (1/2,1/2) = 2 is encoding the target jet momentum
- We can also understand the anomaly detection performance by looking at the encodings of the training data compared to the anomalous signal

## Conclusion
- Develops the Lorentz group autoencoder (LGAE), an autoencoder model equivariant to Lorentz transformations
- argues that incorporating this key inductive bias of high energy physics (HEP) data can have a significant impact on the performance, efficiency, and interpretability of machine learning models in HEP
- applies the LGAE to tasks of compression and reconstruction of input quantum chromodynamics (QCD) jets, and of identifying out-of-training-distribution anomalous top quark, W boson, and Z boson jets
- reports excellent performance in comparison to a baseline graph neural network autoencoder (GNNAE) model, with the LGAE outperforming the GNNAE on several key metrics
- demonstrates the interpretability of the LGAE by analyzing the latent space of the LGAE models for both tasks
- opens up many promising avenues both in terms of performance and model interpretability, with the exploration of new datasets, higher-order Lorentz group representations, and characterization of the derived jet observables in the LGAE latent space all exciting possibilities for future work
