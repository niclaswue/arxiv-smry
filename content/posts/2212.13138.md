---
title: "Large Language Models Encode Clinical Knowledge"
date: 2022-12-26T14:28:24.000Z
author: "Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, Vivek Natarajan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13138v1.webp" # image path/url
    alt: "Large Language Models Encode Clinical Knowledge" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13138)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13138).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/large-language-models-encode-clinical).

# Abstract

# Paper Content

## Introduction
- Evaluation of LLMs on MultiMedQA - Flan-PaLM achieves state-of-the-art (SOTA) performance on MedQA, MedMCQA, PubMedQA, and MMLU clinical topics, often outperforming several strong LLM baselines by a significant margin.
- Instruction prompt tuning - We propose instruction prompt tuning, a data-and parameterefficient alignment technique, to further adapt Flan-PaLM to the medical domain. The resulting model, Med-PaLM, performs encouragingly on the axes of our pilot human evaluation framework.

## Related work

## Methods

### Datasets
- Medical question answering requires reading comprehension skills, ability to accurately recall medical knowledge, and manipulation of expert knowledge
- There are several existing medical question answering datasets for research
- MultiMedQA includes multiple-choice question answering datasets, datasets requiring longer-form answers to questions from medical professionals, and datasets requiring longer-form answers to questions that might be asked by non-professionals
- We augmented MultiMedQA with a new dataset of curated commonly searched health queries: HealthSearchQA.
- All the datasets are English-language and we describe them in detail below
- These datasets vary along the following axes: • Format: multiple-choice vs. long-form answer questions • Capabilities tested: e.g., assessing the recall of medical facts in isolation vs. assessing medical reasoning capabilities in addition to recall of facts • Domain: open domain vs. closed domain questions • Question source: from professional medical exams, medical research, or consumers seeking medical information

### Framework for human evaluation

## Results
- Our model achieved an accuracy of 79.0% outperforming the previous state of the art BioGPT model Luo et al. [56] by 0.8%.
- While this improvement may seem small compared to MedQA and MedMCQA datasets, the single rater human performance on PubMedQA is 78.0% [33], indicating that there may be an inherent ceiling to the maximum possible performance on this task.

### State-of-the-art performance on MMLU clinical topics
- The MMLU dataset contains multiple-choice questions from several clinical knowledge, medicine and biology related topics.
- Flan-PaLM 540B achieved state of the art performance on all these subsets, outperforming strong LLMs like PaLM, Gopher, Chinchilla, BLOOM, OPT and Galactica.
- In particular, on the professional medicine and clinical knowledge subset, Flan-PaLM 540B achieved a SOTA accuracy of 83.5% and 84.0%.
- Figure 4 summarizes the results, providing comparisons with other LLMs where available [79].
- We performed several ablations on three of the multiple-choice datasets -MedQA, MedMCQA and PubMedQA -to better understand our results and identify the key components contributing to Flan-PaLM's performance.
- We present them in detail below:
- Instruction tuning improves performance on medical question answering
- Across all model sizes, we observed that the instruction-tuned Flan-PaLM model outperformed the baseline PaLM model on all three datasets -MedQA, MedMCQA and PubMedQA.
- The models were few-shot prompted in these experiments using the prompt text detailed in A.
- We have not yet completed a thorough analysis of the effect of instruction prompt tuning on multiple-choice accuracy; our analysis is of Flan-PaLM in this section, not Med-PaLM.
- Med-PaLM (instruction prompt-tuned Flan-PaLM) was developed to improve the long-form generation results of Flan-PaLM presented in Section 4.5 by better aligning the model to the medical domain. However, given the success of domain-agnostic instruction tuning for multiple-choice question answering, in-domain instruction prompt tuning appears promising, and we present a preliminary result in Section A.6.
- Scaling improves performance on medical question answering
- A related observation from 5 was the strong performance improvements obtained from scaling the model from 8B to 62B and 540B.
- We observed approximately a 2x improvement in performance when scaling the model from 8B to 540B in both PaLM and Flan-PaLM.
- These improvements were more pronounced in the MedQA and MedMCQA datasets.
- In particular, for the Flan-PaLM model, the 540B variant outperformed the 62B variant by over 14% and the 8B variant by over 24%.
- Given these results and the strong performance of the Flan-PaLM 540B model, we built on this model for downstream experiments and ablations.
- The scaling plots are provided in Section A.4.
- Chain-of-Thought (CoT) prompting
- Somewhat unexpectedly, we did not observe improvements using CoT over the standard few-shot prompting strategy across the three multiple-choice datasets -MedQA, MedMCQA and PubMedQA.
- The CoT prompts used are summarized in Section A.9.
- Self-consistency (SC) leads to strong improvement in multiple-choice performance
- Wang et al. [88] showed that self-consistency prompting can help when CoT prompting hurts performance.
- They showed significant improvements on arithmetic and commonsense reasoning tasks.
- Taking their cue, we apply it to our datasets.
- We fixed the number of chain-of-thought answer explanation paths to 11 for each of the three datasets.
- We then marginalized over the different explanation paths to select the most consistent answer.
- Using this strategy, we observed significant improvements over the standard few-shot prompting strategy for the Flan-PaLM 540B model on the MedQA and MedMCQA datasets.
- In particular, for the MedQA dataset we observed a >7% improvement with self-consistency.
- However, somewhat unexpectedly...
