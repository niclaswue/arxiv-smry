---
title: "Controllable Text Generation with Language Constraints"
date: 2022-12-20T17:39:21.000Z
author: "Howard Chen, Huihan Li, Danqi Chen, Karthik Narasimhan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10466v1.webp" # image path/url
    alt: "Controllable Text Generation with Language Constraints" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10466)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10466).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/controllable-text-generation-with-language).

# Abstract
- We consider the task of text generation in language models with constraints specified in natural language
- To this end, we first create a challenging benchmark Cognac that provides as input to the model a topic with example text,
- Along with a constraint on text to be avoided, Cognac contains knowledge-intensive constraints sourced from databases like Wordnet and Wikidata
- We find that even state-of-the-art language models like GPT-3 fail often on this task, and propose a solution to leverage a language model's own internal knowledge to guide generation
- Our method, called CognacGen, first queries the language model to generate guidance terms for a specified topic or constraint,
- Uses the guidance to modify the model's token generation probabilities
- Proposes three forms of guidance (binary verifier, top-k tokens, textual example), and employs prefix-tuning approaches to distill the guidance to tackle diverse natural language constraints
- Through extensive empirical evaluations, we demonstrate that CognacGen can successfully generalize to unseen instructions and outperform competitive baselines in generating constraint conforming text.

# Paper Content

## Introduction
- As language models (LMs) become increasingly good at generating text indistinguishable from human writing, a key question emerges: 'How can we best control them to produce what is required while preventing unwanted generations?'
- This is especially critical for reducing issues of toxicity and bias (Gehman et al., 2020;Xu et al., 2021; Figure 1: Constraining instructions and model generations. Green highlight specifies the topic to be covered. Red highlight specifies the constraint to conform to. GPT-3 generates continuation that mentioned a politician, thus violating the constraint. COGNACGEN generates continuation that satisfies both the topic requirement and the constraint. Perez et al., 2022) and misinformation (Taylor et al., 2022) in applications that build on these models.
- Prior work has used special control codes (Keskar et al., 2019) to steer the model towards generating text on certain topics, explored the use of classifiers at inference time to modify the LM's probability distribution (Dathathri et al., 2020;Krause et al., 2021;Liu et al., 2021a), or prompting the LM itself to diagnosis and remove bias (Schick et al., 2021).
- While the former requires additional training with control codes, the other two approaches have only been shown to work with a small set of attributes as constraints.
- In this work, we consider the problem of controlling generation in LMs with constraints specified in natural language (Figure 1). Our framework allows for the use of both guidance topics that instructs the model on what to generate, as well as constraints that specifies what not to generate, all described in Dataset Instruction w/ Demonstrations Topic / Constraint WordNet Talk about motor vehicle: Topic: motor vehicle -A cruiser is a type of warship. Constraint: car -In motorsport, a safety car, or a pace car, is an automobile which [...] -A gas guzzler, in informal language, is a vehicle that is [...] Do not talk about car: -Arthur Neville Chamberlain [...] was a British politician [...] Topic: Wikidata -Maurice Harold Macmillan, [...] was a British Conservative statesman [...] citizenship = UK -Peter Edward Cook [...] was an English satirist and comedic actor [...] Constraint: The above are sentences describing people who are citizens of United Kingdom.

### Task Setup
- We study the problem of conditional text generation with topics and constraints provided in natural language.
- As input, each context includes 1) a topic to generate text on (e.g., "List examples of people who are citizens of United Kingdom"), 2) a number of example generations under that topic (demonstrations) and 3) a constraint that specifies what the model should not generate (e.g. "Keep listing examples below, but do not mention any politician.")-all specified in natural language.
- The goal is to train LMs to generate fluent on-topic content while respecting the specified constraint.
- LMs typically learn a probability distribution p θ (x) on sequences of tokens.
- An autoregressive LM can generate text by predicting the probability of the next token conditioned on the previous tokens: p θ (x j | x <j ).
- In our task, we consider the previous tokens in the context to include a task specification t, demonstrations E = {e k } K k=1 , and a constraint c.
- We assume that the task description t is based on a topic entity t. For example, "Talk about sports" is based on the topic entity "sports". Similarly, the constraint text c is generated based on a constraint entity c.
- The topic and constraint entities are added to the demonstrations using a template ( §2.2) into a full instruction I = G(t, c, E).
- We show in Table 1 examples of instructions and their corresponding topic and constraint entities.
- Our task is challenging for three key reasons: 1) the model has to understand the topic and constraint specified in natural language, and 2) the topics and constraints are knowledge-intensive-broader than lexical-level constraint (e.g., 'Include words "car" and "drive" in a sentence.') yet more specific than broad attributes such as toxicity or sentiment, and 3) it has to respect both the topic (which specifies what to generate) and the constraint (which specifies what not to generate) simultaneously.

### Dataset Collection
- There are no datasets that contain topic and constraint specifications in natural language.
- We create two new datasets based on WordNet and Wikidata.
- WordNet. We use WordNet (Miller, 1995) and its hypernymy relation to construct a hierarchical constraint dataset.
- We select five root nodes "animal", "vehicle", "art", "food", and "sport" from which the hierarchical structure is constructed.
- The leaf nodes are instances of their ancestors and are used as the topic and the constraint checker.
- Concretely, when evaluating the generated text x against a constraint entity c using the Word-Net constraint checker:
- where M(•) denotes whether s is a substring of x.
- We sam-ple two nodes as the topic and the constraint entities within the same subtree (higher-level : "vehicle", lower-level : "car" ) from the WordNet hierarchy, where the higher-level node is the topic and the lower-level node is the constraint.
- We sample three leaf nodes under the topic node and use them as demonstrations (|E| = 3), where the demonstration is the first sentence from its Wikipedia page.
- We collect a dataset of train/develop/test split of 3, 000/500/500.
- Wikidata. We also use Wikidata (Vrandečić and Krötzsch, 2014) to construct a second dataset.
- Each property and value pair (e.g., property : "citizenship", value : "United Kingdom"; shown in Table 1) contains a set of names (e.g., Winston Churchill).
- We use 5 properties: occupation, citizenship, education, birthplace, and deathplace from Wikidata.
- In each instance, the topic entity is a sampled property-value pair and the demonstraitons |E| = 3 are from the propertyvalue name set.
- The constraint entity is selected by choosing what the GPT2-XL (Radford et al., 2019) most likely to generate.
- When evaluating a generation x with constarint entity c, the Wikidata constraint checker C wikidata (x, c) = 1[∃s ∈ name-set(c) : M(s, x) = 1].
- We scrape from Wikipedia the corresponding first sentence of each entity.
- We collect a total of 150 unique topics, 261 unique constraints, and they form 540 unique topic-constraint pairs.
- We provide detailed data generation procedure for WordNet and Wikidata in A.1.
- Using the Word-Net and Wikidata databases for the checker functions enjoys the benefit of straightforward and automatic evaluation.
- However, we recognize the knowledge bases come with their fundamental limits to capture all relevant entities.
- Diverse natural language instructions.
- Our goal is to assess the model's ability to understand instructions that are diversely verbalized.
- For example, templates include instructions where the order of the topic and constraint vary and the lexical context differs.
- We collect 35 unique templates.
- We split them across train/develop/test as 3/3/29 templates.
- The templates are collected by PhD students writing the first nine seed templates, which are then expanded by paraphrasing using GPT-3 (Brown et al., 2020).
- The paraphrased templates were edited through human inspection to ensure quality.
- We provide examples in §A.2.

### Evaluation Metrics
- Evaluate different generation methods of LMs
- Use metrics that test for correctness and fluency of the generations
- Correctness is measured by the model's ability to generate text that conforms to the constraint while staying on topic
- Fluency is measured by the model's ability to generate text that is coherent and not overly repetitive or simply copying from the input
- The main metric we use is whether the generation x conforms to the constraint c while staying on topic t: where D is the evaluation dataset
- A higher IC score indicates that the model can generate text that conforms to the constraint while staying on topic
- We also report the on-topic score x∈D (higher is better) and the constraint violation score Copy-BLEU
- We report the extent to which the generation undesirably copies from the demonstrations
- Copy-BLEU score is calculated by taking the maximum BLEU score between the generated text and the |E| demonstrations
- Repetition (Rep-n). We report the ratio of the n-gram repetition (lower is better) in the generated text (Rep-n) proposed in Welleck et al. (2020)
- Perplexity (PPL). The perplexity of the generated text is calculated with respect to a pre-trained GPT2-XL model (Radford et al., 2019) on the generated sentence (lower is better)

### Overview
- The COGNAC model benefits from an explicit use of its own knowledge by querying itself
- To this end, we explicitly factorize the conditional probability
- The desired distribution: can be modeled by three components: 1) p(x | E), which is the probability conditioned only on the demonstrations E and 2) p(t | x) that evaluates if the task is performed, and 3) p(c | x) that evaluates if the constraint is conformed.

### Guided Generation
- COGNACGEN updates the next token prediction probability from the generation model by modifying the logits using the guidance (the "Generate" step in Figure 2).
- Specifically, the next token probability is modified as where s j is the logits corresponding to the original probability p(x j | x <j , E), o t j , o c j ∈ {0, 1} |V | are the guidance logits provided by the guidance model at each generation step j, and α and β are the hyperparameters that control the strength of the guidance.
- We use greedy decoding to generate from the above probability for COGNACGEN in our experiments.

### Guidance Model
- The binary verifier evaluates the probability p guide ("yes" | q), where q = "Is x j a type of c ?", where x j is the token to generate at timestep j.
- The top-k token guidance uses the next token probability distribution from the guidance model p guide (• | q), where q = "What are some examples of wine?".
- The textual example guidance model takes a query q = "What are some examples of wine?" and generates a set of guidance examples such as "cabernet", "merlot", "pinot noir", "pinot gris".
- The trie tree based approach is used to decide what guidance to apply at each step.

### Tackling Diverse Natural Language Instructions
- The method described so far assumes that the topic and constraint are given and can be used in a query template to obtain guidance.
- We propose to train the model to take natural language instruction and demonstrations and generate the guidance directly.
- With the set of diverse instructions (described in §2.2), the model needs to infer the topic and the constraint entities from the full input containing the instruction and demonstrations.
- We finetune the generation model using prefix-tuning (Li and Liang, 2021) on the model's textual example generated examples.
- This can be thought of as distilling the model's own knowledge by mimicking the textual guidance as the output and generalizing the implicit topic and constraint inference to unseen instructions.
- Formal, we fine-tune the added prefix weights and save the prefix activations of the fixed guidance model p guide (y | [I; P ]; θ, φ), where y are the examples generated by the tex-tual example model7 , φ is the added fine-tuning weights to generate the activations (θ remains fixed throughout), and P is the added prefix tokens.
- The fine-tuning objective minimizes the loss L(φ) = − t log p guide (y * t | [I; P ]; θ, φ).
- At the end of the training, only prefix activations φ(P ) are saved.
- This step distills the model's own knowledge and generalizes the implicit topic and constraint inference to unseen natural language instructions (Figure 2 Stage 1).

## Experimental Setup
- Evaluates COGNACGEN under two settings, with and without control codes
- Compared COGNACGEN to a fine-tuned model baseline and the self-debiasing technique
- Provides more details about self-guidance distillation training

## Results
- COGNACGEN (textual example) achieves the best instruction conformance (IC) scores, outperforming the selfdebiasing baseline by 12 points on WordNet and by 16 points on Wikidata.
- Among COGNACGEN's variants, textual example Table 5: Instruction Conformance for different natural language templates using COGNACGEN textual example. These three templates are applied to all the instances in the development set. guidance performs better than top-k token guidance and the binary verifier.
- All model variants of COGNACGEN seem to be equally fluent, with COGNACGEN textual example having a desirable slightly lower 47.9 perplexity.
- In the NL instruction setting (Table 4), COG-NACGEN textual example achieves a higher performance than GPT-3 (legacy) by 10.0 points on WordNet and 11.6 points on Wikidata, despite having much fewer parameters (1.5B vs 175B).
- In-structGPT achieves much higher scores (49 IC on Wordnet and 41.9 IC on Wikidata), but it is also a much larger model(175B) and is also fine-tuned on instruction following using human feedback (RLHF) (Ouyang et al., 2022).
- To analyse model performance on different kinds of templates, we report IC scores for each of the three templates in development sets in Table 5.
- We observe that performance among different templates stays about the same for WordNet, but for Wikidata, the template with the topic and constraint specified at the end proves to be more challenging than others. This highlights challenges due to structural variations in instruction templates and how this may manifest differently in each dataset.
- Performance analysis by category. We analyze the performance of COGNACGEN (textual example) by category for both WordNet (Table 6) and Wikidata (Table 7), revealing how each category provides different challenges.
- COGNACGEN struggles to avoid violating constraints for the 'Art' category at a IC of 15.0, a much lower score compared to other categories which all have > 30.0 IC.
- Moreover, for knowledge-heavy categories such as 'Art' in Word-Net and 'birthplace'/'deathplace' (as topic) in Wikidata, COGNACGEN struggles to stay on topic.
- Model ablations. To provide more insight into the workings of COGNACGEN, we ablate away the trie-based generation and also compare with a database oracle model on Wikidata, which provides an upper bound on IC score when using the In Wikidata, topic and constraint are often from different categories.
- Along the same lines, Unlikelihood training (Welleck et al., 2020) and CRINGE (Adolphs et al., 2022) use auxiliary token-level and sequence-level objectives to discourage models from assigning high probabilities to certain tokens or sequences, while Quark (Lu et al., 2022a) and Liu et al. (2021b) use reinforcement learning to do the same.
- All these approaches are limited by the type of control they exert over the language model, restricted to high-level concepts like sentiment, toxicity or repetition and usually employing a fixed set of predetermined binary or categorical codes.

## Conclusion
- We introduced a new task for controllable generation in language models with constraints specified in natural language.
- We developed COGNAC, a new benchmark containing knowledge-based constraints using data from Wordnet and Wikidata and showed that even state-of-the-art language models like GPT-3 fail to conform to the provided instructions.
- We then develop COGNACGEN, a method to use knowledge internal to a language model to guide its own generations. Our approach involves several key innovations such as guidance self-distillation using prefix-tuning and a trie-based decoding scheme based on the guidance of textual examples. This helps the model generate on-topic text that violates constraints less frequently compared to several baselines, including much larger models like GPT-3.
- Our analysis also revealed that there is still significant room to improve on COGNAC and we hope future approaches will find the benchmark useful for developing better methods to control language models.
