---
title: "DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue"
date: 2022-12-15T18:58:07.000Z
author: "William Held, Christopher Hidey, Fei Liu, Eric Zhu, Rahul Goel, Diyi Yang, Rushin Shah"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08054v1.webp" # image path/url
    alt: "DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08054)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08054).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/damp-doubly-aligned-multilingual-parser-for).

# Abstract
- Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands
- However, prior work has demonstrated that semantic parsing is a difficult multilingual transfer task with low transfer efficiency compared to other tasks
- In global markets such as India and Latin America, this is a critical issue as switching between languages is prevalent for bilingual users
- In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that contrastive alignment pretraining improves both English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.

# Paper Content

## Introduction
- Task-oriented dialogue systems are the backbone of virtual assistants
- Semantic parsing converts unstructured text to structured representations grounded in task actions
- Due to the conversational nature of the interaction between users and task-oriented dialogue systems, speakers often use casual register with regional variation
- Such variation is an essential challenge for the inclusiveness and reach of virtual assistants which aim to serve a global and diverse userbase
- Work partially done during an internship at Google

## Related Work
- Multilingual language model alignment massively multilingual transformers (MMTs) (Pires et al., 2019;Conneau et al., 2020a;Liu et al., 2020;Xue et al., 2021)
- Effective at intra-sentential codeswitching as well (Winata et al., 2021)
- Prior work has studied explicit alignment of individual embeddings (Artetxe et al., 2018;Artetxe and Schwenk, 2019)
- MMTs appear to implicitly perform alignment within their hidden states (Artetxe et al., 2020;Conneau et al., 2020b)
- MMTs are remarkably robust for multilingual and intra-sentential codeswitching benchmarks (Aguilar et al., 2020;Hu et al., 2020;Ruder et al., 2021)
- However, the gap between performance on the training language and zero-shot targets is larger in task-oriented parsing benchmarks (Li et al., 2021;Agarwal et al., 2022;Einolghozati et al., 2021)
- Our work applies the pretraining regime from Hu et al. (2021), which adds multiple explicit alignment objectives to traditional MMT pretraining. We show that this technique is effective both for semantic parsing, a new task, and intra-sentential codeswitching, a new linguistic domain.
- The concept of using an adversary to remove undesired features has been discovered and applied separately in transfer learning (Ganin et al., 2016), privacy preservation (Mirjalili et al., 2020), and algorithmic fairness (Zhang et al., 2018a)
- When applying this technique to transfer learning, Ganin et al. (2016) term this domain adversarial training.
- Due to its effectiveness in domain transfer learning, multiple works have studied applications of domain adversarial learning to cross-lingual transfer (Guzman-Nateras et al., 2022;Lange et al., 2020;Joty et al., 2017)
- Most relevant, Sherborne and Lapata (2022) combine a multi-class language discriminator with translation loss to improve crosslingual transfer.
- In this space, we contribute the 4 following novel findings. Firstly, we show that binary discrimination is more effective than multi-class discrimination and provide intuitive reasoning for this surprising phenomenon. Secondly, we show that adversarial alignment can increase the accidental translation phenomena (Xue et al., 2021) in models with pretrained decoders. Thirdly, we show that tokenlevel adversarial discrimination improves transfer to intra-sentential codeswitching. Finally, we remove the challenge of zero-shot hyperparameter search with a novel constrained optimization technique that can be configured a priori based on our alignment goals.

## Methods
- We utilize two separate stages of alignment to improve zero-shot transfer in DAMP.
- During pretraining, we propose to use contrastive learning to improve alignment amongst pretrained representations.
- During finetuning, we add double alignment through domain adversarial training using a binary language discriminator and a constrained optimization approach.

### Baseline Architecture
- We use a pointer-generator network to generate semantic parses
- We tokenize words [w 0 , w 1 . . . , w m ] from the labeling scheme into sub-words [s 0,w 0 , . . . , s n,w 0 , s 0,w 1 . . . , s n,wm ] and retrieve hidden states [h 0,w 0 , . . . , h n,w 0 , h 0,w 1 . . . , h n,wm ] from our encoder
- We use the hidden state of the first subword for each word to produce word-level hidden states
- At each action-step a, we produce a generation logit vector using a perceptron to predict over the vocabulary of intents and slot types g a and a copy logit vector for the arguments from the original query c a using similarity with Eq. 1
- Finally, we produce a probability distribution p a across both generation and copying by applying the softmax to the concatenation of our logits

### Alignment Pretraining
- Translation language modeling:
- Sentence alignment:
- Word alignment:
- Attention symmetry loss:
- Cross-attention matrices:

### Cross-Lingual Adversarial Alignment
- Uses a binary scheme for classifying languages not found in the training data as a single class
- Introduces a general constrained optimization approach for adversarial training
- Applies the approach to cross-lingual alignment

## Experiments
- Einolghozati et al. ( 2021) is a dataset of Iranian sentences
- Agarwal et al. ( 2022) is a dataset of Indian sentences
- Li et al. ( 2021) is a dataset of Chinese sentences
- We evaluate the effects of our techniques on three benchmarks for task-oriented semantic parsing with hierarchical parse structures.
- Two of these datasets evaluate robustness to intra-sentential codeswitching (Einolghozati et al. , 2021;Agarwal et al., 2022) and the third uses multilingual data to evaluate robustness to inter-sentential codeswitching (Li et al., 2021).
- Examples are divided as originally released into training, evaluation, and test data at a ratio of 70/10/20.

### Datasets
- MTOP is a benchmark to evaluate multilingual transfer
- The benchmark contains queries in English, French, Spanish, German, Hindi, and Thai
- Zero-shot performance on this benchmark to evaluate inter-sentential codeswitching robustness
- Each language has approximately 15,000 total queries which cover 11 domains with 117 intents and 78 slot types
- Hindi-English Task Oriented Parsing (CST5) is a benchmark of Hindi-English intra-sentential codeswitching data
- As part of preprocessing, we use Zhang et al. (2018b) to respectively
- Best results for models which fit on a single consumer GPU in bold. Models marked with † significantly (p = 0.05) improve over all others using the bootstrap confidence interval.

### Results
- We use the same hyperparameter configurations for all settings
- The encoder uses the mBERT architecture (Pires et al., 2019)
- The decoder is a randomly initialized 4-layer, 8-head vanilla transformer for comparison with the 4-layer decoder structure used in Li et al. (2021)
- We use AdamW and optimize for 1.2 million training steps with early stopping using a learning rate of 2e−5, batch size of 16, and decay the learning rate to 0 throughout of training
- We train on a Cloud TPU v3 Pod for approximately 4 hours for each dataset
- For all adversarial experiments, we use the unlabeled queries from MTOP as training data for our discriminator and a loss constraint of 0.3 as justified in 3.3
- The English data from each benchmark is used for training and early stopping evaluation
- We report Exact Match (EM) accuracy on all test splits
- In all tables, results significantly (p = 0.05) improve over all others are marked with a † using the bootstrap confidence interval (Dror et al., 2018)
- In Table 1, we report the results of our architecture with mBERT, AMBER, and DAMP compared to existing baselines from prior work: XLM-R with a pointer-generator network (Li et al., 2021), MT5 (Xue et al., 2021) and byT5 (Xue et al., 2022)
- For both T5 variants, we train with the hyperparameters described in Nicosia et al. (2021)
- Despite being a strong baseline for other tasks (Wu and Dredze, 2019;Aguilar et al., 2020;Liang et al., 2020;Hu et al., 2020;Ruder et al., 2021), mBERT alone is ineffective at cross-lingual transfer for compositional semantic parsing achieving an average multilingual accuracy of 0.5
- The AMBER pretraining process significantly improves accuracy for all languages to an average of 23.6
- Average accuracy across the 5 Non-English languages improves by 47x. English accuracy also improves to 84.2 from 78.6, instead of suffering negative transfer (Wang et al., 2020)
- DAMP further improves accuracy over AMBER by 1.8x to 42.2, outperforming both similarly sized models (byT5-Base; +34.2, mT5-Base; +16.1) and models three times its size (mT5-Large; +10.8, XLM-R; +3.4)
- mT5-XXL maintains state-of-theart performance of 55.1 but requires 33x more parameters and multiple GPUs for inference which increases latency and compute cost. Adversarial alignment improves performance in each language by at least 10 points, with Hindi and Thai, the most distant testing languages from English, having the largest improvements of +20.7 and +26.5 respectively
- DAMP improves over the mBERT baseline by 84x without architecture changes or additional inference cost.

## Adversarial Baseline Comparison

### Adversary Ablation
- The binary adversarial technique used in this study is beneficial to adversarial alignment
- The constrained optimization technique used in this study is beneficial to adversarial alignment

### Regularization Comparison
- Adversarial training is the only method that improves performance across all benchmarks
- Freezing the first 8 layers of the encoder improves performance

### Pretrained Decoder Comparison
- The plain mT5 model does better than adversarial alignment (-9.6)
- Adversarial alignment causes a drop in performance (-9.6)
- MTOP and CSTOP perform better with adversarial alignment than the mT5 decoder

### Improvement Analysis
- Since exact match accuracy is a strict metric, we analyze our improvements through qualitative analysis.
- We examine examples that DAMP predicts correctly but AMBER and mBERT do not.
- We then randomly sampled 20 examples from each language for manual evaluation.
- Improvements in intent prediction are a large portion of the gain.
- If intent prediction fails, the rest of the auto-regressive decoding goes awry as the decoder attempts to generate valid slot types for that intent.

## Alignment Analysis
- We use a twodimensional projection of the resulting encoder embeddings to provide a visual intuition for alignment.
- We provide a more reliable quantitatively evaluate alignment using a post-hoc linear probe.

### Embedding Space Visualization
- Figure 1: Embedding spaces of different model variants on MTOP test set
- UMAP visualization of mBERT provides a strong intuition for its poor results
- By using AMBER instead, this global clustering behavior is removed and replaced by small local clusters of English and Non-English data
- DAMP produces an embedding space with no clear visual clusters of Non-English data without English data intermingled

### Post-Hoc Probing
- Sherborne and Lapata (2022) reports the performance of the training adversary as evidence of successful training
- Elazar and Goldberg (2018) shows mode collapse during training
- Therefore, we train a linear probe on a frozen model after training for each variant using 10-fold cross-validation.
- Supporting the visual intuition, probe performance decreases with each stage of alignment.
- On mBERT, he discriminator achieves 98.07 percent accuracy indicating poor alignment.
- AMBER helps, but the discriminator still achieves 93.15 percent accuracy indicating the need for further removal.
- DAMP results in a 23.62 point drop in discriminator accuracy to 69.53.

## Conclusions and Future Work
- Doubly Aligned Multilingual Parser
- Uses contrastive alignment pretraining and adversarial alignment during finetuning
- Demonstrates that both stages of alignment benefit transfer learning in semantic parsing
- Shows that adversarial learning outperforms baselines on multiple datasets
- Has applicability to pretrained decoders
