---
title: "Recycling diverse models for out-of-distribution generalization"
date: 2022-12-20T17:21:46.000Z
author: "Alexandre Ramé, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Léon Bottou, David Lopez-Paz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10445v1.webp" # image path/url
    alt: "Recycling diverse models for out-of-distribution generalization" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10445)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10445).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/recycling-diverse-models-for-out-of).

# Abstract
- Foundation models are redefining how AI systems are built
- Practitioners now follow a standard procedure to build their machine learning solutions: download a copy of a foundation model, and fine-tune it using some in-house data about the target task of interest
- Consequently, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks
- Yet, these individual fine-tunings often lack strong generalization and exist in isolation without benefiting from each other
- In our opinion, this is a missed opportunity, as these specialized models contain diverse features
- Based on this insight, we propose model recycling, a simple strategy that leverages multiple fine-tunings of the same foundation model on diverse auxiliary tasks, and repurposes them as rich and diverse initializations for the target task
- Specifically, model recycling fine-tunes in parallel each specialized model on the target task, and then averages the weights of all target fine-tunings into a final model
- Empirically, we show that model recycling maximizes model diversity by benefiting from diverse auxiliary tasks, and achieves a new state of the art on the reference DomainBed benchmark for out-of-distribution generalization
- Looking forward, model recycling is a contribution to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to incrementally and reliably update machine learning models

# Paper Content

## Introduction
- Machine learning frameworks are being adopted for real-world applications due to their ability to adapt to downstream tasks
- The framework of foundation models is fueling this adoption
- There are multiple training strategies, each of which starts with a pre-trained foundation model
- Some of the training strategies fine-tune the pre-trained model on auxiliary tasks
- Then, all strategies perform fine-tuning on the target task of interest
- Finally, the weights fine-tuned on the target task are used as is, or are weight-averaged into a final model
- Our proposed model recycling is attractive because it (i) enables compute parallelism throughout auxiliary and target fine-tunings, (ii) maximizes the amount of diversity in the predictions of the fine-tuned models, (iii) achieves state-of-the-art performance in DomainBed [14], the standard benchmark for OOD generalization, and (iv) is without any inference or training overhead compared to a traditional hyperparameter search.

## Fine-tuning for out-of-distribution generalization
- We use a learning setup where the featurizer is parametrized by the weights and the classifier is parametrized by the weights.
- We allow a distribution shift between the two types of data, which is formally called an out-of-distribution problem.
- Our aim is to find a θ maximizing the test accuracy.

### Vanilla fine-tuning
- Turns out, the vanilla fine-tuning baseline is frustratingly difficult to beat in this task
- Fine-tuning is a simple recipe, namely (i) download a pre-trained featurizer with parameters φ pt , (ii) plug a classifier w lp compatible with the target task, and (iii) fine-tune the model using empirical risk minimization (ERM)
- While the classifier w lp could be initialized at random, first learn only the classifier with frozen featurizer (i.e., linear probing) on the target task improves results by preventing feature distortion
- This fine-tuning strategy is the standard practice to transfer the knowledge from models pre-trained on ImageNet

### Weight averaging over epochs
- Fine tuning a pre-trained model saves checkpoints every few epochs and builds a final model whose weights are the average of those multiple checkpoints.
- This was a surprising observation that was called the linear mode connectivity.
- Weight averaging strategies improved performance in OOD classification.

### Weight averaging over runs
- There is no performance barrier between two instances of models trained from pre-trained weights
- Interpolating two random solutions from the same basin could generally produce solutions closer to the center of the basin, which potentially have better generalization performance than the endpoints

### Weight averaging over tasks
- Fine-tune pre-trained model on auxiliary tasks
- Averaging auxiliary fine-tuning weights
- Use averaged model as the initialization for the target fine-tuning

## Model recycling
- Model recycling is a proposal to leverage diverse auxiliary fine-tunings of the same pre-trained model
- It is compared against other fine-tuning strategies in Figure 1 and outlined in detail in Figure 2b
- The 5-steps and parallelizable procedure for model recycling are: 1. Download an appropriate featurizer, pre-trained on task T0

## Experiments
- Model recycling is a technique that can improve the performance of models
- This technique can be used to improve the performance of models for ID tasks
- The technique can be used to improve the performance of models for a variety of tasks

### Recycling achieves state-of-the-art OOD performance
- Our first array of experiments shows the SoTA performance of model recycling on DomainBed [14]
- For each domain in PACS, each run is inter-trained on one of the following auxiliary tasks: VLCS, OfficeHome, TerraIncognita, DomainNet, or ImageNet (which we consider as the auxiliary task "number zero")
- Rather than selecting one single model, recycling applies the uniform or the greedy selection, without any training overhead; in brief, model recycling is to inter-training as Model Soups/DiWA is to ERM
- The symbol † indicates the averaging of all 60 = 20 × 3 weights from the 3 data splits-at the expense of losing error bars
- Fusing averages multiple auxiliary weights but at initialization
- Ensembling strategies perform functional averaging [55] with high inference overhead (indicated by the symbol * ); in particular, we report the scores from [27] for the deep ensembles * [55] baseline of M = 6 models with different random classifier initializations
- Finally, CORAL [61] is the best invariant approach; SWAD [52] and MA [27] average weights along one training trajectory but differ in their selection strategy
- Results: Table 1 summarizes our results on DomainBed, where model recycling achieves a new SoTA: recycling with uniform selection improves its DiWA equivalent by 0.5 points after averaging over all datasets.

### Recycling increases model diversity
- In Figure 3, we investigate how the diversity across models fine-tuned on the target task influences the OOD performance of their weight average.
- Diversity is measured with the prediction q-diversity [53], which increases when models predict differently and fail on different examples; this diversity measure is precisely defined in Appendix C.
- Following [30], let the target task be OfficeHome, with "Art" as the test OOD domain; we train on the "ClipArt", "Product" and "Photo" domains.
- We consider different models, either only pre-trained on ImageNet (as in Model Soups/DiWA), or also inter-trained on DomainNet.
- First, we verify that inter-training influences the final models. Specifically, Figure 3a confirms that networks with different initializations are more diverse than networks initialized similarly.
- Then, Figure 3b verifies that this diversity gain comes from their initialization and remains along fine-tuning on the target task. Moreover, Figure 3c shows that diversity is positively linearly correlated with OOD generalization: specifically, we observe that having different initializations improves diversity and thus the accuracy of their weight average.
- Finally, in Figure 3d, we consider averaging M weights: a proportion (1 − µ) start directly from ImageNet, the others µ were inter-trained on DomainNet. In the simplest case M = 2, using one model from each initialization leads to maximum accuracy; more generally, the best performances are usually obtained around µ ≈ 0.5, where the final weight average has maximal access to diverse initializations.
- In conclusion, each auxiliary task fosters the learning of diverse features [63].
- Recycling increases diversity and improves performance [38,39] by removing a key limitation of Model Soups [29] and DiWA [30]; the need for all fine-tunings to start from a shared initialization.

## Discussion: towards updatable machine learning
- Model recycling within the emerging updatable machine learning paradigm
- The goal is to develop machine learning systems that can be incrementally improved and recombined, allowing for the collaborative creation of increasingly efficient and sophisticated AI systems
- The core idea is to consider networks as pieces of software
- This enables mirroring open-source development in software engineering via version control
- Could it be possible that, someday, we can collaboratively "clone", "commit", and "merge" models towards an ever-improving AI system?
- Recent works [33,34,68,69] and the proposed recycling give some primitives to build such a repository of neural networks
- Here, (i) commits are fine-tunings performed by individual contributors on their specific tasks, and (ii) branch merging is replaced by weight averaging
- In terms of optimization and privacy, a federated learning setup [70] where weights are communicated and averaged only at the end of the learning process does indeed seem desirable and viable
- Moreover, advanced merging operations that consider weighted interpolations [33] or neuron permutations [72,73,74] could help.
- These ideas could lead to collaborative repositories where stakeholders open-source their fine-tuned models learned in a decentralized way on various auxiliary tasks [35,58], all starting from a shared foundation model.
- This also opens the door to volunteer computing (SETI@home [75], Leela Chess Zero [76], etc), and even perhaps to remuneration incentives for contributors.
- If this is the way forward, how can we ensure and maximize the recyclability of the resulting models?
- In software engineering, practices such as unit tests greatly reduce the failure modes of programs; how can we borrow these ideas to specify and test neural networks?
- To this end, we may leverage datasets as test certificates [77].
- The community would monitor statistics such as accuracy, forgetting, fairness, and robustness against spurious correlations on these datasets.
- Then, the reported scores could guide the choice of what models to clone, fine-tune, and merge.
- However, bad actors could directly include these datasets in their training data; then, should these external datasets be watermarked [78], or kept secret by some certifying authority?
- These questions are all the more important as traditional foundation models [1] come with centralization, monetization, data privacy issues, and lack of transparency [79].
- Being able to collaboratively enrich weights opens the transition from proprietary networks training to open-source collaborative networks building.
