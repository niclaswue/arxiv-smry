---
title: "Defending Against Poisoning Attacks in Open-Domain Question Answering"
date: 2022-12-20T05:25:01.000Z
author: "Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van Durme"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10002v1.webp" # image path/url
    alt: "Defending Against Poisoning Attacks in Open-Domain Question Answering" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10002)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10002).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/defending-against-poisoning-attacks-in-open).

# Abstract
- Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the input contexts can cause large drops in accuracy
- To do so, we introduce a new method that uses query augmentation to search for a diverse set of retrieved passages that could answer the original question.
- We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call Confidence from Answer Redundancy, e.g. CAR).
- Together these methods allow for a simple but effective way to defend against poisoning attacks and provide gains of 5-20%

# Paper Content

## Introduction
- Open-domain question answering is the task of answering a given question, based on evidence from a large corpus of documents
- Recent work in ODQA has resulted in many well-curated datasets, which evaluate a model's ability to answer questions on a wide array of topics
- However, most internet users search across lesscarefully curated sources, where malicious actors are able to affect articles that may be input to an ODQA system
- Despite these recent works (see Appendix A for more) highlighting the vulnerability of existing methods, the problem of defending against these data poisoning attacks is still understudied

## Experimental Details
- We seek to simulate realistic misinformation attacks on a curated knowledge source
- For our experiments we use Wikipedia as the knowledge collection for both original and augmented queries
- Poisoning entire articles at a time (although one could use a different collection for the augmented queries)
- Note that we also experimented with poisoning the top retrieved contexts or random contexts and found similar results

### Data
- For our experiments we use Natural Questions (NQ) and TriviaQA datasets
- The Natural Question dataset was gathered by collecting real-user queries typed into Google Search
- The TriviaQA dataset was collected by scraping question and answer pairs from trivia websites
- We simulate the data poisoning through the code available from Longpre et al. (2021)
- Instead of simply providing conflicts for model analysis, we use them as adversarial poisoning strategies
- This method uses the answers to the questions to suggest an entity of the same type, using SpaCY NER

### Models
- Following previous work, we use an encoder-decoder architecture to generate an answer given retrieved evidence.
- The model uses the DPR bi-encoder architecture for retrieval, embedding both documents and queries into a single dense vector.

### Metrics and Hyperparameters
- EM is used in all experiments
- Hyperparameters are tuned using the validation set
- Model is used with default retriever hyperparameters

### Query Augmentation
- Query augmentation is a traditional information retrieval technique to augment a given query to find a better set of documents
- In classical terms, the strategy is usually to expand the query, spelling out acronyms or adding synonyms.
- Recently, work has begun to use neural models to generate these expansions (Wang et al., 2021;Claveau, 2021).
- In our work, we use such a technique to create new queries that will gather a diverse set of passages.
- Confidence and Calibration of QA Many works have focused on calibrating QA models so that they correctly reflect probabilities that equal their actual correct answer rate (Clark and Gardner, 2017;Kamath et al., 2020;Si et al., 2022;Jiang et al., 2021).
- Our proposed confidence method is similar in that it measures when the model will be more likely to be correct, however, it does not do calibration in the sense of calibrated probabilities, instead giving a single value of "confident" or "not confident."

### Confidence from Answer Redundancy
- To identify the best augmented queries with their corresponding new passages, we derive a novel method, CAR, for measuring ODQA confidence.
- This new method of confidence relies on the fact that ODQA models use a large number of contexts (typically around 100).
- Thus, one way of measuring the effectiveness of an ODQA pipeline's 2 Note that if the model retrieves a passage that was initially poisoned we swap the original text for the poisoned text. retrieval is to measure how often the predicted answer string occurs in the retrieved contexts.
- For example, if the predicted answer appears only once in all 100 contexts, this may mean that the retriever was not able to find many documents relevant to the query.
- However, for popular entities (those asked about in Natural Questions and TriviaQA) there are generally many articles containing the correct answer.
- Overall, the more frequently the predicted answer appears in the contexts, the more likely that the retrieval was both successful and plentiful (e.g. redundant).
- Thus, CAR conveys some aspect of the model's confidence in the retrieval.

### Answer Resolution
- The query augmentation technique proposed earlier provides us with a large and diverse set of passages and questions.
- The answer redundancy confidence helps us to know when a particular query is confident about its predicted answer.
- However, how can we effectively combine more than one question and passage set into one final predicted answer?
- We use the following strategies to explore this question, with their shortened names in italics: (1) use the original question's predicted answer, e.g. the baseline, which doesn't use the augmented questions or contexts (2) randomly pick a new augmented question and use its predicted answer (3) take a majority vote of the augmented question's predictions and (4) use answer redundancy to choose which questions to use.
- This last strategy uses CAR to decide whether to choose the original question's prediction and if not, uses a majority vote over the augmented questions that are confident, filtered using CAR.
- We also explore three different data type settings: (1) using the originally retrieved contexts with the original/new questions (Original C), (2) using the new augmented questions and their newly retrieved contexts (New Q, New C), or (3) using the original question with the newly retrieved contexts from the augmented questions (Original Q, New C).

## Results
- Figure 3 highlights the key findings from the FiD model
- As the amount of poisoned data given to the model decreases, performance decreases
- Using a majority vote to resolve multiple answers results in lower performance than simply using the original question, but does do better than selecting an augmented question at random
- When comparing the data setting used with these methods, using the original question with the new contexts performs best, then the new questions with the new contexts, and in last place are those that use the original contexts

## Conclusion
- Our work defends against data poisoning attacks in open-domain question answering through two novel methods: (1) the use of query augmentation to find diverse passages that still correctly answer the question and (2) the use of answer redundancy as a strategy for model confidence in its prediction.
- Our proposed methods do not involve any gradient updates and provide a significant performance improvement.
- Thus, our work shows the effect of data poisoning on state-of-the-art open-domain question-answering systems and provides a way to improve poisoned performance by almost 20 points in exact match.
- We hope that this work encourages future work in defending against data poisoning attacks on knowledge sources.
- Our work focuses on the TriviaQA and Natural Questions benchmarks, which include questions about popular entities in Wikipedia.
- Our proposed defense depends upon finding alternate sources of information outside of the poisoned contexts.
- Thus, for entities that appear less often in the knowledge source, our approach will not be as effective.
- In order to study this for uncommon entities, more datasets will need to be developed, which we leave for future work.
- Our work focuses on defending against data poisoning attacks to high popularity entities, which by definition of popularity will be more frequently asked about and will be more frequently attacked (and are also what is studied in existing literature, e.g. Natural Questions/TriviaQA).
- Our work shows the impact that disinformation attacks could have on Wikipedia and provides an initial attempt to help remedy those attacks.
- We note that our strategy does not have perfect accuracy and is still susceptible to attacks, e.g. if there is no correct information in any context to be found, it will be very difficult for ODQA systems to give the correct answer.
- We welcome additional research to improve the resistance of ODQA systems to adversarial attacks.
- As a larger section of related work did not have space in the main paper, we include more related work here.
