---
title: "DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax"
date: 2023-01-05T14:53:21.000Z
author: "Sadra Safadoust, Fatma Güney"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-02092v1.webp" # image path/url
    alt: "DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02092)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02092).


# Abstract
- Current self-supervised monocular depth estimation methods are mostly based on estimating a rigid-body motion representing camera motion.
- These methods suffer from the well-known scale ambiguity problem in their predictions.
- We propose DepthP+P, a method that learns to estimate outputs in metric scale by following the traditional planar parallax paradigm.
- We first align the two frames using a common ground plane which removes the effect of the rotation component in the camera motion.
- With two neural networks, we predict the depth and the camera translation, which is easier to predict alone compared to predicting it together with rotation.
- By assuming a known camera height, we can then calculate the induced 2D image motion of a 3D point and use it for reconstructing the target image in a self-supervised monocular approach.
- We perform experiments on the KITTI driving dataset and show that the planar parallax approach, which only needs to predict camera translation, can be a metrically accurate alternative to the current methods that rely on estimating 6DoF camera motion.

# Paper Content

## Introduction
- Understanding the 3D structure of a scene is fairly easy for human beings.
- Having this ability is crucial for autonomous vehicles to be able to drive in different environments.
- Training deep networks for estimating depth has proven successful in computer vision research.
- However, many such methods are supervised and require ground truth depth which is costly to achieve.
- Another line of work uses a stereo setup that must be carefully calibrated.
- Both of these approaches cannot use the vast amount of unlabeled videos that are easily available for training.
- On the other hand, self-supervised monocular depth estimation methods that do not rely on stereo supervision do not suffer from these limitations and, in practice, have been closing the gap with their supervised or stereo counterparts.
- Current self-supervised monocular depth estimation approaches all follow the same basic idea proposed in [37].
- They use a pose network to estimate the ego-motion between a source frame and the target frame and a depth network to estimate the depth of the target image.
- These estimations can then be used to sample pixels from the source image to synthesize the target frame.
- The difference between the target frame and the synthesized can be used as the source of supervision for training the networks.
- In this paper, we propose another approach to synthesize the target image.
- Our approach, DepthP+P, illustrated in Fig. 1, uses the traditional planar parallax formulation [15,25], which decomposes the motion into a planar homography and a residual parallax.
- Consider a plane in the scene and its motion represented by a homography from the source to the target image.
- By first warping the source image according to this homography, the motion of the plane is canceled.
- Then the residual image motion depends on two factors: (1) the deviations of the scene structure from the plane, i.e. the depth of points and their perpendicular distance to the plane and (2) only the translational motion of the camera.
- Autonomous driving is a perfect use case for this approach because there is typically a planar surface in front of the vehicle, i.e. the road.
- However, it is important to note that the plane in the planar parallax formulation does not necessarily have to be a real plane and can also be a virtual plane, but choosing the road as the planar surface makes it easier to implement in practice.
- Moreover, our approach does not rely on the availability of a plane to predict depth during inference.
- In this approach, we first align the road plane between the source and target images. This is achieved by calculating the homography between the road regions in two frames and then warping the source frame according to the homography to obtain the aligned image.
- By doing so, the road regions in the aligned image and target image match.
- The residual motion between the aligned image and the target image can be explained as follows:
- We first estimate the depth of each pixel with a monocular depth network and back-project them into 3D.
- Then using a known camera height, we can calculate the perpendicular distance of each point to the road.
- In addition, we estimate the translation between the camera origins.
- Note that this is different from the typical monocular depth approach [10,37] which needs to estimate both the rotation and translation components.
- Finally, the target image can be synthesized from the aligned image using the calculated residual parallax as shown in Fig. 2.

### Planar Parallax
- The Planar Parallax paradigm has been used to understand the 3D structure of a scene from multiple images
- Sawhney [25] proposes a formulation for the residual parallax that uses depth and distance to the plane
- Irani et al. [15] use this formulation to derive a rigidity constraint between pairs of points over multiple images
- Irani et al. [13] derive trifocal constraints and use them to propose a simple method for new view synthesis
- In a follow-up work [14], they extend the planar parallax method to more than two uncalibrated frames
- More recently, MR-Flow [31] uses P+P to refine the optical flow estimations with rigidity constraints
- Chaney et al. [3] use P+P to estimate the height of points in the scene with eventbased cameras
- We propose a method to use the P+P formulation within the view synthesis framework for self-supervised monocular depth estimation.

## Methodology
- Current self-supervised monocular depth estimation approaches suffer from scale ambiguity
- Our proposed approach predicts depth maps in metric scale without using any ground truth depth supervision

### DepthP+P
- Planar Parallax Decomposition is a well-studied method for depth prediction
- We introduce the notation and build our method using that notation
- We first warp the source image I s to obtain the aligned image I w
- We then use Eq. (2) to reconstruct the target image I t by warping the aligned image I w
- We minimize the difference between I t and Îw for supervision

### Self-Supervised Training Loss
- Photometric loss function is the linear combination of the L1 distance and the structural similarity (SSIM) to minimize the difference between the target image I t and the reconstructed image Îw
- We use the per-pixel minimum reprojection error introduced in [10] and calculate the minimum of the L photo for each pixel across the previous and next aligned images
- We also define L smooth as an edge-aware smoothness loss over the mean-normalized inverse depth estimates [29] to encourage the depth predictions to be locally smooth

### Network Architecture
- Our depth network is based on the U-Net architecture
- We use a ResNet pre-trained on the ImageNet as the encoder for our depth network, and for the decoder we use the architecture similar to the one used by [10]
- The difference is that we directly estimate depth by multiplying the output of the last sigmoid layer by 250, which is the maximum depth value that can be predicted, instead of estimating disparity as in [10]
- The depth network takes as input a single target image I t and outputs the per-pixel depth estimates
- In DepthP+P, our second network takes I w and I t and outputs only the translation vector between the views
- The network is similar to the pose network proposed in [10], except that the output is a 3-element vector representing the translation and is metric scale in our case

## Experiments
- We use the Eigen split of the KITTI dataset to train and evaluate our model.
- We use all of the images in the split for which we could accurately estimate the homography for aligning the road between consecutive images.
- This results in 45000 training and 1769 validation samples.
- We evaluate our model on the 697 test images in the split using the original ground truth provided by LiDAR.
- We also report results using the improved ground truth for 652 test images provided by Uhrig et al.
- They use a stereo-reconstruction method to remove the outliers in LiDAR points and increase the ground truth density by accumulating laser scans which result in high-quality ground truth data.
- The camera height in this dataset is d c = 1.65 and we assume that the road is completely horizontal, i.e. N = [0, 1, 0] T .

### Depth Estimation Results
- Our method achieves significantly better results than the initial models by predicting the pose and depth.
- After the initial proposal of SfMLearner by Zhou et al. [37], several improvements have been proposed to improve its performance.
- Therefore, we believe that similar improvements can our proposed approach, DepthP+P, to previous approaches on the KITTI dataset that were trained only with monocular supervision.
- The scale column specifies whether the method can estimate depth in metric scale.
- We provide results with the original and improved ground truth.
- We show the results for the input resolution 640 × 192.
- The best method in each column is shown in bold and the second best is underlined.
- Our approach can achieve comparable results without a separate segmentation network.
- DepthP+P can also be trained with additional stereo supervision.

## Conclusion and Future Work
- Our approach is able to produce metrically accurate depth estimates by using a known camera height.
- Unlike previous methods that rely on estimating the full rigid-body motion of the camera, our method only needs to estimate the camera translation.
- We discussed the advantage of our method compared to the other scale-aware depth prediction methods.
- We see our approach as a first step to unlocking the potential of the plane and parallax for efficient and metric-accurate depth estimation.

## A Derivation
- Let C and C be the camera centers of the target view I t and the source view I s , respectively.
- x = [X, Y, Z] T and x = [X, Y, Z ] T are the coordinates of a 3D point with respect to C and C respectively.
- R is the rotation and t = [t x , t y , t z ] T is the translation between C and C .
- According to the Fig 2 (right), we can calculate h, the perpendicular distance of the 3D point to the plane Π as:
- K and K denote the camera intrinsic for the target view and the source view.
- p = [x, y, 1] T = 1 Z Kx and p = [x , y , 1] = 1 Z K x represent the pixel coordinates of our 3D point in the target and source view, respectively.
- Eq. ( 11) can be written as:
- By multiplying both sides by 1 Z K we obtain:
- where K −1 is the 3 × 3 homography matrix associated with the plane Π between the source and the target view.
- Since K 3 t = t z , we therefore have:
- By dividing each side of Eq. ( 14) by each side of Eq. ( 16) we obtain:
- Adding and subtracting Hp H 3 p to the right-hand side yields:
- The point p w = [x w , y w , 1] T = Hp H 3 p is the point p transformed by the homography matrix H.
- Thus, Eq. ( 22) can be simplified to:

## C Qualitative Results
- Our models are able to capture the boundaries of the objects very well
- Neither of our models suffer from artefacts such as the wrong estimation for the road lane line in the last row
- Input image Monodepth2 [10] DNet [32] Ours-Mono Ours-Stereo
- Quantitative Results for Monocular Training on KITTI
- By using ResNet50, our model performs on par with Monodepth2 [10]
