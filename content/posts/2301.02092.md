---
title: "DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax"
date: 2023-01-05T14:53:21.000Z
author: "Sadra Safadoust, Fatma Güney"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02092v1.webp" # image path/url
    alt: "DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02092)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02092).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/depthp-p-metric-accurate-monocular-depth).

# Abstract
- Current self-supervised monocular depth estimation methods are mostly based on estimating a rigid-body motion representing camera motion.
- These methods suffer from the well-known scale ambiguity problem in their predictions.
- We propose DepthP+P, a method that learns to estimate outputs in metric scale by following the traditional planar parallax paradigm.
- We first align the two frames using a common ground plane which removes the effect of the rotation component in the camera motion.
- With two neural networks, we predict the depth and the camera translation, which is easier to predict alone compared to predicting it together with rotation.
- By assuming a known camera height, we can then calculate the induced 2D image motion of a 3D point and use it for reconstructing the target image in a self-supervised monocular approach.
- We perform experiments on the KITTI driving dataset and show that the planar parallax approach, which only needs to predict camera translation, can be a metrically accurate alternative to the current methods that rely on estimating 6DoF camera motion.

# Paper Content

## Introduction
- Understanding the 3D structure of a scene is fairly easy for human beings.
- Having this ability is crucial for autonomous vehicles to be able to drive in different environments.
- Training deep networks for estimating depth has proven successful in computer vision research.
- However, many such methods are supervised and require ground truth depth which is costly to achieve.
- Another line of work uses a stereo setup that must be carefully calibrated.
- Both of these approaches cannot use the vast amount of unlabeled videos that are easily available for training.
- On the other hand, self-supervised monocular depth estimation methods that do not rely on stereo supervision do not suffer from these limitations and, in practice, have been closing the gap with their supervised or stereo counterparts.
- Current self-supervised monocular depth estimation approaches all follow the same basic idea proposed in [37].
- They use a pose network to estimate the ego-motion between a source frame and the target frame and a depth network to estimate the depth of the target image.
- These estimations can then be used to sample pixels from the source image to synthesize the target frame.
- The difference between the target frame and the synthesized can be used as the source of supervision for training the networks.
- In this paper, we propose another approach to synthesize the target image.
- Our approach, DepthP+P, illustrated in Fig. 1, uses the traditional planar parallax formulation [15,25], which decomposes the motion into a planar homography and a residual parallax.
- Consider a plane in the scene and its motion represented by a homography from the source to the target image.
- By first warping the source image according to this homography, the motion of the plane is canceled.
- Then the residual image motion depends on two factors: (1) the deviations of the scene structure from the plane, i.e. the depth of points and their perpendicular distance to the plane and (2) only the translational motion of the camera.
- Autonomous driving is a perfect use case for this approach because there is typically a planar surface in front of the vehicle, i.e. the road.
- However, it is important to note that the plane in the planar parallax formulation does not necessarily have to be a real plane and can also be a virtual plane, but choosing the road as the planar surface makes it easier to implement in practice.
- Moreover, our approach does not rely on the availability of a plane to predict depth during inference.
- In this approach, we first align the road plane between the source and target images.
- This is achieved by calculating the homography between the road regions in two frames and then warping the source frame according to the homography to obtain the aligned image.
- By doing so, the road regions in the aligned image and target image match.
- The residual motion between the aligned image and the target image can be explained as follows:
- We first estimate the depth of each pixel with a monocular depth network and back-project them into 3D.
- Then using a known camera height, we can calculate the perpendicular distance of each point to the road.
- In addition, we estimate the translation between the camera origins.
- Note that this is different from the typical monocular depth approach [10,37] which needs to estimate both the rotation and translation components.
- Finally, the target image can be synthesized from the aligned image using the calculated residual parallax as shown in Fig. 2.

### Planar Parallax
- The Planar Parallax paradigm, also called Plane + Parallax (P+P), has been used to understand the 3D structure of a scene from multiple images by decomposing the motion into a planar homography and a residual parallax.
- Sawhney [25] proposes a formulation for the residual parallax that uses depth and distance to the plane. Irani et al. [15] use this formulation to derive a rigidity constraint between pairs of points over multiple images. Irani et al. [13] derive trifocal constraints and use them to propose a simple method for new view synthesis.
- In a follow-up work [14], they extend the planar parallax method to more than two uncalibrated frames. More recently, MR-Flow [31] uses P+P to refine the optical flow estimations with rigidity constraints.
- Chaney et al. [3] use P+P to estimate the height of points in the scene with eventbased cameras.
- We propose a method to use the P+P formulation within the view synthesis framework for self-supervised monocular depth estimation.

## Methodology
- Current self-supervised monocular depth estimation approaches suffer from scale ambiguity.
- Our proposed approach predicts depth maps in metric scale without using any ground truth depth supervision.

### DepthP+P
- The Planar Parallax decomposition has been studied in detail before
- We first introduce it here to establish our notation and then build our method to predict depth following that notation
- Notation: Let Π be a 3D plane and H be the homography aligning Π between the target image I t and the source image I s . Let p and p be the images of the 3D point x = [X, Y, Z] T on the I t and I s respectively.
- As shown on the left in Fig. 2, we can warp p by the homography H and obtain the image point p w : where we omit the conversion to the homogenous coordinates.
- Note that by warping the source image I s , we obtain the aligned image I w such that the plane Π matches between them.
- The displacement between p w and p can be computed as follows: where K is the camera intrinsic, t = [t x , t y , t z ] T is the translation vector between the I t and I s , and d c is the distance between the camera for the source view to the plane Π.
- The structure is represented by γ = h Z where h is the distance of x to Π. Note that when x lies on the plane Π, i.e. h = 0, we will have p w = p.
- DepthP+P: Following the typical self-supervised monocular depth approach, our framework has two networks, one for estimating depth and another for estimating the translation between frames.
- Precisely, our pose network takes the source and images I s , I w and outputs the translation vector t.
- The depth network takes the target image I t and outputs the depth map D for I t .
- For every pixel p = [x, y], let D(p) denote its estimated depth.
- We backproject p using the camera intrinsics and the estimated depth to obtain the corresponding 3D point x in the camera coordinate system as follows: Therefore, as demonstrated on the right in Fig. 2, we have the following:
- where N is the normal vector of the plane Π, ĥ is the estimated distance of the point x to the plane Π and γ is our estimate of the structure variable γ.
- As a result, we obtain all the parameters required to use Eq. ( 2) to reconstruct the target image I t by warping the aligned image I w resulting in Îw . In other words, for each pixel p on the I t , we calculate the p w using (2) according to the depth and translation predicted by our two networks and then inverse warp I w and obtain Îw to reconstruct I t :
- We minimize the difference between I t and Îw for supervision as explained in the Section 3.2.
- In order to obtain the aligned images, we perform a pre-processing step on the dataset. We calculate a homography for every consecutive frame by using the road as the plane Π and warp the frames according to the calculated homographies. In other words, we calculate a homography H for every target image I t and source image I s pair, and then warp I s according to H to obtain the warped source image I w . We explain the details of this pre-processing step in Section 4.1.

### Self-Supervised Training Loss
- Photometric loss function is the linear combination of the L1 distance and the structural similarity (SSIM) to minimize the difference between the target image I t and the reconstructed image Îw
- We use the per-pixel minimum reprojection error introduced in [10] and calculate the minimum of the L photo for each pixel across the previous and next aligned images.
- We also define L smooth as an edge-aware smoothness loss over the mean-normalized inverse depth estimates [29] to encourage the depth predictions to be locally smooth.

### Network Architecture
- Our depth network is based on the U-Net architecture
- We use a ResNet pre-trained on the ImageNet as the encoder for our depth network, and for the decoder we use the architecture similar to the one used by [10]
- The difference is that we directly estimate depth by multiplying the output of the last sigmoid layer by 250, which is the maximum depth value that can be predicted, instead of estimating disparity as in [10]
- The depth network takes as input a single target image I t and outputs the per-pixel depth estimates. Note that the output of our depth decoder is in metric scale.

## Experiments
- We use the Eigen split of the KITTI dataset to train and evaluate our model.
- We use all of the images in the split for which we could accurately estimate the homography for aligning the road between consecutive images.
- This results in 45000 training and 1769 validation samples.
- We evaluate our model on the 697 test images in the split using the original ground truth provided by LiDAR.
- We also report results using the improved ground truth for 652 test images provided by Uhrig et al.
- They use a stereo-reconstruction method to remove the outliers in LiDAR points and increase the ground truth density by accumulating laser scans which result in high-quality ground truth data.
- The camera height in this dataset is d c = 1.65 and we assume that the road is completely horizontal, i.e. N = [0, 1, 0] T .

### Depth Estimation Results
- Our method introduces a novel approach to estimating the pose and depth
- Our method achieves significantly better results than the initial models by predicting the pose and depth
- After the initial proposal of SfMLearner by Zhou et al. [37], several improvements have been proposed to improve its performance. Therefore, we believe that similar improvements can our proposed approach, DepthP+P, to previous approaches on the KITTI dataset that were trained only with monocular supervision.
- The scale column specifies whether the method can estimate depth in metric scale. We provide results with the original and improved ground truth. We show the results for the input resolution 640 × 192. The best method in each column is shown in bold and the second best is underlined.
- Using stereo supervision significantly improves the performance of our DepthP+P model, outperforming all methods except for Monodepth2 [10]

## Conclusion and Future Work
- The new approach uses a known camera height to estimate the translation t of the camera.
- The approach is scale-aware and can produce accurate depth estimates without needing to estimate the full rigid-body motion of the camera.
- The approach can be used to detect moving foreground objects.
