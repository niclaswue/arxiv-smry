---
title: "Reproducible scaling laws for contrastive language-image learning"
date: 2022-12-14T10:24:50.000Z
author: "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, Jenia Jitsev"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07143v1.webp" # image path/url
    alt: "Reproducible scaling laws for contrastive language-image learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07143)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07143).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/reproducible-scaling-laws-for-contrastive).

# Abstract
- Neural networks have been found to scale well across a wide range of tasks
- Previous work on scaling laws has primarily used private data and models, or focused on uni-modal language or vision learning
- We investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository
- Our large-scale experiments involve models trained on up to two billion image-text pairs and find power law scaling for multiple downstream tasks

# Paper Content

## Introduction
- Large pre-trained models now achieve state-of-the-art performance on a wide range of tasks.
- In particular, large models have led to substantial advances in speech [56], language [17,57,8,28], vision [38,84], and multi-modal language-vision settings [55,33,54,59,62].
- A key ingredient in these breakthroughs has been self-or weakly-supervised learning, which enabled the use of Internetharvested training sets and reduced the need for explicit human annotation of training data.
- In addition, recent pre-trained models relied on increasing the compute, model, and data scale by orders of magnitude.
- When varying model size, compute amount, and data quantity, several papers have empirically observed that both pre-training loss and downstream task performance reliably improve with scale.
- Specifically, researchers have postulated scaling laws in the form of power law relationships between model performance and model compute, or data scale [35,73,61,84].
- Such scaling laws allow practitioners to predict model performance for a given model and compute scale, extrapolate to larger Data Arch. ImageNet VTAB+ COCO CLIP [55] WIT-400M L/14 75
- We study the scaling behavior of large CLIP models using fully open-source training code and data.
- All models in our investigation will be made available and include the largest public CLIP models.
- This table shows zero-shot performance at 224 pixel resolution, displaying accuracy on ImageNet [15], average accuracy on 35 VTAB+ datasets [65,85], and image retrieval recall at 5 on MS-COCO image retrieval [46].
- So far, the literature on empirical scaling laws has focused on language-only [35,73,28] or visiononly models [83,25,61].
- In the multimodal domain of language and vision, contrastive language-image models such as CLIP [55] have recently achieved large performance gains in zero-image classification, for instance improving zero-shot ImageNet accuracy from the prior state-of-the-art of 12% to 76%.
- Moreover, these models demonstrate unprecedented robustness to distribution shifts compared to prior supervised models [71,55,78].
- However, there is currently no systematic investigation for scaling trends in contrastive language-image learning.
- One substantial challenge in this direction is that until recently, there were no datasets of sufficiently large scale openly available for the research community to undertake such experiments.
- In this work, we conduct a scaling laws study for contrastive language-vision learning by utilizing the recently released LAION-5B [65] dataset of 5 billion image-text pairs.
- To ensure that our experiments are fully reproducible, we use the open source OpenCLIP [32] code to train CLIP models while varying model, data, and samples seen.
- We evaluate our CLIP models on several downstream tasks, including zero-shot classification, image retrieval, and fine-tuning via linear probing and end-to-end optimization.
- We observe a consistent increase in performance when scaling model, data, and compute, and derive scaling laws of power law form across different downstream tasks (Figure 1a, 1b).
- Interestingly, when comparing our OpenCLIP and OpenAI's original CLIP models, we find larger scaling coefficients for OpenCLIP models on zero-shot retrieval, while OpenAI CLIP models show stronger scaling for zero-shot classification.

## Background and related work
- The power law relation between scale and model performance was highlighted in [26]
- Empirical work stimulated theoretical studies that provided justification for the observed generalization boost with scale, investigating generalization error in overparameterized networks in the interpolation regime [6,9]
- Early empirical studies focused on the effect of training scale on upstream performance, measuring the test loss from the same distribution used for training
- Subsequent studies of large language models such as GPT-3 [8] demonstrated broad generalization capabilities in models with substantially larger scale
- Moreover, neural scaling laws of the power law form were derived for language models, connecting model, data, and training compute scale to performance [35,73,28]
- Scaling law studies were also studied in the vision domain [61,84], also observing a power law dependency of performance on scale
- Scaling law studies were also conducted for transfer and out-of-distribution performance [35,73,84]
- In these studies, researchers observed that performance on downstream tasks benefits from increasing model, data, and training compute scale [38,8,35,84]
- Since downstream performance most accurately reflects a practical use cases, examining scaling behavior on downstream tasks is increasingly important
- Recent work has also studied the effect of scale on other model characteristics, such as performance after pruning and compression [64,11] and on susceptibility to catastrophic forgetting [58]
- Learning from very large amounts of weakly aligned imagetext pairs has led to the development of models with broad generalization capabilities
- While these studies already show clear merits of scaling up, they do not conduct a thorough scaling investigation by systematically scaling model, data and, training compute
- Conducting scaling law studies requires sufficiently large pre-training datasets

### Open large-scale datasets LAION-400M/2B
- The LAION-400M and LAION-5B datasets are open, public image-text datasets
- The LAION-5B dataset has an English image-text subset of 2.32 billion samples
- Due to its scale, transparency and open-source nature, LAION has already been adopted by various works on language-vision modelling

### Pre-training OpenCLIP across various scales
- We selected a scale range for each dimension.
- For model scale, we choose CLIP architectures with ViT-B/32, ViT-B/16, ViT-L/14, ViT-H/14 and ViT-g/14 as visual encoders, scaling the text encoder in accord.
- For data scale, we use LAION-80M (an 80M subset of LAION-400M), LAION-400M, and LAION-2B.
- For training duration, we choose 3B, 13B and 34B samples seen scales.
- Due to compute constraints, for the larger H/14 and g/14 model scales, we conduct only restricted measurements (done for LAION-2B, with 34B samples seen for H/14, and with 13B samples seen for g/14).
- This selection provides coverage at the scale where we cannot afford to sample with the same density as at the intermediate and lower model scales.
- To verify that LAION-80M and LAION-400M are valid subsets of LAION-2B, we conduct a control experiment by extracting a random 400M subset of LAION-2B and comparing our reference OpenCLIP ViT-B/32 models pre-trained on both datasets.
- When doing so, we found no significant difference (see Appendix Sec. Compared to the original CLIP training procedure [55], we work with larger batch sizes and adapt the learning rate accordingly.
- We opt for larger batch sizes to allow for more efficient distributed training; maximizing the local batch size per GPU and using close to one thousand GPUs lead us to global batch sizes in the range of 86-88K samples.
- In order to assess the validity of re-using measurements obtained with different batch sizes, we perform a number of control experiments varying batch size from 32K to 86-88K, and observe a difference of 0.2 − 0.5% across different settings (see Appendix Sec. B.2.3), which is small enough not to confound observations on the effect of scale.
- For each number of samples seen scale, we execute a separate training experiment with a cosine annealing learning schedule adapted to the number of samples.
- This allows us to assess performance of models pre-trained with different training durations and avoid suboptimal training when using the same schedule for runs of different length.
- We tune a small number of hyper-parameters (see Appendix Table 17), each scale point to optimize validation loss and prevent training instabilities, and otherwise closely follow the original CLIP training procedure [55], using the InfoNCE loss, Adam with decoupled weight regularization [47] (i.e., AdamW) as an optimizer, with β 1 = 0.9, β 2 = 0.98 and weight decay of 0.2.
- We train the models using mixed precision.
- For larger model scales (ViT-L/14, H/14, g/14), we observed loss spikes during training which had an adverse effect on performance.
- We fixed the issue by switching from mixed precision with float16 to mixed precision with bfloat16.

## Scaling laws for different downstream tasks

### Zero-shot transfer and robustness
- CLIP models can be used on downstream classification tasks without requiring any labeled training examples
- CLIP models are observed to excel on out-of-distribution robustness benchmarks
- Scale has a positive effect on CLIP performance

### Retrieval
- Retrieval is another common way to evaluate zero-shot capabilities of the models
- In this section, we study the effect of scale on both text and image zero-shot retrieval
- Evaluation setup. We compute text-image scores using the cosine similarity between image and text embeddings and rank the top-K images (resp. text captions) for each text caption (resp. images) when evaluating on image (resp. text) retrieval.
- We evaluate on MS-COCO [46] and Flickr30K [80], following the evaluation setup and test splits from [36].
- Again we observe performance consistently improves when increasing scale following power law trends
- We measure scaling coefficients α openCLIP = −0.08 and α CLIP = −0.05 for zero-shot retrieval on MS-COCO and α openCLIP = −0.19 and α CLIP = −0.10 for Flickr30K. In contrast to zero-shot accuracy, retrieval performance shows a scaling advantage for OpenCLIP pretrained on LAION-400M/2B over CLIP pre-trained on WIT-400M.
- We also observe scale bottleneck effects. For instance, OpenCLIP ViT-L/14 model shows almost no improvement on LAION-400M when increasing the number of samples seen scale from 13B to 34B, indicatating a data scale bottleneck.
- When increasing data scale to 2B, we then observe clear improvements when going from 13B to 34B samples (see also Appendix Table 20 and 21).

### Full and few-shot linear probing
- Linear classifiers are trained on the frozen representations of various CLIP models
- Linear classifiers achieve higher accuracy than end-to-end fine-tuning
- Scaling up the model and data size leads to continued accuracy improvements

### Fine-tuning
- Next, we evaluate the effect of scale on fine-tuning performance.
- We fine-tune and evaluate on ImageNet with the timm [77] library, using the image encoder from CLIP models trained on 2B data, 34B samples seen scale.
- To get the best results, we consider two different schemes, (A) fine-tune directly on ImageNet (B) first fine-tune on a subset of the full ImageNet-22k we call ImageNet-12k2 then continue fine-tuning on ImageNet, similar to [4].
- We compare the results with OpenAI CLIP models fine-tuned with the same settings, evaluating the models using top-1 accuracy on ImageNet and the ImageNet distribution shift datasets [22,23,24,75,5].
- The OpenCLIP models range from 82.6 to 88.5% top-1 on ImageNet, comparable to the best released ImageNet models pretrained on public datasets [4].
- For additional details, including strong supervised baselines, see Appendix sec. B.2.2.
- In addition, we fine-tune and evaluate on eight diverse datasets where zero-shot models perform poorly [55,31]: Cars [41], DTD [14], EuroSAT [21], GTSRB [69], MNIST [44], RESISC45 [13], SUN397 [79], and SVHN [50].
- We fine-tune a single model jointly on the eight downstream tasks
- Effect of scale. For ImageNet fine-tuning, only the models with the largest data and samples seen were fine-tuned. Despite the narrower scale range, a similar relationship in the slope of the OpenAI CLIP vs OpenCLIP fit lines is observed across the model scales (Figure 4). Moreover, scale consistently improves accuracy when fine-tuning on other downstream tasks (Figure 5). While trends vary with the task, we find that the slope of the linear trend relating accuracy and total compute used for pre-training depends on the pre-training dataset, typically favors CLIP WIT-400M, as we observe in zero-shot experiments.

## Discussion
- Larger scale improves performance across different downstream tasks
- In line with previous studies, our work observes scaling laws of power law form across various downstream tasks
- We empirically find that scaling model, data and training samples seen results in consistent improvements on downstream zero-shot classification, retrieval, linear probing, and fine-tuning performance
- We also observe bottleneck behaviors that occur when fixing one scaling dimension while increasing others
- For instance, OpenCLIP ViT-B/32 and ViT-B/16 are bottlenecked by the number of samples seen at the 13B scale
- Increasing the number of samples seen to 34B reveals that LAION-2B brings clear improvement over LAION-400M, which would remain hidden when fixing the number of samples seen scale to a lower value
- Similar observations may occur along other scaling dimensions
- OpenCLIP ViT L/14 shows an example of data scale bottleneck on LAION-400M scale, as increasing the number of samples seen from 13B to 34B does not lead to improvements
- The benefit of using a larger number of samples seen is then revealed when going to the larger LAION-2B dataset
- Having derived scaling laws from our experimental observations, we are able to make predictions for both smaller and larger scales
- Extrapolation has its limits, as saturation effects at both lower and higher scale ranges have been previously observed
- We can however extrapolate to scales close to the ones we have already measured
- A prediction for larger ViT-g/14 trained on LAION-2B with 34B samples delivers an estimate of 79.1% ImageNet top-1 accuracy. This may appear at first sight modest compared to results reported by BASIC (85.7% [54]), LiT (85.2% [86]) or CoCA (86.1% [81]). However, these works leverage an internal JFT dataset with labels which can be used for supervised pre-training. Moreover, for 973/1000 ImageNet classes, researchers were able to manually identify a correspondance from a JFT class
- These works also use larger encoders, larger private data, and pre-train the encoders in multiple stages. Nonetheless, we estimate based on our empirical findings that further increasing model and data scale could result in competitive models even without using labeled data, additional supervised pre-training stages or additional losses
- Finally, we observe that the improvement of zero-shot ImageNet accuracy due to scaling up is accompanied by closely aligned improvements on robustness benchmarks.
- Scaling behavior depends on task type and pre-training dataset. When measuring scaling coefficients for the observed power laws, we see that OpenAI CLIP and OpenCLIP have distinct scaling advantages over each other depending on the downstream task. OpenCLIP pre-trained on LAION-400M/2B data has stronger scaling trends for zero-shot retrieval, while OpenAI CLIP pre-trained on private WIT-400M data shows stronger scaling for zero-shot ImageNet classification. We hypothesize that the observed differences are due to differences in the pre-training data, as we closely follow the architectures and pre-training recipes used for the OpenAI CLIP models. WIT-400M may have a stronger affinity to ImageNet as a result of the curation procedure, while LAION-400M/2B was filtered by a pre-trained OpenAI ViT-B/32 model relying on its similarity measurements for image-text pairs, which may have rendered the dataset more suitable for retrieval based tasks. This hypothesis can be tested by systematically varying dataset composition procedure (for example by using a stronger L/14 model for filtering crawled data) and observing the effect on scaling behavior across various task types.

## Conclusion
- We present a systematic study of scaling laws for contrastive language-image learning, investigating how scale affects performance on several downstream tasks and across adaptation methods.
- We find-in accord with previous works on uni-modal learning [35,84]-a power law relation between scale (model, data and the number of samples seen) and downstream performance in a broad range of settings, including zero-shot classification, retrieval, few-and full-shot linear probing and fine-tuning.
- Interestingly, the scaling behavior for OpenCLIP-LAION pre-trained models and for OpenAI-WIT-400M pre-trained models differ, showing distinct benefits of one over another on different downstream tasks.
- We hypothesize that such task-specific scaling differences originate from the different pre-training datasets.
- Predictions for model performance on larger scales made on the basis of the scaling laws estimate 81.9% zero-shot top-1 accuracy on ImageNet for a ViT-G/14 CLIP model trained on 68B image-text samples from scratch.
- Our study opens many directions for further investigations. Obtaining more data points for smaller and intermediate scales can provide enough sampling density to better understand the optimal configuration of model size, dataset size and number of samples seen given a fixed compute, similar to works such as [28,39].
- Scaling laws for robustness benchmarks [71] can be derived when controlling for larger accuracies observed at larger scales.
- Further, treating vision and text encoder scales separately may lead to modality specific scaling laws.
- A promising direction is to study the effect of the pre-training dataset on scaling behavior. Our observations so far hint that the data source may strongly influence task-specific scaling. This paves the road for studies on foundation datasets [68].
- Having open datasets [66,65] and open source tools [32] opens many doors for future research.
