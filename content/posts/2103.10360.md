---
title: "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
date: 2021-03-18T16:30:26.000Z
author: "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2103-10360v2.webp" # image path/url
    alt: "GLM: General Language Model Pretraining with Autoregressive Blank Infilling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2103.10360)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2103.10360).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/all-nlp-tasks-are-generation-tasks-a-general).

# Abstract
- There have been various types of pretraining architectures, but none of them perform as well as a GLM on all tasks.
- A GLM is based on autoregressive blank infilling, which improves pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans.
- GLM can be pretrained for different types of tasks by varying the number and lengths of blanks.
- On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT.

# Paper Content

## Introduction
- Language models pretrained on unlabeled texts have improved the state of the art in various NLP tasks
- Downstream task performance and the scale of the parameters have also increased in the past few years
- Existing pretraining frameworks can be categorized into three families: autoregressive, autoencoding, and encoder-decoder models
- Autoregressive models, such as GPT (Radford et al., 2018a), learn left-to-right language models
- While they succeed in long-text generation and show fewshot learning ability when scaled to billions of parameters, the inherent disadvantage is the unidirectional attention mechanism
- Autoencoding models, such as BERT (Devlin et al., 2019), learn bidirectional context encoders via denoising objectives, e.g. Masked Language Model (MLM)
- The encoders produce contextualized representations that suit natural language understanding tasks, but could not be directly applied for text generation
- Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and cross attention between them (Song et al., 2019;Bi et al., 2020;Lewis et al., 2019)
- They are typically deployed in conditional generation tasks, such as text summarization and response generation
- T5 (Raffel et al., 2020) unifies NLU and conditional generation via encoder-decoder models, but requires more parameters to match the performance of BRET-based models such as RoBERTa (Liu et al., 2019) and DeBERTa (He et al., 2021)
- None of these pretraining frameworks is flexible enough to perform competitively across all NLP tasks
- Previous works have tried to unify different frameworks by combining their objectives via multi-task learning (Dong et al., 2019;Bao et al., 2020)
- However, since the autoencoding and autoregressive objectives differ by nature, a simple unification cannot fully inherit the advantages of both frameworks
- In this paper, we propose a pretraining framework named GLM (General Language Model), based on autoregressive blank infilling
- We randomly blank out continuous spans of tokens from the input text, following the idea of autoencoding, and train the model to sequentially reconstruct the spans, following the idea of autoregressive pretraining
- While blanking filling has been used in T5 (Raffel et al., 2020) for text-to-text pretraining, we propose two improvements, namely span shuffling and 2D positional encoding
- Empirically, we show that with the same amount of parameters and computational cost, GLM significantly outperforms BERT on the SuperGLUE benchmark by a large margin of 4.6% -5.0% and outperforms RoBERTa and BART when pretrained on a corpus of similar size (158GB)
- GLM also significantly outperforms T5 on NLU and generation tasks with fewer parameters and data
- Inspired by Pattern-Exploiting Training (PET) (Schick and Schütze, 2020a), we reformulate NLU tasks as manually-crafted cloze questions that mimic human language
- Different from the BERTbased models used by PET, GLM can naturally handle multi-token answers to the cloze question via autoregressive blank filling
- Furthermore, we show that by varying the number and lengths of missing spans, the autoregressive blank filling objective can pretrain language models for conditional and unconditional generation
- Through multi-task learning of different pretraining objectives, a single GLM can excel in both NLU and (conditional and unconditional) text generation

### Pretraining Objective
- GLM is trained by optimizing an autoregressive blank infilling objective
- Given an input text, each span is replaced with a single [MASK] token, forming a corrupted text
- The model predicts the missing tokens in the spans from the corrupted text in an autoregressive manner, which means when predicting the missing tokens in a span, the model has access to the corrupted text and the previously predicted spans
- To fully capture the interdependencies between different spans, we randomly permute the order of the spans, similar to the permutation language model (Yang et al., 2019)
- Formally, let Z m be the set of all possible permutations of the length-m index sequence [1, 2, • • • , m], and s z <i be [s z 1 , • • • , s z i−1 ], we define the pretraining objective as
- We always generate the tokens in each blank following a left-to-right order, i.e. the probability of generating the span s i is factorized as:
- We implement the autoregressive blank infilling objective with the following techniques.
- x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3
- Position 1 1 2 3 4 5 5 5 5 3 3
- Position 2 0 0 0 0 0 1 2 3 1 2
- x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3
- Position 1 1 2 3 4 5 5 5 5 3 3
- Position 2 0 0 0 0 0 1 2 3 1 2
- x 4 [M] [S] x 5 x 6 [S] x 3
- Position 1 1 2 3 4 5 5 5 5 3 3
- Position 2 0 0 0 0 0 1 2 3 1 2

### Multi-Task Pretraining
- Then we evaluate the GLM's performance in a multi-task setting
- Within one training batch, we sample short spans and longer spans (document-level or sentence-level) with equal chances
- We evaluate the multi-task model for NLU, seq2seq, blank infilling, and zero-shot language modeling
- SuperGLUE. For NLU tasks, we evaluate models on the SuperGLUE benchmark
- The results
- (Bao et al., 2020)
- 43.2
- 20.4
- 40.1
- 44.0
- 21.1
- 36.1
- T5 Large (Raffel et al., 2020)
- 42.5
- 20.7
- 39.8
- 40.9
- 17.3
- 33.0
- BART Large (Lewis et al., 2019)
- 44
- are also shown in Table 1.
- We observe that with multi-task pretraining, GLM Doc and GLM Sent perform slightly worse than GLM Large , but still outperform BERT Large and UniLM Large .
- Among multitask models, GLM Sent outperforms GLM Doc by 1.1% on average.
- Increasing GLM Doc 's parameters to 410M (1.25×BERT Large ) leads to better performance than GLM Large .
- GLM with 515M parameters (1.5×BERT Large ) can perform even better.
- Sequence-to-Sequence. Considering the available baseline results, we use the Gigaword dataset (Rush et al., 2015) for abstractive summarization and the SQuAD 1.1 dataset (Rajpurkar et al., 2016) for question generation (Du et al., 2017) as the benchmarks for models pretrained on BookCorpus and Wikipedia. Additionally, we use the CNN/DailyMail (See et al., 2017) and XSum (Narayan et al., 2018) datasets for abstractive summarization as the benchmarks for models pretrained on larger corpora.
- The results for models trained on BookCorpus and Wikipedia are shown in Tables 3 and 4.
- GLM Large can achieve performance matching the other pretraining models on the two generation tasks.
- GLM Sent can perform better than GLM Large , while GLM Doc performs slightly worse than GLM Large . This indicates that the documentlevel objective, which teaches the model to extend the given contexts, is less helpful to conditional generation, which aims to extract useful information from the context.
- Increasing GLM Doc 's parameters to 410M leads to the best performance on both tasks.
- The results for models trained on larger corpora are shown in Table 2.
- GLM RoBERTa can achieve performance matching the seq2seq BART model, and outperform T5 and UniLMv2.
- Text Infilling. Text infilling is the task of predicting missing spans of text which are consistent with the surrounding context (Zhu et al., 2019;Donahue et al., 2020;Shen et al., 2020). GLM is trained with an autoregressive blank infilling objective, thus can straightforwardly solve this task.
- We evaluate GLM on the Yahoo Answers dataset (Yang et al., 2017) and compare it with Blank Language Model (BLM) (Shen et al., 2020), which is a specifically designed model for text infilling.
- From the results in Table 5, GLM outperforms previous methods by large margins (1.3 to 3.9 BLEU) and achieves the state-of-the-art result on this dataset.
- We notice that GLM Doc slightly underperforms GLM Large , which is consistent with our observations in the seq2seq experiments.

### Model Architecture
- GLM uses a single Transformer with several modifications to the architecture
- The order of layer normalization and the residual connection is critical for large-scale language models to avoid numerical errors
- We use a single linear layer for the output token prediction
- We replace ReLU activation functions with GeLUs
- One of the challenges of the autoregressive blank infilling task is how to encode the positional information
- Transformers rely on positional encodings to inject the absolute and relative positions of the tokens
- We propose 2D positional encodings to address the challenge
- Specifically, each token is encoded with two positional ids
- The first positional id represents the position in the corrupted text x corrupt . For the masked spans, it is the position of the corresponding [MASK] token. The second positional id represents the intra-span position. For tokens in Part A, their second positional ids are 0. For tokens in Part B, they range from 1 to the length of the span
- The two positional ids are projected into two vectors via learnable embedding tables, which are both added to the input token embeddings
- Our encoding method ensures that the model is not aware of the length of the masked span when  6 2 0 8 O m d N d n b g D 6 z 3 H 7 R 5  reconstructing them
- It is an important difference as compared to other models
- For text generation tasks, the given context constitutes the Part A of the input, with a mask token appended at the end
- The model generates the text of Part B autoregressively

### Discussion and Analysis
- GLM is a pretrained model that is better at inferring the probability of an answer of a certain length than other pretrained models
- GLM is better at inferring the probability of an answer of a certain length than BERT, which fails to capture the interdependencies of masked tokens
- GLM is better at inferring the probability of an answer of a certain length than T5, which always predicts spans in a fixed left-to-right order
- GLM is better at inferring the probability of an answer of a certain length than UniLM, which combines different pretraining objectives under the autoencoding framework

## Experiments
- We use a convolutional neural network to pretrain the classifier.
- We evaluate the classifier on a set of images from the MNIST dataset.
- The pretrained classifier is able to correctly classify the images with a accuracy of around 80%.

### Pretraining Setup
- For a fair comparison with BERT (Devlin et al., 2019), we use BooksCorpus (Zhu et al., 2015) and English Wikipedia as our pretraining data.
- We use the uncased wordpiece tokenizer of BERT with 30k vocabulary.
- We train GLM Base and GLM Large with the same architectures as BERT Base and BERT Large , containing 110M and 340M parameters respectively.
- For multi-task pretraining, we train two Largesized models with a mixture of the blank infilling objective and the document-level or sentencelevel objective, denoted as GLM Doc and GLM Sent . Additionally, we train two larger GLM models of 410M (30 layers, hidden size 1024, and 16 attention heads) and 515M (30 layers, hidden size 1152, and 18 attention heads) parameters with documentlevel multi-task pretraining, denoted as GLM 410M and GLM 515M .
- To compare with SOTA models, we also train a Large-sized model with the same data, tokenization, and hyperparameters as RoBERTa (Liu et al., 2019), denoted as GLM RoBERTa .

### SuperGLUE
- pretrained GLM models are evaluated on the SuperGLUE bench-mark
- the classification tasks are reformulated as blank infilling with human-crafted cloze questions
- finetuning the pretrained GLM models is described in Section 2.3
- on average, GLM outperforms BERT on most tasks with either base or large architecture
- GLM RoBERTa can still achieve improvements over the baselines, but with a smaller margin

### Ablation Study
- Table 6: GLM ablation analysis
- First, to provide an apple-to-apple comparison with BERT, we train a BERT Large model with our implementation, data, and hyperparameters (row 2).
- The performance is slightly worse than the official BERT Large and significantly worse than GLM Large .
- It confirms the superiority of GLM over Masked LM pretraining on NLU tasks.
- Second, we show the SuperGLUE performance of GLM finetuned as sequence classifiers (row 5) and BERT with clozestyle finetuning (row 3).
- Compared to BERT with cloze-style finetuning, GLM benefits from the autoregressive pretraining.
- Especially on ReCoRD and WSC, where the verbalizer consists of multiple tokens, GLM consistently outperforms BERT.
- This demonstrates GLM's advantage in handling variable-length blank.
- Another observation is that the cloze formulation is critical for GLM's performance on NLU tasks.
- For the large model, clozestyle finetuning can improve the performance by 7 points.
- Finally, we compare GLM variants with different pretraining designs to understand their importance.
- Row 6 shows that removing the span shuffling (always predicting the masked spans from left to right) leads to a severe performance drop on SuperGLUE.
- Row 7 uses different sentinel tokens instead of a single [MASK] token to represent different masked spans. The model performs worse than the standard GLM.
- We hypothesize that it wastes some modeling capacity to learn the different sentinel tokens which are not used in downstream tasks with only one blank.

## Related Work
- Pretrained language models significantly improve the performance of downstream tasks
- There are three types of pretrained models- Second, autoregressive models are trained with a left-to-right language modeling objective (Radford et al., 2018a,b;Brown et al., 2020)
- Among encoder-decoder models, BART (Lewis et al., 2019) conducts NLU tasks by feeding the same input into the encoder and decoder, and taking the final hidden states of the decoder.
- Instead, T5 (Raffel et al., 2020) formulates most language tasks in the text-to-text framework. However, both models require more parameters to outperform autoencoding models such as RoBERTa (Liu et al., 2019).
- UniLM (Dong et al., 2019;Bao et al., 2020) unifies three pretraining models under the masked language modeling objective with different attention masks.
- NLU as Generation. Previously, pretrained language models completed classification tasks for NLU with linear classifiers on the learned representations.
- GPT-2 (Radford et al., 2018b) and GPT-3 (Brown et al., 2020) show that generative language models can complete NLU tasks such as question answering by directly predicting the correct answers without finetuning, given task instructions or a few labeled examples.
- However, generative models require much more parameters to work due to the limit of unidirectional attention.
- Recently, PET (Schick and Schütze, 2020a,b) proposes to reformulate input examples as cloze questions with patterns similar to the pretraining corpus in the few-shot setting. It has been shown that combined with gradient-based finetuning, PET can achieve better performance in the few-shot setting than GPT-3 while requiring only 0.1% of its parameters.
- Similarly, Athiwaratkun et al. ( 2020 2020) also study blanking infilling models.

## Conclusions
- GLM is a general pretraining framework for natural language understanding and generation
- We show that the NLU tasks can be formulated as conditional generation tasks, and therefore solvable by autoregressive models
- GLM unifies the pretraining objectives for different tasks as autoregressive blank infilling, with mixed attention masks and the novel 2D position encodings
- Empirically we show that GLM outperforms previous methods for NLU tasks and can effectively share parameters for different tasks
- The hyperparameters for all the pre-training settings are summarized in Table 7
- Our pretraining implementation is based on Megatron-LM (Shoeybi et al., 2019) and Deep-Speed (Rasley et al., 2020)
- We include our code in the supplementary material
- Due to the size limit of supplementary material, we cannot include the pretrained models, but will make them public available in the future
- The SuperGLUE benchmark consists of 8 NLU tasks
- We formulate them as blank infilling tasks, following (Schick and Schütze, 2020b)
- Table 8 shows the cloze questions and verbalizers we used in our experiments
- For 3 tasks (ReCoRD, COPA, and WSC), the answer may consist of multiple tokens, and for the other 5 tasks, the answer is always a single token
- When finetuning GLM on the SuperGLUE tasks, we construct the input using the cloze questions in Table 8 and replace the blank with a [MASK] token. Then we compute the score of generating each answer candidate. For the 5 single-token tasks, the score is defined to be the logit of the verbalizer token. For the 3 multi-token tasks, we use the sum of the log-probabilities of the verbalizer tokens.
- Thanks to the autoregressive blank infilling mechanism we proposed, we can obtain all the log-probabilities in one pass
- We compute the cross entropy loss using the groundtruth label and update the model parameters
- For the baseline classifiers, we follow the standard practice to concatenate the input parts of each task (such as the premise and hypothesis for textual entailment, or the passage, question and answer for ReCORD and MultiRC) and add a classification layer on top of the [CLS] token representation
- We also implemented cloze-style finetuning for the other pre-trained models, but the performance was usually similar to the standard classifier, as we shown in the ablation study
- Models with blank-infilling objectives, such as T5 and our GLM, benefits more from converting the NLU tasks into cloze questions. Thus for T5 and GLM, we report the performance after such conversion in our main results
- Fot the text summarization task, we use the dataset Gigaword (Rush et al., 2015) for model fine-tuning and evaluation
- We finetune GLM LARGE on the training set for 4 epochs with AdamW optimizer. The learning rate has a peak value of 3e-5, warmup over the 6% training steps and a linear decay. We also use label smoothing with rate 0.1 (Pereyra et al., 2017)
- The maximum document length is 192 and the maximum summary length is 32. During decoding, we use beam search with beam size of 5 and remove repeated trigrams. We tweak the value of length penalty on the development set.
- The evaluation metrics are the F1 scores of Rouge-1, Rouge-2, and Rouge-L (Lin, 2004) on the test set.
