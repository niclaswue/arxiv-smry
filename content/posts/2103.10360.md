---
title: "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
date: 2021-03-18T16:30:26.000Z
author: "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2103-10360v2.webp" # image path/url
    alt: "GLM: General Language Model Pretraining with Autoregressive Blank Infilling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2103.10360)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2103.10360).


# Abstract
- There have been various types of pretraining architectures, but none of them perform as well as a GLM on all tasks.
- GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks.
- GLM can be pretrained for different types of tasks by varying the number and lengths of blanks.
- On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data.

# Paper Content

## Introduction
- Language models pretrained on unlabeled texts have substantially advanced the state of the art in various NLP tasks
- Downstream task performance as well as the scale of the parameters have also constantly increased in the past few years
- In general, existing pretraining frameworks can be categorized into three families: autoregressive, autoencoding, and encoder-decoder models
- Autoregressive models, such as GPT (Radford et al., 2018a), learn left-to-right language models
- While they succeed in long-text generation and show fewshot learning ability when scaled to billions of parameters (Radford et al., 2018b;Brown et al., 2020), the inherent disadvantage is the unidirectional attention mechanism, which cannot fully capture the dependencies between the context words in NLU tasks
- Autoencoding models, such as BERT (Devlin et al., 2019), learn bidirectional context encoders via denoising objectives, e.g. Masked Language Model (MLM). The encoders produce contextualized representations that suit natural language understanding tasks, but could not be directly applied for text generation
- Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and cross attention between them (Song et al., 2019;Bi et al., 2020;Lewis et al., 2019). They are typically deployed in conditional generation tasks, such as text summarization and response generation
- 2 . T5 (Raffel et al., 2020) unifies NLU and conditional generation via encoder-decoder models but requires more parameters to match the performance of BRET-based models such as RoBERTa (Liu et al., 2019) and DeBERTa (He et al., 2021)
- None of these pretraining frameworks is flexible enough to perform competitively across all NLP tasks
- Previous works have tried to unify different frameworks by combining their objectives via multi-task learning (Dong et al., 2019;Bao et al., 2020)
- However, since the autoencoding and autoregressive objectives differ by nature, a simple unification cannot fully inherit the advantages of both frameworks
- In this paper, we propose a pretraining framework named GLM (General Language Model), based on autoregressive blank infilling. We randomly blank out continuous spans of tokens from the input text, following the idea of autoencoding, and train the model to sequentially reconstruct the spans, following the idea of autoregressive pretraining (see Figure 1).
- While blanking filling has been used in T5 (Raffel et al., 2020) for text-to-text pretraining, we propose two improvements, namely span shuffling and 2D positional encoding. Empirically, we show that with the same amount of parameters and computational cost, GLM significantly outperforms BERT on the SuperGLUE benchmark by a large margin of 4.6% -5.0% and outperforms RoBERTa and BART when pretrained on a corpus of similar size (158GB). GLM also significantly outperforms T5 on NLU and generation tasks with fewer parameters and data.
- Inspired by Pattern-Exploiting Training (PET) (Schick and Schütze, 2020a), we reformulate NLU tasks as manually-crafted cloze questions that mimic human language.
- Different from the BERTbased models used by PET, GLM can naturally handle multi-token answers to the cloze question via autoregressive blank filling. Furthermore, we show that by varying the number and lengths of missing spans, the autoregressive blank filling objective can pretrain language models for conditional and unconditional generation. Through multi-task learning of different pretraining objectives, a single GLM can excel in both NLU and (conditional and unconditional) text generation. Empirically, compared with standalone baselines, GLM with multi-task pretraining achieves improvements in NLU, conditional text generation, and language modeling tasks altogether by sharing the parameters.

### Pretraining Objective
- GLM is trained by optimizing an autoregressive blank infilling objective
- Given an input text, each span is replaced with a single MASK token, forming a corrupted text
- The model predicts the missing tokens in the spans from the corrupted text in an autoregressive manner, which means when predicting the missing tokens in a span, the model has access to the corrupted text and the previously predicted spans
- To fully capture the interdependencies between different spans, we randomly permute the order of the spans, similar to the permutation language model (Yang et al., 2019)
- Formally, let Z m be the set of all possible permutations of the length-m index sequence [1, 2, • • • , m], and s z <i be [s z 1 , • • • , s z i−1 ], we define the pretraining objective as
- We always generate the tokens in each blank following a left-to-right order, i.e. the probability of generating the span s i is factorized as:
- We implement the autoregressive blank infilling objective with the following techniques.
- x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 0 1 2 3 1 2 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 0 1 2 3 1 2 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 0 1 2 3 1 2 x 4 [M] [S] x 5 x 6 [S] x 3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 0 1 2 3 1 2 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 0 1 2 3 1 2 x 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3 Position 1 1 2 3 4 5 5 5 5 3 3 Position 2 0 0 0 0 0 0 1 2 3 1 2

### Multi-Task Pretraining
- Then the paper evaluates the performance of a number of machine learning models on a variety of tasks, including language modeling, text infilling, and conditional generation.
- GLM Large outperforms the other models on both the BookCorpus and Wikipedia datasets, and also achieves the state-of-the-art on the Yahoo Answers dataset.
- GLM Doc slightly underperforms GLM Large , which is consistent with the findings from the seq2seq experiments.

### Model Architecture
- GLM uses a single Transformer with several modifications to the architecture
- The order of layer normalization and the residual connection is critical for large-scale language models to avoid numerical errors
- We use a single linear layer for the output token prediction
- We replace ReLU activation functions with GeLUs
- One of the challenges of the autoregressive blank infilling task is how to encode the positional information
- Transformers rely on positional encodings to inject the absolute and relative positions of the tokens
- We propose 2D positional encodings to address the challenge
- Specifically, each token is encoded with two positional ids
- The first positional id represents the position in the corrupted text x corrupt . For the masked spans, it is the position of the corresponding [MASK] token. The second positional id represents the intra-span position. For tokens in Part A, their second positional ids are 0. For tokens in Part B, they range from 1 to the length of the span
- Our encoding method ensures that the model is not aware of the length of the masked span when  6 2 0 8 O m d N d n b g D 6 z 3 H 7 R 5  reconstructing them
- It is an important difference as compared to other models
- For text generation tasks, the given context constitutes the Part A of the input, with a mask token appended at the end
- The model generates the text of Part B autoregressively

### Discussion and Analysis
- GLM is a pretrained model that is better at inferring the probability of an answer of a certain length than other pretrained models
- GLM is better at inferring the probability of an answer of a certain length than BERT, which fails to capture the interdependencies of masked tokens
- GLM is better at inferring the probability of an answer of a certain length than T5, which always predicts spans in a fixed left-to-right order
- UniLM is a pretrained model that combines different pretraining objectives under the autoencoding framework, but it always replaces masked spans with [MASK] tokens, which limits its ability to model the dependencies between the masked spans and their context

## Experiments
- We use a convolutional neural network to pretrain the classifier.
- We evaluate the classifier on a set of images from the MNIST dataset.
- The pretrained classifier is able to correctly classify the images with a accuracy of around 80%.

### Pretraining Setup
- We use BooksCorpus (Zhu et al., 2015) and English Wikipedia as our pretraining data
- We use the uncased wordpiece tokenizer of BERT with 30k vocabulary
- We train GLM Base and GLM Large with the same architectures as BERT Base and BERT Large , containing 110M and 340M parameters respectively
- For multi-task pretraining, we train two Largesized models with a mixture of the blank infilling objective and the document-level or sentencelevel objective, denoted as GLM Doc and GLM Sent
- Additionally, we train two larger GLM models of 410M (30 layers, hidden size 1024, and 16 attention heads) and 515M (30 layers, hidden size 1152, and 18 attention heads) parameters with documentlevel multi-task pretraining, denoted as GLM 410M and GLM 515M

### SuperGLUE
- pretrained GLM models are evaluated on the SuperGLUE bench-mark
- the classification tasks are reformulated as blank infilling with human-crafted cloze questions
- finetuning the pretrained GLM models is described in Section 2.3
- the cloze questions and other details can be found in Appendix B.1
- on average, GLM outperforms BERT on most tasks with either base or large architecture
- GLM RoBERTa can still achieve improvements over the baselines, but with a smaller margin

### Ablation Study
- Table 6: GLM outperforms BERT on NLU tasks
- Table 7: removing the span shuffling (always predicting the masked spans from left to right) leads to a severe performance drop on SuperGLUE
- Figure 4: removing the second dimension of 2D positional encoding hurts the performance of long text generation

## Related Work
- Pretrained language models improve the performance of downstream tasks.
- There are three types of pretrained models: autoencoding models learn a bidirectional contextualized encoder for natural language understanding via denoising objectives (Devlin et al., 2019;Joshi et al., 2020;Yang et al., 2019;Liu et al., 2019;Lan et al., 2020;Clark et al., 2020), autoregressive models are trained with a left-to-right language modeling objective (Radford et al., 2018a,b;Brown et al., 2020), and encoder-decoder models are pretrained for sequence-to-sequence tasks (Song et al., 2019;Lewis et al., 2019;Bi et al., 2020;Zhang et al., 2020).
- Among encoder-decoder models, BART (Lewis et al., 2019) conducts NLU tasks by feeding the same input into the encoder and decoder, and taking the final hidden states of the decoder.
- However, both models require more parameters to outperform autoencoding models such as RoBERTa (Liu et al., 2019).
- UniLM (Dong et al., 2019;Bao et al., 2020) unifies three pretraining models under the masked language modeling objective with different attention masks.
- NLU as Generation. Previously, pretrained language models completed classification tasks for NLU with linear classifiers on the learned representations. GPT-2 (Radford et al., 2018b) and GPT-3 (Brown et al., 2020) show that generative language models can complete NLU tasks such as question answering by directly predicting the correct answers without finetuning, given task instructions or a few labeled examples.
- However, generative models require much more parameters to work due to the limit of unidirectional attention.
- Recently, PET (Schick and Schütze, 2020a,b) proposes to reformulate input examples as cloze questions with patterns similar to the pretraining corpus in the few-shot setting. It has been shown that combined with gradient-based finetuning, PET can achieve better performance in the few-shot setting than GPT-3 while requiring only 0.1% of its parameters. Similarly, Athiwaratkun et al. ( 2020 2020) also study blanking infilling models.
- Different from their work, we pre-train language models with blank infilling objectives and evaluate their performance in downstream NLU and generation tasks.

## Conclusions
- GLM is a general pretraining framework for natural language understanding and generation
- We show that the NLU tasks can be formulated as conditional generation tasks, and therefore solvable by autoregressive models
- GLM unifies the pretraining objectives for different tasks as autoregressive blank infilling, with mixed attention masks and the novel 2D position encodings
- Empirically we show that GLM outperforms previous methods for NLU tasks and can effectively share parameters for different tasks
- The hyperparameters for all the pre-training settings are summarized in Table 7
- Our pretraining implementation is based on Megatron-LM (Shoeybi et al., 2019) and Deep-Speed (Rasley et al., 2020)
- We include our code in the supplementary material
- Due to the size limit of supplementary material, we cannot include the pretrained models, but will make them public available in the future
- The SuperGLUE benchmark consists of 8 NLU tasks
- We formulate them as blank infilling tasks, following (Schick and Schütze, 2020b)
- Table 8 shows the cloze questions and verbalizers we used in our experiments
- For 3 tasks (ReCoRD, COPA, and WSC), the answer may consist of multiple tokens, and for the other 5 tasks, the answer is always a single token
- When finetuning GLM on the SuperGLUE tasks, we construct the input using the cloze questions in Table 8 and replace the blank with a [MASK] token
- Then we compute the score of generating each answer candidate
- For the 5 single-token tasks, the score is defined to be the logit of the verbalizer token
- For the 3 multi-token tasks, we use the sum of the log-probabilities of the verbalizer tokens
- Thanks to the autoregressive blank infilling mechanism we proposed, we can obtain all the log-probabilities in one pass
- We compute the cross entropy loss using the groundtruth label and update the model parameters
- For the baseline classifiers, we follow the standard practice to concatenate the input parts of each task (such as the premise and hypothesis for textual entailment, or the passage, question and answer for ReCORD and MultiRC) and add a classification layer on top of the [CLS] token representation
- We also implemented cloze-style finetuning for the other pre-trained models, but the performance was usually similar to the standard classifier, as we shown in the ablation study
- Models with blank-infilling objectives, such as T5 and our GLM, benefits more from converting the NLU tasks into cloze questions. Thus for T5 and GLM, we report the performance after such conversion in our main results
- Fot the text summarization task, we use the dataset Gigaword (Rush et al., 2015) for model fine-tuning and evaluation
- We finetune GLM LARGE on the training set for 4 epochs with AdamW optimizer. The learning rate has a peak value of 3e-5, warmup over the 6% training steps and a linear decay. We also use label smoothing with rate 0.1 (Pereyra et al., 2017)
- The maximum document length is 192 and the maximum summary length is 32. During decoding, we use beam search with beam size of 5 and remove repeated trigrams. We tweak the value of length penalty on the development set. The evaluation metrics are the F1 scores of Rouge-1, Rouge-2, and Rouge-L (Lin, 2004) on the test set.
- For the question generation task, we use the SQuAD 1.1 dataset (Rajpurkar et al., 2016) and follow the dataset split of (Du et al., 2017)
- The optimizer hyperparameters are the same as those of abstractive summarization
- The maximum passage length is 464 and the maximum question length is 48. During decoding, we use beam search with beam size 5 and tweak the value of length penalty on the development set. The evaluation metrics are the scores of BLEU-1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and Rouge-L (Lin, 2004).

## C Results on Other NLU Benchmarks
- GLUE is a NLU benchmark
- GLM can outperform BERT with the same amount of parameters, but with a smaller margin
- SQuAD is an NLU benchmark
- GLM can outperform BERT with the same amount of parameters, but with a smaller margin

## D Text Generation Samples
- He was drafted in the first round of the 1962 NFL Draft by the Los Angeles Rams.
- He played for the Rams from 1962 to 1965.
- He was traded to the Detroit Lions in 1965.
- He played for the Lions from 1965 to 1966.
