---
title: "Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized Photography"
date: 2022-12-22T18:54:34.000Z
author: "Ilya Chugunov, Yuxuan Zhang, Felix Heide"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12324v1.webp" # image path/url
    alt: "Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized Photography" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12324)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12324).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/shakes-on-a-plane-unsupervised-depth).

# Abstract
- Modern mobile burst photography pipelines often disregard the 3D nature of the scene they capture, treating pixel motion between images as a 2D aggregation problem.
- We show that in a "long-burst", 42 12-megapixel RAW frames captured in a two-second sequence, there is enough parallax information from natural hand tremor alone to recover high-quality scene depth.
- To this end, we devise a test-time optimization approach that fits a neural RGB-D representation to long-burst data and simultaneously estimates scene depth and camera motion.
- Our plane plus depth model is trained end-to-end, and performs coarse-to-fine refinement by controlling which multi-resolution volume features the network has access to at what time during training.
- We validate the method experimentally, and demonstrate geometrically accurate depth reconstructions with no additional hardware or separate data pre-processing and pose-estimation steps.

# Paper Content

## Introduction
- Over the last century we saw not only the rise and fall in popularity of film and DSLR photography, but of standalone cameras themselves.
- Modern cellphones offer doubledigit megapixel image streams at high framerates; optical image stabilization; on-board motion measurement devices such as accelerometers, gyroscopes, and magnetometers; and, most recently, integrated active depth sensors [43].
- As users often photograph people, plants, food items, and other complex 3D shapes, depth can play a key role in object understanding tasks such as detection, segmentation, and tracking [32,63,80].
- 3D information can also help compensate for non-ideal camera hardware and imaging settings through scene relighting [21,55,79], simulated depth-offield effects [1,71,72], and frame interpolation [3].
- Beyond helping improve or understand RGB content, depth itself is a valuable output for simulating objects in augmented reality [6,14,44,64] and interactive experiences [26,36].
- Active depth methods such as pulsed time-of-flight [46] (e.g. LiDAR), correlation timeof-flight [38] (e.g. Kinect v2), and structured light [61,83] (e.g. Kinect v1) use controlled illumination to help with depth reconstruction.
- While this can help these methods operate more independent of image content, it also comes with complex circuitry and power demands [28].
- Miniaturization for mobile applications results in very lowresolution sub-kilopixel sensors [9,27,74].
- The Apple iPhone 12-14 Pro devices, which feature one of these miniaturized sensors, use depth derived from RGB, available at 12 mega-pixel resolution, to recover scene details lost in the sparse LiDAR measurements.
- While how exactly they use the RGB stream is unknown, occluding camera sensors reveals that the estimated geometry is the result of monocular RGB-guided depth reconstruction.
- Existing passive monocular depth estimation methods leverage training data to learn shape priors [7,30,59] -e.g. what image features imply curved versus flat objects or occluding versus occluded structures -but have a hard time generalizing to out-of-distribution scenes [48,60].
- Multiview depth estimation methods lower this dependence on learned priors by leveraging parallax information from camera motion [17,69] or multiple cameras [45,67] to recover geometrically-guided depth.
- The recent explosion in neural radiance field approaches [49,50,66,81] can be seen a branch of multi-view stereo where a system of explicit geometric constraints is swapped for a more general learned scene model.
- Rather than classic feature extraction and matching, these models are fit directly to image data to distill dense implicit 3D information.
- Returning to the context of mobile imaging, even several seconds of continuous mode photography, which we refer to as a "long-burst", contain only millimeter-scale view variation from natural hand tremor [12].
- While these microbaseline [33] shifts are effectively used in burst superresolution and denoising methods [58,76] as indirect observations of content between sensor pixels, 3D parallax effects on pixel motion are commonly ignored in these models as the depth recovered from this data is too coarse for sub-pixel refinement [31,33,82].

## Related Work
- Active depth reconstruction relies on patterned illumination to directly infer object shape
- In contrast, time-of-flight (ToF) depth sensors use the round trip time of photons themselves -how long it takes light to reach and return from an object -to infer depth
- Indirect ToF does this by calculating phase changes in continuously modulated light
- Direct ToF times how long a pulse of light is in flight to estimate depth
- The LiDAR system found in the iPhone 12-14 Pro devices is a type of direct ToF sensor built on low-cost single-photon detectors
- While active LiDAR depth measurements can help produce metric depth estimates, without scale ambiguity, existing mobile depth sensors have very limited sub-kilopixel spatial resolution
- Passive Depth Reconstruction relies on single-image passive methods to leverage the correlation between visual and geometric features to estimate 3D structure
- Examples include depth from shading [4,77], focus cues [78], or generic learned priors [7,30,59]
- Learned methods have shown great success in producing visually coherent results, but rely heavily on labeled training data and produce unpredictable outputs for out-of-distribution samples
- Multiview and structure from motion works leverage epipolar geometry [24], the relationship between camera and image motion, to extract 3D information from multiple images
- Methods typically either directly match RGB features [18,65], or higher-level learned features [42,67], in search of depth and/or camera parameters which maximize photometric consistency between frames
- COLMAP [62] is a widely adopted multi-view method which many neural radiance works [49,50] rely on for camera pose estimates
- In the case of long-burst photography, this problem becomes significantly more challenging as many different depth solutions produce identical images under small view variations
- Work in this space either relies on interpolation between sparse feature matches [31,33,82] or additional hardware [12] to produce complete depth estimates
- Our work builds on these methods to produce both dense depth and high-accuracy camera motion estimates from long-burst image data alone, with a single model trained end-to-end rather than a sequence of disjoint data processing steps

## Long-Burst Photography

## Unsupervised Depth Estimation
- Projection model: Given an image stack I(u, v, N), we aim to condense the information in I(u, v, N) to a single compact projective model.
- In our problem setting, we are given a long-burst image stack I(u, v, N) and device rotation values R(N), supplied by an on-board gyroscope, and are tasked with recovering depth D(u, v) and translation T (N) which make these observations consistent.
- Given the sheer number of pixels in I(u, v, N), in our case about 500 million, exhaustively matching and minimizing pixel-topixel loss is both computationally intractable and ill-posed.
- Under small camera motion, many depth solutions for a pixel can map it to identical-colored pixels in the image, especially in textureless parts of the scene.
- Traditional multiview stereo (MVS) and bundle adjustment methods tackle this problem with feature extraction and matching, optimizing over a cost-volume orders of magnitude smaller than the full image space.
- Here we strongly diverge from previous small motion works [12,31,82]. Rather than divide the problem into feature extraction and matching, or extract features at all, we propose a single fully differentiable forward model trained end-to-end.
- Depth is distilled as a product of fitting this neural scene model to long-burst data.
- We start by redefining I(u, v) from a static reference image to a learned implicit representation where f I is a multi-layer perceptron (MLP) [29] with learned weights θ.
- This MLP learns a mapping from γ I (u, v), a positional encoding of sampled camera coordinates, to image color. Specifically, we borrow the multiresolution hash encoding from Müller et al. [53] for its spatial aggregation properties.
- The parameters in params γI determine the minimum N γI min and maximum N γI max grid resolutions, number of grid levels L γI , number of feature dimensions per level F γI , and overall hash table size T γI .
- Implicit Depth on a Plane Model.

## Assessment
- Our approach outperforms the most similar purely multi-view methods BARF [41] and Structure From Small Motion (SfSM) [31], both of which estimated depth and camera motion directly from an input image stack.
- We also compare to learned monocular methods: iPhone's 14 Pro's native depth output and MiDaS [60], a robust single-image approach.
- Lastly, we compare to Robust Consistent Video Depth Interpolation (RCVD) [37] and Handheld Multi-frame Neural Depth Refinement (HNDR) [12], which both use multi-view information to refine initial depth estimates initialized from a learned monocular approach.
- The latter of which is most directly related to our approach as it targets close-range objects imaged with multi-view information from natural hand tremor, but relies on iPhone LiDAR hardware for depth initialization and pose estimation.
- All baselines were run on processed RGB data synchronously acquired by our data capture app, except for HNDR which required its own data capture software which we ran in tandem to ours.
- We note that other neural scene volume methods such as [49] require COLMAP as a pre-processing step, which fails to find pose solutions for our long-burst data.
- To assess absolute performance and geometric consistency, we scan a select set of complex 3D objects, illustrated in Fig. 8, with a commercial high-precision turntable structured light scanner (Einscan SP).
- We use this data to generate ground truth object meshes, which we register and render to depth with matching camera parameters to the real captures.
- For quantitative depth assessment, we use relative absolute loss and log loss as described and commonly used in monocular depth literature [70]; see the Supplemental Document for details.
- Tested on a variety of scenes, illustrated in Fig. 7, we demonstrate high-quality object depth reconstruction outperforming existing learned, mixed, and multi-view only methods.
- Of particular note is how we are able to reconstruct small features such as Dragon's tail, Harold's scarf, and the ear of the Tiger statue consistent to the underlying scene geometry.
- Our coarse-to-fine approach also allows us to reconstruct scenes with larger low-texture regions, such as Harold's head, which produces striped depth artifacts for HNDR as it can only refine depth within a patch-size of high-contrast edges.
- Compared to other purely multi-view methods, our plane plus depth offset approach avoids spurious depth solutions in low-parallax regions around the objects, cleanly segmenting them from their background.
- In Fig. 8 we highlight our method's ability to produce highquality object reconstructions.

## Discussion and Future Work
- From only a stack of images acquired during long-burst photography, with parallax information from natural hand-tremor, it is possible to recover high-quality, geometrically-accurate object depth
- Forward Models. While we focus on static scenes modeled by a single implicit RGB-D representation, our approach could potentially be modified to include differentiable models of object motion, deformation, or occlusion
- Image Refinement. We primarily use the learned image I(u, v) as a vehicle for depth optimization, but is possible to flip this. The learned depth D(u, v) can potentially be used as a vehicle for aggregating RGB information between frames for image deblurring and denoising tasks.

## Supplemental Document
- Data is from a real-world image dataset
- Train and evaluate a deep learning network on the data
- Find that the network is able to generalize well to new images
- Find that the network is also sensitive to changes in the encoding parameters
- Find that the network is also sensitive to changes in the network parameters
- Find that the network is also sensitive to changes in the data
- Find that the network is also sensitive to changes in the reconstruction parameters
- Find that the network is also sensitive to changes in the imaging parameters
- Find that the network is also sensitive to changes in the depth data
- Find that the network is also sensitive to changes in the image matting parameters

## A. Implementation Details
- We acquire long-burst data through a custom app built on the AVFoundation camera framework for iOS 16.
- While the vanilla AVFoundation framework offered a default method to capture burst or bracketed sequences, it was limited to only four frame sequences with significant overhead between captures, necessitating custom streaming code to save a longer continuous sequence of RAW data.
- A restriction we could not lift, however, is the inability to stream RAW captures from multiple cameras simultaneously.
- If this were possible, you could potentially use parallax and focus cues between two synchronized camera streams -for example the wide and ultra-wide cameras -to further improve reconstruction in the overlap of their fields of view.
- During capture, we record the following: Bayer CFA RAWs (42 frames 4032×3024px), processed RGB images (42 frames 1920×1440px), depth maps (42 frames 320×240px), frame timestamps, ISO, exposure time, brightness estimates, black level, white level, camera intrinsics, lens distortion tables, device acceleration estimates ( ∼ 200 measurements at 100Hz), device rotation estimates ( ∼ 200), and motion data timestamps ( ∼ 200).
- To account for lens shading effects in bright scenes, as illustrated in Fig. 1, we estimate a shade map with the help of a simple diffuser and uniform light source.
- We sample 1024 points (u, v) per iteration of training, projecting these to 42×1024 points in the image stack I(u, v, N), corresponding to 1024 points per frame.
- We perform 256 iterations per epoch, for 100 epochs of training with the Adam optimizer [34] with betas (0.9, 0.99) and epsilon 10 −15 .
- We exponentially decay learning rate during training with a factor of 0.98 per epoch.
- Training on a single Nvidia A100 takes approximately 15 minutes.
- Evaluation.
- To generate depth maps we sample D(u, v) at a grid of (H, W) = (1920, 1440) points (u, v) ∈ [0, 1].
- To reduce noise introduced by the stochastic training process we median filter this result with kernel size 13 before visualization.
- For depth evaluation, we use relative absolute error L1-rel and scale invariant error sc-inv metrics, that is Table Size 2^6 which are often used in the monocular depth estimation literature [70] to compare approaches with varying scales and representations of depth.
- For methods such as MiDaS and RCVD we first convert inverse depth to depth before applying these metrics.
- We purposely avoid using photometric loss or reprojection error as comparison metrics [12] for similar arguments as discussed in Gao et al. [20].

## B. Additional Ablation Experiments
- γ D is a multiresolution hash encoding that directly controls what spatial information f D has access to during training
- Increasing the number of levels L γD and effective max resolution N γD max increases the spatial frequency of reconstructed depth features
- Scenes such as Branch contain both high-frequency image and depth content, thin textured needles, and are best reconstructed by a fine resolution grid with L γD = 16
- The Desk Gourds, however, have small image features in the patterns on the gourds, but relatively low-frequency depth features. Setting L γD = 16 allows the network to overfit to these features and bleed image texture into the depth reconstruction
- We select L γD = 8 as a compromise between these imaging settings, but in practice, like other neural scene fitting task [53], different scenes have different optimal encoding parameters for maximum reconstruction quality
- We find hash table size T γD significantly easier to tune, as choosing an overly large table size primarily affects model storage size, rather than reconstruction quality
- We thus choose T γD = 2 14 , the smallest table size which does not lower the detail of depth reconstruction

## C. Additional Reconstruction Results
- Our approach performs multi-view depth estimation through ray reprojection
- Each of these scenarios presents its own set of challenges and direction of study
- In the Dynamic scene, we fail to reconstruct accurate depth for the majority of the plant leaves as they undergo deformation far larger than the parallax effects we observe in the long-burst
- In the Textureless and Distant scenes, we are able to reconstruct the plant, but the textureless planter provides no multi-view information from which to estimate depth except for along its edges, which we can track relative to the motion of the background
- The church in Distant is so far from the camera that it exhibits only fractions of pixel in disparity over the entire long-burst
- In both these scenarios we need a mechanism to aggregate information in image space to make up for the lack of parallax
- In the Thin Structures reconstruction, we are partially successful, as in the foreground region we are able to track and reconstruct the depth of the thin orange mesh, but breaks down when it begins to overlap with the traffic cone
- Depth-from-defocus cues [78] could potentially help regularize reconstruction in these areas
- Additional Comparisons
- Fig. 8 provides additional qualitative comparisons of our proposed approach to a wide set of baseline methods

## D. Synthetic Evaluation
- To further validate our approach we use the highfidelity structured light object scans we acquired for quantitative evaluation to generate simulated long-burst captures.
- Illustrated in Fig 9, we apply a Voronoi color texture to the surface of these meshes, and place them in front of a tilted background plane with an outdoor image texture.
- We add depth-of-field effects and match camera intrinsics to our real captures -using the ARKit poses captured by the software from Chugunov et al. [12] to generate realistic hand tremor motion paths -and render frames at 16-bit color depth with Blender's Eevee engine.
- This synthetic data allows us to not only validate the fidelity of our object reconstructions, but also our estimated camera motion paths, for which we cannot otherwise get ground truth during ordinary captures.
- We find that for this synthetic data, in the absence of noise, lighting changes, and other imaging non-idealities, we are able to recover nearly ground truth reconstructions of both the objects and background planes.
- This supports our plane plus offset depth model, which fits the simple plane to the out-of-focus background content instead of generating spurious depth estimates for regions without reliable parallax information.
- Though the colorful object textures make single-view depth estimation visually difficult, as illustrated by artifacts in the MiDaS reconstructions, these high-contrast cues allow our method to reconstruct even tiny features such as the tusks of the Synth-Ganesha.
