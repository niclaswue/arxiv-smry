---
title: "ART: Automatic multi-step reasoning and tool-use for large language models"
date: 2023-03-16T01:04:45.000Z
author: "Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer and 1 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-09014v1.webp" # image path/url
    alt: "ART: Automatic multi-step reasoning and tool-use for large language models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.09014)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.09014).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/art-automatic-multi-step-reasoning-and-tool).

# Abstract
- LLMs can perform complex reasoning in few- and zero-shot settings
- LLMs can generate intermediate chain of thought reasoning steps
- LLMs can use external tools to support computation
- Prior work requires hand-crafting task-specific demonstrations
- ART uses frozen LLMs to automatically generate intermediate reasoning steps
- ART selects demonstrations from a task library
- ART integrates external tool output before resuming generation
- ART improves performance on unseen tasks in BigBench and MMLU benchmarks
- ART is extensible and can be improved with minimal human intervention

# Paper Content

## Introduction
- In-context learning allows LLMs to quickly adapt to new tasks with natural language instructions and a few demonstrations
- LLMs can be used without annotating large datasets or hosting the LLM
- There are limitations around multi-step reasoning
- Recent work proposes prompting LLMs to mimic a chain of thought or providing them with access to tools
- Existing methods for chained reasoning with tool use are difficult to extend to new tasks and tools
- ART is a framework that automatically generates decompositions for instances of new tasks and selects and uses the most appropriate available tools
- ART retrieves demonstrations of related tasks from a task library to enable few-shot decomposition and tool use
- ART provides the LLM with demonstrations of how to decompose instances of several related tasks and how to select and use tools
- ART matches or outperforms automatically generated CoT reasoning chains on tasks
- Tool-use improves performance on test tasks
- ART improves over direct few-shot prompting
- ART enables human intervention and improvement of the reasoning process
- ART with additional human feedback surpasses the best-known results for GPT3

## Related work
- Finetuning LLMs on public NLP datasets is an effective technique for cross-task generalization
- Aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback further improves in-context learning performance
- Finetuning on an aggregated mixture of tasks with scaling models to 540B parameters achieves state-of-the-art in-context learning performance
- Chain-of-thought prompting is a popular gradient-free technique that encourages LLMs to generate intermediate reasoning steps
- LLMs can generate CoT-style multi-step reasoning in a zero-shot manner
- LLMs can automatically generate CoT-style prompts
- ART uses API access to InstructGPT and Codex to leverage their emergent in-context learning abilities
- ART introduces a common language that enables cross-task demonstrations and flexible and extensible tool use

### Overview
- ART is presented with a new task description and input instance
- ART retrieves similar tasks from a task library
- Task library is written in a specific format defined by a custom parsing expression grammar
- Grammar decomposes each task instance into a sequence of sub-steps
- Sub-steps contain symbols corresponding to tools in a tool library
- LLM writes its own program at generation time
- ART pauses generation whenever a tool call is encountered
- Humans can add new decomposition demonstrations or tools to improve performance

### Task library
- Constructed a library of programs for a small seed set of tasks from Big-Bench
- Identified five skills useful across more than half of the tasks in Big-Bench
- Grouped tasks into clusters such as Arithmetic and Algebra
- Wrote programs for a few instances of each task, including calls to external tools
- Defined a query language to represent decomposed reasoning steps and incorporate function calls to external tools

### Tool library
- Whenever a sub-task query name matches a tool name, generation is stopped and resumed after the tool is called and its output is incorporated
- A tool library is seeded with tools that have demonstrations in the task library
- Search is done using SerpAPI2
- Codex model is used for code generation
- Python code is run in a virtual environment with pre-installed packages

### Human feedback
- Pilot use of task-specific feedback in Table 6
- Edit 5 random instances of model-generated programs that resulted in errors
- Corrections include sub-steps, adding missing sub-steps, and defining new tools
- Comparing human feedback applied to CoT-style reasoning
- Corrections include 35% of tokens in baseline and 15.7% of tokens in ART programs
- Corrections lead to significant gains in performance on task

## Experimental setup
- Evaluated on 15 tasks from task library and 19 additional test tasks from BigBench
- Also evaluated on random subset of tasks from MMLU benchmark
- Also evaluated on subset of tasks used to evaluate Toolformer
- Used InstructGPT and Codex with temperature set to 0.3
- Number of seed tasks in prompt set to 3, used 2 demonstration programs from each task
- Compared with Few-shot/Direct, Auto-CoT, ART-tool and GPT-3 Best baselines

## Results
- Evaluated ART on tasks from task library and BigBench, MMLU, and QA benchmarks
- ART can be improved with more compute and human feedback

### Results on the task library
- ART improves performance over fewshot learning by 14.9%
- ART does not do as well on language games, code description, and auto debugging
- ART outperforms AutoCoT on most tasks without any tool use
- Tool use improves performance by 7.91%
- ART is stronger or comparable to best published GPT-3 results in 5/8 tasks

### Test tasks (cross-task transfer)
- ART outperforms few-shot learning on BigBench test tasks
- ART has significant improvements on arithmetic tasks
- ART is competitive on BigBench without supervision for decompositions
- ART outperforms all baselines on 5/6 tasks from MMLU benchmark
- ART outperforms Toolformer by a large margin on 5/6 tasks
- Self-consistency consistently improves performance for ART

## Conclusion
- Introduced ART, a gradient-free approach for automatic multi-step reasoning generation and automatic tool-use
- Lightweight grammar to represent multi-step reasoning as a program
- Extensible library of seed tasks for which programs are authored
- Tool library consisting of useful external utilities
- Interpretable reasoning framework allows humans to improve task decomposition and tool use
- ART achieves substantial improvement over few-shot prompting and automatic generation of CoT reasoning
- ART substantially exceeds performance on hand-crafted CoT prompts when human feedback is incorporated
- ART benefits from approaches such as self-consistency, or from new and more powerful LLMs trained for tool use
- Analyzed input-output instances of all 200 tasks in BigBench, filtered out text classification and short answer generation tasks in English
- Created a list of reasoning skills relevant to solving each task
- Focused on five most used skills that cover a significant proportion of BigBench tasks
- Defined a parsing expression grammar to describe language used to write multi-step reasoning programs
- Added Prolog Engine tool in formal fallacies task
- Few-shot baseline uses 3 randomly chosen input-output instances
- Auto CoT baseline generates automatic CoT-style multi-step reasoning in a free-form natural language
- Best GPT-3 results reported in approaches that use multi-step reasoning (like CoT) and tool use, with human supervision
- Human-authored CoT reasoning for several tasks in BigBench
- Human feedback provided by humans for five BigBench tasks
