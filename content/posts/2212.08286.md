---
title: "ALERT: Adapting Language Models to Reasoning Tasks"
date: 2022-12-16T05:15:41.000Z
author: "Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08286v1.webp" # image path/url
    alt: "ALERT: Adapting Language Models to Reasoning Tasks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08286)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08286).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/alert-adapting-language-models-to-reasoning).

# Abstract
- Current large language models can perform reasonably well on complex tasks
- These models are applying reasoning skills they have learnt during pre-training and reason
- To tease apart these possibilities, we introduce ALERT, a benchmark and suite
- ALERT provides a test bed to asses any language model on fine-grained

# Paper Content

## Introduction
- Large language models (LLMs) (e.g., GPT-3 (Brown et al., 2020a), PALM (Chowdhery et al., 2022a), OPT (Zhang et al., 2022)) have shown increasing in-context learning capabilities with scaling up the model and data size.
- Even the largest of these models still struggle with tasks such as commonsense reasoning (West et al., 2022), and math word problems (Hendrycks et al., 2021b) which require arithmetic reasoning or symbolic manipulation (Rytting and Wingate, 2021).
- Table 1 presents some examples that require certain reasoning skills.
- Predictions by ChatGPT are shown in Table 8 in Appendix.
- powerful LLMs (text-davinci-0031 and ChatGPT2 ) fail to make correct predictions.
- To improve large LLMs' performance on tasks that require multiple steps of reasoning, recent work used different prompting methods which included a rationale with the final answer in the form of: scratchpad for arithmetic and logical reasoning (Nye et al., 2021), chain-of-thought (Wei et al., 2022) for practically any task, or adding let's think step-by-step (Kojima et al., 2022) to prompt models to generate explanations.
- Other works such as Chung et al. (2022) integrate step-by-step explanations into the finetuning stage (CoT-finetuning).
- Although these approaches improve downstream task performance and interpretability on many tasks, it is less known which skills these models are triggering and to what extent they require reasoning skills.
- It also remains unclear how often the stated reasoning steps actually support the final task predictions.
- For instance to correctly answer the questions in Table 1 a combination of logical, commonsense, math and spatial reasoning skills are required.
- In this work, to gain a deeper understanding of LLMs reasoning abilities in in-context learning settings, we introduce ALERT, a new pipeline to benchmark different LLMs on various reasoning skills.
- Unlike existing commonly used benchmarks (e.g., Mishra et al. (2022); Wang et al. (2022c); Srivastava et al. (2022)), we designed ALERT to evaluate LLMs on fine-grained reasoning skills.
- It spans over 20 datasets and covers 10 different reasoning skills including logistic, causal, commonsense, abductive, spatial, analogical, argument and deductive reasoning as well as textual entailment, and mathematics (see Figure 2).
- ALERT enables easy benchmarking of any LM (e.g., pre-trained, finetuned, CoT-finetuned) on a rich set of new inference methods including zero-shot, few-shot and chain-of-thought (CoT).
- Using ALERT we further investigate whether finetuning can improve LM's performance on downstream reasoning tasks. Specifically, we are interested in diagnosing what has been really improved when we see a performance increase on reasoning tasks.
- Is it because models have seen similar data in finetuning stage? Or is it because models have seen prompts in a specific template and memorize the template? Or have the reasoning skills really been improved?
- We further explored the above three possibilities.
- To study the above questions, we compare three different model types: a pre-trained model and two types of finetuned models.
- The first finetuned model (which we refer as finetuned model) is a standard finetuned LM while the second one is the rationale-based finetuned model (Chung et al., 2022) (which we refer as CoT-finetuned model).
- We use these three...

## Motivation and Our Benchmark
- The analyses in ALERT are inspired by a scientific question: To what extent do LLMs learn generalizeable reasoning abilities?
- This question motivates our focus on measuring LLMs performance on tasks that require contextual understanding and perform multi-step operations, which are crucial to perform well on complex downstream tasks.
- In order to construct the datasets of ALERT, we select some tasks from NIV2 benchmark (Wang et al., 2022c) and perform the following operations: (1) Omit extremely hard tasks. We design ALERT so that it can be used to benchmark a variety of LLMs, from pre-trained to finetuned to instruction-tuned models.
- To select such tasks, we apply several heuristics: firstly, we manually omit tasks that heavily rely on instructions. Some tasks are hard to solve when only in-context examples (demonstrations) are provided (e.g., the example in Figure 1). Secondly, we apply pretrained OPT-13B model (Zhang et al., 2022) on each NIV2 task and keep the tasks with decent performance (ROUGE-L > 5.0). Thirdly, we omit tasks on which humans fail to get decent performance given the ground truth labels from NIV2.
- For example, task963_librispeech_asr_next_word_ prediction (Weir et al., 2020) provides a prompt "Joey's favourite food is ___", with the ground truth answer "sandwiches". Without any context or background information, the answer can be any food thus it is extremely hard for humans to accurately predict "sandwiches".
- After the above steps, we select tasks that represent a variety of reasoning skills and construct ALERT reasoning benchmark, where Table 2 shows details about our benchmark.
- In order to make it easier to analyze reasoning skills in Section 4.2.2, reasoning skills in ALERT have been divided into in-domain skills and held-out skills in Figure 2.
- The "in-domain" here are referred to these reasoning skills can be learned from finetuning data, but these in-domain datasets are actually on-held datasets, which are not seen in finetuning.

## Experiment Setup

### Models
- Pre-trained models are used
- OPT (Zhang et al., 2022) is used
- For finetuned models (OPT-FT), the OPT models are finetuned on datasets without explanations
- For COT-finetuned models (OPT-CoT), the OPT models are finetuned on data with rationales

### Finetuning Details
- Our finetuning corpus is comprised of 10 datasets
- The datasets have been finetuned for different purposes
- The finetuning has been done by different researchers
- The finetuning has been done in different ways

### Evaluation
- Templates used during inference stage
- Classification accuracy used for evaluation
- ROUGE-L used to measure performance
- Exact-match score used for evaluation

## Analysis

### Does finetuning help?
- Figure 4: Average performance across all evaluation tasks in our benchmark
- We evaluate three models: OPT, OPT-FT, OPT-CoT at two different scales: 1.3B and 13B
- We report both the average and the best (max) ROUGE-L score and exact-match score across 5 templates (as detailed in Section 3.3)
- The best ROUGE-L score is calculated by obtaining max ROUGE-L score across 5 different templates for each task and then getting the average score of 20 tasks.

### What does LLMs learn during finetuning?
- We find that CoT-finetuning improves performance on reasoning tasks in general.
- However, what exactly does the LM learn during the finetuning stage is still under explored.
- Thus, we study the role of finetuning from three perspectives: data memorizing, reasoning skills transfer, and prompt template memorizing.
- Gururangan et al. ( 2020) finds that the performance gain is larger when the finetuning dataset is more dissimilar to the pre-training dataset.
- However, their conclusion is made by a single-task finetuning.
- They evaluate their model on the same dataset that was used for finetuning.
- A more thorough evaluation dictates that finetuned models (Wei et al., 2021b;Chung et al., 2022) be evaluated on heldout datasets.
- As such, in Figure 3 inference as we adopt in this paper.
- To verify that the improvement of finetuning is attributed to seeing more data during the finetuning stage, we measure how dissimilar are our training and test data used in finetuning and evaluation respectively.
- If higher similarity leads to better performance, it may indicate that the improvements of finetuned LLMs are due to seeing more similar data during the finetuning stage.
- Following (Gururangan et al., 2020), we use unigram vocabulary overlap to measure the data similarity.
- More specifically, we divide our tasks into three categories.
- The first category has 10 datasets and there is up to 10% overlap between the finetuning data and evaluation data.
- The second category has 3 datasets with an overlap between 10% and 30%.
- The third category has 7 datasets with an overlap over 30%.
- Details can be found in Table 6 in appendix A.4.
- We measure performance improvements of OPT-FT and OPT-CoT compared against the pretrained OPT model.
- We present both ROUGE-L score (top) and relaxed-match score (down) in Figure 5.
- The results indicate that there is no strong correlation between the vocabulary overlap between fineuning and evaluation datasets and the performance of the model (neither a higher nor a lower vocabulary overlap always translate to a performance improvement).
- OPT-CoT achieves the best ROUGE-L and relaxed-match scores both in settings when there is a medium (10%-30%) level of vocabulary overlap.
- We don't observe a consistent pattern on OPT-FT models either.
- Overall, for these challenging tasks, seeing similar data during finetuning stage does not guarantee performance improvement.
- In this subsection, we measure whether finetuning can improve reasoning skills.
- The average ROUGE-L scores are calculated for each reasoning skill on 6 models (1.3B OPT, 1.3B OPT-FT, 1.3B OPT-CoT, 13B OPT, 13B OPT-FT, 13B OPT-CoT).
- Figure 6 shows the difference between OPT-FT and OPT, and the difference between OPT-CoT and OPT models' performances.
- For example, OPT-FT 1.3B model yields on average 3.5 less ROUGE-L points than OPT 1.3B model on the tasks of logistic reasoning.
- We measure 10 reasoning skills, 6 of which are in-domain skills, i.e. similar reasoning skills are seen during finetuning.
- From Figure 6 we can see that OPT-CoT generally improves in-domain reasoning skills, but OPT-FT does not show the same pattern.
- All four LLMs showed improved reasoning abilities on textual entailment, abductive reasoning, and analogical reasoning tasks.
- We find that these three skills can not be easily obtained during the pre-training stage since pretraining stage only contains plain text....

## Related Works
- Large language models (e.g. GPT-3 (Brown et al., 2020b), OPT (Zhang et al., 2022), PaLM (Chowdhery et al., 2022b)) have demonstrated remarkable performance on various NLP tasks.
- On the other hand, reasoning tasks (e.g. mathematical reasoning and commonsense reasoning) remain more challenging.
- A few followup research trajectories have been proposed to improve LLMs' reasoning abilities.
- Kojima et al. (2022) shows that LLMs can be decent zero-shot reasoners by simply appending "Let's think step by step" to the prompt.
- Wei et al. (2022) adds a series of intermediate reasoning steps to improve LLMs' reasoning abilities.
- Wang et al. (2022a) further proposes to expand prompts to include rationales in each few-shot example. Additionally, Fu et al. (2022) discovers that prompting with higher reasoning complexity achieves substantial gains on math word tasks.
- To tackle problems harder than demonstration examples, Zhou et al. (2022) first reduces a complex problem into a list of subproblems and then solve subproblems sequentially.
- Another line of research is to improve the naive decoding strategy, Wang et al. (2022b) introduces a self-consistency strategy which selects the most consistent answer among a set of reasoning paths.
- Existing Reasoning Benchmarks. Many benchmarks are used for evaluating language models' performance, such as BIG-Bench (Srivastava et al., 2022), Natural Instruction V2 (NIV2) (Wang et al., 2022c), MMLU (Hendrycks et al., 2020).
- Although they contain some reasoning tasks, none of them are specifically designed to test models' reasoning skills.
- For example, NIV2 contains 172 datasets and a total of 1554 tasks, including some reasoning tasks. It has several issues which make it inappropriate to be directly used as a reasoning benchmark: (1) it is designed for instruction-tuned models and some tasks might be unsuitable for evaluating pretrained models or non-instruction finetuned models, as shown in Figure 1; (2) reasoning skills have been divided into 27 categories while some of them have large overlaps, e.g. numerical reasoning, quantitative reasoning, reasoning on numbers; (3) some reasoning labels are wrongly labeled, e.g. task393_plausible_result_generation gives textual entailment label but this task can hardly examine the entailment skill.
- The Curriculum benchmark (Chen and Gao, 2022) is deigned for probing language models' reasoning abilities and covers 8 different reasoning skills. However, this work only focuses on classification tasks and it converts all examples into the Natural Language Inference (NLI) format to fit into a unified framework. We argue that the NLI format can make the original tasks more complicated and causes some confusions. We observed some SOTA language models (GPT-3 davinci-003) fail to solve some tasks, e.g. examples in Table 1.
- More discussion and results are shown in Appendix B.

## Conclusions
- ALERT is a benchmark for assessing LLMs' reasoning abilities
- With this benchmark, we further explore what is the meaning of finetuning for these complex tasks
- Our experiments reveal that LLMs do not rely on memorizing training data, but are capable to learn diverse reasoning skills, such as textual entailment, abductive reasoning and analogical reasoning
- While we found that in general finetuning yields performance improvement, we also show some side ef-fects. LLMs memorize the data template representation and templates seen during finetuning, thus reducing the robustness of the model to generalized settings.
- CoT-finetuning alleviates this problem to a certain extent, but it is still less robust compared to the pre-trained model.
