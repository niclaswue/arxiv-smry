---
title: "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models"
date: 2023-01-10T21:26:08.000Z
author: "Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04213v1.webp" # image path/url
    alt: "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04213)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04213).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/does-localization-inform-editing-surprising).

# Abstract
- Language models are known to learn a great quantity of factual information
- Recent work localizes this information to specific model weights like mid-layer MLP weights (Meng et al., 2022)
- In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific parameters in models would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods.

# Paper Content

## Introduction
- Language models learn a variety of facts about the world during pretraining that can be elicited via natural language prompts
- Recent work explores how these facts are stored in model weights and expressed in response to particular prompts, suggesting that MLP weights act as key-value memories that support factual association
- Besides improving our scientific understanding of pretrained language models, this kind of investigative work may enable the design of better model editing methods for injecting new facts into model weights
- Model editing methods could be broadly useful for correcting factual errors in pretrained models, avoiding morally undesirable outputs, and updating models with changing knowledge over time

## Related Work
- Localization aims to interpret what certain hidden representations represent or to understand how a given concept is represented in a model
- We group these methods based on the kind of model components they consider (e.g. layers, neurons, etc.)
- In this paper, we adopt the layer-wise localization method from Meng et al. (2022a) known as Causal Tracing
- Related to analysis at the layer level, other work aims to localize concepts to directions in a latent space
- One might also place "key-value memory" theories of weight matrices in this category since a key vector represents a direction in the latent space
- Neurons, meanwhile, are the most common focus of localization analysis
- Relating Localization to Editing. Many works on localization validate the quality of their conclusions by editing neuron activations or layer weights corresponding to a particular concept, then checking that the network behavior changes appropriately.
- For example, Dai et al. (2022) check that their "knowledge neurons" have localized a specific fact by amplifying or suppressing the expression of that fact via adjusting the corresponding neuron activations.
- However, in isolation, it paints an incomplete picture that has led to misleading interpretations about the connections between localization and editing.
- Such experiments alone do not show whether editing that specific component is (1) successful in proportion to the strength of the localization result, (2) necessary to achieve the desired behavior, or (3) the best option for editing.
- In particular, these experiments do not show whether the same change in behavior can be achieved elsewhere in the network.
- Recent work has started looking into this question by measuring editing success across layers, averaging the results across data, then comparing the results with Causal Tracing conclusions also averaged across data.

## Notation and Background

### Data Notation
- Subject entity (e.g. Paris)
- Binary relation (e.g. is located in)
- Object (e.g. France)
- Tuple (s, r, o)
- Prompt (P)
- Subject and relation token representations
- Gaussian noise added to token representations of the subject
- Object that correctly completes the tuple (s, r, •)

### Causal Tracing
- Causal tracing is a method for localizing information in the forward pass of an autoregressive Transformer to specific hidden representations
- For a model with L layers, the input is a prompt containing T tokens (including a subject and relation)
- Given this input, the forward pass produces T × L layer outputs (one representation per T tokens and L layers)
- The algorithm aims to estimate the amount of information about the fact (s, r, o true ) that is contained in each of these representations
- We denote the representation at token t and layer as v (t, )
- The amount of factual information in v (t, ) is estimated by copying this representation into a different forward pass obtained from using a noised subject in the input
- In practice, a set of representations from multiple adjacent layers is copied from the clean forward pass rather than a single layer's representation
- A window size of, e.g., three implies that the tracing effect at layer estimates the amount of information contained in the three representations

### Model Editing with ROME
- The ROME editing method is used in the paper
- A desirable model edit is described in Section 3.4

### Editing Metrics
- Editing methods are typically evaluated according to their ability to change the model prediction on the input provided at runtime, generalize appropriately to paraphrases of the prompt, and avoid over-generalizing to unrelated data.
- We adopt metrics for each desideratum that we compute with available CounterFact data.
- Instead of the exact "magnitude" metrics from Meng et al. (2022a), we use normalized versions of each metric that we design to scale from 0 to 1 depending on whether the edit was maximally (un)successful, for purposes of making scores more comparable across data points.
- We denote the new edited weights of the LM as θ * and its pre-edit weights as θ. See Fig. 3 for an example of the kinds of data these metrics are computed on.

## Does Edit Success Follow From Localization?
- Localization results should inform editing methods because it should help to know where information is stored in a model if you are going to manipulate the model's expression of that information.
- In this section, we investigate the validity of this assumption as it applies to autoregressive Transformers, suggesting that edit success appears to be unrelated to localization results.

### Experiment Design
- ROME achieves an average rewrite score of 99% at layer 6 of GPT-J and above 96% at layers besides the last layer of the model.
- We operationalize this outcome and explanatory variable as follows: 1. Edit Success. We primarily consider Rewrite Score as our measure of edit success, given that this is the main optimization objective of ROME.
- Note ROME achieves an average rewrite score of 99% at layer 6 of GPT-J and above 96% at layers besides the last layer of the model.
- Tracing is a T × L grid of estimates, we obtain a single tracing effect per layer by taking the max across the T token effects at each layer (i.e., we collapse the grid in Fig. 2 down to a single curve across layers).
- Like our other metrics, we use a fractional tracing effect where 0 means the intervention had no effect and 1 means it fully restored the original probability p θ (o true |s, r):
- Lastly, note we use a tracing window size of 5 (smaller than the value of 10 used in Fig. 2).
- To confirm the robustness of our results, we include a few experiments in Appendix C showing that results are similar for (1) other measures of edit success including Paraphrase Score, Neighborhood Score, and an Overall Score (Tables 4, 5 and 6), (2) different values of the tracing window size (Fig. 12), (3) GPT2-XL rather than GPT-J (Fig. 13), (4) the original unscaled metrics from Meng et al. (2022a) (Fig. 14), and (5) using the tracing effect at the last subject token rather than the max across tokens (Fig. 16).

### Model and Data
- GPT-J is a 6 billion parameter autoregressive language model
- We use the CounterFact dataset to study editing performance
- We find that ROME achieves an average rewrite score of 99% at layer 6 and above 96% at layers besides layer 28
- We select data for experiments from 10% of the CounterFact dataset

### Experiment Results
- R 2 values for a linear regression model predicting the rewrite score based on (1) the choice of edit layer treated as a categorical variable, or (2) that variable interacted with the tracing effect are reported in Fig. 6.
- The Layer-only regression explains most of the variance in the outcome, while the tracing effect explains only an additional 3.2% of the variance.
- The results are robust across several experiment design choices listed in Sec. 4.1 and described in Appendix C.
- The strongest positive relationship between edit success and tracing effects is found for Fact Forcing with finetuning methods.

## Reconciling Localization and Editing
- If injecting a new fact has little to do with where an existing fact is stored in the model, perhaps there is some other editing intervention that would be more closely related to insights from tracing analysis.
- In this section, we propose a few variants of the model editing problem that appear more and more like Causal Tracing in terms of their input, target, and objective.
- Then, we repeat and extend our analysis from Sec. 4 for all of these editing problems.

### Editing Problem Variants
- Editing problems
- Rewrite score, paraphrase score, and neighborhood score
- Different target outputs for rewrite and paraphrase metrics

### Experiment Design and Additional Edit Methods
- We use the same experimental procedure as in Sec. 4, except that we consider a broader set of editing methods besides ROME.
- We list the four methods below:
- 1. ROME. The edit method from Sec. 4, ROME edits a single MLP layer's down-projection weight.
- 2. MEMIT. Though designed to edit multiple facts at once, when editing a single fact this method differs from ROME only by spreading out its update over several layers rather than a single layer (Meng et al., 2022b).
- 3. Constrained Finetuning (window size 1). We adopt a simple Adam-based optimization approach with an ∞ -norm constraint, following Zhu et al. (2020).
- 4. Constrained Finetuning (window size 5). This is the above finetuning method applied to five adjacent layers.
- We select these methods for their simplicity and since ROME and MEMIT are designed specifically to edit MLP layers.

## Discussion
- Does causal tracing tell us anything?
- In this paper, we show that causal tracing is not indicative of which layer to select for model editing.
- However, this does not mean that localization insights from causal tracing have been useless.
- Rather, causal tracing has helped reveal the role that early-to-mid-range MLP representations at the last subject token index play in factual association in autoregressive language models, and ROME does perform better on average when optimizing the last subject token representation rather than another token representation (Meng et al., 2022a).
- 3 dditional editing experiments then demonstrate the further fact that editing MLP weights is preferable to editing other weights that also have large causal tracing effects, like late-layer attention weights.
- We focus our analysis specifically on the choice of MLP layer following evidence that MLPs play a key role in factual association (Geva et al., 2021;Meng et al., 2022a;Geva et al., 2022).
- What do our results imply about using model editing to validate localization claims?
- We interpret our results to suggest that causal tracing answers a different question than model editing does.
- That is, causal tracing answers a question about where factual information is carried in representations in a Transformer forward pass, and this question turns out to be a different question than the editing question of where is best to intervene in the Transformer in order to change the factual information it expresses.
- It seems critical, then, to carefully formalize the questions that one wishes to answer via localization analysis and model editing before (1) validating the results of localization via editing or (2) motivating the design of an editing method via localization, because the conclusions that can be drawn from a particular localization method might not be relevant for the performance of a given model editing method.

## Conclusion
- We obtain the surprising result that model edit success is essentially unrelated to where factual information is stored in models, as measured by Causal Tracing.
- Faced with this result, we attempt to reconnect tracing-based localization with edit success by introducing four variants of the Error Injection problem using the CounterFact dataset.
- We find that edit success and tracing effects correlate best in our Fact Forcing setting. However, even in this case, tracing effects explain only a small fraction of the variance in editing performance, while the choice of edit layer is a much more important factor.
- This suggests that, counterintuitively, better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.
