---
title: "On the Role of Parallel Data in Cross-lingual Transfer Learning"
date: 2022-12-20T11:23:04.000Z
author: "Machel Reid, Mikel Artetxe"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10173v1.webp" # image path/url
    alt: "On the Role of Parallel Data in Cross-lingual Transfer Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10173)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10173).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/on-the-role-of-parallel-data-in-cross-lingual).

# Abstract
- Parallel data is conducive for cross-lingual learning
- Unsupervised machine translation can generate synthetic parallel data
- Supervised machine translation and gold parallel data are still the best results

# Paper Content

## Introduction
- Multilingual models generalize across languages in a zero-shot fashion
- Pretrained models are finetuned using labeled downstream data in the source language (typically English), which makes them capable of generalizing to the target language
- Parallel data is incorporated either at pretraining or finetuning time
- The explicit modeling of parallel interactions is beneficial

## Experimental setup

### Tasks
- We run experiments on 3 tasks: NLI on XNLI (Conneau et al., 2018), extractive QA on XQuAD (Artetxe et al., 2020), and NER on WikiANN (Pan et al., 2017).
- In all cases, we use the original training set in English, and evaluate transfer performance in other languages.
- Due to compute constraints, we restrict evaluation to the following set of languages: English (en), Arabic (ar), German (de), Hindi (hi), French (fr), Swahili (sw), Russian (ru), Thai (th) and Vietnamese (vi).
- Our finetuning incorporation experiments in ยง3.2 involve machine translating the training data into the target languages.

### Model
- We use XLM-R base (Conneau et al., 2020) for all of our experiments
- This XLM-R base was trained through Masked Language Modeling (MLM) on CC-100 (a monolingual corpus covering 100 languages)
- For finetuning, we experiment with learning rates of 1e-5, 5e-5, and 1e-4 using the Adam optimizer
- We train for up to 10 epochs and choose the checkpoint with the best validation performance averaged across the languages in consideration

### Parallel data sources
- We use the same parallel data as Reid and Artetxe (2022), which combines data from IWSLT, WMT, and other parallel corpora.
- We use the 420M M2M-100 model (Fan et al., 2020).
- We replace words that are included in our dictionary with a probability of 0.4.
- We compare finetuning XLM-R on the original English data (1), and machine translated data through either word-by-word replacement (2), unsupervised MT (3) or supervised MT (4).

### Pretraining incorporation
- XLM-R is pretrained on monolingual data through MLM and TLM, with 70k steps and a learning rate of 5e-5
- Parallel data is incorporated into the pretraining process, with supervised MT outperforming gold data and unsupervised MT lagging behind
- The results suggest that parallel data is helpful even when not using any new data, but incorporating groundtruth parallel data brings further improvements

### Finetuning incorporation
- Parallel data is incorporated into the finetuning process
- Parallel data outperforms the baseline in all tasks and target languages
- Simple ways to incorporate parallel signals can bring improvements

### Discussion
- Parallel data helps cross-lingual transfer learning
- Explicit use of parallel data is more effective than unsupervised MT
- Multilingual pretrained models can benefit from monolingual data

## Reconsidering the categorization of cross-lingual learning approaches
- There are different data types that can be used for cross-lingual learning
- Different procedures can be used to incorporate different data types into the cross-lingual learning pipeline
- Cross-lingual learning should aim to understand how the variants in each dimension and the interactions between them impact downstream performance.

## Related work
- Monolingual pretraining relies on knowledge transfer from unlabeled corpora
- Cross-lingual learning also relies on knowledge transfer from parallel data
- Our unsupervised MT variant does not use any additional data compared to regular pretraining

## Conclusions
- In this work, we show that even model-generated parallel data can be useful for cross-lingual learning-greatly expanding the possibilities for multilingual models to improve their performance by taking advantage of their own machine translation capabilities.
- Given this, we advocate for investigating the optimal way to leverage monolingual and/or parallel data for cross-lingual learning, which might require thinking beyond the boundaries of the conventional zero-shot, translate-train and translate-test categories.
- Figure 1: Cross-lingual transfer settings. Monolingual and parallel data can be used at different stages of the pipeline, either directly or indirectly through MT (b), but the traditional categorization falls short at capturing them (a).
- + unsup MT 83.4 72.4 77.1 72.2 78.2 67.8 75.2 86.7 70.2 80.7 81.5 75.8 79.6 79.0 81.3 54.1 82.1 74.9 71.1 3.8 80.7 71.7 64.9
- + sup MT 83.2 74.4 77.5 72.7 78.3 70.1 76.0 86.6 73.5 81.1 83.0 77.4 81.9 80.7 81.6 57.0 82.3 75.4 71.6 5.8 81.6 73.4 66.1
- + gold 84.0 75.2 77.7 72.4 78.6 70.4 76.4 86.3 72.3 82.3 82.7 78.2 81.9 80.6 82.4 57.3 82.4 75.6 71.8 4.6 81.5 73.7 66.2
