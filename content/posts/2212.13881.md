---
title: "Feature learning in neural networks and kernel machines that recursively learn features"
date: 2022-12-28T15:50:58.000Z
author: "Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, Mikhail Belkin"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13881v1.webp" # image path/url
    alt: "Feature learning in neural networks and kernel machines that recursively learn features" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13881)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13881).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/feature-learning-in-neural-networks-and).

# Abstract
- Neural networks have achieved impressive results on many technological and scientific tasks
- By identifying mechanisms driving the successes of neural networks, we can provide principled approaches for improving neural network performance
- In this work, we isolate the key mechanism driving feature learning in fully connected neural networks by connecting neural feature learning to the average gradient outer product. We subsequently leverage this mechanism to design Recursive Feature Machines (RFMs), which are kernel machines that learn features
- We show that RFMs (1) accurately capture features learned by deep fully connected neural networks, (2) close the gap between kernel machines and fully connected networks, and (3) surpass a broad spectrum of models including neural networks on tabular data. Furthermore, we demonstrate that RFMs shed light on recently observed deep learning phenomena such as grokking, lottery tickets, simplicity biases, and spurious features. We provide a Python implementation to make our method broadly accessible

# Paper Content

## Introduction
- Neural networks have been shown to be effective at a variety of tasks, including image generation, protein folding, and language understanding and generation.
- Different lines of investigation connect feature learning to various aspects of neural network methodology, such as model capacity, the initialization scheme, or the optimization method.
- In this work, we isolate a key mechanism of feature learning in deep fully connected neural networks by connecting feature learning with a statistical estimator for feature selection known in the literature as the expected gradient outer product.
- We subsequently leverage this connection to develop a class of kernel machines, Recursive Feature Machines (RFMs), that learn features.
- We demonstrate that RFMs (1) accurately capture features learned by deep networks, (2) close the gap between kernel machines and neural networks, and (3) achieve state-of-the-art performance on tabular data, in particular outperforming a spectrum of machine learning models including modern neural networks.
- We also provide theoretical evidence for the connection between feature learning in fully connected networks and the expected gradient outer product. In particular, under certain simplifying conditions on network initialization and training scheme, we show that the features learned in linear and nonlinear fully connected networks are equivalent to those given by expected gradient outer product.
- In addition, we show that RFMs shed light on several striking phenomena recently identified in deep learning literature, including grokking [50], lottery tickets [20], simplicity biases [59], and spurious feature identification [62].

## Neural Feature Learning and Recursive Feature Machines
- The Neural Feature Ansatz is the idea that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz implies that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network generalize well.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are similar to the features captured by the feature matrix of the network when it is trained.
- The Neural Feature Ansatz is based on the assumption that the features learned by a neural network are close to the optimal predictor....

## Empirical Results
- Features learned by deep fully connected networks are accurately captured by RFMs
- On real-world and synthetic datasets where feature learning is important, RFMs close the gap between kernel machines and neural networks
- RFMs provide state-of-the-art results on large tabular datasets and in particular, outperform a spectrum of models including modern neural networks such as transformers and ResNets on a broad set of classification and regression tasks

### RFMs close the gap between kernels and fully connected networks.
- RFMs outperform previous kernel methods in predictive performance
- RFMs improve error for large datasets with high data dimension

### RFMs outperform neural networks and NTK on tabular data benchmarks.
- RFMs outperform modern neural networks (transformers and ResNets) and modern tree-based models (gradient boosted trees and XGBoost) on large tabular tasks from [17]
- RFMs achieve the best performance on these benchmarks across a variety of commonly used performance metrics
- In Fig. 4a and b, we compare RFMs to all previous methods across all metrics on both the subset of 90 small datasets and all 121 tasks
- In Fig. 4c, we compare the difference in error (100% -accuracy) between RFMs and the classical Laplace kernel, which is equivalent to an RFM where the feature matrix M is the identity matrix
- In particular, we observe that the Laplace kernel generally results in high error than the RFM for larger datasets
- In Fig. 4d, we report the relative decrease in error between RFMs and the Laplace kernel
- We observe sizable improvement in relative error for larger datasets and those with higher dimensional data

## Deep Learning Phenomena through the Prism of RFM
- Empirical studies of deep neural networks have shown that there are a spectrum of phenomena that are absent in analytically tractable kernel methods.
- In this section, we show that such phenomena are reproducible in RFMs, and we envision that RFMs will provide a framework for rigorous analysis of these phenomena.
- Specifically, we discuss four specific phenomena below.

### Grokking in RFMs and deep networks.
- Deep networks can begin to exhibit a dramatic increase in test accuracy when training past the point where training accuracy is 100%.
- This phenomenon is due to the fact that the network learns the feature corresponding to the 5 × 5 pixel during this stage of training.
- RFM also exhibit this phenomenon.

### Lottery tickets in RFMs and deep networks.
- The lottery ticket hypothesis refers to the claim that a randomly-initialized neural network contains a sub-network that can match or outperform the trained network when trained in isolation.
- The sparsity of feature matrices identified in this work provides direct evidence for this hypothesis. Indeed, such sparsity is immediately evident when visualizing the diagonals of the feature matrix as in Fig. 2b.
- In line with the lottery ticket hypothesis, we demonstrate that retraining an RFM after thresholding coordinates of the data corresponding to these sparse regions in the feature matrix leads to a consistent increase in performance in many settings.
- We note that such thresholding drastically reduces the number of training examples needed.

### Inductive biases of RFMs and deep networks.
- A recent trend in deep learning is to use neural networks that are over-parameterized
- Despite their over-parameterization, such models yield improved performance
- To understand why such over-parameterized models generalize, recent works analyzed their inductive biases
- Recent works [28,59] have identified a particular form of inductive bias in deep networks, referred to as simplicity bias
- As an example, consider the experimental setup similar to that from [59] where we train a deep fully connected network on concatenated images of CIFAR-10 and MNIST
- The work [59] showed through perturbative analyses that that neural networks trained on such a dataset relied only on the MNIST half of the concatenated image to make predictions

### RFMs capture spurious features and biases in deep networks.
- Recent works demonstrate that deep networks can leverage spurious features for prediction, which can make these models unreliable in mission-critical domains.
- In image classification, such features are those correlated with but not part of objects of interest.
- Examples include food co-occurring with plates or fingers co-occurring with band-aids.
- It has also been argued that such features are responsible for so-called "adversarial examples" where small, often visually imperceptible alterations of images lead to dramatic loss of accuracy.
- We now demonstrate that RFMs can serve as a useful tool for identifying spurious features used by deep networks during prediction.
- In particular, we showcase how such feature matrices can be used to identify spurious features leveraged by deep networks in CelebA lipstick and earring prediction tasks.

## Summary, Discussion, and Outlook

## B Background on Kernel Ridge Regression
- Kernel ridge regression is a non-parametric estimation technique that uses a kernel function to fit a model to data.
- The ridge regularization parameter, λ, controls how much the kernel function is smoothed.
- There is a unique solution to the kernel ridge regression problem in the span of the data.
- To solve the kernel ridge regression problem, we use the EigenPro solver.

## C Dataset and Experimental Details
- All datasets, models, and training methodology considered in this work are from [17].
- For all binary classification tasks on CelebA in this work, we normalize all images to be on the unit sphere.
- We train 2-hidden layer ReLU networks with 1024 hidden units per layer using stochastic gradient descent (SGD) for 500 epochs with a learning rate of 0.1 and a mini-batch size of 128.
- We train RFMs for 1 iteration, use a ridge regularization term of 10 −3 , and average the gradient outer product of at most 20000 examples.
- All RFMs use Laplace kernels as the base kernel and use a bandwidth parameter of L = 10.
- For all classification tasks, we split available training data into 80% training and 20% validation for hyperparameter selection.
- We report accuracy on a held out test set provided by PyTorch [48].
- In addition, since there can be large class imbalances in this data, we ensure that the training set and test set are balanced by limiting the number of majority class samples to the same number of minority class samples.
- For RFM-T experiments in Fig. 7, we retrained a Laplace kernel with bandwidth 10 and ridge regularization of 10 −3 after thresholding the diagonals of the feature matrix with pixel intensity larger than 0.05 after min-max scaling the feature matrix.
- We consider the low rank polynomials from [73] and [16].
- We use 1000 examples for training and 10000 samples for testing.
- For all kernel methods (RFMs, Laplace kernel and NTK), we grid search over ridge regularization from the set {10, 1, .1, .01, .001} and grid search over 5 iterations for RFMs and used a bandwidth of 10 for all Laplace kernels.
- For NTK ridge regression experiments, we grid search over NTKs corresponding to ReLU networks with between 1 and 5 hidden layers.
- For the dataset with 130000 samples, we use EigenPro to train all kernel methods and RFMs.
- We run EigenPro for at most 50 iterations and select the iteration with best validation accuracy for reporting test accuracy.
