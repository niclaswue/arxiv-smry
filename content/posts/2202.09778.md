---
title: "Pseudo Numerical Methods for Diffusion Models on Manifolds"
date: 2022-02-20T10:37:52.000Z
author: "Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2202-09778v2.webp" # image path/url
    alt: "Pseudo Numerical Methods for Diffusion Models on Manifolds" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2202.09778)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2202.09778).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/pseudo-numerical-methods-for-diffusion-models-1).

# Abstract
- Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples.
- However, DDPMs require hundreds to thousands of iterations to produce final samples.
- Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)).
- However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability.
- To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds.
- Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods.
- We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations.
- According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules. Our implementation is available at https://github.com/luping-liu/PNDM.

# Paper Content

## INTRODUCTION
- Denoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015;Ho et al., 2020) is a class of generative models which model the data distribution through an iterative denoising process reversing a multi-step noising process.
- DDPMs have been applied successfully to a variety of applications, including image generation (Ho et al., 2020;Song et al., 2020b), text generation (Hoogeboom et al., 2021;Austin et al., 2021), 3D point cloud generation (Luo & Hu, 2021), textto-speech (Kong et al., 2021;Chen et al., 2020) and image super-resolution (Saharia et al., 2021).
- Unlike Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which require careful hyperparameter tuning according to different model structures and datasets, DDPMs can use similar model structures and be trained by a simple denoising objective which makes the models fit the noise in the data.
- To generate samples, the iterative denoising process starts from white noise and progressively denoises it into the target domain according to the noise predicted by the model at every step.
- However, a critical drawback of DDPMs is that DDPMs require hundreds to thousands of iterations to produce high-quality samples and need to pass through a network at least once at every step, which makes the generation of a large number of samples extremely slow and infeasible.
- In contrast, GANs only need one pass through a network.
- There have been many recent works focusing on improving the speed of the denoising process. Some works search for better variance schedules, including Nichol & Dhariwal (2021) and Watson et al. (2021). Some works focus on changing the inference equation, including Song et al. (2020a) 1 arXiv:2202.09778v2 [cs.CV] 31 Oct 2022 and Song et al. (2020b).
- Denoising Diffusion Implicit Models (DDIMs) (Song et al., 2020a) relying on a non-Markovian process accelerate the denoising process by taking multiple steps every iteration.
- Probability Flows (PFs) (Song et al., 2020b) build a connection between the denoising process and solving ordinary differential equations and use numerical methods of differential equations to accelerate the denoising process.
- Additionally, we introduce more related works in Appendix A.1.

## BACKGROUND
- The classical understanding of DDPMs is that they are a way to speed up the denoising process of diffusion models
- The understanding based on Song et al. (2020b) is that DDPMs can be used to accelerate the denoising process of diffusion models
- Numerical methods are used later in this paper

## DENOISING DIFFUSION PROBABILISTIC MODELS
- DDPMs model the data distribution from Gaussian distribution to image distribution through an iterative denoising process
- Let x 0 be an image, then the diffusion process is a Markov process and the reverse process has a similar form to the diffusion process, which satisfies:
- Here, β t controls the speed of adding noise to the data, calling them the variance schedule. N is the total number of steps of the denoising process. µ θ and β θ are two neural networks, and θ are their parameters.
- Ho et al. (2020) get some statistics estimations of µ θ and β θ . According to the properties of the conditional Gaussian distribution, we have:
- According to the properties of the Gaussian distribution, we have:
- The objective function is defined by:
- Here, x t (x 0 , ) = √ ᾱt x 0 + √ 1 − ᾱt , ∼ N (0, 1), θ is an estimate of the noise . The relationship between µ θ and θ is µ Because ∼ N (0, 1), we assume that the mean and variance of θ are 0 and 1.

## STOCHASTIC DIFFERENTIAL EQUATION

## FORMULA TRANSFORMATION

## CLASSICAL NUMERICAL METHOD
- After getting the target ODE, the easiest way to solve it is through classical numerical methods.
- However, We notice that classical numerical methods can introduce noticeable noise at a high speedup rate, making high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs.
- To make better use of numerical methods, we analyze the differences between Equation (10) and usual differential equations and find two main problems when we directly use numerical methods with diffusion models.
- The first problem is that the neural network θ and Equation (10) are well-defined only in a limited area.
- The second problem is that Equation (10) is unbounded at most cases.
- We find that for most linear variance schedules β t , Equation (10) tends to infinity when t tends to zero (see Appendix A.4), which does not satisfy the condition of numerical methods mentioned in Section 2.3.
- This is an apparent theoretical weakness that previous works have not explored.
- On the contrary, in the original DDPMs and DDIMs, the prediction of the sample x t and the noise θ in the data are more and more precise as the index t tends to zero (see Appendix A.5).

## PSEUDO NUMERICAL METHOD ON MANIFOLD
- The first problem above shows that we should try to solve our problems on certain manifolds.
- Here, target manifolds are the high-density region of the data x t of DDPMs, which is defined by Ernst & Gerhard (1996).
- Unfortunately, it's challenging to use the above expression of manifolds.
- Because we do not know the target x 0 in the reverse process and random items are hard to handle, too.
- In this paper, we design a different way that we make our new equation of denoising process more fits with the equation of original DDIMs to make their results share similar data distribution.
- Firstly, we divide the classical numerical methods into two parts: gradient and transfer parts.
- The gradient part determines the gradient at each step, while the transfer part generates the result at the next step.
- For example, linear multi-step method can be divided into the gradient part f = δ 24 (55f t −59f t−δ + 37f t−2δ − 9f t−3δ ) and the transfer part x t+δ = x t + δf .
- All classical numerical methods have the same linear transfer part, while gradient parts are different.
- We define those numerical methods which use a nonlinear transfer part as pseudo numerical methods.
- And an expected transfer part should have the property that when the result from the gradient part is precise, then the result of the transfer part is as close to the manifold as possible and the error of this result is as small as possible.
- We find that Equation (9) satisfies such property.
- Property 3.1 If is the precise noise in x t , then the result of x t−δ from Equation ( 9) is also precise.
- And we put the proof of this property in Appendix A.5. Therefore, we use: as the transfer part and θ as the gradient part.
- That if θ is precise, the result of x t−δ is also precise, which means that θ can determine the direction of the denoising process to generate the final results.
- Therefore, such a choice also satisfies the definition of a gradient part.
- Now, we have our gradient part θ and transfer part φ. This combination solves the two problems mentioned above successfully.
- Firstly, our new transfer parts do not introduce new errors.
- This property also means that it keeps the results at the next step on the target manifold because generating samples away is a kind of error.
- This shows that we solve the first problem.
- Secondly, we know that the prediction of θ is more and more precise in the reverse process in the above subsection.
- And our new transfer part can generate precise results according to the precise prediction of θ .
- Therefore, our generation results are more and more precise using pseudo numerical methods, while classical numerical methods can introduce obvious error at the last several steps.
- This shows that we solve the second problem, too.
- We also find that their combination φ(x t , θ (x t , t), t, t − 1) is just the inference equation used by DDIMs, so DDIMs is a simple case of pseudo numerical methods.

## GRADIENT PART
- The gradient part from different classical numerical methods can be used freely
- The transfer part of the new inference equation is important
- PNDMs combine the benefits of higher-order classical numerical methods and DDIMs

## ALGORITHM
- According to Song et al. (2020a), the algorithm of the original method satisfies Algorithm 1.
- Our new algorithm of PNDMs uses the pseudo linear multi-step and pseudo Runge-Kutta method, which satisfies Algorithm 2.
- Here, we cannot use linear multi-step initially because the linear multi-step method cannot start automatically, which needs at least three previous steps' information to generate results. So we use the Runge-Kutta method to compute the first three steps' results and then use the linear multi-step method to calculate the remaining.
- We also use the gradient parts of two second-order numerical methods to get another pseudo numerical method. We introduce the details of this method in Appendix A.3. We call it S-PNDMs, because its gradient part uses information from two steps at every step. Similarly, we also call our first PNDMs F-PNDMs, which use data from four steps, when we need to distinguish them.

## CONVERGENCE ORDER
- The local error is smaller than the global error.
- The new transfer part does not introduce any new error.

## EXPERIMENT

## SETUP
- We conduct unconditional image generation experiments on four datasets: Cifar10 (32 × 32), CelebA (64 × 64), LSUN-church (256 × 256) and LSUN-bedroom (256 × 256)
- According to the analysis in Section 3.1, we can use pre-trained models from prior works in our experiments
- The pre-trained models for Cifar10, LSUNchurch and LSUN-bedroom are taken from Ho et al. (2020) and the pre-trained model for CelebA is taken from Song et al. (2020a)
- In these models, the number of total steps N is 1000 and the variance schedule is linear variance schedule

## SAMPLE EFFICIENCY AND QUALITY
- To analyze the acceleration effect, we test Fenchel Inception Distance (FID (Heusel et al., method and linear multi-step method).
- On Cifar10 and CelebA, we first provide the results of previous works DDIMs.
- Then, we use the same pre-trained models to test numerical methods mentioned in this paper and put the results in Cifar10 / CelebA (linear).
- We also use models from iDDPMs to test nonlinear variance schedules and put the results in Cifar10 (cosine).
- Song et al. (2020b) do not provide detailed FID results of probability flows (PFs) under different steps, so we retest the results using its pretrained models by ourselves.
- Efficiency
- Our two baselines are DDIM and PF. DDIM is a simple case of pseudo numerical methods, and PF is a case of classical numerical methods.
- However, PF uses a much bigger model than DDIM and uses some tricks to improve the sample quality.
- To ensure the experiment's fairness, we use fourth-order numerical methods on Equation ( 10) and the model from DDIM.
- In Table 2, we find that the performance of FON is limited when the number of steps is small. By contrast, our new methods, including S-PNDM and F-PNDM, can improve the generated results regardless of whether the number of steps used is large or small.
- According to Cifar10 / CelebA (linear), F-PNDM can achieve lower FID than 1000 steps DDIM using only 50 steps, making diffusion models 20x faster without losing quality.
- We draw a line chart of computation cost with FID according to the results of Cifar10 (linear) above in Figure 3. Because F-PNDM uses the pseudo Runge-Kutta method to generate the first three steps, it is slower than other methods at the first several steps. Therefore, S-PNDM can achieve the best FID initially, then F-PNDM becomes the best and the acceleration is significant.
- Quality
- When the number of steps is relatively big, the results of FON become more and more similar to that of pseudo numerical methods. This is because all the methods are solving Equation ( 10), and their convergent results should be the same.
- However, pseudo numerical methods still work better using a large number of steps empirically.
- F-PNDM can improve the best FID around 0.4 using pretrained models and achieves a new SOTA FID score of 2.71 on CelebA, which shows that our work can not only accelerate diffusion models but also improve the sample quality topline.
- We also notice that the FID results of F-PNDM converge after more than 250 steps. The FID results will fluctuate around a value then. This phenomenon is more pronounced when we test our methods on LSUN (see Table 5, 6).
- According to Cifar10 (cosine), the cosine variance schedule can lower FID using a relatively large number of steps.
- More analyses about variance schedule can be found in Appendix A.7.
- What's more, we test our methods on other datasets and provide our FID results in Appendix A.9 and image results in Appendix A.10.
- We can draw similar conclusions on our methods' acceleration and sampling quality, regardless of the datasets and the size of the images.

## DISCUSSION
- PNDMs can generate high-quality images without loss of quality successfully.
- PNDMs can generate images with lower global error than other numerical methods.
- PNDMs can be used on more general applications.
- The order of a numerical method can be determined by its local and global error.
