---
title: "RoFormer: Enhanced Transformer with Rotary Position Embedding"
date: 2021-04-20T09:54:06.000Z
author: "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2104-09864v4.webp" # image path/url
    alt: "RoFormer: Enhanced Transformer with Rotary Position Embedding" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2104.09864)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2104.09864).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/roformer-enhanced-transformer-with-rotary).

# Abstract
- Position encoding recently has shown effective in the transformer
- It enables valuable supervision for dependency modeling between elements at different positions of the sequence.
- In this paper, we first investigate various methods to integrate positional information into the
- Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the
- Notably, RoPE enables valuable properties, including the flexibility of sequence length,
- Finally, we evaluate the enhanced transformer with rotary position
- Furthermore, we provide a theoretical analysis to explain some experimental

# Paper Content

## Introduction

### Preliminary
- Let S N = {w i } N i=1 be a sequence of N input tokens
- The corresponding word embedding of S N is denoted as
- The self-attention first incorporates position information to the word embeddings and transforms them into queries, keys, and value representations
- where q m , k n and v n incorporate the m th and n th positions through f q , f k and f v , respectively
- The query and key values are then used to compute the attention weights, while the output is computed as the weighted sum over the value RoFormer representation
- The existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form Equation (1)

### Absolute position embedding
- Equation (1) is a typical choice for representing a context
- RoPE uses sinusoidal functions to incorporate relative position information

### Relative position embedding
- Equation (1) is used to decompose q m k n of Equation (2)
- The key idea is to replace the absolute position embedding p n with its sinusoid-encoded relative counterpart pm−n
- The authors of Raffel et al. [2020] reformed Equation ( 6) as: where b i,j is a trainable bias
- The authors of Ke et al. [2020] investigated the middle two terms of Equation ( 6) and found little correlations between absolute positions and words
- The authors of Raffel et al. [2020] proposed to model a pair of words or positions using different projection matrices
- The authors of He et al. [2020] argued that the relative positions of two tokens could only be fully modeled using the middle two terms of Equation ( 6)
- As a consequence, the absolute position embeddings p m and p n were simply replaced with the relative position embeddings pm−n

## Proposed approach
- The RoPE is a method for encoding relative positions in a rotary encoder
- The RoPE has a few properties that make it a good candidate for encoding relative positions
- The RoPE is able to encode relative positions with high accuracy

### Formulation
- Transformer-based language modeling usually leverages the position information of individual tokens through a selfattention mechanism.
- In order to incorporate relative position information, we require the inner product of query q m and key k n to be formulated by a function g, which takes only the word embeddings x m , x n , and their relative position m − n as input variables.

### Rotary position embedding
- The geometric property of vectors on a 2D plane is used to prove that a solution to the formulation Equation (11) is: where Re[•] is the real part of a complex number and (W k x n ) * represents the conjugate complex number of (W k x n ).
- θ ∈ R is a preset non-zero constant.
- We can further write f {q,k} in a multiplication matrix: where (x m ) is x m expressed in the 2D coordinates. Similarly, g can be viewed as a matrix and thus enables the solution of formulation in Section (3.1) under the 2D case.
- Specifically, incorporating the relative position embedding is straightforward: simply rotate the affine-transformed word embedding vector by amount of angle multiples of its position index and thus interprets the intuition behind Rotary Position Embedding.
- In order to generalize our results in 2D to any x i ∈ R d where d is even, we divide the d-dimension space into d/2 sub-spaces and combine them in the merit of the linearity of the inner product, turning f {q,k} into: is the rotary matrix with pre-defined parameters
- Applying our RoPE to self-attention in Equation ( 2), we obtain: Θ is an orthogonal matrix, which ensures stability during the process of encoding position information.
- In contrast to the additive nature of position embedding method adopted in the previous works, i.e., Equations ( 3) to (10), our approach is multiplicative. Moreover, RoPE naturally incorporates relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding when applied with self-attention.

### Properties of RoPE
- Following Vaswani et al. [2017], we set θ= 10000.
- One can prove that this setting provides a long-term decay property (refer to Section (3.4.3) for more details), which means the inner-product will decay when the relative position increase.
- This property coincides with the intuition that a pair of tokens with a long relative distance should have less connection.
- The self-attention can be rewritten in a more general form. The original self-attention chooses sim(q, k) = exp(qk/√d).
- Note that the original self-attention should compute the inner product of query and key for every pair of tokens, which has a quadratic complexity O(N).
- Follow Katharopoulos et al. [2020], the linear attentions reformulate Equation (17) as where φ(•), ϕ(•) are usually non-negative functions.
- The authors of Katharopoulos et al. [2020] have proposed φ(x) = ϕ(x) = elu(x) + 1 and first computed the multiplication between keys and values using the associative property of matrix multiplication.
- A softmax function is used in Shen et al. [2021] to normalize queries and keys separately before the inner product, which is equivalent to φ(q) = softmax(q) and φ(k) = exp(k).
- For more details about linear attention, we encourage readers to refer to original papers.
- In this section, we focus on discussing incorporating RoPE with Equation ( 18).
- Since RoPE injects position information by rotation, which keeps the norm of hidden representations unchanged, we can combine RoPE with linear attention by multiplying the rotation matrix with the outputs of the non-negative functions.
- It is noteworthy that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in the numerator could contain negative terms.
- Although the weights for each value vin Equation ( 19) are not strictly probabilistic normalized, we kindly argue that the computation can still model the importance of values.

### Theoretical Explanation
- The function g that defines the inner product between vectors produced by f {q,k} can be found by solving the equation
- The vectors with empty position information encoded are
- The solution to f q , f k can be found by taking the advantage of the geometric meaning of vector and its complex counter part and decomposing the functions into
- The initial condition for the solution is
- The solution to f q , f k can be written as a complex number multiplication
- The solution is a constant irrelevant to m

## Experiments and Evaluation
- We evaluate the proposed RoFormer on various NLP tasks
- We validate the performance of the proposed solution on machine translation task
- We compare our RoPE implementation with BERTDevlin et al. [2019] during the pre-training stage
- Based on the pre-trained model, in Section (4.3), we further carry out evaluations across different downstream tasks from GLUE benchmarksSingh et al. [2018]
- In Addition, we conduct experiments using the proposed RoPE with the linear attention of PerFormer Choromanski et al. [2020]
- By the end, additional tests on Chinese data are included in Section (4.5)

### Machine Translation
- We first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks.
- We compare to the transformer-based baseline alternative Vaswani et al. [2017].
- We incorporate RoPE into the 12 layer char-based PerFormer with 768 dimensions and 12 heads2 .
- To better illustrate the efficacy of RoPE, we report the loss curves of the pre-training process with and without RoPE under the same settings, i.e., learning rate 1e-4, batch size 128 and a fixed maximum sequence length of 1024, etc.
- We apply the pre-trained RoFormer model to CAIL2019-SCM with different input lengths.
- The model is compared with the pre-trained BERT and WoBERT model on the same pre-training data, as shown in Table (5).

### Pre-training Language Modeling
- The second experiment is to validate the performance of our proposal in terms of learning contextual representations.
- To achieve this, we replace the original sinusoidal position encoding of BERT with our RoPE during the pre-training step.
- We look at several datasets from GLUE, i.e. F1-score for MRPC and QQP dataset, spearman correlation for STS-B, and accuracy for the remaining as the evaluation metrics.

### Fine-tuning on GLUE tasks
- Pre-trained RoFormer is used on GLUE tasks
- Evaluates its generalization ability on downstream NLP tasks

### Performer with RoPE
- Performer Choromanski et al. [2020] introduces an alternative attention mechanism, linear attention, which is designed to avoid quadratic computation cost that scales with input sequence length.
- The RoPE can be easily implemented in the PerFormer model to realize the relative position encoding while keeping its linearly scaled complexity in self-attention.
- We demonstrate its performance with the pre-training task of language modeling.

### Evaluation on Chinese Data
- In addition to experiments on English data, we show additional results on Chinese data.
- To validate the performance of RoFormer on long texts, we conduct experiments on long documents whose length exceeds 512 characters.
- In these experiments, we carried out some modifications on WoBERT Su [2020]

## Conclusions
- In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures.
- Our theoretical analysis indicates that relative position can be naturally formulated using vector production in self-attention, with absolution position information being encoded through a rotation matrix.
- In addition, we mathematically illustrated the advantageous properties of the proposed method when applied to the Transformer.
- Finally, experiments on both English and Chinese benchmark datasets demonstrate that our method encourages faster convergence in pre-training.
- The experimental results also show that our proposed RoFormer can achieve better performance on long texts task.
