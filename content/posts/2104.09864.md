---
title: "RoFormer: Enhanced Transformer with Rotary Position Embedding"
date: 2021-04-20T09:54:06.000Z
author: "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2104-09864v4.webp" # image path/url
    alt: "RoFormer: Enhanced Transformer with Rotary Position Embedding" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2104.09864)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2104.09864).


# Abstract
- Position encoding recently has shown effective in the transformer architecture.
- It enables valuable supervision for dependency modeling between elements at different positions of the sequence.
- In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models.
- Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information.
- Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation.
- Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.
- Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives.
- Furthermore, we provide a theoretical analysis to explain some experimental results.

# Paper Content

## Introduction
- Sequential order of words is valuable to natural language understanding
- Recurrent neural networks (RRNs) and Convolution neural networks (CNNs) based models encode tokens' order by recursively computing a hidden state
- Pre-trained language models (PLMs) were typically considered position-agnostic, but recent work has shown that the padding operation can implicitly learn position information
- Recently, the pre-trained language models (PLMs) have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including context representation learning, machine translation, and language modeling
- Unlike RRNs and CNNs-based models, PLMs utilize the self-attention mechanism to semantically capture the contextual representation of a given corpus
- As a consequence, PLMs achieve a significant improvement in terms of parallelization over RNNs and improve the modeling ability of longer intra-token relations compared to CNNs

### Preliminary
- Let S N = {w i } N i=1 be a sequence of N input tokens
- The corresponding word embedding of S N is denoted as
- The self-attention first incorporates position information to the word embeddings and transforms them into queries, keys, and value representations
- where q m , k n and v n incorporate the m th and n th positions through f q , f k and f v , respectively
- The query and key values are then used to compute the attention weights, while the output is computed as the weighted sum over the value RoFormer representation
- The existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form Equation (1)

### Absolute position embedding
- Equation (1) is a typical choice for representing the position of a object in a 3D space
- RoPE uses sinusoidal functions to incorporate the relative position information
- This allows for more accurate predictions of the object's movement

### Relative position embedding
- Equation (1) is used to derive different variants of relative position embeddings
- The variants proposed in Shaw et al. [2018], Dai et al. [2019], Raffel et al. [2020], He et al. [2020], Ke et al. [2020], Huang et al. [2020] all attempt to modify Equation (6) based on the decomposition of Equation (3) under self-attention settings in Equation (2)
- Our approach aims to derive the relative position encoding from Equation (1) under some constraints

## Proposed approach
- The RoPE is a method for encoding relative positions in a rotary encoder.
- The RoPE is formulated in terms of the relative position encoding problem.
- The RoPE has properties that are investigated in Section (3.3).

### Formulation
- Transformer-based language modeling usually leverages the position information of individual tokens through a selfattention mechanism.
- In order to incorporate relative position information, we require the inner product of query q m and key k n to be formulated by a function g, which takes only the word embeddings x m , x n , and their relative position m − n as input variables.
- The ultimate goal is to find an equivalent encoding mechanism to solve the functions f q (x m , m) and f k (x n , n) to conform the aforementioned relation.

### Rotary position embedding
- We use the geometric property of vectors on a 2D plane and its complex form to prove that a solution to our formulation Equation (11) is: where Re[•] is the real part of a complex number and (W k x n ) * represents the conjugate complex number of (W k x n ).
- θ ∈ R is a preset non-zero constant.
- We can further write f {q,k} in a multiplication matrix: where (x m ) is x m expressed in the 2D coordinates. Similarly, g can be viewed as a matrix and thus enables the solution of formulation in Section (3.1) under the 2D case.
- Specifically, incorporating the relative position embedding is straightforward: simply rotate the affine-transformed word embedding vector by amount of angle multiples of its position index and thus interprets the intuition behind Rotary Position Embedding.
- In order to generalize our results in 2D to any x i ∈ R d where d is even, we divide the d-dimension space into d/2 sub-spaces and combine them in the merit of the linearity of the inner product, turning f {q,k} into: is the rotary matrix with pre-defined parameters
- Applying our RoPE to self-attention in Equation (2), we obtain: Θ is an orthogonal matrix, which ensures stability during the process of encoding position information.
- In contrast to the additive nature of position embedding method adopted in the previous works, i.e., Equations (3) to (10), our approach is multiplicative. Moreover, RoPE naturally incorporates relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding when applied with self-attention.

### Properties of RoPE
- Following Vaswani et al. [2017], we set θ i = 10000 −2i/d .
- This setting provides a long-term decay property, which means the inner-product will decay when the relative position increase.
- The self-attention can be rewritten in a more general form.
- The original self-attention chooses sim(q m , k n ) = exp(q m k n / √ d).
- Note that the original self-attention should compute the inner product of query and key for every pair of tokens, which has a quadratic complexity O(N 2 ).
- Follow Katharopoulos et al. [2020], the linear attentions reformulate Equation (17) as where φ(•), ϕ(•) are usually non-negative functions.
- The authors of Katharopoulos et al. [2020] have proposed φ(x) = ϕ(x) = elu(x) + 1 and first computed the multiplication between keys and values using the associative property of matrix multiplication.
- A softmax function is used in Shen et al. [2021] to normalize queries and keys separately before the inner product, which is equivalent to φ(q i ) = softmax(q i ) and φ(k j ) = exp(k j ).
- For more details about linear attention, we encourage readers to refer to original papers.
- In this section, we focus on discussing incorporating RoPE with Equation ( 18).
- Since RoPE injects position information by rotation, which keeps the norm of hidden representations unchanged, we can combine RoPE with linear attention by multiplying the rotation matrix with the outputs of the non-negative functions.
- It is noteworthy that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in the numerator could contain negative terms.
- Although the weights for each value v i in Equation ( 19) are not strictly probabilistic normalized, we kindly argue that the computation can still model the importance of values.

### Theoretical Explanation
- The vectors x q and x k correspond to query and key, and their position is encoded in m and n.
- The function g that defines the inner product between vectors is required.
- The solution to f q , f k can be found by taking advantage of the geometric meaning of vector and its complex counter part.
- The solution is a complex number multiplication.

## Experiments and Evaluation
- We evaluate the proposed RoFormer on various NLP tasks
- We validate the performance of the proposed solution on machine translation task
- We compare our RoPE implementation with BERTDevlin et al. [2019] during the pre-training stage
- Based on the pre-trained model, in Section (4.3), we further carry out evaluations across different downstream tasks from GLUE benchmarksSingh et al. [2018]
- In Addition, we conduct experiments using the proposed RoPE with the linear attention of PerFormer Choromanski et al. [2020]
- By the end, additional tests on Chinese data are included in Section (4.5)

### Machine Translation
- We first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks.
- We compare to the transformer-based baseline alternative Vaswani et al. [2017].
- We carry out tests on the Enwik8 dataset Mahoney [2006], which is from English Wikipedia that includes markup, special characters and text in other languages in addition to English text.
- We incorporate RoPE into the 12 layer char-based PerFormer with 768 dimensions and 12 heads2 .
- To better illustrate the efficacy of RoPE, we report the loss curves of the pre-training process with and without RoPE under the same settings, i.e., learning rate 1e-4, batch size 128 and a fixed maximum sequence length of 1024, etc.
- We apply the pre-trained RoFormer model to CAIL2019-SCM with different input lengths.
- The model is compared with the pre-trained BERT and WoBERT model on the same pre-training data, as shown in Table (5).
- With short text cut-offs, RoFormer i.e., 512, the result from RoFormer is comparable to WoBERT and is slightly better than the BERT implementation. However, when increasing the maximum input text length to 1024, RoFormer outperforms WoBERT by an absolute improvement of 1.5%.

### Pre-training Language Modeling
- The second experiment is to validate the performance of our proposal in terms of learning contextual representations.
- To achieve this, we replace the original sinusoidal position encoding of BERT with our RoPE during the pre-training step.
- We look at several datasets from GLUE, i.e. MRPC Dolan and Brockett [2005], SST-2 Socher et al. [2013], QNLI Rajpurkar et al. [2016], STS-B Al-Natsheh [2017], QQP Chen et al. [2018b] and MNLI Williams et al. [2018].
- We use F1-score for MRPC and QQP dataset, spearman correlation for STS-B, and accuracy for the remaining as the evaluation metrics.

### Fine-tuning on GLUE tasks
- We fine-tune the weights of our pre-trained RoFormer across various GLUE tasks.
- The pre-trained RoFormer is able to generalize well on the downstream NLP tasks.

### Performer with RoPE
- Performer Choromanski et al. [2020] introduces an alternative attention mechanism, linear attention,
- which is designed to avoid quadratic computation cost that scales with input sequence length
- as discussed in Section (3.3), the proposed RoPE can be easily implemented in the PerFormer model
- to realize the relative position encoding while keeping its linearly scaled complexity in self-attention
- We demonstrate its performance with the pre-training task of language modeling.

### Evaluation on Chinese Data
- In addition to experiments on English data, we show additional results on Chinese data.
- To validate the performance of RoFormer on long texts, we conduct experiments on long documents whose length exceeds 512 characters.
- In these experiments, we carried out some modifications on WoBERT Su [2020]

## Conclusions
- In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures.
- Our theoretical analysis indicates that relative position can be naturally formulated using vector production in self-attention, with absolution position information being encoded through a rotation matrix.
- In addition, we mathematically illustrated the advantageous properties of the proposed method when applied to the Transformer.
- Finally, experiments on both English and Chinese benchmark datasets demonstrate that our method encourages faster convergence in pre-training.
- The experimental results also show that our proposed RoFormer can achieve better performance on long texts task.
