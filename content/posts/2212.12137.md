---
title: "Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing"
date: 2022-12-23T04:12:52.000Z
author: "William Brannon, Yogesh Virkar, Brian Thompson"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12137v1.webp" # image path/url
    alt: "Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12137)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12137).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/dubbing-in-practice-a-large-scale-study-of).

# Abstract
- Humans perform the task of dubbing video content from one language into another, leveraging a novel corpus of 319.57 hours of video from 54 professionally produced titles.
- The results challenge a number of assumptions commonly made in both qualitative literature on human dubbing and machine-learning literature on automatic dubbing, arguing for the importance of vocal naturalness and translation quality over commonly emphasized isometric (character length) and lip-sync constraints.
- We also find substantial influence of the source-side audio on human dubs through channels other than the words of the translation, pointing to the need for research on ways to preserve speech characteristics, as well as semantic transfer such as emphasis/emotion, in automatic dubbing systems.

# Paper Content

## Introduction
- Considerable attention has been paid to the dubbing of video content from one language into another, both in the literature of several disciplines and in the daily practice of the entertainment industry.
- One influential line of work, in the fields of film studies and audiovisual translation, studies human dubbing from a qualitative perspective (Chaume, 2012;Zabalbeascoa, 1997Zabalbeascoa, , 2008;;Freddi and Pavesi, 2009).
- This literature has developed a rich body of theory on the nature of the human dubbing task, and the ways humans approach it, but has little engagement with large-scale data.
- More recently, machine-learning practitioners have taken up the task of building multimodal systems for automatic dubbing (e.g., Saboo and Baumann (2019); Federico et al. (2020a); Tam et al. (2022)), but lack deep empirical or theoretical bases for how to organize their work.
- What is missing from both literatures, and can help bridge the gap between them, is a large-scale study of human dubbing in practice: a data-driven examination of the way humans actually perform this task.
- Such an analysis can have benefits for both the qualitative study of human dubbing, by providing empirical evidence of how dubbing teams approach their work, and informing future machine-learning work on automatic dubbing systems.
- We, accordingly, aim to understand human dubbing by studying not its process, but its product: a large set of actual dubbed dialogues from real TV shows, obtained from Amazon Studios.
- As compared to qualitative work or interviews with dubbers, this approach has the particular virtue of capturing tacit knowledge brought to bear in the human dubbing process but difficult to write down or explain.
- We organize our investigation around one of the most fundamental insights from the qualitative literature, that of human dubbing (and subtitling, which we do not consider here) as "constrained translation" (Titford, 1982;Mayoral et al., 1988).
- A dub, after all, is not just a translation of the original content -indeed it is not a purely textual product at all.
- As a translation, it should preserve the meaning of the original; as spoken language, it should sound natural; as an accompaniment to a video track, it should fit with the timing of actors' mouth movements, body language and the flow of the story (Chaume, 2020).
- Simultaneously satisfying all of these constraints is very difficult, and in general may not be possible.
- We are accordingly interested in how human dubbers balance the competing interests of semantic fidelity, natural speech, timing constraints, and convincing lip-sync.
- Each can be traded off against the others, with varying effects on the audience's experience of the resulting product.
- We operationalize this broad question as several more specific ones about the human dubbing process:
- Do dubbers respect timing constraints imposed by the video and original audio?
- Do the original and dub texts have approximately the same number of characters?
- How much do voice actors vary their speaking rates, possibly compromising speech naturalness, to meet timing constraints?
- How closely do the voice actors' words match visible mouth movements of the original actors?
- How much will dubbers reduce translation accuracy (i.e. adequacy and fluency) to meet other constraints?
- Do source speech traits influence the target in ways not mediated by the words of the dub, indicating emotion transfer?
- After exploring each of these questions, we provide insights on several research directions to address weaknesses we uncover in current automatic dubbing approaches.

## Related Work

### Qualitative
- Modern qualitative research on human dubbing began with a seminal monograph by Fodor (1976), himself a translator and writer of dubbed dialogue.
- Dubbing (both human and automatic) has subsequently come to be viewed as a type of constrained translation, with more constraints than settings like comics, songs, or voice-over video content (Mayoral et al., 1988).
- Most of the constraints stem from the need for a close match to the original video track. In particular, dubs have isochronic constraints: they should be about the same duration as the source, and should respect perceptible pauses within a speaker turn (Miggiani, 2019). Similarly, dubs benefit from complying with phonetic synchrony2 or lip sync: compatibility between the articulatory mouth movements required to produce the dub and the mouth movements, when visible, of the original actors (Fodor, 1976;Miggiani, 2019).
- Dubs also need to consider kinesic synchrony: the plausibility of the dubbed dialogue in light of visible body movements of the original actors (Chaume, 2012).
- These three constraintsisochronic, phonetic and kinesic -are true 'synchronies' in modern usage as they relate to time. Kinesic synchrony is also an example of the broader category of semiotic or iconic constraints, or constraints "inherent to film language" (Chaume, 2020).
- Dubs, of course, have non-temporal constraints as well. As cultural products, they should be readily intelligible to a member of the target linguistic and cultural community, with foreign references avoided or used for effect. (As Chaume (2020) puts it, they must comply with "sociocultural constraints.")
- As speech, they should sound natural, as though originally recorded in the target language. Dubs which fail to meet this criterion are often considered examples of "dubbese" (Myers, 1973).
- The peculiarities of "dubbese" have been studied extensively in a wide range of language pairs (see Herbst (1997), Nencioni (1976), Pavesi (1996), Freddi and Pavesi (2009), and many others), especially as it may be specific to a national or linguistic community of dubbers (Chaume, 2020).
- Turning to content, dubs have the same goal as any translation of preserving the semantic meaning of the source. However, some leeway is allowed; Chaume (2020) provides two examples: (1) In a Spanish-to-English dub, an off-screen omelet may be turned into a pie, as the word for pie better adheres to lip-sync constraints. (2) In a Japaneseto-English dub, non-visible chopsticks might be changed to a fork to adhere to sociocultural constraints.
- Viewed through this lens, dubbing is a form of non-literal translation called "transcreation" (Zanotti, 2014). However, it is often desirable to keep such changes to a minimum to preserve fidelity to the source film (Martí Ferriol, 2010).

### Automatic Dubbing
- Several works have explored the automatic generation of dubs, focusing on a variety of constraints.
- One line of work has focused on integrating lip sync constraints into the dub generation process.
- Taylor et al. (2015) developed a method for automatic dubbing that matches the visemes of the original speech.
- Saboo and Baumann (2019) integrated lip-sync constraints into an encoder-decoder machine translation architecture.
- Taking a different approach, Kim et al. (2019) have explored adjusting mouth movements in the original video to match a dubbed audio track.
- Other literature has examined "isometric" machine translation: producing a translation for use in automatic dubbing which has a similar length (in characters) to the input.
- It's argued that this property is "a proxy for the duration of its spoken realization" (Lakew et al., 2021), and that similarity in character length makes TTS-generated speech sound more natural (Lakew et al., 2022).
- A third line of work has focused on controlling the speaking rate in automatic dubbing systems to achieve prosodic alignment, or "synchronizing the translated transcript with the original utterances" (Federico et al., 2020b).
- Öktem et al. (2019) focused only on the linguistic content matching between source-target phrases as a way to improve TTS, while Federico et al. (2020a) focused on fluency.
- Their subsequent works (Federico et al., 2020b;Virkar et al., 2021) further enhanced prosodic alignment by addition of features controlling for TTS speaking rate variation and linguistic content matching. Additionally, they introduced a time-boundary relaxation mechanism that can help to control speaking rate and speech fluency.
- Virkar et al. (2022) extended the time-boundary relaxation to further relax timing constraints for sentences that are off-screen.
- Tam et al. (2022) examined integrating pause constraints directly into MT.
- Finally, in contrast to the pipeline architecture used in most automatic dubbing works, Hu et al. (2021) explored end-to-end dubbing.

### Empirical Studies
- Studies have attempted to quantify human dubbing through quantitative methods,
- Some research has looked at prosody in human dubs, while other work has looked at audience reaction to lip sync,
- Recent work has concluded that on-screen human dubs have lower translation quality than human off-screen dubs.

## Corpus Description & Preprocessing
- We analyzed 674 episodes of 54 TV shows produced by Amazon Studios
- Each episode was hand-curated for transcripts or dubbing scripts
- The transcripts or dubbing scripts were produced as part of the human dubbing process
- The dataset contains 319.57 hours of content from 9,215 distinct speakers
- A detailed summary of the dataset is provided in Table 1
- The dataset includes statistics for all genres for which we have at least 400 lines of manual on/off annotations
- Much of the analysis relies on a subset of 35.68 hours of content with both Spanish and German dubs
- The dataset is processed and filtered in the following ways:
- Data amounts for the entire corpus and each genre/language subset are provided in Table 2

### Segmentation and Forced Alignment
- Script timecodes are used to segment audio tracks
- Each dialogue line is associated with the audio between its start time and the start time of the next line (or the end of the episode for the last line)
- Lines are roughly, but not exactly, the same as speaker turns
- The timecode-based segmentation process produces 234,322 dialogue lines for English, 29,210 for German, and 28,720 for Spanish
- Next, the Montreal Forced Aligner (McAuliffe et al., 2017) is used to force align each dialogue line with its corresponding audio
- MFA successfully aligns 87.37% of English lines (204,734), 89.35% of German lines (26,099) and 80.81% of Spanish lines (23,209)
- Speaker fundamental frequency (i.e. F0 or, less formally, "pitch") is extracted using pyworld3 and linearly interpolated to fill in missing values
- Energy is computed from Mel spectrograms of the speech signals
- Both pitch and energy are averaged on a per-phone basis

### Filtering
- There are several ways our data collection, segmentation, and alignment procedures might fail
- We extensively filter the English side of the dataset to identify and remove erroneous dialogue lines
- Specifically, we filter out the following: Foreign-language text We identify dialogue lines in the English originals whose text is not in English. We use a language identification model for text4 and exclude anything with a low probability of being English, as well as one entire show whose script text appeared not to be in English. Foreign-language audio Similarly, we identify lines with non-English audio (from original non-English speech and errors in the corpus), using an audio language identification model trained on the VOXLINGUA107 corpus from the SpeechBrain toolkit (Ravanelli et al., 2021;Valk and Alumae, 2021). We excluded an entire show whose supposedly English audio was actually German, several characters who spoke only in non-English languages, and any lines with low probability of being English. Multiple speakers or overlapping speech Because overlapped speech is likely to confuse MFA, we ran overlapped speech detection (Bredin et al., 2020;Bredin and Laurent, 2021), and excluded anything with a detected fraction of overlap higher than 30%. Incorrect alignments We performed ASR on each line's audio using an in-house tool and excluded dialogue lines with a) empty ASR output, b) an exact match to the gold text except for an inclusion at the front (these indicate segmentation errors), or c) a Levenshtein distance to the original greater than 80% of the original length.
- After The number of speakers as estimated by the number of distinct characters in the given show ("Speakers"), and total run time for the show ("Duration"), and the number of distinct dialogue lines ("Dialogue Lines") for the show. We report statistics for the entire corpus ("All") as well as four genres (Drama, Kids, Comedy, and Suspense), in each of the 3 languages considered in this work (English, the source language, as well as German and Spanish dubs).
- 355.36 hours of source and target content. Manual inspection with Praat (Boersma and Weenink, 2022) suggests that post-filtering alignment quality is acceptably high. Most words and phones are correctly aligned, with only 12% of words in a hand-audited sample containing any phones with major problems.
- Errors most frequently occurred on foreign words and at silence boundaries, with word-initial or word-terminal phones incorrectly aligned into a preceding or following silence.

### Cross-lingual alignment
- Given sets of force-aligned and filtered content in each language, we still need to align across languages to create a single corpus of parallel (English, dub) examples.
- Offset finding: Many of the dubbed shows have episode-initial inserts, such as recaps of previous episodes or intro segments. Our dataset lacks targetside videos, and in lieu of manually identifying these segments from the audio tracks, we rely on cross-correlation of the aligned speech signals.
- For each (English, dub) pair of a given episode we sample at 100Hz a binary indicator of whether MFA has aligned a non-silence phone, and find the offset that maximizes the cross-correlation of these two signals. By inspection, these offsets work well and produce closely correlated patterns of silence between English and dubbed content.
- Finally, we need to align groups of sentences occurring at approximately the same time in each (English, dub) episode pair. Note that because voice actors do not have to respect the exact distribution of silences in the original audio track, we have a many-to-many alignment problem: many stretches of speech in English may correspond to many, indeed potentially a different number, of stretches of speech in the dub.
- Accordingly, we align mostly according to content, using the Vecalign algorithm (Thompson andKoehn, 2019, 2020) on multilingual LASER embeddings (Artetxe et al., 2018) of the English and dubbed lines.
- We align contiguous stretches of speech. After alignment, we perform two final filtering steps to remove any spurious alignments, dropping sentence pairs where: either duration is exactly one frame, or the midpoint times of the source and target speech segments differ by more than 1s. The final dataset of parallel cross-lingual alignments contains 42,850 aligned dialogue line pairs, from 49 episodes and 11 shows, comprising 35.68 hours of source and target content.

### Gender Annotations
- We extract information from the dramatis personae lists in the original scripts on characters' genders.
- The scripts do not list all characters from whom we have speech, and differences in name formatting mean that some characters' gender information is lost.
- We are able to collect gender annotations for 23,304 dialogue lines (54.39% of the filtered corpus).

### On-Screen Annotations
- We used annotations in the German dubbing scripts to identify on-screen and off-screen speech
- Because these are the scripts actually used by dubbing professionals, they are not only human judgments of when characters' mouths are visible, but also directly influenced the actual human dubbing process
- Only approximately 9.68% of aligned pairs have on-screen/off-screen annotations
- Because much of our analysis rests on comparing onscreen and offscreen dialogue lines, we would also like to test for systematic differences in what type of content is onscreen or offscreen

### Data Release Considerations
- Content licensing restrictions prevent us from releasing our data.
- We believe this target language subsets (German and Spanish).
- The "Orig" column gives the number of lines before any filtering (see § 3).
- The "Filter" column gives the number of lines after quality filtering described in sections § 3.1 and § 3.2.
- The "Align" column gives the number of lines after cross-lingual alignment described in § 3.3.
- Finally, the "On/Off" column gives the number of lines which have manual on-screen / off-screen annotations (see § 3.5).

## Analysis

### Isochrony
- Dubs should line up in time with the original speech
- Many qualitative works have considered isochronic constraints
- Automatic dubbing work has explored integrating them, usually with a proxy for isochrony such as length in syllables (Saboo and Baumann, 2019;Öktem et al., 2019) or in characters (Federico et al., 2020a;Lakew et al., 2021Lakew et al., , 2022;;Tam et al., 2022)
- We are accordingly interested in exploring how much human dubbers respect this constraint
- First, we simply compare the durations of aligned dialogue line pairs
- Source duration ought to be a strong predictor of dub duration, which indeed it is: the correlation between the two is quite high at r = 0.877
- But duration does not consider the actual start and stop times of lines, and may simply reflect the need to convey the same amount of information in source and target
- As a further check, we look at the overlap fraction of speech time: the amount of time in each dialogue line when both the original (source language) actor and the dubbing voice actor (target language) are speaking (i.e. the intersection), divided by the amount of time when either is speaking (i.e. the union)
- A value of 1.0 indicates perfect time alignment, while 0.0 indicates the source and target speech occur at entirely different times
- The mean overlap fraction in our corpus is 0.658 and the median is 0.731 -in 4.3% of lines, overlap is exactly 0, pulling down the mean. Thus, while human dubbed speech mostly co-occurs with source speech, isochronic constraints are also frequently violated by human dubbers
- Off-screen dubs are more isochronic than on-screen, but to a surprisingly small degree
- For significance tests in this work, unless otherwise noted we tests for the entire corpus for which the test is valid, as well as for subsets of the valid corpus corresponding to each target language (German and Spanish) and the genres listed in Table 1 (Drama, Kids, Comedy, and Suspense).

### Isometry
- Past work has examined similarity of text length (measured in characters) as a way to constrain translation for automatic dubbing
- This literature uses isometry mainly as a proxy for similarity of duration and for isochrony
- We aim to test these assumptions: how good a proxy is isometry for isochrony in human dubs, and how much do human dubbers preserve character length?
- We examined the text length (measured in characters) of aligned (source, human dub) dialogue line pairs, and especially the percentage change in character length from source to dub
- Character lengths on both sides included punctuation and spaces (except at the start or end of a dialogue line).
- To measure how well character length similarity proxies for similarity of duration, we compared the ratio of target to source length to the ratio of target to source duration.
- We examine here only known onscreen lines, as these are subject to the greatest pressure to be isochronic; results are very similar if using all lines.
- We find first that isometry is a weak-to-moderate proxy for isochrony.
- The ratio of human dub to English character lengths has a correlation of only r = 0.279 (r 2 = 0.078) with the time overlap fraction of source and target, though it is somewhat more correlated with the ratio of target to source durations (r = 0.620, r 2 = 0.385).
- Our results on character length similarity in human dubs, meanwhile, are summarized in Figure 1.
- Overall, there are large changes in character length from source to human dub. Most sentence pairs differ in length by more than prior work's 10% threshold.
- The absolute percentage change in character length is significantly different from 0 under a one-sample t-test (t = 20.3, p = 3.9e-83).
- These changes are significant for both languages and all genre subsets with p < 1e-38.
- Character lengths are more similar for longer sentences (and the distribution of character count change is smoother), but nearly 60% of pairs in which the source sentence is at least 50 characters long differ in length by more than 10%.
- We observe that human dubs are largely nonisometric in both Spanish and German, with neither language differing from the English source lines by more than 10% in less than 69% of cases.
- The length differences are, however, distributed differently. German skews toward longer dub lines, with 53% of all lines longer than the matched English lines by >=10%, and 16% shorter by >=10%; Spanish displays a smaller skew in the opposite direction, with 30% at least 10% longer and 42% at least 10% shorter.

### Speaking Rate
- Previous literature has paid considerable attention to the naturalness of human dubbed speech
- A frequent, though not universal, conclusion is that dubs sound "artificial and contrived"
- From another angle, the isometric MT literature argues that TTS models, which are less flexible than humans in varying speaking rate, may require isometric input to produce natural sounding isochronic output
- Because naturalness is a broad topic, and in general may require human evaluation, we focus on examining speaking rates
- We're particularly interested in whether dubbing voice actors are willing to vary their speaking rates, and perhaps compromise naturalness, in order to meet other constraints, like isochrony
- We examined both the dub speaking rate and the ratio of human dub duration to source duration as functions of the number of words in source and target dialogue lines
- As the dub-to-source ratio of word lengths increases, in other words, what happens to dub speaking rate and the duration ratio?
- Perhaps counterintuitively, it seems that the duration ratio is much more closely related to relative length of content than the dub speaking rate
- When forced to pick one or the other, human dubbers appear more willing to break timing constraints than vary speaking rate

### Lip Sync
- Both qualitative (Chaume, 2012;Fodor, 1976;Miggiani, 2019) and technical work (Taylor et al., 2015;Hu et al., 2021;Kim et al., 2019) have considered "lip sync" constraints in human and automatic dubbing, respectively.
- The idea is that dubbed audio should match the (visible) mouth movements of the original actors.
- Failing to do so may be jarring to the audience and reduce the quality of the dub.
- Some recent empirical studies, however, have found that this constraint may not be as binding as previously assumed (Perego et al., 2016).
- We ask here whether human dubbers produce speech which matches the mouth movements of the original actors.
- Rather than relying on the video tracks, we use the notion of a "viseme", or visual phoneme (Fisher, 1968), to capture alignment between source and human dub mouth movements.
- Phones in the same viseme are produced with similar articulatory movements of the lips and tongue, and look visually similar.
- We use viseme tables 10 to map each MFA-aligned phone to its corresponding viseme.
- The glottal stop and four guttural German sounds made without moving the lips are dropped.
- To measure cooccurrence, we sample the viseme active on both source and dub sides and compute the viseme-viseme cooccurrence matrix.
- We normalize the matrix so that the observed frequency of each viseme pair is a fraction of the frequency expected if source and target visemes were independent, but with the observed marginal distributions.
- Over all the data (onscreen, offscreen, and unannotated), the average within-viseme cooccurrence rate as a fraction of the rate under independence is 1.575, with an average across-viseme rate of 0.981.
- For onscreen the average within-viseme cooccurrence rate is 1.613, while for offscreen it is 1.463.
- We believe both on-and off-screen rates are above 1.0 due to the presence of names and cognates, where the phones may be (nearly) the same for the source and dub.
- The difference is statistically significant at the α = 0.05 level under a percentile-bootstrap test (p = 0.017) for the entire corpus, as well as for both languages and all genre subsets.
- Moreover, we see similar patterns by language: The amount of excess cooccurrence is 40.8% for 10 German (1.638 to 1.898)11 and 37.8% for Spanish (1.439 to 1.605).
- But though the effect is significant, it is not large in absolute terms: even in onscreen speech, only about 12.4% of speech time has the same viseme on the source and target sides. This suggests that human dubbers do sometimes lip-sync their output, but it is a fairly soft constraint.

### Translation Quality
- The human dubbing process is complicated and has many constraints
- To get at how faithful the translation is, the paper uses onscreen and offscreen annotations and measures translation quality
- The results show that there is no substantial difference between onscreen and offscreen speech
- For both metrics, the results were the same whether the data was broken out by language or genre.

### Non-Text Transfer
- Source audio properties explain a substantial fraction of target variance
- Source speaking rate correlates with target speaking rate
- Line-level mean pitch is more closely related to target mean pitch
- Gender of source character is only a weak predictor of line-level mean pitch

## Insights for Automatic Dubbing
- The human dubbing process points to several directions that should (and perhaps should not) be pursued in automatic dubbing
- Speaker gender and number issues are especially critical since the audience can often both hear and see the speakers and addressees
- While research does exist in this space, we suspect there is much room for improvement
- We find strong evidence for several levels of non-textual transfer of source audio properties into human dubs: speaker characteristics, dialogue line-level effects, and emotion/emphasis transfer when considering semantic alignments at the word level
- This points to a glaring issue with pipeline approaches employed by the vast majority of automatic dubbing literature: Without a mechanism to encode emotion/emphasis, individual vocal profiles and other traits of the source speech, we expect them to be nearly impossible to replicate in synthetic target speech
- The high rates of isochrony that we observe in human dubs support the need for continued research on isochronic MT, especially given the observed unwillingness of human dubbers to vary their speaking rate, which shows that automatic dubbing systems should not simply vary speaking rates to achieve isochronic constraints
- However, our findings do not support the use of isometric MT.

## Future Work
- The work focused on two language pairs: English-German and English-Spanish
- In future work, we hope to analyze more distant language pairs (e.g. English-Chinese or English-Arabic), as well as non-English source material
- Analysis has shown that isometry is a poor proxy for isochrony in human dubs
- However, in future work, we hope to verify some of these findings (e.g. translation quality of on-vs off-screen) using human annotators
- Finally, the aggregate analysis in this work is necessary to provide high-level insights for automatic dubbing
- However, it likely also hides interesting variations across different individual translators, adaptors, dubbers, dubbing studios, etc.

## Conclusion
- Humans display less respect for isochrony and especially lip sync than is suggested by qualitative literature
- They are surprisingly unwilling to vary speaking rates or sacrifice translation quality to hit other constraints
- The alignment process produces (source, tar-r 2 values for linear models predicting various properties of target audio (dubbing voice actor) from source audio (original actor). The first column reflects models containing only indicator or dummy variables for the speaker, while models in the second column add the line-level property for source side. All increases in explained variance are significant at the α = 10 −6 level by F-test.
