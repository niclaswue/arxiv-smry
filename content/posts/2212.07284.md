---
title: "MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"
date: 2022-12-14T15:33:44.000Z
author: "Nathan Godey, Roman Castagné, Éric de la Clergerie, Benoît Sagot"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07284v1.webp" # image path/url
    alt: "MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07284)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07284).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/manta-efficient-gradient-based-tokenization).

# Abstract
- Static subword tokenization algorithms have been an essential component of recent works on language modeling.
- However, their static nature results in important flaws that degrade the models' downstream performance and robustness.
- In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.
- MANTa is a differentiable tokenizer trained end-to-end with the language model.
- The resulting system offers a trade-off between the expressiveness of byte-level models and the speed of models trained using subword tokenization.
- In addition, our tokenizer is highly explainable since it produces an explicit segmentation of sequences into blocks.
- We evaluate our pre-trained model on several English datasets from different domains as well as on synthetic noise.
- We find that MANTa improves robustness to character perturbations and out-of-domain data.
- We then show that MANTa performs comparably to other models on the general-domain GLUE benchmark. Finally, we show that it is considerably faster than strictly byte-level models.

# Paper Content

## Introduction
- Most of the system-induced biases in the last few years have been removed from the field of natural language processing
- Tokenization is an essential part of natural language processing, and most recent models use subword tokenizers
- Tokenizers produce token sequences whose length is greatly reduced compared to the original character sequence, which is helpful because limitations in compute power and architectural constraints prevent models from processing arbitrary long sequences
- Tokenization-free models produce character-based or byte-based embeddings for LMs instead of subword embeddings, which improves the robustness of LMs to naturally occurring noise as well as their expressiveness when dealing with out-of-domain or multilingual data
- MANTa learns a simple but explainable segmentation using only the LM objective while effectively reducing the length of byte sequences.

## Related Work
- Non-neural subword-level tokenization methods have dominated in the last few years
- These methods have inherent flaws that limit their multilingual performance
- Tokenization-free (or character-level) models leverage characters instead of subwords to build text representations
- Some of the first neural networks for sequence generation used characters directly as inputs
- Following works modified the approach to create input word representations based on characters
- However, they still rely on fixed tokenization heuristics
- Recent works have tried to remove these induced biases by working purely with characters or bytes as input
- However, they either have to use various tricks to reduce the sequence lengths based on other induced biases like downsampling rates or have extremely low training and inference speeds

### Differentiable Tokenization
- Our main contribution is the introduction of an end-to-end differentiable tokenization architecture that consists of softly aggregating input bytes into what we refer to as blocks.
- Blocks can be compared to tokens with smooth borders, and the tokenization process is decomposed into several differentiable operations.
- The frontier predictor consists of a parameterized module mapping each byte to the probability of being a block frontier.
- In a first part, we embed each byte to an embedding e b .
- Working with bytes instead of characters allows modeling a larger array of symbols while having very small embedding matrices with 256 × hidden size parameters.
- Since the input sequences fed to the frontier predictor may be particularly long, we use a Transformer with sliding window attention.
- This layer achieves a linear complexity with respect to sequence length by computing attention using only a local context.
- We make the assumption that long-range dependencies are not relevant for segmentation and that this reduced context window should not harm the quality of the tokenization.
- Once the frontier probabilities (p F ) are predicted for the whole sequence, we use them to model an assignment between bytes and block slots.
- Each byte is given a probability distribution over the available block slots, and the expected block position of a byte in the block sequence increases along the byte sequence (i.e. the next byte is always more likely to be assigned to the next block).
- Let us introduce (B, b i ), the slot random variables for each byte b i , describing the position of the block containing b i in the block sequence.
- In other words, the event (B = k, b i ) describes the fact that the i-th byte belongs in the k-th block.
- These variables can only take values in [1, L], as there cannot be more blocks than there are bytes.
- We can model the (B, b i ) as a cumulative sum of the random variables F i : the position of the block in which a byte belongs is exactly the number of frontier bytes before this one.
- Since F i ∼ B(p F ), we can model the block index variables B depending on the index of the bytes b using the Poisson Binomial distribution PB which models the cumulative sum of Bernoulli variables: (B, b i ) ∼ PB (p F k ) k≤i .
- There exists no closed form for this distribution's mass function, but some fast methods have been developed to avoid exploring the whole event tree (Biscarri et al., 2018;Zhang et al., 2017).
- However, to reduce computational cost, we use a truncated Gaussian kernel G with the same mean and variance to approximate the (B, b i ) probability mass function: where term, and:
- We denote by P k,i the approximation of the probability of membership of the byte i to block k.
- We display an example of this map at different steps during training in Figure 2.
- At this point in the forward pass, we have estimated the position of the block in which each input byte belongs, along with the block sequence maximum plausible length L B .
- In order to provide block embeddings to the LM, we now focus on the contribution of each byte to the block given by the block-byte assignment map.
- For each block position k ∈ [1, L B ], this map actually provides an unnormalized contribution (P k,i ) of each byte in this block.
- We can then use the byte embeddings e b from the frontier predictor described in Section 3.1.1 and, for the k-th block, build a block embedding where each byte b i contributes based on its probability of being in this block P k,i...

### Model Training
- We obtain from the differentiable tokenizer and pooling module a sequence of block embeddings that can be used exactly like subword embeddings.
- Thus, we use an encoder decoder architecture identical to T5 (Raffel et al., 2020).
- Nevertheless, since we do not have a fixed subword vocabulary, our decoder operates at the byte level similarly to ByT5 (Xue et al., 2022).
- Our objective is identical to the one used in ByT5.
- We mask 15% of bytes randomly and choose a number of spans such that each has an average length of 20 bytes.
- Each span is then replaced by an <extra_id_i> token with i identifying the order of the span in the sequence.
- On the decoder side, the model has to predict in an autoregressive way the span identifier and the masked bytes.
- We pre-train our model on English text data using C4 (Raffel et al., 2020), a large corpus scraped from the Internet.
- This corpus is particularly suited to our pre-training due to its diversity in terms of content and linguistic variations.
- In addition, it enables a better comparison with other tokenizer-free models trained using it such as Charformer.
- Since this dataset is not available publicly, we use the English split of the mC4 distributed by AllenAI.
- We filter long documents containing more than 2 15 bytes, which is a simple proxy to remove important quantities of unwanted code data.
- Hyperparameters We pre-train two versions of our model: MANTa-LM Small and MANTa-LM Base .
- Each of them stacks a MANTa Small (resp. MANTa Base ) tokenizer and embedding module and a T5 Small (resp. T5 Base ) encoder-decoder model stripped of its tokenizer and subword embedding matrix.
- Details about MANTa hyperparameters can be found in Appendix B.
- Following T5 and ByT5, we use the Adafactor optimizer with a learning rate of 10 −2 for the encoder-decoder model, parameter scaling for the whole system and no weight decay.
- However, to maintain stability of our differentiable tokenizer, we use a learning rate of 10 −3 for the parameters of the byte embeddings, the frontier predictor, and the pooling module.
- We also use a triangular learning rate schedule with 1000 (resp. 5000) warm-up steps for batch size 1024 (resp. Training We train T5 Small , MANTa-LM Small , and MANTa-LM Base for 65k steps with a batch size of 1024.
- Sequence lengths are respectively 1024 for Small models and 2048 for the Base model. Thus, the models are trained on roughly the same amount of bytes as in Tay et al. (2021), where a batch size of 64 is used for 1M steps.
- We also train a ByT5 Small model on the same data, using a batch size of 64 and a sequence length of 1024.
- We consider the "Scaled" architecture which provides the encoder with more layers than the decoder (Xue et al., 2022).
- To avoid prohibitive computation costs and ensure fairness in terms of available resources between models, we limit its training time to the one of MANTa-LM Small . Hence, our ByT5 Small is only trained for 200k steps.

## Experiments and Results

### Evaluation on GLUE
- Uses GLUE as a benchmark to evaluate the model
- Shows slightly better performance than Charformer but is still within a small margin on average
- Main objective of the model is to balance decent performance with robustness and speed

### Robustness to Domain Change
- Static subword tokenizers tend to show important limitations when used with texts originating from a domain unseen during training.
- For instance, El Boukkouri et al. (2020) show that tokenizing medical texts with a tokenizer trained on Wikipedia data often results in an over-segmentation of technical terms which in turn affects the downstream performance.
- By removing this static bottleneck in MANTa-LM, we hope that it should be able to adapt more easily to new domains.
- To test this hypothesis, we finetune it on a medical Natural Language Inference dataset.
- We finetune MANTa-LM on MEDNLI (Romanov and Shivade, 2018), a dataset consisting of 14,049 sentence pairs extracted from clinical notes.
- We follow the same finetuning setup than for the GLUE Benchmark i.e. use the same batch size and learning rate.
- We compare our results to the ones obtained by El Boukkouri et al. (2020) with models pretrained on the general domain.
- We found that models performed similarly for the different noise levels in the DEV-ONLY setting.
- On the contrary, in the TRAIN-DEV setting, MANTa-LM can be finetuned as well as ByT5 for all levels of noise, while the performance of T5 quickly degrades.

### Robustness to Noisy Data
- MANTa is not static and can be finetuned on non-standard data
- MANTa is more robust to variation/noise than a subword tokenizer paired with a LM

## Training Speedups
- MANTa-LM is approximately 4 times faster than Byte-level T5 Small
- MANTa-LM is only 2.3 times slower than T5 Small

## Discussion

### Truncating Embedding Sequences
- Once we obtain block embeddings, the final step in MANTa consists in truncating sequences to a length 4 times smaller than the original byte sequence
- This is essential to make MANTa-LM work. First, it increases the control over the encoderdecoder's computation cost. Without this bottleneck, the Transformer can receive sequences varying from a single block containing the whole sequence (L B = 1) to one block per byte in the sequence (L B = L). In the latter case, which mimics ByT5's input segmentation, the computation becomes extremely slow due to the quadratic cost of the attention with respect to the sequence length. Using the bottleneck ensures that we can control the worst case complexity of the encoder Transformer and keep it similar to that of a subwordbased encoder model.

### Learnt Block Segmentation
- Segmentation examples can be found in Table 6.
- For each byte, we retrieve the expected block position produced by MANTa and approximate it with the closest integer to mimic hard tokenization.
- We found that MANTa is not keen to produce subword level segmentations.
- Most of the key symbols for word separation have been identified as block delimiters during pre-training.
- As expected, MANTa is less prone to over-segmentation of unknown words like named entities.
- We also found that a trained MANTa produced spiked separation probabilities, meaning that it converged towards a "hard" segmentation.

### Gradient-Based Segmentation
- Employs a radically different downsampling approach
- After byte contextualization, down samples sequences using a fixed rate
- Down samples right before the LM
- Limits the length of block sequences
- Can build word-level representations of arbitrary length as long as it divides the whole byte sequence length by a fixed factor
- Segmentation of blocks is disentangled from their representations

### Main hyperparameters
- We discuss here some of the major hyperparameters of our method.
- Constrained by limited computational resources, we were unable to assess their exact importance on MANTa's performance.
- We try to give some intuitions on their influence.
- Frontier Predictor
- We used a small Transformer network with sliding window attention for this module. A much larger network would be slower and may not bring significant improvements to the overall performance of the model, since it is only used for predicting the block byte assignment but does not "expand" the overall expressivity of the model.
- Convolution kernel applied on byte embeddings
- This kernel adds positional information to the byte embeddings and expressivity when constructing the block embeddings.
- Using a larger kernel or a concatenation of kernels might help for better block representations.
- However, our experiments did not show any significant difference in the pretraining performance.
- Trimming block sequences was instrumental to produce meaningful input segmentations and blocks containing more than a single byte.
- We settled for a factor of 4 since other values led to minor degradations early in training.
- This factor roughly corresponds to the average number of bytes in a subword created by an English tokenizer.
- We believe that a more thorough hyperparameter search could improve the performance of our model.

## Conclusion
- In this work, we present MANTa, a fully differentiable module that learns to segment input byte sequences into blocks of arbitrary lengths, and constructs a robust representation for these blocks.
- We train this module jointly with an encoderdecoder LM on a span denoising objective to obtain MANTa-LM.
- MANTa-LM is more robust when applied to noisy or out-ofdomain data than models using static subword tokenizers.
- At the same time, it performs on par with fully byte-level models on these setups while operating with a much reduced computational cost.
- Beyond the noisy and out-of-domain settings, we believe that our approach could lead to interesting results for a number of languages, especially those whose writing system do not use whitespace separators, such as Chinese.
- Finally, tokenizers are hypothesized to be an important limiting factor when segmenting multilingual data (Rust et al., 2021). We believe MANTa could be used in the multilingual setting to ensure a more balanced segmentation between languages.
