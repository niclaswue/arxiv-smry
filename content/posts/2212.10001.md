---
title: "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"
date: 2022-12-20T05:20:54.000Z
author: "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10001v1.webp" # image path/url
    alt: "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10001)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10001).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/towards-understanding-chain-of-thought).

# Abstract
- Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs).
- CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations.
- Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance.
- In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.
- Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning.

# Paper Content

## Introduction

## Study Formulation
- Chain-of-Thought rationale consists of a series of reasoning steps
- Bridging objects: the key and necessary objects that the model needs to traverse in order to make a successful final prediction
- Language templates: the complementary parts of bridging objects, which serve as textual hints and relations/predicates that guide the model to derive the correct bridging objects along the reasoning process
- In Chain-of-Thought prompting, correct bridging objects and language templates are provided as demonstrations to show the LLM how to reason

### Datasets & In-context Exemplars
- We experiment on two representative tasks involving multi-step reasoning: arithmetic reasoning & multi-hop factual question answering (QA).
- For arithmetic reasoning, we experiment on GSM8K (Cobbe et al., 2021), one of the most challenging mathematical reasoning benchmarks available which is also repeatedly adopted by prior work as a key benchmark for arithmetic reasoning; for multihop factual QA, we experiment on Bamboogle, a dataset of compositional questions constructed by Press et al. (2022).
- Due to budget considerations, we uniformly sample 800 out of the 1319 test examples for GSM8K for evaluation.
- We evaluate on all 125 test samples for Bamboogle.
- Based on the original prompt exemplars, i.e., the set of (query, rationale, answer) pairs released by Wei et al. (2022) andPress et al. (2022), with slight editing to make the struc-ture more consistent and reduce redundancy, our experiments only slightly affect the performance of CoT.
- We show our edited demonstration examples and include more details in Appendix A.1.

### Backbone Language Model
- We use the InstructGPT-175B2 LLM as our backbone
- LLM is one of the most performant and widely-used
- LLM has strong performance under CoT prompting
- We report its results and analyze them in the main content
- In addition, we also test on its very recent improved version text-davinci-003

### Evaluation
- Answer Accuracy for GSM8K: measures the correctness of the final answer
- Answer F1 for Bamboogle: measures the recall/f1 of the bridging objects which need to be derived by the LLM
- Intrinsic evaluation: measures the recall/f1 of the bridging objects which need to be derived by the LLM

### Constructing Invalid Chain of Reasoning
- We manually write rationales with invalid reasoning for all the in-context demonstration examples.
- Since our focus here is to investigate the importance of the validity of reasoning, we only ablate the parts in a CoT rationale which are involved with derivations that are logically sound and helpful for answering the query.
- Importantly, we are not adopting an adversarial/counterfactual perturbation setting where minimal alterations are applied to make the reasoning invalid; instead, we apply rather drastic changes where we change both the bridging objects and language templates and hence little valid reasoning exists to help solve the query.
- The full prompts in our setting are included in Appendix A.4.

### Results & Analysis
- Relevance and coherence are key for the performance of CoT prompting.
- It can be seen that most of the settings for this section (2-7) have rather large performance drops from CoT, where the low-performing ones approach or even underperform standard prompting.
- This suggests that overall, relevance and coherence are key for the performance of CoT.
- Keeping relevance is crucial.
- The no relevance setting 7 where both components of the rationale have no relevance achieves significantly poorer performance than other ablation settings, and even underperforms standard prompting (STD) where no rationale is given on GSM8K.
- To see why such low performance happens, we manually examine the generated rationales under this setting for 20 examples on GSM8K.
- The LLM is indeed generating irrelevant rationales (both bridging objects and language templates) for 15 out of 20 examples, many of which have recurring topics (e.g., "cats and dogs", "passengers and buses") which we hypothesize are frequent patterns in the portion relevant to mathematics in the pretraining corpora.
- Overall, this suggests that a certain level of relevance is crucial for the LLM to stick to the query being asked.
- Relevance matters more than coherence for bridging objects.
- Providing incoherent bridging objects (2) achieves better performance than providing irrelevant bridging objects (3), especially on the more challenging GSM8K dataset (39.2 v.s. 26.2 Inter. F1).
- which indicates that it is important for the bridging objects to be relevant, but not as important to have them in the right order to guide the LLM along the reasoning process.
- We also quantitatively measure the coverage of bridging objects from the query for the generated rationales, and find that the settings with no relevance for bridging objects (3, 7) indeed have significantly lower coverage (below 60%) than other settings (around 80%).
- Coherence of language templates is important.
- Different from the coherence of bridging objects 2, the coherence of language templates 4 matters a lot to the performance of CoT prompting.
- By examining the predicted rationales, we find that the LLM is indeed generating rationales with incoherent language templates (14 out of 20 examples), which negatively affects reasoning.

### Ablation Settings
- Based on the CoT prompts instead of the settings in §4
- There are 4 ablation settings where each could examine 1 aspect of a certain component
- We also experiment with 2 other settings: no relevance where neither bridging objects nor language templates have relevance, and no coherence which is defined analogously (6,7 in Table 4)

## Discussion
- The results from §4 and §5 open up new questions regarding learning to reason in-context for LLMs
- What the LLM learns from the demonstrations about how to reason properly is limited-rather, the LLM has already gained a lot of such complex reasoning ability from pretraining (at least for arithmetic & multihop factual QA that we experiment on), and the provided reasoning steps serve more as the role of an output format/space, that regularizes the LLM to generate rationales that look step-by-step while being coherent and relevant to the query being asked.
- From another perspective, if we view the invalid reasoning setting as a task, where the goal is to generate invalid reasoning steps for the query, then the LLM has basically failed to capture the task as it still tries to predict valid reasoning steps.
- This extends the previous findings that LLMs face difficulties in capturing task semantics that are presumably rare in the pretraining distribution (Jang et al., 2022).
- Can LLMs learn to reason in-context? We note that what we find does not in any way diminish the potential of learning to reason in-context for LLMs; recent work has also shown evidence that learning in-context is possible and could be powerful (Garg et al., 2022;Akyürek et al., 2022).
- Rather, our findings show that the existing successes of CoT are not sufficient for establishing that LLMs are good few-shot learners of reasoning; instead, the rich pretraining corpora have already forged them to be good reasoners on the tasks being evaluated, where they learn little about how to reason from the demonstrations.
- This is not an issue if the aim is only to elicit reasoning skills and other types of knowledge from LLMs, however, our findings would give a warning sign if the goal is to build models that are truly intelligent and learn new reasoning skills efficiently.

## Conclusion
- The original 9 computers plus the 5 computers each day from Monday to Thursday equals 14 computers in the server room.
