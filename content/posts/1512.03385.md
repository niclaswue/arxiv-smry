---
title: "Deep Residual Learning for Image Recognition"
date: 2015-12-10T19:51:55.000Z
author: "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1512-03385v1.webp" # image path/url
    alt: "Deep Residual Learning for Image Recognition" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1512.03385)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1512.03385).


# Abstract
- Deeper neural networks are more difficult to train
- We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously
- We reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions
- We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth
- On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity
- An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task.

# Paper Content

## Introduction
- Deep convolutional neural networks have led to a series of breakthroughs for image classification
- The "levels" of features can be enriched by the number of stacked layers (depth)
- With the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly
- The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize
- The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers
- Our extremely deep residual nets are easy to optimize, but the counterpart "plain" nets (that simply stack layers) exhibit higher training error when the depth increases

## Related Work
- In image recognition, VLAD is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector can be formulated as a probabilistic version of VLAD.
- For vector quantization, encoding residual vectors is shown to be more effective than encoding original vectors.
- In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.
- An alternative to Multigrid is hierarchical basis preconditioning, which relies on variables that represent residual vectors between two scales. It has been shown that these solvers converge much faster than standard solvers that are unaware of the residual nature of the solutions.
- Shortcut connections have been studied for a long time and practices and theories that lead to shortcut connections have been studied for a long time.
- In [44], an "inception" layer is composed of a shortcut branch and a few deeper branches. Concurrent with our work, "highway networks" present shortcut connections with gating functions.
- When a gated shortcut is "closed" (approaching zero), the layers in highway networks represent non-residual functions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned.
- In addition, high-way networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers).

## Deep Residual Learning

### Residual Learning
- Let us consider H(x) as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.
- If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., H(x) − x (assuming that the input and output are of the same dimensions).
- So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function F(x) := H(x) − x.
- The original function thus becomes F(x)+x.
- Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.
- This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left).
- As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart.
- The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.
- With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.
- In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem.
- If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.

### Identity Mapping by Shortcuts
- We adopt residual learning to every few stacked layers
- The form of the residual function F is flexible
- Experiments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible. But if F has only a single layer, Eqn.( 1) is similar to a linear layer: y = W 1 x + x, for which we have not observed advantages.

### Network Architectures
- We have tested various plain/residual nets, and have observed consistent phenomena.
- To provide instances for discussion, we describe two models for ImageNet as follows.
- Plain Network. Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig. 3, left).
- The convolutional layers mostly have 3×3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer.
- We perform downsampling directly by convolutional layers that have a stride of 2.
- The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax.
- The total number of weighted layers is 34 in Fig. 3.
- It is worth noticing that our model has fewer filters and lower complexity than VGG nets [41] (Fig. 3

### Implementation
- The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation
- A 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted
- The standard color augmentation in [21] is used
- We initialize the weights as in [13] and train all plain/residual nets from scratch
- We use SGD with a mini-batch size of 256
- The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60 × 10 4 iterations
- We use a weight decay of 0.0001 and a momentum of 0.9
- We do not use dropout

## Experiments

### ImageNet Classification
- We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes.
- The models are trained on the 1.28 million training images, and evaluated on the 50k validation images.
- We also obtain a final result on the 100k test images, reported by the test server.
- We evaluate both top-1 and top-5 error rates.
- Plain Networks. We first evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for detailed architectures.
- The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net.
- To reveal the reasons, in Fig. 4 (left) we compare their training/validation errors during the training procedure. We have observed the degradation problem -the Here the ResNets have no extra parameter compared to their plain counterparts.
- Fig. 4 shows the training procedures.
- 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one.
- We argue that this optimization difficulty is unlikely to be caused by vanishing gradients.
- These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances.
- We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish.
- In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the solver works to some extent.
- We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error 3 .
- The reason for such optimization difficulties will be studied in the future.
- Residual Networks. Next we evaluate 18-layer and 34layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3×3 filters as in Fig. 3 (right).
- In the first comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts.
- We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learning -the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.
- Second, compared to its plain counterpart, the 34-layer 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC '15) 3.57
- Table 5. Error rates (%) of ensembles. The top-5 error is on the test set of ImageNet and reported by the test server.
- ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left). This comparison verifies the effectiveness of residual learning on extremely deep systems.
- Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left). When the net is "not overly deep" (18 layers here), the current SGD solver is still able to find good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster convergence at the early stage.

### CIFAR-10 and Analysis
- The plain/residual networks have the same depth, width, and number of parameters as the deep versions.
- The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper.
- ResNets have smaller responses than their plain counterparts.

### Object Detection on PASCAL and MS COCO
- Our method has good generalization performance on other recognition tasks
- Table 7 and 8 show the object detection baseline results on PASCAL VOC 2007 and 2012
- We adopt Faster R-CNN [32] as the detection method
- Here we are interested in the improvements of replacing VGG-16 [41] with ResNet-101.
- The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks.
- Most remarkably, on the challenging COCO dataset we obtain a 6.0% increase in COCO's standard metric (mAP@[.5, .95]), which is a 28% relative improvement.

## A. Object Detection Baselines
- In this section we introduce our detection method based on the baseline Faster R-CNN system.
- The models are initialized by the ImageNet classification models, and then fine-tuned on the object detection data.
- We have experimented with ResNet-50/101 at the time of the ILSVRC & COCO 2015 detection competitions.
- Unlike VGG-16 used in [32], our ResNet has no hidden fc layers.
- We adopt the idea of "Networks on Conv feature maps" (NoC) [33] to address this issue.
- We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv layers in ResNet-101; Table 1).
- We consider these layers as analogous to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels).
- These layers are shared by a region proposal network (RPN, generating 300 proposals) [32] and a Fast R-CNN detection network [7].
- RoI pooling [7] is performed before conv5 1.
- On this RoI-pooled feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16's fc layers.
- The final classification layer is replaced by two sibling layers (classification and box regression [7]).
- For the usage of BN layers, after pre-training, we compute the BN statistics (means and variances) for each layer on the ImageNet training set. Then the BN layers are fixed during fine-tuning for object detection. As such, the BN layers become linear activations with constant offsets and scales, and BN statistics are not updated by fine-tuning.
- We fix the BN layers mainly for reducing memory consumption in Faster R-CNN training.
- We revisit the PASCAL VOC dataset based on the above model.
- With the single model on the COCO dataset (55.7% mAP@.5 in Table 9), we fine-tune this model on the PAS-CAL VOC sets.
- The improvements of box refinement, context, and multi-scale testing are also adopted.
- By doing so, Table 12. Our results (mAP, %) on the ImageNet detection dataset.

## B. Object Detection Improvements
- MS COCO box refinement
- Non-maximum suppression
- Global context
- Multi-scale testing
- Ensemble

## C. ImageNet Localization
- The ImageNet localization task [36] requires to predict the location of objects in an image.
- We adopt the "per-class regression" (PCR) strategy [40,41], learning a bounding box regressor for each class.
- We pre-train the networks for Im-ageNet classification and then fine-tune them for localization.
- We train networks on the provided 1000-class Ima-geNet training set.
- Our localization algorithm is based on the RPN framework of [32] with a few modifications.
- Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form.
- This RPN ends with two sibling 1×1 convolutional layers for binary classification (cls) and box regression (reg), as in [32].
- The cls and reg layers are both in a per-class from, in contrast to [32].
- Specifically, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not being an object class; the reg layer has a 1000×4-d output consisting of box regressors for 1000 classes.
- As in [32], our bounding box regression is with reference to multiple translation-invariant "anchor" boxes at each position.
- As in our ImageNet classification training (Sec. 3.4), we randomly sample 224×224 crops for data augmentation.
- We use a mini-batch size of 256 images for fine-tuning.
- To avoid negative samples being dominate, 8 anchors are randomly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1 [32].
- For testing, the network is applied on the image fully-convolutionally.
- Table 13 compares the localization results.
- Following [41], we first perform "oracle" testing using the ground truth class as the classification prediction.
- VGG's paper [41]  ports a center-crop error of 33.1% (Table 13) using ground truth classes.
- Under the same setting, our RPN method using ResNet-101 net significantly reduces the center-crop error to 13.3%.
- This comparison demonstrates the excellent performance of our framework.
- With dense (fully convolutional) and multi-scale testing, our ResNet-101 has an error of 11.7% using ground truth classes.
- Using ResNet-101 for predicting classes (4.6% top-5 classification error, Table 4), the top-5 localization error is 14.4%.
- The above results are only based on the proposal network (RPN) in Faster R-CNN [32].
- One may use the detection network (Fast R-CNN [7]) in Faster R-CNN to improve the results. But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar RoI-pooled features. As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training.
- Motivated by this, in our current experiment we use the original R-CNN [8] that is RoI-centric, in place of Fast R-CNN.
- Our R-CNN implementation is as follows. We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals.
- For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classifier.
- The image region is cropped from a proposal, warped to 224×224 pixels, and fed into the classification network as in R-CNN [8].
- The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form.
- This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the RoI-centric fashion.
- For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals' scores and box positions.
- This method reduces the top-5 localization error to 10.6% (Table 13).
- This is our single-model result on the validation set.
- Using an ensemble of networks for both classification and localization, we achieve a top-5 localization error of 9.0% on the test set.
