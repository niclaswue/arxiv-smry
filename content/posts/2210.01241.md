---
title: "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
date: 2022-10-03T21:38:29.000Z
author: "Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-01241v2.webp" # image path/url
    alt: "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.01241)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.01241).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural).

# Abstract
- We tackle the problem of aligning pre-trained large language models (LMs) with human preferences.
- If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework.
- However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment.
- Thus, a question rises in the research community: is RL a practical paradigm for NLP?
- To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL.
- The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function.
- Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks.
- Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluation.

# Paper Content

## INTRODUCTION
- The ultimate aim of language technology is to interact with humans
- Most language models are trained without direct signals of human preference, with supervised target strings serving as (a sometimes crude) proxy
- One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient
- Automated metrics offer a promising compromise: models of human preference like pairwise learned preference models (Ouyang et al., 2022), BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate
- But -these functions are usually not per-token differentiable: like humans, metrics Here, the LM (i.e., the policy) needs to produce a positive sentiment continuation given a review prompt (we cover other models of human preference in Sec. Two objectives are balanced: 1) an automated proxy of human preference that serves as a reward (here: a sentiment classifier); and 2) "naturalness" as measured by a KL divergence from an LM not trained with explicit human feedback.
- The plots show validation learning curves comparing our NLPO to the popular policy gradient method PPO. (Top plot:) RL methods can easily achieve high reward if the KL penalty is removed, (Bottom:) but at the cost of higher perplexity. NLPO+KL, our proposed approach, succeeds in balancing reward and naturalness more effectively than prior work.

## RELATED WORK
- Imitation learning
- Algorithms inspired by DAGGER and SEARN
- The cliff MDP problem
- Reward-conditioned transformers
- Non-public datasets don't enable comparison

## RL4LMS: A LIBRARY FOR TRAINING LMS WITH RL
- RL4LMs is an open-source library with building blocks for fine-tuning and evaluating RL algorithms on LM-based generation
- The library is built on HuggingFace (Wolf et al., 2020) and stable-baselines-3 (Raffin et al., 2021), combining important components from their interfaces
- RL4LMs can be used to train any decoder only or encoder-decoder transformer models from Hug-gingFace with any on-policy RL algorithm from stable-baselines-3
- Furthermore, we provide reliable implementations of popular on-policy RL algorithms that are tailored for LM fine-tuning such as PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015a), A2C (Mnih et al., 2016), and our own NLPO ( §4)
- The library is modular, which enables users to plug-in customized environments, reward functions, metrics, and algorithms

## ENVIRONMENTS: GENERATION AS A TOKEN-LEVEL MDP
- Each environment is an NLP task: we are given a supervised dataset of N examples, where x is an input language and y is the target string.
- Generation can be viewed as a Markov Decision Process (MDP) using a finite vocabulary V.
- Each episode in the MDP begins by sampling a datapoint (x, y) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.
- The input x = (x 0 , • • • , x m ) is a task-specific prompt that is used as our initial state s 0 = (x 0 , • • • , x m ), where s 0 ∈ S and S is the state space with x m ∈ V.
- An action in the environment a t ∈ A consists of a token from our vocabulary V.
- The transition function P : S × A → S deterministically appends an action a t to the end of the state s t−1 = (x 0 , • • • , x m , a 0 , • • • , a t−1 ). This continues until the end of the horizon t ≤ T and we obtain a state s T = (x 0 , • • • , x m , a 0 , • • • , a T ).
- At the end of an episode a reward R : S × A × Y → R 1 that depends on the (s T , y) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted.

## REWARD FUNCTIONS AND EVALUATION METRICS
- Because RL4LMs provides a generic interface for per-token or per-sequence generation rewards, it is possible to quickly apply a wide array of RL algorithms to a similarly diverse range of textual metrics-as-rewards.
- Specifically, we provide interfaces to 1) n-gram overlap metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), SacreBLEU (Post, 2018), METEOR (Banerjee & Lavie, 2005); (2) model-based semantic metrics such as BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020) which generally provide higher correlation with human judgment; 3) task-specific metrics such as CIDER (Vedantam et al., 2015), SPICE (Anderson et al., 2016) (for captioning/commonsense generation), PARENT (Dhingra et al., 2019) (for data-to-text) and SummaCZS (Laban et al., 2022) (for factuality of summarization); 4) diversity/fluency/naturalness metrics such as perplexity, Mean Segmented Type Token Ratio (MSSTR) (Johnson, 1944), Shannon entropy over unigrams and bigrams (Shannon, 1948), the ratio of distinct n-grams over the total number of n-grams (Distinct-1, Distinct-2) and count of n-grams that appear only once in the entire generated text (Li et al., 2015); 5) task-specific, model-based human preference metrics such as classifiers trained on human preference data collected in the methodology of Ouyang et al. (2022).

## ON-POLICY ACTOR-CRITIC ALGORITHMS

## NLPO: NATURAL LANGUAGE POLICY OPTIMIZATION
- Natural Languge Policy Optimization
- Inspired by action elimination/invalid-action masking
- Masks out less relevant tokens in-context
- Periodically updates policy π ψ

## GRUE (GENERAL REINFORCED-LANGUAGE UNDERSTANDING EVAL)
- GRUE is a collection of 7 generative NLP tasks
- To combat reward hacking for any single metric, each task is evaluated at test time according to a task-specific mix of metrics, detailed in Table 1
- At training time, there are no special restrictions: models are free to use the supervised data, compute metrics on intermediate generations, etc.
- For each task, to isolate the effect of training method, we select a single pre-trained LM backbone.
- For IMDB text continuation we use GPT-2 (117m parameters), and for the rest of the tasks we use T5-base (220m parameters).
- For our RL models (PPO, NLPO, Supervised+PPO, Supervised+NLPO), for a thorough investigation of how reward-hacking might interplay with GRUE, we run a separate set of experiments optimizing multiple task rewards for each task independently
- We gather human judgments for five of the tasks in GRUE. In doing so, our goals are 1) to validate that the automated metrics we selected for GRUE correlate with human judgments with respect to relative ranking between models; and 2) to provide additional empirical comparisons regarding NLPO vs. PPO, ablations to study the effects of the KL naturalness penalty, etc.
- We find that trends in our human evaluations generally match those seen in the automated metrics for both task and naturalness metrics (see Figures 2(c), 2(d) which summarize Appendix Tables 10,15,21,26, 35-Supervised+NLPO > Supervised ≥ Supervised+PPO > NLPO ≥ PPO > Zero-shot-with the exception of Supervised outperforming Supervised+PPO on 2 out of 5 tasks when automated metrics would indicate that Supervised+PPO outperforms Supervised on all of the tasks.

## PREFERENCE REWARD LEARNING, SELECTION, AND HACKING
- The GRUE benchmark's metric for each task is an average over several measures
- Many possible single metric rewards provide task performance gains over supervised methods
- The KL constraint -models are penalized from straying too far from a base LM in their pursuit of high reward
- But which model works best as a base regularizing LM?
- When the initial policy (i.e., the raw, pretrained model) has low performance on the task, the KL penalty pushes the policy towards nonsense
- If the base regularizing LM is the supervised model, the reward encourages the policy to balance the task-specific reward and a more reasonable regularization term
- Deriving KL penalties from warm-started initial policies is critical for performance on such tasks

## DATA BUDGET: IMPROVE YOUR REWARD OR GATHER MORE DEMONSTRATION?
- The trade-off between gathering feedback to improve a learned reward function or to gather more expert demonstrations is between 1) sentiment labels (improving the reward); or 2) positive sentiment reviews (improving supervised training).
- We train a classifier on varying amounts of training data and evaluate on the held out test datasetfinding as expected that more training data improves test accuracy and so results in a higher quality reward.
- Further, we trained a supervised model with at least as many samples used to train each of these reward classifiers. We find that a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data. This implies that improving reward models can be more data efficient than collection expert demonstrations for a task-and that's not accounting for the fact that assigning sentiment labels is likely a simpler task than writing full demonstrations.

## PRACTICAL CONSIDERATIONS: WHICH IMPLEMENTATION DETAILS MATTER MOST?
- Generation is a token-level MDP, not a bandit environment
- Most recent works that tune LMs using RL do so by calculating a reward for all the tokens in the sentence (Wu et al., 2021;Ouyang et al., 2022;Lu et al., 2022)
- This setting is equivalent to a bandit feedback environment where the action space is the space of all possible generations for the task (Sutton & Barto, 2018)
- This type of environment can be simulated within our RL formulation by setting the discount factor γ = 1.
- Table 3 (and Appendix Table 6) shows that this causes instability in training with respect to naturalness in both PPO and NLPO for IMDB
- Our standard setting is γ = 0.95 when calculating discounted rewards-to-go in the token-level MDP formulation, which reduces the magnitude of the reward that is applied to tokens selected at the beginning.
- The sentiment scores are approximately the same between both settings but the naturalness of language in the bandit setting is significantly less -indicating that discounting rewards with γ < 1 via a token-level MDP formulation is at least sometimes more effective for language generation.

## CONCLUSIONS
- We're hopeful that the GRUE benchmark and the RL4LMs library can push progress in aligning language models to human preferences via RL methods by providing the community with a standard means of comparing methods.
- Furthermore, we're optimistic that, as the stability and consistency of training improves, our methods provide a path towards iterative improvement of language technologies, with deployment, user feedback collection, and re-optimization enabling better user experiences when interacting with generative models.
- A.1 PPO DETAILS
- Given discussion and equations in Section 3.3, we further note that we follow (Ziegler et al., 2019) and dynamically adapt the KL coefficient β during training where, where KL target is user-specified KL divergence between initial model h and current policy π and K β is rate of update which we generally set to 0.2 in our experiments.
- To increase stability during training, we further use Generalized Advantage Estimation (GAE) (Schulman et al., 2015b) and define the advantage estimator Â(s n , a n ) based on the Temporal Difference residual as:
- where λ provides the trade-off between bias and variance.
- A.2 NLPO DETAILS
- NLPO learns to mask irrelevant language by maintaining a masking policy π ψ : the masking policy is a copy of the current policy (π θ ), but is updated only every µ steps.
- Given Z(π θ ) = a∈V π θ0 (a|s) the normalization value of the sum of probabilities of all action a ∈ A given a particular State s ∈ S, let the parameterized top-p vocabulary V p π θ ⊂ V be the subset of the vocab, consisting of the top-p highest probability vocabulary tokens with respect to π θ .
- Formally, let Z p be the normalization value for the parameterized top-p vocabulary, can be defined as the subset of tokens that maximizes Z p (π θ ) = a∈V k π θ π θ (a|s).
- Then optimizing a policy according to the parameterized top-p vocabulary can be defined as:
- B EXPERIMENTAL DETAILS
- Qualification round
- We ran a qualification round using the IMDB task.
- We opened the qualification around to users from {AU, CA, NZ, GB, US} with 5K prior approved HITs and a minimum acceptance rate of 97% on their previous HITs.
- We gathered judgments over 600 generations from 3 annotators per generation.
- One of the authors of this paper also completed 17 random HITs to serve as a proxy for "ground truth."
- After gathering these annotations, we selected workers who:
- 1) didn't significantly disagree with other annotators on the same instance more than 20% of the time;
- 2) who completed at least 5 HITs;
- 3) who didn't disagree with the author annotator on the 17 HITs by more than 1 point; and
- 4) (likely) spent a reasonable amount of time reading the instructions/examples provided.
- In the end, 56 annotators were qualified.
- Additional per-task details are provided in the per-task sections of the Appendix.
- Compensation details
- As per Amazon Mechanical Turk policy, annotators were compensated on a per-HIT basis.
- In addition, we used a timing script to estimate hourly wages to ensure our target of $15/hr was met.
- In cases where this minimum hourly rate was not met,...
