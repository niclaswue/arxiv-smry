---
title: "CORL: Research-oriented Deep Offline Reinforcement Learning Library"
date: 2022-10-13T15:40:11.000Z
author: "Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, Sergey Kolesnikov"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2210-07105v2.webp" # image path/url
    alt: "CORL: Research-oriented Deep Offline Reinforcement Learning Library" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.07105)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.07105).


# Abstract
- CORL is an open-source library that provides single-file implementations of Deep Offline Reinforcement Learning algorithms
- It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool
- In CORL, we isolate methods implementation into distinct single files, making performance-relevant details easier to recognise
- Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud
- Finally, we have ensured the reliability of the implementations by benchmarking a commonly employed D4RL benchmark

# Paper Content

## Introduction
- Deep Offline Reinforcement Learning has been showing significant advancements in numerous domains such as robotics, autonomous driving, and recommender systems
- On the one hand, introduced abstractions may hinder the learning curve for newcomers and the ease of adoption for researchers interested in developing new algorithms
- On the other hand, the unadorned design allows practitioners to read and understand the implementations of the algorithms straightforwardly
- CORL library supports optional integration with experiments tracking tool such as Weighs&Biases

## Related Work
- Numerous open-source RL frameworks and libraries have been developed over the last years
- For example, stable-baselines provides many deep RL implementations that carefully reproduce results to back up RL practitioners with reliable baselines during methods comparison
- Ray is focusing on implementations scalability and production-friendly usage
- More nuanced solutions exist, such as Dopamine, which emphasizes different DQN variants
- ORL branch, which we are interested in this paper, is not yet covered as much: the only library that precisely focus on offline RL setting is d3rlpy
- CORL do also focus on ORL methods, similar to d3rlpy, but takes a different perspective on library design and provides non-modular independent algorithms implementations
- To be more concrete, we put environment details, algorithms hyperparameters, and evaluation parameters into a single file
- Without additional abstractions, implementation simplifies to a single for-loop with a global python name scope
- Such flat architecture makes it easier to access and inspect any created variable during the training process

### Configuration files
- CORL allows for predefined configuration files for each experiment
- This makes it easier to set up experiments, as you only need to remember one set of parameters

### Experiment Tracking
- ORL evaluation is another challenging aspect of the current ORL state
- CORL supports integration with Wandb, a modern experiment tracking tool
- With each experiment, CORL automatically saves: (1) source code, (2) dependencies (requirements.txt), (3) hardware setup, (4) OS environment variables, (5) hyperparameters, (6) training and system metrics, (7) logs (stdout, stderr)

## Benchmarks
- In our library we implemented the following algorithms: N %7 Behavioral Cloning (BC), TD3 + BC [Fujimoto and Gu, 2021], CQL [Kumar et al., 2020], IQL [Kostrikov et al., 2021], AWAC [Nair et al., 2020], SAC-N, EDAC [An et al., 2021], and Decision Transformer (DT) [Chen et al., 2021].
- We evaluated every algorithm on the D4RL benchmark [Fu et al., 2020], focusing on Locomotion, Maze2D, and AntMaze tasks.
- Each algorithm was run for 1 million gradient steps8 and evaluated every 5000 steps using 10 and 100 episodes for locomotion and maze tasks, respectively.
- For our experiments, we used hyperparameters proposed in the original works (see Appendix C for details).
- The final performance results are reported in Tables 1 and 2.
- According to the observed results, AWAC, SAC-N, and EDAC show the most competitive performance in both the last and best evaluation settings. At the same time, TD3 + BC performs well across all tasks without any hyperparameters tuning while other methods rely on it. Finally, simple yet effective BC-10% performs well on locomotion tasks when high-quality data is available and converges there almost instantly (see Appendix A).

## Conclusion
- CORL is a single-file implementation library for Offline Reinforcement Learning
- All implemented algorithms were benchmarked on D4RL datasets
- Focusing on implementation clarity and reproducibility, we hope that CORL will help RL practitioners in their research and applications.

## A Additional Benchmark Information
- Figure 1: Illustration of the CORL library design.
- Figure 2: Graphical representation of the normalized performance of the last trained policy on D4RL averaged over 4 random seeds.
- (a) Locomotion datasets. (b) Maze2d datasets (c) AntMaze datasets
- (TD3+BC hyperparameters. SAC-N and EDAC hyperparameters.)
