---
title: "Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning"
date: 2022-12-21T20:43:46.000Z
author: "Chris Lengerich, Gabriel Synnaeve, Amy Zhang, Hugh Leather, Kurt Shuster, Fran√ßois Charton, Charysse Redwood"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11353v1.webp" # image path/url
    alt: "Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11353)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11353).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/contrastive-distillation-is-a-sample).

# Abstract
- Traditional approaches to RL focus on learning decision policies from episodic decisions
- While some approaches have been adopted to refine representations via auxiliary self-supervised losses while simultaneously learning decision policies, learning compositional representations from hand-designed and context-independent self-supervised losses (multi-view) still adapts relatively slowly to the real world
- In contrast, supervised language model cascades have shown the flexibility to adapt to many diverse manifolds
- However, to date, transfer methods for language models like few-shot learning and fine-tuning still require human supervision and transfer learning using self-learning methods has been underexplored
- We propose a self-supervised loss policy called contrastive distillation which manifests latent variables with high mutual information with both source and target tasks from weights to tokens
- We show how this outperforms common methods of transfer learning and suggests a useful design axis of trading off compute for generalizability for online transfer

# Paper Content

## Introduction
- The test of a first-rate intelligence is the ability to hold two opposed ideas in mind
- Online generation of contrastive self-learning data in regimes of extremely limited data is a domain that humans excel at
- By asking questions like "why", "how" via rollouts of writing or thought, humans manifest latent axes of variation as sequences
- This additional data parameterizes online loss surfaces expressed as expectations and values which are attended to contrastively
- Generating this self-learning data efficiently may rely heavily on transfer learning and a diversity of online learning algorithms to collect contrastive evidence
- In contrast, current machine learning approaches to transfer learning are substantially less data-efficient
- The current paradigm is arguably one of "unsupervised pre-training then online supervised transfer", where the pre-training step is training on massive datasets using unsupervised losses like InfoNCE, language modeling loss, or VICReg, among others
- Since this Bayesian contrastive distillation can be invoked recursively, it allows for iterative generation (meta-learning) of the edges of compositional hierarchies from small batches of sequential data via the extension of past hierarchical representations
- It also generates the training data needed to bootstrap verification classifiers needed for self-learning

### Core Contributions
- We synthesize 4 disparate literatures from RL, NLP, self-supervised learning and linguistics into a concrete unifying model for self-learning
- This formalizes a model of language dynamics as iterative world models for belief propagation through discrete sequences which manifest latent axes of variation of distribution shifts, especially for transfer learning through memory.
- We introduce contrastive distillation -a self-learning loss policy created by manifesting latent variables as sequences which have high mutual information with both source and target tasks. This is consistent with the constraints of transfer learning and empirically improves transfer learning of a language model in limited-data regimes vs. fine-tuning and few-shot prompting baselines. Overall, this suggests a general tradeoff between generalizability and compute for self-learning.
- We show that sampling from episodic memory improves contrastive distillation. Flat memory of sequences, in contrast to hierarchical representations, has inductive biases for constant-time connectivity between world models, multi-task support (since memory lookups can load arbitrary models and mix results from several world models in scope at once), and short belief-space path length between world models for value propagation during self-learning.
- We present a simple, yet generalizable, algorithm for sampling negative examples for contrastive losses like NCE more efficiently than random negative sampling. This can be used to iteratively construct hierarchical decision rules.

## Related Works

### Self-Learning Via Expert Iteration
- Recent papers show promising results for self-learning of large language models via CoT rollouts and majority voting
- STaR (Zelikman et al., 2022) uses a rationalization bootstrapping technique to sample posterior updates given data, however, they do not consider contrastive trajectories and episodic memory, which are important components for sample-efficient compositional representations.
- Micheli et al. (2022) can also be considered to be a form of self-learning which clusters continuous observations to form a vocabulary for a world model, demonstrating sample-effient learning on Atari.
- Our approach also distills latent axes of variation into a distribution over tokens used as a world model, however, we argue that distillation is essential for transfer learning, and show that is improved via contrastive memory lookups, which may provide a mechanism to solving some of shortcomings of their method when clustering fails to identify the correct axes of variation of the environment.
- Polu et al. (2022) demonstrate that expert iteration along with a curriculum of starting proof contexts of various difficulties can be used to self-train GPT-f to solve IMO math problems.
- Expert iteration was originally proposed for well-defined games like Hex (Anthony et al., 2017;Silver et al., 2017) and consists of alternating epochs of rolling out trajectories using an expensive expert model such as MCTS biased by an apprentice model, followed by fine-tuning the apprentice model on expert-summarized information.
- While these works were useful, they were relatively compute and sample inefficient, requiring 2000 A100 days for training in the case of Polu et al. (2022), and initially had challenges with overfitting and context disambiguation, handling most semantic (ie. context-free) memory rather than episodic memory (contextrequired).
- In comparison, we introduce a method to improve the sample and compute efficiency of the expert policy via contrastive distillation and via a task-agnostic retrieval and summarization policy over episodic memory.

### Self-Learning Via In-Context Learning
- Self question-asking can improve language model performance
- Hand-designed actor-critic-like transition functions have been applied to text editing and story-writing to improve in-context updates
- However, the in-context updates added by these algorithms are less persistent and reusable than representations learned from contrastive distillation.

### Self-Learning Via Contrastive Methods
- Recently, Deng et al. (2022) and Li et al. (2022) have shown that contrasting likely sequences from paired sequence models under language modeling loss can efficiently manifest latent axes of variation between the two models, either for analysis or downstream use as a signal for contrastive decoding.
- However, using these methods for self-learning settings or transfer learning requires some work to adapt, and in the case of Deng et al. (2022) requires a explicit latent model of the data generating process.
- Contrastive methods for distillation from teacher to student have also been explored in Tian et al. (2022); Nguyen et al. (2021).
- Brown et al. ( 2020) and Radford et al. demonstrate that large language models perform few-shot learning after exhaustive pre-training on natural language data.
- Chan et al. (2022) show that data distributions similar to those in first-person datasets and natural language can induce few-shot learning on Transformer architectures, suggesting that a Transformer-like architecture may have inductive biases for few-shot learning from 1st-person perception data, an approach that has been leveraged by hand-iterated approaches to prompt engineering (Wei et al. , 2022;Kojima et al., 2022;Creswell et al., 2022).
- Recent works have proposed improved methods for multi-hop causal reasoning in NLP tasks.
- Creswell & Shanahan (2022) demonstrate a framework for faithful reasoning on EntailmentBank and Proofwriter (Dalvi et al., 2022;Tafjord et al., 2021).
- Rajani et al. (2019) demonstrate how training a generator on human-collected explanations for NLP, then fine-tuning on this data can improve generative performance for downstream NLP models.
- This builds on a rich literature for question-answering, especially using memoryaugmented architectures, described further below.
- Adding explanation data (similar to self-QA) has been shown to improve few-shot performance via prompt manipulation in language models and for RL agents (Lampinen et al., 2021a;2022), but has been less clearly connected to transfer learning.

### Memory
- Several architectures have introduced per-cycle "working memory" for tasks requiring relatively short context windows.
- End-to-end memory networks introduce fully differentiable memory modules which improve performance on single-pass question-answering tasks.
- Universal Transformers interpose a per-token recurrent memory layer between Transformer layers, showing improvement on NLP tasks.
- Similarly, the Neural Turing Machine augments networks with a lower-level working memory that resembles the register/RAM architecture of a CPU and shows that the network can learn basic policies for computation using this architecture.
- However, to date, most of these approaches do not leverage the latent natural language policies available in self-supervised models, especially the memory compression policies like summarization, and use memory which is in-scope only for a single pass through the network, rather than episodic and persistent which is needed for a continual learning agent.
- Persistent memory architectures have also been used to introduce episodic memory in RL agents.
- The sparsity and top-k retrieval structure shows improvement over past episodic memories which use RNNs or LSTMs, however, the fixed-width chunking strategy of HCAM still limits its ability to recognize arbitrary patterns and the memory is read-only.
- Goyal et al. (2022) 2022) augment a MuZero agent with a retrieval-only episodic memory using a pre-trained retrieval policy.
- In contrast, we use a flat key-value I/O summarization scheme which can learn to attend equally well to high-level and low-level perceptions (or mixtures thereof) and is read-write, rather than read-only.
- BlenderBot 2.0 uses a supervised summarization scheme to persist important summaries into a long-term memory and uses these, along with results from a learned web search strategy, to improve conversational fluency of the agent via directly embedding matched summaries during decoding.
- Our approach is similar to this strategy, however, we frame this in the context of transfer learning and our approach is considerably less hand-engineered.

## Self-Learning Decision-Making Process
- A self-learning decision-making process is formalized as an semi-MDP agent
- A emits actions as input to an environment which produces a sequential observation
- We consider primarily a discrete observation and discrete action space, nonstochastic environment and agent with stochasticity derived from sampling during inference and retrieval
- Let c t be a context window for the agent and M t the representation of an internal R/W memory
- We represent a self-learning agent as the tuple (P, S, V, U ) where
- P (c t , M t ) ‚Üí x t , y t is a proposal policy which generates a prediction task x t from the current context and memory, along with an expected observation y t .
- S(x t , M t ) ‚Üí (r, f ) 0 , .., (r, f ) l‚àí1 = s t is a solver policy which generates a selection-inference chain of adaptive length l, s t , where r( an inference conditioned on the past inferences and retrievals.
- One output f j is also defined as ≈∑t , the prediction, for a task-specific j.
- One specific type of selection in training environments is querying a label oracle, which yields y.
- For any retrieval, r i , the selection-inference chain can be split into prior and posterior rollouts with respect to r i , s priorr i := (r 0 , f 0 ...f i‚àí1 ) and s posteriorr i := (f i , ..., r l‚àí1 , f l‚àí1 ), respectively.
- V (x t , (r, f ) 0 , .., (r, f ) l‚àí1 ) ‚Üí vt ‚àà [0, 1] is a verifier which discriminates a noisy judgement vt as to whether ≈∑t is consistent with the proposed task x t and evidence chain (r, f ) 0 , .., (r, f ) l‚àí1 .
- Consistency is defined informally for labeled data as to whether the input x t ‚äï (r, f ) 0 , .., (r, f ) l‚àí1 activates the same latent variables z as the input with the true label x t ‚äï y t and is learned as a classifier over labeled examples.
- U (c t , x t , (r, f ) 0 , .., (r, f ) l‚àí1 , vt ) ‚Üí (u t , m t , a t ) is an update policy which generates updates u t , m t , which are parameterized as discrete sequences used as inputs to a self-training process and memory, respectively, and a discrete action a t .

### Connection to Linguistics: A (Type 0) Grammar That Learns A Posterior Grammar Conditioned on Evidence
- The self-learning process can be interpreted as a distribution over tokens (terminals)
- The self-learning process uses its own likely sequences to update its posterior distribution over tokens (terminals).

### Constraints on Update Representations from Multi-Task Self-Learning
- Multi-task self-learning imposes additional constraints on the self-learning process
- Given an environment with distribution shift from a source environment E to a target environment E at time t + 1, we have the constraints: 1. the length of u t is task-adaptive, as the number of latent variables may vary in a multi-task setting (task-adaptive length constraint) 2. I(u t , v t ) is high (source mutual information constraint) 3. I(u t , v t+1 ) is also high (target mutual information constraint) 4. u t is compact, which helps to prevent overfitting and encourages compositionality across multiple retrievals (information bottleneck constraint) 5. the latent variables z t activated by u t are activated regardless of the location of u t in the solver's input, which helps to prevent overfitting (position-invariance constraint)

## Contrastive Distillation
- We define contrastive distillation (for sequences) as conditionally sampling a sequence of tokens (rollout) u from p(u|x t , x t+1 ) for task observations x t , x t+1 where u satisfies the source mutual information constraint and the target mutual information constraint with regard to E, E .
- It is abbreviated as cd(x t , x t+1 ) and as cd(x t+1 ) if x t is an empty observation.
- A simple example of where this empirically occurs via handengineering is after the prompt "why?" for a large pre-trained language model, as we will demonstrate -this can be considered to be a form of rationalization which also satisfies the target mutual information constraint (Zelikman et al., 2022).
- Informally, we can think of this as manifesting the latent axes of variation of source and target distributions from weights to tokens, and is contrasted with direct weight updates ‚àá Œ∏s , which do not fulfill the target mutual information constraint.

### Contrastive Distillation Through Fine-Tuning
- The update is a self-learning sequence
- The update includes x t and u t in later iterations
- We refer to this as contrastive distillation through fine-tuning.

### Contrastive Distillation Through Memory
- When the update is stored in memory, it can be used to fulfill the task-adaptive length constraint.
- Using memory better fulfills the position-invariance and iterative compositionality constraint.

### Bayesian Contrastive Distillation
- Contrastive distillation rollouts present an easy way to generate negative samples for contrastive losses like InfoNCE (Oord et al., 2019).
- In contrast to sampling randomly from a noise distribution or sampling from a fixed view augmentation or time-lagged policy, these are sampled from task-adaptive prior distributions.
- Contrastive distillation can be used for a variety of downstream applications -for example, for data augmentation directly for losses like InfoNCE, or for compression via meta-sampling -if y is a long sequence, recursively sampling u meta ‚àº cd(u prior , u posterior ) will produce a shorter self-learned sequence that has mutual information with y and can be stored in memory as a compressed representation of a policy update given the new evidence y.
- Following this process iteratively can build a hierarchy of tokenized updates representing distribution shifts (an iterative world model).

## Experiment: Contrastive Distillation Improves NLP Transfer Learning
- We experiment with the update policy as applied to transfer learning
- In this experiment, we use a teacher oracle (GPT-3) to test the contrastive distillation and memory mechanics rather than generation capabilities of the (small) model
- However, future experiments will use self-generation with larger student models

### Datasets
- We train and test on low-data configurations of bAbI datasets (5 datapoints per task)
- In comparison to semantic memory, episodic memory often requires recontextualization of past episodes for new task instances.
- In the case of bAbI, for example, token statistics of the current context may be similar to those of an episodic memory, however, simply copying the answer from the memory without de novo reasoning will fail.
- In contrast, in Com2Sense and other one-hop semantic QA tasks, copying with a small bit of adaptation is often a successful strategy, conditioned on having useful retrievals (multi-hop reasoning requires a more advanced skill of planning and generating intermediate episodic memories).
- However, both forms of reasoning are necessary for a successful self-learning agent.

### Proposal Policy
- We implement our proposal policy as a deterministic iterator.
- This yields 5 examples for each of the 20 bAbI tasks in a linear order, and similar for the 100 datapoints of the singleton Com2Sense task.

### Solver Policy
- We implement our solver policy as single-hop solver fine-tuned from T5-3B
- Various update policies are described below

### Memory Policy

### Verifier Policy
- We use the BLEURT metric to score predictions against known labels.
- The metric is not perfect, but it correlates well with held-out datapoints evaluated by hand for accuracy.

### Update Policy
- For the contrastive distillation experimental models, we generate updates for datapoints m t , q t , a t by prompting an Instruct GPT-3 davinci-002 (Ouyang et al., 2022) with the following "why" prior and posterior prompts:
- In the contrastive distillation agents (t5-3bcd and t5-3b-cd-memory), an update is randomly sampled and added as a target prefix during fine-tuning.
- For the memory-based contrastive distillation agent (t5-3b-cd-memory), updates are also indexed under key(x, u) in memory, for a total memory size of 1,000 sequences which are available in both source and target environments.

### Source Environment
- We randomly sample updates from the teacher model at each epoch.
- In the baseline case, no updates are sampled.
- In the contrastive distillation models, updates are added as prefixes of proposed target y as u ‚äï a ‚äï y, where a is the fixed string "Answer:" and u is the update.
- The full string is weighted using target loss to prioritize outputting a well-formatted final answer.
- Letting ≈ù = √ª ‚äï √¢ ‚äï ≈∑, s = u ‚äï a ‚äï y, we have the contrastive distillation loss function: l cd,Œ∏s (≈ù, s) = 0.1 * l Œ∏s (≈ù :‚àí5 , s :‚àí5 ) + 0.9 * l Œ∏s (≈ù ‚àí5: , s ‚àí5: )
- where l Œ∏s is the standard cross-entropy loss w.r.t. to solver policy parameters Œ∏ s and the notation s :‚àí5 indicates a vector composed of all elements of s until the 5th-to-last.
- The baseline model is trained using unweighted language modeling loss l Œ∏s (≈∑, y).
- In the contrastive distillation agent with memory (t5-3b-cd-memory), updates are also added as memory examples.
- All models are fine-tuned on the source task until validation error plateaus.

### Target Environment
- The trained networks are decoded greedily in the target environment
- Generations are parsed into answers by matching on the regex ".*Answer:(.*).*" if it occurs in the generation, otherwise the full generation is used verbatim.

### Baselines
- We test against T5-3B fine-tuned on the source task without contrastive distillation through fine-tuning or memory (t5-3b).
- We also experimented with in-context few-shot learning for t5-3b as a second baseline, however, were not able to obtain performance above noise, even on the source task.
- Question: John had to get up early Saturday, so he went to bed early on Friday instead of staying up late. Is this plausible?
- t5-3b:<unk> <unk> <unk> <unk> <unk> <unk>
- t5-cd: Rationale: No. The only reason John would have stayed up late on Friday is because he had to get up early Saturday morning.
- Is this plausible?
- t5-cd-memory: Rationale: Yes, because he had to get up early on Saturday.
- Answer: yes

## Results
- The kitchen is south of the bathroom.
- The bedroom is south of the kitchen.
- Rationale: The bedroom is south of the kitchen because it is closer to John.
- Rationale-memory: The bedroom is south of the kitchen because it is closer to John.

### Manifesting Latent Variables From Weights to Tokens Improves Transfer At the Cost of Compute
- While the contrastive distillation models achieve comparable performance to the baseline on the source task, they substantially outperform on the target task
- However, this comes with a cost in terms of both training time (more iterations) and inference time (more number of tokens decoded during inference)
- This may reflect a generic design tradeoff between online generalization and compute time

### Contrastive Retrievals Improve Transfer
- Transfer using distillation through memory improves over distillation which uses fine-tuning only.
- We hypothesize that this is due to the ability to incorporate snippets of partially relevant information as explicit contrastive examples, similar to phenomenon observed in few-shot learning and QA in works like Borgeaud et al. (2022); Izacard et al. (2022).
- However, in contrast to these works, the memory contents are generated by a teacher model, rather than humans, use learned embeddings with respect to the source task, and have substantial distribution shift from the target task.
- As a result, many memory lookups on the target task have no obvious connection to the generated updates, eg. ['As fall is ending, the nights are usually getting colder. Is this plausible?', 'Rationale: The passage does not mention whether or not the man is in the bathroom. Therefore, we cannot say for sure. Answer: no</s>', 'True', [[['t=173. Prior reasoning: Gertrude is a mouse and mice are afraid of wolves.</s>', 0.002189338207244873, 'retrieval_order_0'], ['t=856. Prior reasoning: The passage does not mention whether or not Daniel is in the hallway, so we cannot say for certain. It is possible that he is, but we cannot say for sure based on the information given.</s>', 0.0020872140303254128, 'retrieval_order_1'], ['t=985. Prior reasoning: The most direct route from the garden to the bathroom would be to go through the office and then the hallway. However, if the garden is south of the hallway and the hallway is south of the bathroom, then the bathroom must be south of the garden. Therefore, the most direct route from the garden to the bathroom would be to go through the hallway.</s>', 0.0025377562269568443, 'retrieval_order_2'], ['t=195. Prior reasoning: because he is tired</s>', 0.0053531392477452755, 'retrieval_order_3']]],
- However, in generations with higher BLEURT score, the model seems to have learned to improvise from existing memory examples: 'Jim needed to buy drinks for his family for their week-long vacation, so he bought many bottles of water. Is this plausible?', 'Rationale: Yes, he will. Answer: yes</s>', 'True',[[['t=790. Prior reasoning: Jason will go to the store to buy a drink.</s>', 0.0019759121350944042, 'retrieval_order_0'], ['If it is raining, I should not ask my guests to take off their shoes before they come in my house. Is this plausible?', 'Rationale: No, it is not possible to answer this question with the given information.</s>', 'False ', [[['t=791. Prior reasoning: It is not possible to answer this question with the given information .</s>', 0.002464243909344077, 'retrieval_order_0'], Overfitting is a more serious problem in limited data regimes than data-abundant ones, and larger feature representations created by memory retrievals can exacerbate this problem.
- We experimented initially with including the full task context alongside the update on the prompt, however, significant gains were observed when using only updates, likely due to overfitting on the longer contexts. Additionally, we observed that adding randomness to memory retrievals at training time significantly improved source task performance (and this is included in t5-3b-cd-memory). Randomness in query selection can be compared to the masked language modeling objective which has been seen to improve task transfer in Transformer language models (Raffel et al., 2020).

### Error Cases
- Generated selection-inference chains are not always autoregressively coherent
- This may be partially due to sampling without filtering from the teacher oracle, especially the prior updates, as this can sample updates which diverge from the label, but are consistent with the context
- Filtering these updates represents an important avenue for improvement via a bootstrapping loop in environments with oracle verifiers, while learning to contrast these updates represent an important data source for iterative improvements to the solver and to bootstrap verifiers in self-learned curriculums

## Conclusion
- Designing loss policies for decision-making agents to adapt to distribution shift is a challenging problem
- We present one promising approach via iteratively manifesting latent variables from weights into tokens
- These create update representations which better fulfill the constraints of multi-task self-learning and empirically outperform standard adaptation approaches for language models like fine-tuning and few-shot learning in limited data transfer learning settings
- This suggests a design tradeoff between generalizability and compute for language models used as self-learning decision-making agents which is improved by sampling from memory
- While this is a promising phenomenon, much work remains
