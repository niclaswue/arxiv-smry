---
title: "GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities"
date: 2023-01-11T11:30:42.000Z
author: "Jillian Bommarito, Michael Bommarito, Daniel Martin Katz, Jessica Katz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04408v1.webp" # image path/url
    alt: "GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04408)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04408).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/gpt-as-knowledge-worker-a-zero-shot).

# Abstract
- The global economy is increasingly dependent on knowledge workers to meet the needs of public and private organizations
- The most comprehensive assessment of capability readiness for professional knowledge workers is the Uniform CPA Examination developed by the American Institute of Certified Public Accountants
- In this paper, we experimentally evaluate OpenAI's `text-davinci-003` and prior versions of GPT on both a sample Regulation (REG) exam and an assessment of over 200 multiple-choice questions based on the AICPA Blueprints for legal, financial, accounting, technology, and ethical tasks
- First, we find that `text-davinci-003` achieves a correct rate of 14.4% on a sample REG exam section, significantly underperforming human capabilities on quantitative reasoning in zero-shot prompts
- Second, `text-davinci-003` appears to be approaching human-level performance on the Remembering & Understanding and Application skill levels in the Exam absent calculation
- For best prompt and parameters, the model answers 57.6% of questions correctly, significantly better than the 25% guessing rate, and its top two answers are correct 82.1% of the time, indicating strong non-entailment

# Paper Content

## Introduction

## AICPA Exam
- The Uniform CPA Examination is a modern, computerized assessment based on psychometric and statistical techniques.
- While prior paper-based generations of the Exam might have been compared to traditional linear exams, the current Exam is a dynamic, adaptive exam [28], best compared to exams like the current GRE or GMAT.
- Linear exams present the test-taker with a preset sequence of test questions, while dynamic exams adapt to each test-taker in response to the answers provided in prior questions.
- The examination and study of the interrelationships of separate areas in order to identify causes and find evidence to support inferences.
- The examination and use of judgment to draw conclusions.
- The examination and division into four sections that test-takers sit for independently: Auditing and Attestation (AUD), Business Environment and Concepts (BEC), Financial Accounting and Reporting (FAR), and Regulation (REG).
- Each section of the Exam is divided up into at least four testlets that feature scenarios, multiple choice questions, calculated amounts, short answer, and related evidence and research material.
- The passage rates of Exam sections are presented in Table 1; the AICPA does not publish statistics related to per-question or per-section test-taker accuracy.
- Based on prior research and experience with LLMs, we strongly suspected that GPT-3.5 would struggle with zero-shot quantitative reasoning in this context.

## Data
- There is an active body of research on quantitative reasoning with fine-tuning or few-shot contexts
- We constrain our results in this study to zero-shot prompts to better assess the "intrinsic" capability of these models
- Therefore, we prepared two separate assessments to allow us to isolate the arithmetic or quantitative capabilities from other elements of the Exam.

### Assessment 1: Sample Exam -Regulation
- The first assessment is intended to approximate the real Uniform CPA Examination
- The AICPA's online, publiclyavailable sample exams "include two multiplechoice testlets and three task-based simulation testlets for [...] Auditing and Attestation (AUD), Financial Accounting and Reporting (FAR) and Regulation (REG);" the fourth section, BEC, is shorter
- Between AUD, FAR, and REG, we utilize the REG section as it contains the most balanced distribution of skill types and quantitative and qualitative reasoning
- Therefore, a test session of the REG exam as provided on the AICPA's site was transcribed on January 3rd, 2023, including correct answers
- All questions are formatted as simple text or, where evidence or workpapers are formatted in tables or lists, as Markdown
- This process results in 40 test questions across five testlets
- Two of these five testlets consist of multiple-choice questions, with a total of 15 questions ranging from four to six options each
- Of the remaining 25 questions, 24 require the test-taker to indicate the correct financial amount and one requires the testtaker to research authoritative material made available within the exam
- While we cannot redistribute these test questions directly, interested readers can directly access and take the AICPA's online sample exams at no cost

### Assessment 2: Synthetic MCQ Assessment
- The Uniform CPA Examination is organized around Bloom's cognitive taxonomy
- The AICPA has adapted these skill levels into four simpler groups
- The questions in the test are designed to mimic the nature and difficulty of real questions on the Exam
- The authors also reviewed material and sample questions prepared by McGraw-Hill Education and Becker Professional Education to ensure that our test questions were at least as difficult and broad as theirs

## Methods
- In prior work on the Bar Exam, we outlined a method for experimentally evaluating OpenAI's models
- For multiple choice question (MCQ) assessments in this paper, we follow this approach as closely as possible; calculated amounts and short answers are compared to the correct answer after stripping and reformatting answers
- For example, (10, 000), (10000), and âˆ’10, 000 are identical in the automated scoring of the model's responses
- Our evaluation is based on generating zero-shot prompts for the text-davinci-003 text completion API
- Unlike in our prior research [27], we are able to fully opensource the source code and questions created in Assessment 2
- While replication of results requires an OpenAI account and accepting the AICPA's terms of use, we have again attempted to provide researchers with as much replication detail as is possible under the circumstances

### Prompt Engineering and Responses
- OpenAI's large language models are constrained by our limited scientific understanding and the proprietary nature of the models
- Prompt engineering is critical to replication of studies involving large language models
- The following prompt variations were tested: entailment or recall tasks, openended problems, and questions that evaluate traditional entailment tasks
- Most prompts produced similar performance, clustering near the central tendency of 55% noted in Table 8
- Contextual variations suggest differences in the nature of advice between professions
- Justification variations suggest differences in the complexity or state of codification across subject areas

### Model (hyper)parameters
- The AICPA curriculum itself notes that LLMs are sensitive to small changes in their inputs
- In addition to prompt sensitivity, they are often highly sensitive to the parameters set in training and inference
- While our ability to intepret results or identify all (hyper)parameters is limited by the proprietary nature of GPT, we did evaluate how altering some model parameters impacts the performance of the model
- We do not vary the maximum token output or attempt nucleus sampling; however, we do evaluate the following parameters for at least one prompt: 1. temperature: Sampling temperature; 0.0 is deterministic, higher is more "random." We tested values in {0.0, 0.5, 1.0}. 2. best of: "Generates [N] completions server-side and returns the "best" (the one with the highest log probability per token)." We tested values in {1, 2, 4}.

### Fine-tuning and Historical Models
- OpenAI provides an API for fine-tuning models
- This publication is focused on the zero-shot performance of the model itself
- Based on prior experience in similar problems, we do not believe that fine-tuning text completion at small sample sizes would improve the models' performance
- In some circumstances, others have found success in subsequent supervised or unsupervised re-training of some or all layers of an LLM
- While it is possible that this performance decrease is explained by the 50% head layer contraction required by OpenAI's API, we are unable to test further without access to details of fine-tuning or resulting weights

## Results
- Text-davinci-003 was able to answer over 50,000 questions in 700 independent assessment sessions
- The range of performance values observed over all experiments is summarized in Table 6

### Assessment 1
- As expected, the quantitative reasoning and arithmetic required in Assessment 1 resulted in substantially lower zero-shot performance than observed in Assessment 2.
- Out of 24 questions that required the test-taker to provide a numeric answer based on facts and work papers, GPT-3.5 frequently only answered one, two, or three questions correctly, resulting in an average range across all parameters and prompts of 5.7 to 9.4%.
- While it is arguable whether 0% is the true baseline for this task, it is clear that such zero-shot performance is not on par with human test-takers.
- GPT-3.5 also struggled with arithmetic on the 15 MCQs on Assessment 1, scoring above random chance for some, but not all, prompts and parameters.
- As a number of questions include more than four choices, the true baseline rate of guessing is 22.67%, not 25%, but despite this, the best prompts and parameters were only 4-6% above the baseline rate.
- Based on a qualitative review of these questions and the model's responses, we believe that performance could be improved somewhat in few-shot evaluations.
- Further, we believe that even some zero-shot performance improvements could be achieved by expanding the prompt to include "scratchpads" for common relationships or equations, as might be seen on problems that feature common workpapers like a statement of cash flows; however, in this paper, we focus on a zero-shot, "out-of-the-box" evaluation, and so these improvements are left for future research.

### Assessment 2
- Assessment 2 evaluated GPT-3.5's capabilities at the foundation of knowledge work.
- 208 MCQs were created for this assessment, each with four options.
- The baseline guessing rate for the model is 25%.
- GPT-3.5 was assessed on these 208 questions 180 times, three samples for each combination of 10 prompts, three temperature (T ) values, and two best of (n) parameter values.
- Across these 10 prompts, mean performance ranged between 51.1% and 56.9%, with a worst run of 50.0% and a best run of 57.6%.
- 23 of the 53 questions from the Regulation section in Assessment 2 require some degree of Application or Analysis.
- While these skill levels may be subjective in the context of realistic questions, we encourage readers to examine the complete set of 208 questions in the SI for themselves and to self-assess their own performance.
- GPT-3.5 is demonstrating performance significantly in excess of guessing, achieving approximately 70% in questions on Business Environment and Concepts (BEC), 57% for Auditing and Attestation (AUD), 53% for Regulation (REG), and 51% for Financial Accounting and Reporting (FAR).
- In addition to reviewing models for single correct answers, some prompts also required models to provide explanations or justifications.
- We performed a qualitative review of explanations and justifications for a sample of sessions, and found that more than half of the model's correct answers were also correctly explained with the correct reference or authority.

### GPT Model Progression
- Text-davinci-003 demonstrated material improvements from prior generations of GPT models
- In this work, we also compare our results against older or smaller GPT-3 models
- Table 8 and Figure 2 summarize these findings, demonstrating a qualitatively-identical story from our work on the Bar Exam
- Only text-davinci-001 exhibits the ability to follow instructions and answer above random chance

## Conclusion and Future Work
- In this paper, we document and develop two assessments of knowledge worker readiness based on the AICPA's Uniform CPA Examination Blueprints.
- Assessment 1 is a sample Regulation test as provided by the AICPA, including quantitative reasoning and calculations; Assessment 2 covers foundational skill levels, excluding quantitative reasoning and calculations, for all four sections of the Blueprints.
- In total, these assessments cover a broad, practical curriculum including law, finance, accounting, and technology.
- We then experimentally evaluate GPT-3.5 on these two assessments, including detailed steps to replicate this evaluation, and share source code and data for all questions not covered by copyright.
- First, we find that text-davinci-003 achieves a correct rate of 14.4% on Assessment, significantly underperforming testtakers.
- As many authors have documented in research on large language models [31,32,33], arithmetic and quantitative reasoning are often outside the scope of zero-shot use cases, and these results are consistent with these prior findings.
- As arithmetic and quantitative reasoning are the subjects of substantial active research, we look forward to exploring zeroshot approaches as new models or techniques become available.
- Further, as many industrial applications will support iterative or few-shot approaches, we are continuing to investigate applied use cases like the calculation of financial or operational metrics or the analysis of specific financial statements using more mature techniques like [39].
- Second, we find that text-davinci-003 can achieve an accuracy of 57% on Assessment 2, significantly better than a 25% guessing rate, and approaching or on par with anecdotal testtaker performance.
- It also demonstrates strong non-entailment capabilities and improving explanation capabilities, as its top two answers are correct 82% of the time and explanations are correct more often than not.
- While this assessment is not identical to the CPA Exam and the AICPA does not publish directly comparable statistics, approximately 45-55% of test-takers fail the exams annually, as an indication of general difficulty.
- All questions in this assessment are available for readers to review and self-assess, and we encourage others to suggest improvements or perform their own assessment on this material.
- Finally, as in prior research, we find that recent generations of GPT-3 demonstrate material improvements on this assessment.
