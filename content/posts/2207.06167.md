---
title: "Unsupervised Visual Representation Learning by Synchronous Momentum Grouping"
date: 2022-07-13T13:04:15.000Z
author: "Bo Pang, Yifan Zhang, Yaoyi Li, Jia Cai, Cewu Lu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2207-06167v1_SmQ0pfxoV.jpg" # image path/url
    alt: "Unsupervised Visual Representation Learning by Synchronous Momentum Grouping" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2207.06167)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2207.06167).


# Abstract
- The proposed method outperforms vanilla supervised learning
- The proposed method integrates the advantages of clustering-based and instance-level contrastive methods
- The proposed method achieves better performance than clustering-based methods and representation learning methods that rely on supervisory signals

# Paper Content

## Introduction
- Deep learning and data-driven frameworks are becoming more popular
- The quality of the learned representation largely determines the performance of models on a majority of tasks
- For a long time, people have used supervised tasks and adopted a large amount of annotations to train models, but this simple and efficient scheme faces the problems of expensive and time-consuming annotating costs and non-ideal generalization performance caused by bias from the annotation information
- To solve these problems, people extensively study the unsupervised representation learning method, including the generative models providing expressive latent features and self-supervised methods with different pretext tasks, but there are still performance gaps between these methods and the supervised scheme
- In recent years, unsupervised representation learning achieves great improvements, and SOTA contrastive learning based unsupervised methods reduce the performance gap to less than 3%
- We continue the study of the unsupervised representation learning, stand on the shoulder of previous contrastive and clustering-based methods, and finally, push the performance of unsupervised methods above the level of the vanilla supervised scheme
- One major design of the proposed SMoG method is the momentum grouping scheme that allows gradient propagating through groups to synchronously conduct the instance grouping process together with the representation learning
- The proposed SMoG method is effective and pretty simple, and we evaluate it on several standard self-supervised benchmarks and multiple downstream tasks
- Under the linear protocol, it achieves 76.4% top-1 accuracy on ImageNet with a standard ResNet-50

## Related Work

### Handcraft Pretext Task
- pretext tasks are effective unsupervised representation learning schemes
- pretext tasks include jigsaw solving, colorization, denoising, inpainting, super-resolution, and patch position prediction
- for video visual inputs with temporal dimension, sequential-related pretext tasks are proved useful

### Instance Discrimination Method
- Mainstream contrastive learning methods learn representations by instance discrimination which treats each image (pixel) in a training set as its own category.
- This scheme achieves state-of-the-art performances on downstream tasks.
- Its common training direction provided by In-foNCE [50,8,22] or its variations is to maximize the mutual information [50,27], which requires a large number of negative pairs to get good performances [8].
- Adopting a large batch size is a straightforward method, but it consumes lots of resources.
- To solve this, MoCo [23] and [67] propose to utilize memory structures.
- Some latest designs [21,9,7] conduct contrasting without negative samples by an asymmetry Siamese structure or normalization techniques.
- Recent work [37] proposes self-supervised learning with the Hilbert-Schmidt Independence Criterion, which yields a new understanding of InfoNCE.
- While RELIC [47] improves the generalization performance through an invariance regularizer under the causal framework.
- NNCLR [15] adopts the nearest neighbour from the dataset as the positives.
- UniGrad [59] provides a unified contrastive formula through gradient analysis.

### Group Discrimination Method
- DeepClustering [5] adopts the K-means clustering method to get the groups and learn features from them in an iterative manner
- However, the two-step training scheme (clustering, learning) leads to delayed supervisory signal, against effective representation learning
- ODC [71] and CoKe [54] shortens the two-step circulation period but does not solve the delay problem
- SeLa [3] and SwAV [6] treat the grouping problem as pseudo-labelling and solve it as a optimal transport task
- But, to avoid degeneracy, they add an equipartition constraint, decreasing the validity of grouping

## Approach
- Instance contrastive learning is an eminent and robust structure that has been proved by many models
- However, there are two potential problems with using this structure: 1) In terms of the problem setting, instance discrimination is too finegrained and the learned representation goes against the downstream tasks that rely on the high-level semantics to some extent, because for high-level semantics, instance-level contrasting introduces many false-negative pairs which damage the quality of representation learning
- 2) In terms of technical practice, contrastive learning based on InfoNCE needs a large number of negative pairs to improve the upper bound of the theoretical performance, which poses a challenge on computing resources or needs specific model designs

### Group-Level Contrastive Learning
- The instance contrastive learning framework can be expressed as: where sim(u, v) is commonly instantiated as the normalized inner product, fη is just f θ or a variant of f θ such as its momentum updated version or the version without the last few layers (predictor).
- x a i and x b i are two different augmented views of x i .
- In theory, x j should come from the training set X, but in practical, taking computing complexity into account, x j is commonly selected from a much smaller subset Y ⊆ X.
- With the group-level features, we can derive the group-level contrastive learning framework as: where G is the set of group features.
- Since the number of groups is a certain small value, we can conduct group contrasting globally with all the group pairs.
- The left part illustrates the group contrastive target described by Eq. 2.
- As the training progresses, preliminary formed meaningful representations have a large chance to make x a i and x b i belong to the same group.
- In this case, the gathering force (the green arrow) cannot make them get closer. Thus, we modify it to the version in Eq. 3 illustrated in the right part, where the push and pull force required by contrastive learning can always take effect.
- The algorithm of gaining group features (ϕ) will be detailed introduced in the next section.
- Here, we emphasize that besides well expressing a group of similar instances without extra limitation, a qualified ϕ also needs to make sure every group feature c i is updated synchronously with the instance feature f θ (x i ) since the gradients need to be back-propagated through c i to f θ (x i ) for training the parameters which means c i can replace the latest f θ (x i ) to participate in contrasting.
- This is the core difference from the previous clustering-based method, since their group features gained by clustering cannot back-propagate the gradient, and directly contrasting group features cannot train the network.
- Intuitively, the newly designed group contrastive learning method aims at directly adjusting the distribution of group features, and since gradients can propagate through group features to instance features, this algorithm can grad- ually learn the instance representation.
- However, unfortunately, it has flaws in the second half of the training process.
- With the meaningful representations gradually formed, c a i and c b i tend to be the same group feature. This will make the supervisory signal fail to gather similar instances and get them closer (see Fig. 2).
- To solve this problem, considering that c a i is the combination of a group of f θ (x), we change the group contrastive learning loss to: When c a i ̸ = c b i Eq. 3 can be seen as the "stochastic-batch" version of Eq. 2, which splits an intact loss into several batches, similar to SGD and GD.
- When c a i = c b i , Eq. 3 can still gather instances, which solves the problem.

### Synchronous Momentum Grouping
- SMoG's network structure and training settings are identical to conventional contrastive learning frameworks
- Following MoCo [23], we adopt a momentum network: fη ← α * fη + (1 − α) * f θ
- So far, SMoG is consistent with the typical contrastive learning methods.
- By adding the group assigning function ϕ, we can get the complete SMoG method.
- Generating Group Features As mentioned in last section, group-level contrasting optimizes instance features through group features. Thus, group features must represent the latest instance ones to propagate gradients, namely, c i needs to be synchronously updated with f θ (x i ) and calculated by a differentiable function.
- Directly adopting conventional global clustering as ϕ is not feasible to synchronously update c i with f θ (x i ) at each iteration due to the computing cost. Thus, we simply modify it to the momentum grouping scheme, generating c i through an iterative algorithm synchronized with representation learning.
- Before the training start, we initialize the l group features {g 1 , ..., g k , ..., g l } randomly or using a clustering method such as k-means. Then along with the training process, we update g k and get c i in each iteration by: where β is the momentum ratio, x t comes from a mini-batch, and we omit the normalization.
- Importantly, this mechanism does not introduce extra limitation.
- The momentum grouping scheme assigns each instance to the closest group and iteratively updates the group features in a momentum manner. In this way, the group features are always the latest representative of the instance visual features and more importantly, for each iteration, the updated g k is adopted to conduct contrasting and the gradient can be back-propagated through g k to f θ (x i ).
- To deal with this, we periodically apply an extra grouping process on a cached relatively large feature set to relocate the groups.

### Compared with Previous Clustering Methods
- Group discrimination is not a first proposed concept.
- Previous clustering-based methods DeepClustering [5] and SwAV [6] also conduct it and in terms of the loss function, the three algorithms have a similar form.
- The main difference lies on the method to generate the groups and the way to utilize them. Our SMoG aims at conducting the contrasting among the groups like Eq. 2 shows. This is impossible for previous clustering-based methods since their group features have to be detached out and cannot back-propagate the gradients to the parameters. Thus, they do not contrast the groups but classify the instances into groups.
- To achieve the group contrasting, we propose the momentum grouping scheme which allows directly contrasting the groups and propagate gradients. Although we modify the final loss to Eq. 3 for better performance, our SMoG is still a group-level contrastive algorithm, since the group features directly guide the optimization direction.

### Implementation Details
- We adopt the asymmetrical augmentation method used in BYOL [21], where there are two augmentation schemes for the two streams of the Siamese network.
- Since the contrasting loss we adopt is also asymmetrical (contrasting among instance and group features), the two streams of the Siamese network are not assigned to a fixed augmentation scheme, instead, they adopt one of them in an alternate manner.
- In this way, the two streams can generate instance features under the same distribution so that they can be mapped to the same group feature set.
- SMoG Setting. Like many previous works [21,10,68], both f θ and fη have a backbone and a projection head. And fη has an extra prediction head stacked on the projection head.
- The backbone's output is adopted as the learning representation.
- The two kinds of head are both a two-layer MLP and their hidden layer is followed by a BatchNorm [28] and an activation function (ReLU for ResNet and GELU [26] for SwinTransformer [40]). The output layer of projection head also has BN.
- The dimension of the hidden layer is 2048, while for the output layer, it is 128 for CNN and 256 for Transformer.
- The Siamese's momentum ratio α is 0.999.
- In experiments, we group all the instances into 3k groups and the group features g are initialized with the K-means algorithm. The momentum ratio β of grouping follows a linear schedule from 1.0 to 0.99.
- To avoid collapse, we reset the group features every 300 iterations with K-means on cached features of the past 300 iterations and fη is synchronously reset with the parameters of f θ .
- Pre-Training Details. We pre-train the models on ImageNet dataset [12].
- For CNN backbones, we train with the LARS [69] optimizer with 4096 mini-batchsize on 64 GPUS (when adopting ResNet50). The base learning rate is set to lr = 0.3 × batchsize/256, following first a 10-epoch warm-up and then the cosine scheduler. The weight decay and temperature τ are set to 10 −6 and 0.1.
- For Transformer based backbones, we adopt the Adamw optimizer [42]. The base learning rate and weight decay are 5e −4 ×batchsize/2048 and 0.05.
- Other settings are the same with CNN backbones.
- For efficient training, we also adopt the multi-crop training scheme [6,7] with two large views (224×224) and 4 small views (96×96). When adopting multi-crop, the scale of the random crops are [0.2, 1.0] and [0.05, 0.2] respectively for large and small views.
- We evaluate SMoG firstly on the standard benchmark on Ima-geNet with both CNN and Transformer backbones. We then compare the performance on several downstream tasks and give a detailed ablation study.

## Experiments

### Linear Evaluation
- The proposed SMoG improves the best performance of group-based methods by 2.3% top-1 accuracy and achieves the SOTA performances among all the unsupervised representation methods.
- For the first time, the unsupervised representation of ResNet-50 surpasses the performance of the vanilla supervised one (+ 0.3%) on ImageNet.
- Most Transformer-based backbones perform worse than the CNNs with similar parameters. But Transformers benefit more from the multi-view training.
- SMoG, in the two-view setting, achieves SOTA performances compared to DINO (+2%), MoCo (+2%), EsViT (+4%), and MOBY (-0.5%).

### Semi-Supervised Fine-Tune Evaluation
- Next, we evaluate the proposed SMoG through semi-supervised fine-tuning the unsupervised representation on a subset of training data of ImageNet.
- We follow the protocol adopted in [8] and the 1% and 10% labeled splits of ImageNet we adopt are the fixed ones provided in [8].
- 4: ResNet linear evaluation on ImageNet compared with supervised training.
- We can see that for different model sizes, our SMoG achieves comparable performance with supervised method.
- We further transfer the unsupervised representations of ResNet-50 learned with the proposed SMoG to several downstream tasks, namely semantic segmentation, object detection, and instance segmentation.
- We first evaluate SMoG on Cityscapes [11] and VOC-2012 [16] semantic segmentation tasks. For fair comparison, we align all the methods with the same training recipe of FCN [41].
- Results of mean accuracy and mean IoU are provided in Tab. 5, we can see that nearly all the unsupervised representations outperform the conventional supervised one which reveals that in this downstream task, unsupervised representation is already a better choice.
- And again, SMoG performs better than the original SOTAs on Cityscapes (+1.1 mIoU) and VOC-2012 (+1.4 mIoU) datasets.
- Then, we evaluate on object detection and instance segmentation tasks on COCO [38] dataset. Similarly, all the unsupervised representations are transferred with the same fine-tuning recipe of Mask-RCNN [24].
- We provide AP results in Tab. 6. The same with semantic segmentation, on these two downstream tasks, our unsupervised pre-training representations are better than the supervised one.
- And still, SMoG improves the current SOTA results, +0.7 AP on object detection and +1.1 AP on instance segmentation.
- We provide ablation studies on key components of SMoG.
- ResNet-50 is adopted.
- Network is trained for 100 epochs without multi-crop.
- The representation is evaluated under the linear protocol.

### Ablation Study
- SMoG has twice more low-entropy groups than the two baselines, proving its superiority.
- The linearly decreasing schedule for β is much better than the fixed schedule.
- After adopting the linearly decreasing schedule, our SMoG is not sensitive to the final value of β and we adopt 0.99 as the default value for all the experiments on both CNN and Transformer.
- Still, because the group feature updating process cannot access the global feature distribution, there is a possibility of collapse during training. Thus, we adopt the periodical clustering (pd) trick to avoid this.
- The first two rows of Tab. 7b prove its necessity. We can see that it successfully makes the backbone learn useful representations but the performance is not good enough. We believe this is because the group features are always generated and updated based on the instance features generated by only f θ , leading to misalignment with features from fη . Thus, we also reset fη with the parameters of f θ after each clustering.
- The integrated pd with fη reset leads to ideal performance. And surprisingly, we find that only resetting fη without the clustering can avoid the collapse too.
- In Tab. 7c, we evaluate the influence of the number of groups used in SMoG under the linear protocol. The results show that SMoG is not sensitive to the group number. Even if we tune it in a wide range (1k∼30k), the performance is maintained at a stable level (± 0.2), as long as there are enough groups (300 groups are too few to perform well).
- This is consistent with the conclusion of SwAV [6].
- More groups increase the computation time consuming of the momentum grouping algorithm (ϕ) since it needs node communication to synchronize the group features among all the nodes. Thus, we adopt 3K as the default setting for all the experiments.

## Conclusion
- The contrastive learning algorithm is extended to group level
- The algorithm is able to synchronously conduct the learning and grouping process
- The algorithm is hoped to be useful for the community to further develop visual unsupervised methods
