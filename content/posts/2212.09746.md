---
title: "Evaluating Human-Language Model Interaction"
date: 2022-12-19T18:59:45.000Z
author: "Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E. Wang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani, Michael Bernstein, Percy Liang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09746v2.webp" # image path/url
    alt: "Evaluating Human-Language Model Interaction" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09746)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09746).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/evaluating-human-language-model-interaction).

# Abstract
- Many real-world applications of language models involve human-LM interaction.
- To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE).
- We find that non-interactive performance does not always result in better human-LM interaction.
- First-person and third-party metrics can diverge, suggesting the importance of examining the nuances of human-LM interaction.

# Paper Content

## Introduction
- Language models (LMs) have rapidly advanced, demonstrating unprecedented capabilities for generation and striking generality for tackling a wide range of tasks (Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022).
- However, these models are at present primarily evaluated non-interactively: given an input text, a model generates a completion with the focus solely on the quality of the completion. Almost all benchmarks, even those with a diverse range of tasks such as BIG-Bench (Srivastava et al., 2022), GEM (Gehrmann et al., 2021), and HELM (Liang et al., 2022), encode this non-interactive view.
- We propose a framework, HALIE, that expands on non-interactive evaluation along three dimensions: (i) we capture the full process in addition to the final output (targets); (ii) we capture the first-person subjective experience of users interacting with the LM in addition to the perspective of a thirdparty (perspectives), and (iii) we consider notions of preference beyond quality (criteria).
- These dimensions interact to define the full space of evaluation metrics (i.e., a 2x2x2 cube ), which goes beyond standard, non-interactive evaluation (i.e., gray cell). Our goal is to evaluate human-LM interaction instead of just model completions.
- LMs are already deployed in interactive settings, where humans work with them to brainstorm ideas (e.g., Jasper, Copy.ai), paraphrase sentences (e.g., Wordtune, QuillBot), reformulate queries (Nogueira and Cho, 2017), autocomplete sentences (Chen et al., 2019), write code (e.g., Coilot, TabNine), and so forth.

### Social dialogue
- Dialogue is a natural and flexible way to engage in communication
- In this section, we evaluate human-LM interaction in the context of open-ended dialogue about social situations
- We defer related work and experimental details to Appendix B.1 for space
- Task. We consider the task where given a social scenario, users converse with a system (or chatbot) about the scenario
- Users initiate the conversation and take turns with the chatbot to carry out the conversation until users choose to finish it
- We randomly select ten scenarios from the Empathetic-Dialogues (Rashkin et al., 2019) and Common-senseDialogues datasets (Zhou et al., 2021)
- System logic. Figure 3 shows a dialogue system state consisting of scenario, dialogue history, and user input and possible actions: pressing a key, to modify user input, clicking the "send" button to submit their input, and finishing the dialogue
- When users click the "send" button, the system creates a prompt with five dialogue examples for in-context learning and the current dialogue history, invokes an LM with the prompt, fetches a completion, and shows the completion in the dialogue history in the interface (ShowCompletions)
- We recruited a total of 189 crowd workers (or users) on Amazon Mechanical Turk
- For each of the scenarios and models, we had three users each produce a dialogue
- To ensure each dialogue is long enough, the interface allowed users to finish the conversation only after taking more than ten turns or exceeding 250 words in total
- At the end of the conversation, users completed a survey about their experience
- Survey questions. We ask users to evaluate the interestingness, sensibleness (i.e., whether a chatbot response makes sense), and specificity (i.e., whether a chatbot response is specific to what a user had said) of a chatbot, following the survey questions proposed by Thoppilan et al. (2022a)
- We also ask users to evaluate the fluency of the chatbot and their inclination to continue talking with the chatbot, as proposed by Smith et al. (2022a)
- After a dialogue, we ask if users are willing to talk to the chatbot again using a 5-point Likert scale: reuse
- Results. For social dialogue, we specifically care about models' ability to show an emotional connection to users, by being empathetic and having commonsense about social situations (humanness), in addition to being fluent and sensible, and specific to what they have said
- Independently, users may have their own subjective views on whether system responses are interesting and make them want to continue interacting with the system, which are measured by preference metrics interestingness and inclination, respectively
- 1 Instruction tuning improves performance on most quality metrics, but not specificity
- 5 Throughout the paper, results are denoted by * if models had a significant effect relative to TextDavinci, * if significant relative to TextBabbage, * if significant relative to Davinci, and * if significant relative to Jumbo at the p = 0.05 level using a Tukey-Kramer test.

### Question answering
- Question answering is a canonical task in NLP
- We consider an interactive version of question an- Users assisted by TextDavinci achieved the highest accuracy while requiring the least effort (queries, and ease) and being perceived to be the most fluent and helpful.
- The numbers indicate means and standard errors, and the markers denote statistical significance, 5 conditioning on the use of AI assistance; when the assistance was provided, users queried the system 86% of the time.
- Figure 4: [Question answering] The system's state consists of a multiple-choice question, user input, and system output.
- When users take an action (e.g., clicking the "generate" button), the system updates the state (e.g., updating the system output with a model completion).
- swering where a user is asked to answer a question given access to an LM that they can query.
- In practice, users tend to query information systems repeatedly, refining and reformulating their queries in response to the system's output (Huang and Efthimiadis, 2009a;Jansen et al., 2009;Jiang et al., 2013).
- For related work and details of experiments, see Appendix B.2.
- Task. Given a sequence of multiple-choice questions (or quiz), users try to select the correct answer with advice from an LM.
- As an example, consider this multiple-choice question: Before Nixon resigned how many believed he should be removed from office?
- A. 79%
- B. 98 %
- C. 33 %
- D. 57%
- Users can query the system multiple times with free-form text inputs; they might start with the question verbatim, re-phrase the question to increase specificity (e.g., "Did 79 percent of people believe Nixon should have been removed from office?"), or find a different way to query, perhaps bringing in their own prior knowledge (e.g., "Nixon approval rating after watergate").
- We use questions from the Measuring Massive Multitask Language Understanding (MMLU) dataset (Hendrycks et al., 2021).
- Concretely, we first chose five diverse subjects from the dataset (Global facts, Nutrition, US foreign policy, College chemistry, and Miscellany), and selected 6 questions from each subject to construct a pool of 30 questions.
- We constructed quizzes by randomly selecting ten questions from the pool and adding one attention check question in the middle.
- System logic. Figure 4 shows a state of our QA system consisting of a multiple-choice question, user input, and system output.
- Users can take the following actions: pressing a key to modify user input, clicking the "generate" button to query the system, selecting one of the multiple choices, clicking the "next" button to submit their answer, and finishing the quiz.
- When users click the "generate" button, the system creates a prompt with the user input, invokes an LM with the prompt, fetches a completion, and shows the completion in the system output box in the interface (ShowCompletions).
- See Appendix B.3.1 for the exact implementation of CreatePrompt and QueryLM.
- User study procedure. We recruited 342 crowd workers (users) on Amazon Mechanical Turk.
- Each user answered half of the questions with assistance from a single LM (treatment) and answered the other half without assistance (control).
- We instructed users not to use search engines to answer questions, and alerted users ("Please do not switch tabs or open a new window during your participation.") when we detected the tab switching.
- At the end of each quiz, users completed a survey about their experience.
- Survey questions. We asked users for firstperson evaluation of fluency, helpfulness, and...

### Crossword puzzles
- Crossword puzzles have been studied as a challenging task for AI systems
- Crossword puzzles require solving a series of word clues (e.g., "Oscar the Grouch's home") that correspond to either rows ("Across" clues) or columns ("Down" clues) of white squares on a rectangular grid
- As users solve one clue and places letters in squares, they reveal partial information (e.g., shared letters) that can affect their strategy for solving future clues
- In our setting, users try to complete an entire crossword puzzle with the ability to query an LM-based "AI Teammate" via a chat-based interface
- System logic. Figure 6 shows a state of our crossword system, which we adapt from Down
- Users assisted by TextDavinci found their model more fluent, helpful, and easy and enjoyable to interact with compared to other models, and in general provided more accurate solutions across all puzzles
- However, while users with Davinci and Jumbo performed worst on the self-reported survey metrics, users with TextBabbage had the worst accuracy, suggesting a disconnect between first-person preference and automated quality metrics
- The numbers indicate means and standard errors, and the markers denote statistical significance.
- After playing with provided crossword puzzles, users are then asked to rank different qualities of the AI assistant on a 5-point Likert scale, including fluency, helpfulness, enjoyment, and ease of the interaction with the AI Teammate
- Users are additionally asked to describe why they found the teammate helpful or unhelpful, what adjectives best describe the assistant, and whether their interaction changed over the course of answering questions.
- Results. Solving a crossword puzzle is simultaneously an open-ended goal-oriented task and a form of entertainment-therefore, third-party evaluation should consider both accuracy and engagement of the human-LM interaction.

### Text summarization
- Text summarization is a long-standing problem in NLP
- Notably, it has been studied in interactive settings for multi-document summarization, where the users interact with the system to query for additional information to expand upon an initial summary
- In contrast, we focus on human-LM interaction for single-document summarization, where users interact with the system to correct LM-generated summaries
- We consider the task, where given a document and a model-generated summary, users edit the summary to be consistent, relevant, and coherent
- We randomly select 964 documents from XSum dataset (Narayan et al., 2018) and construct 20 summarization sessions by randomly choosing ten documents per session without replacement
- System logic. Figure 8 shows the system state and actions.
- A summarization system state consists of a document, user summary history (not visible to users), user input, and survey questions. Users can take the following actions: pressing a key to modify a model-generated summary, clicking the "next" button, responding to survey questions, and finishing the session.
- One notable difference in the summarization system's state is that CreatePrompt generates a prompt by concatenating all the previous document and user-edited summary pairs in the user's summary history as part of the prompt. This enables us to study whether an LM can learn from user examples and improve over time, as well as how users behavior change as the models learn or fail to learn from their examples. See Appendix B.5.2 for details on CreatePrompt and QueryLM.
- We recruited 39 crowd workers (or users) on Amazon Mechanical Turk. For each model, we collected 20 summarization sessions (80 in total), while allowing the same users to participate multiple times. Upon receiving a model-generated summary for each document, users were asked to edit the summary to be consistent, relevant, and coherent. To encourage users to pay attention to the three quality criteria when editing, we asked users to evaluate the consistency, relevance, and coherence of the original (modelgenerated) summary and edited summary before and after editing. At the end of a summarization session, users completed a survey about their overall experience interacting with the system.
- Survey questions. We ask per-summary and persession questions to the users. Summary-level questions ask consistency, relevance, and coherence of the original and edited summaries, following Fabbri et al. ( 2021). Session-level questions evaluated users' overall perceptions of the summarization system with respect to its helpfulness and improvement over time. See Appendix B.5.2 for the full list of questions.
- We also asked third-party evaluators to evaluate the consistency, relevance, and coherence of a subset of the summaries generated by LMs. To this end, we randomly sampled 100 documents from our user study and recruited 18 workers (third-party evaluators) on Amazon Mechanical Turk to assess the summaries written for the documents. Each summary was assessed by 3 different evaluators.

### Metaphor generation
- Metaphors are used throughout poetry, journalism, and science education to communicate complex or abstract ideas
- Creating metaphors requires divergent, lateral thinking
- To help with ideation, prior work designed metaphor generation tools where users could query multiple times to get suggestions and showed that these suggestions enabled people to write metaphors that they might never have thought of
- In this section, we want to compare degrees in which different LMs support such ideation process
- We defer related work and experimental details to Appendix B.6 for space
- Task. Given a seed metaphor, the task is to write metaphorical sentences that evoke the given metaphor in a limited amount of time.
- For each seed metaphor, we randomly assigned one of the four LMs.
- In each session, each user was given 10 minutes to come up with as many metaphorical sentences as they could using the system.
- At the end of the session, users completed a survey about their experience.

## Framework
- HALIE is a framework for evaluating human-LM interaction
- It describes tasks and system construction for studying human-LM interaction
- It also proposes dimensions and metrics for evaluating human-LM interaction

### Solving tasks interactively
- We study human-LM interaction in the context of tasks
- Figure 2 shows five tasks we study in this paper
- The tasks span the spectrum from goal-oriented (e.g., question answering) to open-ended tasks (e.g., metaphor generation)
- They also have high coverage on the common usages of LMs reported by Ouyang et al. (2022)

### Constructing an interactive system
- Language models are a black box that takes a text prompt and decoding parameters as input and stochastically returns a set of text completions.
- The system logic is a set of states, potential user actions, and a transition function that specifies how states get updated.
- An interaction trace is a sequence of state-action pairs as a result of human-LM interaction.
- Important design considerations include how much control users have over prompts, how decoding parameters influence the quality of completions, and how model completions are shown to users.

### Evaluating human-LM interaction
- Figure 1: Three dimensions in human-LM interaction evaluation.
- Standard evaluation only considers the final output as a target, which is often simply a model completion c, but can be a result of more complicated processes (e.g., sample multiple completions and use heuristics to construct the final output).
- In contrast, we consider the entire interaction process, which is represented as an interaction trace [(s 1 , a 1 ), (s 2 , a 2 ), ...] in HALIE.
- Noninteractive evaluation solely depends on a third-party perspective; automatic metrics can be computed on an interaction trace without human intervention; likewise, human evaluation is done by third parties who did not interact with LMs in any way.
- In the interactive setting, however, evaluation should reflect the first-person perspective of the user who actually takes actions a t to interact with an LM through a system.
- The last dimension is criteria (i.e., how to evaluate). Standard evaluation focuses on quality, which tend to be objective and include metrics like accuracy and fluency (defined for outputs) or helpfulness and edit distance (defined for processes).
- On the other hand, interactive evaluation adds attitudinal measures of a human, which we define as preference. These measures tend to be subjective and often captured by metrics like enjoyment and satisfaction.
- Note that preference criteria apply to both first-person and third-party perspectives.
- One example of preference metrics with third-party perspectives is creative story writing: some readers may not like certain kinds of stories (low preference), although they think that they are good (high quality).
- Also, sometimes metrics can reflect both preference and quality when human preference is aligned with the quality metrics of interest.
- For each task, we define a set of metrics that cover the space defined by the three dimensions.
- Some metrics are a function of interaction traces (e.g., number of queries users made), whereas some metrics only depend on outputs (e.g., accuracy in solving a crossword puzzle).
- Some metrics are defined based on survey responses from users (e.g., satisfaction over the final output) and third-party evaluators (e.g., consistency of a summary given a document).

## Experiments
- There is a discrepancy between metrics based on third-party and first-person perspectives.
- Table 5 shows that users found summaries generated by TextDavinci to be the most helpful starting point for editing, receiving the smallest edit distance and revision score along with the highest helpfulness score.
- From thirdparty evaluation, original summaries from Text-Babbage were rated as the most consistent and relevant among the four models.
- According to the density metric (i.e., the average length of the extractive fragment to which each word in the summary belongs) (Grusky et al., 2018), Text-Davinci is better at in-context learning the user's preferences for summarization.
- The plot shows the rolling average of edit distance between original and edited summaries over ten summaries (1st through 10th summary) across all summarization sessions, with the window size of 2.
- Users save time and effort with Text-Davinci, suggesting it is better at in-context learning.
- We are interested in how users behavior change over time as LMs succeed or fail to learn from previous examples. To that end, we observed how the edit distance between the original and edited summaries change as a summarization session progresses.
- Figure 9 shows the change of edit distance across ten summaries in a sessions.
- We observe that users had to edit less with Text-Davinci over time, whereas they had to edit more with TextBabbage.
- The survey response also indicated that users perceived TextDavinci to be best at improving over time (improvement in Table 5).
- From this observation, we hypothesize that Text-Davinci is better at in-context learning the user's preferences for summarization.

## Related work
- Evaluation of LM-based language generation systems has a long history
- Traditionally, evaluations for generative systems have centered on specific tasks: for example, WMT for machine translation (WMT, 2006), TIPSTER SUMMAC for text summarization (Mani et al., 1999)
- Overall, users perceived TextDavinci to be the most helpful and satisfied with the results from working with the model. However, users perceived Davinci to be the easiest to work with and were willing to reuse the model the most.
- Overall, perceived helpfulness of models and user satisfaction had a strong positive correlation (r = 0.95), while ease of interaction and users' willingness to reuse the system were also positively correlated (r = 0.74)
- The numbers indicate means and standard errors. For these outcomes, no results were found to have statistical significance using a Tukey-Kramer test.

## Discussion
- Benchmarking LMs in interactive settings introduces new and unique challenges.
- In this section, we share the challenges we encountered while de-signing both our tasks and systems as well as working with users.
- Given these experiences, we also discuss potential solutions and paths forward.

### Low latency matters
- Latency significantly influences human-LM interaction and how humans perceive models
- Guidelines in HCI research recommend that interactive systems respond within 0.1 seconds to appear instantaneous and within 1 second to avoid interrupting the user's flow of thought
- When we conducted our user study, some models were simply too slow (e.g., 50x slower than Davinci), which significantly influenced user perception and eventually caused us to exclude these models
- Meeting these latency standards can be a challenge, potentially exacerbated by the growing scale of existing LMs
- With that said, the observed latency is largely determined by the factors beyond model scale alone

### Complexity of interactive study design
- User study design requires researchers to account for individual difference
- Due to these individual effects, especially when the number of recruited users is not especially large, it is generally desirable for each user to interact with most/all models
- This would requires the set of models themselves to be decided upon in advance
- If users interact with multiple models, sequential effects need to be controlled for (e.g., through the randomization of model order across users)

### Potential impact on users
- LMs are prone to generating toxic, biased, or otherwise undesirable text
- This can cause psychological harm
- For QA and crossword puzzles, we asked users to answer the survey question "Did the way you chose to interact with the AI Teammate change over time? If so, how?," and report example responses in the Appendix.
- Oftentimes, user accommodation reflected underlying properties of the models-for example, in QA, prompts phrased as questions yield successful outputs mainly from TextBabbage and TextDavinci, so users assisted with Davinci or Jumbo were more likely to switch to declarative, "fill-in-the-blank" style prompting strategies.
- Further, for some scenarios, we studied user accommodation quantitatively by either measuring how the edit distance or query strategies of user prompts changed over time. We found that task framing can also affect accommodation-while for QA, users engaged in more reformulation over time and became less inclined to copy the provided question verbatim, for crossword puzzles, users became more likely to copy clue text over time, perhaps due to later relying on the LM to understand unfamiliar clue texts.
- Because of these results, we believe future work should closely study more long-term interactive scenarios, where the time it takes a user to develop familiarity with a tool is outweighed by the overall interaction period.
- Lasting impact. In our work, the emphasis is on the short-term interaction of users with LMs: we evaluate this localized experience. However, human-LM interaction, and human-AI interaction more broadly, can have more lasting and longitudinal impacts on users.
- In the context of LMs specifically, we expect interactions with LMs may influence human writing practices, opinions and beliefs, broader perception of language technology and AI systems, and potentially many other aspects of human experience that are mediated by language.
