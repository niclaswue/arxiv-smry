---
title: "A Survey for In-context Learning"
date: 2022-12-31T15:57:09.000Z
author: "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-00234v1.webp" # image path/url
    alt: "A Survey for In-context Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00234)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00234).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-survey-for-in-context-learning).

# Abstract
- In-context learning is a new paradigm for natural language processing
- LLMs make predictions only based on contexts augmented with a few training examples
- ICL is correlated with related studies, including formal definition, training strategies, and so on
- Advanced techniques of ICL include training strategies, prompting strategies, and so on

# Paper Content

## Introduction
- Large language models demonstrate new abilities that learn from a few examples in the context
- Many studies have shown that LLMs can perform a series of complex tasks with ICL
- The key idea of in-context learning is to learn from analogy
- Fig. 1 gives an example describing how language models make decisions with ICL
- First, ICL requires a few examples to form a demonstration context
- Then, Label 1 0 0 … is fed into the language model for prediction
- Different from supervised learning requiring a training stage that uses backward gradients to update model parameters, ICL does not require parameter updates and directly performs predictions on the pretrained language models
- The model is expected to learn the pattern hidden in the demonstration and accordingly make the right prediction
- As a new paradigm, ICL has multiple attractive advantages
- First, since the demonstration is written in natural language formats, it provides an interpretable interface to communicate with large language models
- Second, in-context learning is similar to the decision process of human beings by learning from analogy
- Third, compared with supervised training, ICL is a training-free learning framework
- This could not only greatly reduce the computation costs for adapting the model to new tasks, but also make language-model-as-a-service (Sun et al., 2022)
- MetaICL (Min et al., 2022b), OPT-IML (Iyer et al., 2022), FLAN (Wei et al., 2022a), Super-NaturalInstructions (Wang et al., 2022c), Scaling Instruction (Chung et al., 2022) Self-supervised In-context Tuning ( §4.2) Self-supervised ICL (Chen et al., 2022a) Inference Prompt Designing ( §5) Organization ( §5.1) Selecting ( §5.1.1) GlobalE&LocalE (Lu et al., 2022) Formatting ( §5.2) Instruction ( §5.2.1) Instruction Induction (Honovich et al., 2022), APE (Zhou et al., 2022b), Boostrapping (Wang et al., 2022b) Reasoning Steps ( §5.2.2) CoT (Wang et al., 2022b), Complexity (Fu et al., 2022), AutoCoT (Zhang et al., 2022b), Self-Ask (Press et al., 2022), iCAP (Wang et al., 2022a), Least-to-Most (Zhou et al., 2022a) Scoring Function ( §6) Channel (Min et al., 2022a) Figure sible and can be easily applied to large-scale realworld tasks.

## Overview
- Language models are directly trained on language modeling objectives, such as left-to-right generation.
- ICL still remains a surprising ability.
- Existing ICL studies basically take a well-trained language model as the backbone, and thus this survey will not cover the details of pretraining language models.
- Towards the inference stage, as the input and output labels are all represented in interpretable natural language templates, there are multiple directions for improving ICL performance.
- This paper will give a detailed description and comparison, such as selecting suitable examples for demonstrations and designing specific scoring methods for different tasks.
- We organize the current progress in ICL following the taxonomy above (as shown in Fig. 2), With a formal definition of ICL ( §3), we provide a detailed discussion of the warmup approaches ( §4), the demonstration designing strategies ( §5), and the main scoring functions( §6).
- §7 provides in-depth discussions of current explorations on unveiling the secrets behind the ICL.
- We further provide useful evaluation and resources for ICL ( §8) and introduce potential application scenarios where ICL shows its effectiveness ( §9).
- Finally, we summarize the challenges and potential directions ( §10) and hope this could pave the way for researchers in this field.

## Model Warmup
- LLMs have shown promising ICL capability
- Warmup is an optional procedure for ICL, which adjusts LLMs before ICL inference
- Unlike finetuning, warmup does not aim to train the LLM for specific tasks

### Supervised In-context Training
- To enhance ICL capability, researchers proposed a series of supervised in-context finetuning strategies
- Since the pretraining objectives are not optimized for in-context learning, Min et al. proposed a method MetaICL to eliminate the gap between pretraining and downstream ICL usage
- The pretrained LLM is continually trained on a broad range of tasks with demonstration examples, which boosts its fewshot abilities, e.g., MetaICL obtains performance comparable to supervised finetuning on 52 unique datasets
- Besides, there is a research direction focusing on supervised instruction tuning. Instruction tuning enhances the ICL ability of LLMs through training on task instructions. Tuning the 137B LaMDA-PT (Thoppilan et al., 2022) on over 60 NLP datasets verbalized via natural language instruction templates, FLAN (Wei et al., 2022a) improves both the zero-shot and the few-shot ICL performance. Compared to MetaICL, which constructs several demonstration examples for each task, instruction tuning mainly considers an explanation of the task and is easy to scale up.

### Self-supervised In-context Training
- To utilize raw corpus for warmup, Chen et al. (2022a) proposed to construct self-supervised training data according to the ICL formats in the downstream tasks.
- They transferred the original raw text into input-output pairs and considered four self-supervised objectives, including masked token prediction and classification tasks.
- Although ICL does not strictly require model warmup, we recommend adding a warmup stage before ICL inference.
- The performance advancement made by warmup encounters a plateau when increasingly scaling up the training data. This phenomenon appears both in supervised in-context training and self-supervised in-context training, indicating that LLMs only need a small amount of data to adapt to learn from the context during warmup.

## Prompt Designing
- Many studies have shown that the performance of ICL strongly relies on the demonstration surface, including prompt format, the order of demonstration examples, and so on (Zhao et al., 2021;Lu et al., 2022).
- In this section, we survey prompt designing strategies and classify them into two groups: demonstration organization and demonstration formatting, as shown in Table 1.

### Demonstration Organization
- Unsupervised methods: KATE, Liu et al. (2022)
- Supervised methods: Rubin et al. (2022), Zhang et al. (2022a)
- Reinforcement learning: Lu et al. (2022)

### Demonstration Formatting

## Scoring Function
- The scoring function decides how we can transform the predictions of a language model into an estimation of the likelihood of a specific answer.
- A direct estimation method (Direct) adopts the conditional probability of candidate answers that can be represented by tokens in the vocabulary of language models.
- The answer with a higher probability is selected as the final answer. However, this method poses some restrictions on the template design, e.g., the answer tokens should be placed at the end of input sequences.
- Perplexity (PPL) is another commonly-used metric, which computes the sentence perplexity of the whole input sequence S j = {C, s(x, y j , I)} consists of the tokens of demonstration examples C, input query x and target label y j .
- As PPL evaluates the probability of the whole sentence, it removes the limitations of token positions but requires extra computation time.
- Note that in generation tasks such as machine translation, ICL predicts the answer by decoding tokens with the highest sentence probability combined with diversity-promoting strategies such as beam search or Top-p and Top-k (Holtzman et al., 2020) sampling algorithms.
- Different from previous methods, which estimate the probability of the label given the input, Min et al. (2022a) proposed to utilize channel models (Channel) to compute the conditional probability in a reversed direction, i.e., estimating the input query given the label.
- In this way, language models are required to generate every token in the input, which could boost the performance under imbalanced training data regimes.
- We summarize all three scoring functions in Table 2.

## Analysis
- The number of layers in the network
- The number of neurons in the network
- The number of layers in the input layer
- The number of layers in the output layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the hidden layer
- The number of neurons in the hidden layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the input layer
- The number of layers in the output layer
- The number of neurons in the hidden layer
- The number of neurons in the hidden layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the network
- The number of neurons in the network
- The number of layers in the input layer
- The number of layers in the output layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the hidden layer
- The number of neurons in the hidden layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the input layer
- The number of layers in the output layer
- The number of neurons in the hidden layer
- The number of neurons in the hidden layer
- The number of layers in the network
- The number of neurons in the network
- The number of layers in the input layer
- The number of layers in the output layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the hidden layer
- The number of neurons in the hidden layer
- The number of neurons in the input layer
- The number of neurons in the output layer
- The number of layers in the input layer
- The number of layers in the output layer
- The number of neurons in the hidden layer
- The number of neurons in the hidden layer

### What Influences ICL Performance
- The influence of the pretraining corpora.
- The ICL ability grows as the parameters of LLMs increase.
- In the inference stage, the properties of the demonstration samples also influence the ICL performance.
- Learning Mechanism: Transformers can encode effective learning algorithms to learn unseen linear functions.
- Other work attempted to build connections between ICL and finetuning.
- Extending analysis on extensive tasks and large models may be the next step to be considered.
- Among existing work, understanding ICL as a process of metaoptimization seems to be a reasonable and promising direction for future research.

## Evaluation and Resources

### Traditional Tasks
- Can be examined on various traditional datasets and benchmarks
- GPT-3 can achieve results comparable to state-of-theart (SOTA) finetuning performance on COPA and ReCoRD, but still falls behind finetuning on most NLU tasks
- Scaling up the number of demonstration examples can bring limited improvement

### New Challenging Tasks
- The era of large language models with in-context learning capabilities is more interested in evaluating the intrinsic capabilities of large language models without downstream task finetuning
- To explore the capability limitations of LLM on various tasks, Srivastava et al. (2022) proposed the BIG-bench (Srivastava et al., 2022), a large benchmark covering a large range of tasks, including linguistics, chemistry, biology, social behavior, and beyond
- The best models have already outperformed the average reported human-rater results on 65% of the BIG-Bench tasks through ICL (Suzgun et al., 2022)
- To further explore tasks actually unsolvable by current language models, Suzgun et al. (2022) proposed a more challenging ICL benchmark, BIG-Bench Hard (BBH)
- BBH includes 23 unsolved tasks, constructed by selecting challenging tasks where the state-of-art model performances are far below the human performances
- Besides, researchers are searching for inverse scaling tasks2 , that is, tasks where model performance reduces when scaling up the model size
- Such tasks also highlight potential issues with the current paradigm of ICL
- To further probe the model generalization ability, Iyer et al. (2022) proposed OPT-IML Bench, consisting of 2000 NLP tasks from 8 existing benchmarks
- Specifically, a series of studies focus on exploring the reasoning ability of ICL
- Saparov and He (2022) generated an example from a synthetic world model represented in first-order logic and parsed the ICL generations into symbolic proofs for formal analysis
- They found that LLMs can make correct individual deduction steps via ICL
- Shi et al. (2022) constructed the MGSM benchmark to evaluate the chain-of-thought reasoning abilities of LLMs in multilingual settings
- Finding that LLMs manifest complex reasoning across multiple languages
- To further probe more sophisticated planning and reasoning abilities of LLMs, Valmeekam et al. (2022) provided multiple test cases for evaluating various reasoning abilities on actions and change
- Existing ICL methods on LLMs show poor performance on these tasks

## Application
- ICL outperforms traditional NLP methods
- ICL can be used for meta-learning
- ICL can be used for data annotation
- ICL is 50% to 96% cheaper than traditional methods

### New Pretraining Strategies
- Researchers have proposed to bridge the gap between pretraining objectives and ICL through intermediate tuning before inference
- To take it further, tailored pretraining objectives and metrics for ICL have the potential to raise LLMs with superior ICl capabilities.

### Distill the ICL Ability to Smaller Models
- Previous studies have shown that in-context learning for reasoning tasks emerges as the scale of computation and parameter exceed a certain threshold
- Exploring whether the ICL ability could be transferred to smaller models could facilitate deployments greatly
- Magister et al. (2022) showed that it is possible to distill the reasoning ability to small language models such as T5-XXL. The distillation is achieved by finetuning the small model on the Chain-of-Thought data (Wei et al., 2022c) generated by a large teacher model. Although promising performance is achieved, the improvements are likely task-dependent.

### Knowledge Augmentation and Updating
- The knowledge of LLMs is entirely derived from the pretrained corpus.
- LLMs may lack certain knowledge and generate hallucinations during ICL inference, especially towards long-tailed factual knowledge or commonsense knowledge rarely described in texts.
- Therefore, it is essential to augment knowledge for ICL.
- Different from traditional work, which adds knowledge adapters (Wang et al., 2021a) or provides structured knowledge during pretraining (Zhang et al., 2019;Peters et al., 2019), retrieving correct knowledge and integrating the correct knowledge with the context in a lightweight manner is possibly promissing for ICL.

### Robustness to Demonstration
- Previous studies have shown that the performance of ICL is extremely unstable, from random guess to SOTA
- To effectively improve the robustness of ICL, we need deeper analysis of the working mechanism of the ICL
- We believe that the analysis of the robustness of the ICL from a more theoretical perspective rather than an empirical perspective can highlight future research on more robust ICL

### ICL for Data Engineering
- ICL takes only a few examples to learn the data engineering objective, which saves the cost of annotating training data.
- The strong reasoning ability and text-generating ability of LLM show great potential to generate high-quality data. However, how to use ICL for data annotation remains an open question.

## Conclusion
- Supervised training and self-supervised training propose to train the LLMs before ICL inference.
- The key idea is to bridge the gap between pretraining and downstream ICL formats by introducing objectives close to in-context learning.
- Compared to in-context finetuning involving demonstration, instruction finetuning without a few examples as demonstration is simpler and more popular.
- Understanding why ICL works distribution of training data concentration on the pretraining data, Chan et al. (2022) showed that the ICL ability is driven by some data distributional properties.
- They found that ICL ability emerges when the training data have examples appearing in clusters and have enough number of rare classes.
- Xie et al. (2022) defined the global and local entropy metrics. They found a positive correlation between the entropy metric and the ICL performance.
- With the entropy metric, they directly used the entropy metric to select the best ordering of examples.
- With chain-of-thought prompting, LLMs predict chain-of-thoughts context and then give the final answer.
- CoT prompting can learn complex reasoning by decomposing input-output mappings into many intermediate steps.
- There are also many pieces of research that focus on how to design CoTs to improve the CoT prompting ability of LLMs.
- Similar to demonstration selection, CoT designing also considers CoT selection.
- Different from Wei et al. (2022c) manually writing CoTs, Auto-CoT (Zhang et al., 2022b) used LLMs with Let's think step by step to generate CoTs.
- In addition, Fu et al. (2022) proposed a complexity-based demonstration selection method. They selected demonstrations with more CoTs (reasoning steps) for CoT prompting.
