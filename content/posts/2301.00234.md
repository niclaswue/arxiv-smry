---
title: "A Survey for In-context Learning"
date: 2022-12-31T15:57:09.000Z
author: "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-00234v1.webp" # image path/url
    alt: "A Survey for In-context Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00234)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00234).


# Abstract
- In-context learning is a new paradigm for natural language processing
- LLMs make predictions only based on contexts augmented with a few training examples
- ICL is correlated with related studies, including formal definition, training strategies, and so on
- Advanced techniques of ICL include training strategies, prompting strategies, and so on

# Paper Content

## Introduction
- Large language models can learn from a few examples in the context
- ICL requires a few examples to form a demonstration context
- Demonstration context is usually written in natural language templates
- Label 1 0 0 … is fed into the language model for prediction
- Different from supervised learning requiring a training stage that uses backward gradients to update model parameters, ICL does not require parameter updates and directly performs predictions on the pretrained language models
- The model is expected to learn the pattern hidden in the demonstration and accordingly make the right prediction
- As a new paradigm, ICL has multiple attractive advantages
- Since the demonstration is written in natural language formats, it provides an interpretable interface to communicate with large language models
- This paradigm makes it much easier to incorporate human knowledge into language models by changing demonstration and templates
- In-context learning is similar to the decision process of human beings by learning from analogy
- Compared with supervised training, ICL is a training-free learning framework
- This could not only greatly reduce the computation costs for adapting the model to new tasks, but also make language-model-as-a-service possible
- MetaICL (Min et al., 2022b), OPT-IML (Iyer et al., 2022), FLAN (Wei et al., 2022a), Super-NaturalInstructions (Wang et al., 2022c), Scaling Instruction (Chung et al., 2022), Self-supervised In-context Tuning ( §4.2), Self-supervised ICL (Chen et al., 2022a) are some examples of in-context learning
- Inference Prompt Designing ( §5) is an important task for in-context learning
- While the vanilla GPT-3 model itself shows promising ICL abilities, several studies observed that the ability could be significantly boosted via adaption during pretraining
- Furthermore, the performance of ICL is sensitive to specific settings, including the prompting template, the selection of in-context examples, and example orders, and so on

## Overview
- Language models are directly trained on language modeling objectives, such as left-to-right generation.
- ICL still remains a surprising ability.
- Existing ICL studies basically take a well-trained language model as the backbone, and thus this survey will not cover the details of pretraining language models.
- Towards the inference stage, as the input and output labels are all represented in interpretable natural language templates, there are multiple directions for improving ICL performance.
- This paper will give a detailed description and comparison, such as selecting suitable examples for demonstrations and designing specific scoring methods for different tasks.
- We organize the current progress in ICL following the taxonomy above (as shown in Fig. 2), With a formal definition of ICL ( §3), we provide a detailed discussion of the warmup approaches ( §4), the demonstration designing strategies ( §5), and the main scoring functions( §6).
- §7 provides in-depth discussions of current explorations on unveiling the secrets behind the ICL.
- We further provide useful evaluation and resources for ICL ( §8) and introduce potential application scenarios where ICL shows its effectiveness ( §9).
- Finally, we summarize the challenges and potential directions ( §10) and hope this could pave the way for researchers in this field.

## Model Warmup
- LLMs have shown promising ICL capability
- Many studies also show that the ICL capability can be further improved through a continual training stage between pretraining and ICL inference
- Warmup is an optional procedure for ICL, which adjusts LLMs before ICL inference

### Supervised In-context Training
- To enhance ICL capability, researchers proposed a series of supervised in-context finetuning strategies
- Since the pretraining objectives are not optimized for in-context learning, Min et al. (2022b) proposed a method MetaICL to eliminate the gap between pretraining and downstream ICL usage
- The pretrained LLM is continually trained on a broad range of tasks with demonstration examples, which boosts its fewshot abilities, e.g., MetaICL obtains performance comparable to supervised finetuning on 52 unique datasets
- Besides, there is a research direction focusing on supervised instruction tuning
- Instruction tuning enhances the ICL ability of LLMs through training on task instructions. Tuning the 137B LaMDA-PT (Thoppilan et al., 2022) on over 60 NLP datasets verbalized via natural language instruction templates, FLAN (Wei et al., 2022a) improves both the zero-shot and the few-shot ICL performance
- Compared to MetaICL, which constructs several demonstration examples for each task, instruction tuning mainly considers an explanation of the task and is easy to scale up. Chung et al. ( 2022) and Wang et al. (2022c) proposed to scale up instruction tuning with more than 1000+ task instructions

### Self-supervised In-context Training
- To utilize raw corpus for warmup, Chen et al. (2022a) proposed to construct self-supervised training data according to the ICL formats in the downstream tasks.
- They transferred the original raw text into input-output pairs and considered four self-supervised objectives, including masked token prediction and classification tasks.
- Although ICL does not strictly require model warmup, we recommend adding a warmup stage before ICL inference.
- The performance advancement made by warmup encounters a plateau when increasingly scaling up the training data. This phenomenon appears both in supervised in-context training and self-supervised in-context training, indicating that LLMs only need a small amount of data to adapt to learn from the context during warmup.

## Prompt Designing
- Many studies have shown that the performance of ICL strongly relies on the demonstration surface
- In this section, we survey prompt designing strategies and classify them into two groups: demonstration organization and demonstration formatting
- The first group includes strategies like using prompt format, the order of demonstration examples, and so on
- The second group includes strategies like using demonstration organization to improve the performance of ICL

### Demonstration Organization
- Unsupervised methods: KATE, Liu et al. (2022)
- Supervised methods: Rubin et al. (2022), Zhang et al. (2022a)
- Reinforcement learning: Lu et al. (2022)

### Demonstration Formatting
- A common way to format demonstrations is concatenating examples (x 1 , y 1 ), . . . , (x k , y k ) with a specific template T directly. However, in some tasks that need complex reasoning (e.g., math word problems, commonsense reasoning), it is not easy to learn the mapping from x i to y i with only k demonstrations.
- Although template engineering has been studied in prompting (Liu et al., 2021), some researchers aim to design a better format of demonstrations for ICL by describing tasks with the instruction I ( §5.2.1) and adding intermediate reasoning steps (chain-of-thoughts) between x i and y i ( §5.2.2).
- Except for the well-designed demonstration examples, good instructions which describe the task precisely are also helpful to the inference performance.
- However, unlike the demonstration examples, which are common in traditional datasets, the task instructions depend heavily on human-written sentences. As input-output mappings are decomposed into step-by-step reasoning, some researchers apply multi-stage ICL for CoT prompting and design CoT demonstrations for each step.
- Multi-stage ICL queries LLMs with different prompts in each reasoning step. Self-Ask (Press et al., 2022) allows LLMs to generate follow-up questions for the input and ask themselves these questions. Then the questions and intermediate answers will be added to CoTs.
- Least-to-Most Prompting (Zhou et al., 2022a) is a two-stage ICL including question reduction and subquestion solution. The first stage decomposes a complex question into subquestions; in the second stage, LLMs answer subquestions sequentially, and previously answered questions and generated answers will be added into the context.

## Scoring Function
- The scoring function decides how we can transform the predictions of a language model into an estimation of the likelihood of a specific answer
- A direct estimation method (Direct) adopts the conditional probability of candidate answers that can be represented by tokens in the vocabulary of language models
- The answer with a higher probability is selected as the final answer
- However, this method poses some restrictions on the template design, e.g., the answer tokens should be placed at the end of input sequences
- Perplexity (PPL) is another commonly-used metric, which computes the sentence perplexity of the whole input sequence S j = {C, s(x, y j , I)} consists of the tokens of demonstration examples C, input query x and target label y j
- As PPL evaluates the probability of the whole sentence, it removes the limitations of token positions but requires extra computation time
- Note that in generation tasks such as machine translation, ICL predicts the answer by decoding tokens with the highest sentence probability combined with diversity-promoting strategies such as beam search or Top-p and Top-k (Holtzman et al., 2020) sampling algorithms
- Different from previous methods, which estimate the probability of the label given the input, Min et al. (2022a) proposed to utilize channel models (Channel) to compute the conditional probability in a reversed direction, i.e., estimating the input query given the label
- In this way, language models are required to generate every token in the input, which could boost the performance under imbalanced training data regimes
- We summarize all three scoring functions in Table 2.

## Analysis
- Many analytical studies attempt to investigate what factors may influence the performance and aim to figure out why ICL works
- The factors that have a relatively strong correlation to ICL performance are:
- The size of the data set, the number of iterations, and the number of training samples
- Table 3 summarizes the factors that have a relatively strong correlation to ICL performance

### What Influences ICL Performance
- LLM Assessment Suite (Valmeekam et al., 2022)
- IEEE Transactions on Knowledge and Data Engineering (TKDE)
- IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- IEEE Transactions on Cybernetics (TC)
- IEEE Transactions on Systems, Man, and Cybernetics (TSMC)
- IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
- IEEE Transactions on Computational Intelligence (TCI)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics (TSMC-C)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics and Applications (TSMC-CA)
- IEEE Transactions on Systems, Man, and Cybernetics: Applications and Services (TSMC-AS)
- IEEE Transactions on Systems, Man, and Cybernetics: Computer Architecture and Design (TSMC-CAD)
- IEEE Transactions on Systems, Man, and Cybernetics: Embedded Systems (TSMC-ES)
- IEEE Transactions on Systems, Man, and Cybernetics: Robotics and Automation (TSMC-R&A)
- IEEE Transactions on Systems, Man, and Cybernetics: Services and Applications (TSMC-SA)
- IEEE Transactions on Knowledge and Data Engineering (TKDE)
- IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- IEEE Transactions on Cybernetics (TC)
- IEEE Transactions on Systems, Man, and Cybernetics (TSMC)
- IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
- IEEE Transactions on Computational Intelligence (TCI)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics (TSMC-C)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics and Applications (TSMC-CA)
- IEEE Transactions on Systems, Man, and Cybernetics: Applications and Services (TSMC-AS)
- IEEE Transactions on Systems, Man, and Cybernetics: Computer Architecture and Design (TSMC-CAD)
- IEEE Transactions on Systems, Man, and Cybernetics: Embedded Systems (TSMC-ES)
- IEEE Transactions on Systems, Man, and Cybernetics: Robotics and Automation (TSMC-R&A)
- IEEE Transactions on Systems, Man, and Cybernetics: Services and Applications (TSMC-SA)
- IEEE Transactions on Knowledge and Data Engineering (TKDE)
- IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- IEEE Transactions on Cybernetics (TC)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics (TSMC-C)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics and Applications (TSMC-CA)
- IEEE Transactions on Systems, Man, and Cybernetics: Applications and Services (TSMC-AS)
- IEEE Transactions on Systems, Man, and Cybernetics: Computer Architecture and Design (TSMC-CAD)
- IEEE Transactions on Systems, Man, and Cybernetics: Embedded Systems (TSMC-ES)
- IEEE Transactions on Systems, Man, and Cybernetics: Robotics and Automation (TSMC-R&A)
- IEEE Transactions on Systems, Man, and Cybernetics: Services and Applications (TSMC-SA)
- IEEE Transactions on Knowledge and Data Engineering (TKDE)
- IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- IEEE Transactions on Cybernetics (TC)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics (TSMC-C)
- IEEE Transactions on Systems, Man, and Cybernetics: Cybernetics and Applications (TSMC-CA)
- IEEE Transactions on Systems, Man, and Cybernetics: Applications and Services (TSMC-AS)
- IEEE Transactions on Systems, Man, and Cybernetics: Computer Architecture and Design (TSMC-CAD)
- IEEE Transactions on Systems, Man, and Cybernetics: Embedded Systems (TSMC-ES)
- IEEE Transactions on Systems, Man, and Cybernetics: Robotics and Automation (TSMC-R&A)
- IEEE Transactions on Systems, Man, and Cybernetics: Services and Applications (TSMC-SA)
- IEEE Transactions on Knowledge and Data Engineering (TKDE)
- IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- ...

## Evaluation and Resources

### Traditional Tasks
- ICL can be examined on various traditional datasets and benchmarks
- Implementing ICL with 32 randomly sampled examples on SuperGLUE, Brown et al. (2020) found that GPT-3 can achieve results comparable to state-of-theart (SOTA) finetuning performance on COPA and ReCoRD, but still falls behind finetuning on most NLU tasks
- Hao et al. (2022) showed the potential of scaling up the number of demonstration examples. However, the improvement brought by scaling is very limited

### New Challenging Tasks
- The era of large language models with in-context learning capabilities
- Researchers are more interested in evaluating the intrinsic capabilities of large language models without downstream task finetuning
- To explore the capability limitations of LLM on various tasks, Srivastava et al. (2022) proposed the BIG-bench (Srivastava et al., 2022), a large benchmark covering a large range of tasks, including linguistics, chemistry, biology, social behavior, and beyond
- The best models have already outperformed the average reported human-rater results on 65% of the BIG-Bench tasks through ICL (Suzgun et al., 2022)
- To further explore tasks actually unsolvable by current language models, Suzgun et al. (2022) proposed a more challenging ICL benchmark, BIG-Bench Hard (BBH)
- BBH includes 23 unsolved tasks, constructed by selecting challenging tasks where the state-of-art model performances are far below the human performances
- Besides, researchers are searching for inverse scaling tasks2 , that is, tasks where model performance reduces when scaling up the model size
- To further probe the model generalization ability, Iyer et al. (2022) proposed OPT-IML Bench, consisting of 2000 NLP tasks from 8 existing benchmarks, especially benchmark for ICL on held-out categories
- Specifically, a series of studies focus on exploring the reasoning ability of ICL
- Saparov and He (2022) generated an example from a synthetic world model represented in first-order logic and parsed the ICL generations into symbolic proofs for formal analysis
- They found that LLMs can make correct individual deduction steps via ICL
- Shi et al. (2022) constructed the MGSM benchmark to evaluate the chain-of-thought reasoning abilities of LLMs in multilingual settings
- They found that LLMs manifest complex reasoning across multiple languages
- To further probe more sophisticated planning and reasoning abilities of LLMs, Valmeekam et al. (2022) provided multiple test cases for evaluating various reasoning abilities on actions and change, where existing ICL methods on LLMs show poor performance
- Existing ICL methods on LLMs show poor performance on multiple tasks

## Application

### New Pretraining Strategies
- Researchers have proposed to bridge the gap between pretraining objectives and ICL through intermediate tuning before inference
- To take it further, tailored pretraining objectives and metrics for ICL have the potential to raise LLMs with superior ICl capabilities

### Distill the ICL Ability to Smaller Models
- Previous studies have shown that in-context learning for reasoning tasks emerges as the scale of computation and parameter exceed a certain threshold
- Exploring whether the ICL ability could be transferred to smaller models could facilitate deployments greatly
- Magister et al. (2022) showed that it is possible to distill the reasoning ability to small language models such as T5-XXL
- The distillation is achieved by finetuning the small model on the Chain-of-Thought data (Wei et al., 2022c) generated by a large teacher model
- Although promising performance is achieved, the improvements are likely task-dependent

### Knowledge Augmentation and Updating
- LLMs are entirely derived from the pretrained corpus
- ICL inference may generate hallucinations due to incorrect knowledge in LLMs
- Augmenting knowledge for ICL is possible, but has not been rigorously evaluated

### Robustness to Demonstration
- Previous studies have shown that the performance of ICL is extremely unstable, from random guess to SOTA
- To effectively improve the robustness of ICL, we need deeper analysis of the working mechanism of the ICL
- We believe that the analysis of the robustness of the ICL from a more theoretical perspective rather than an empirical perspective can highlight future research on more robust ICL

### ICL for Data Engineering
- ICL takes only a few examples to learn the data engineering objective, which saves the cost of annotating training data.
- The strong reasoning ability and text-generating ability of LLM show great potential to generate high-quality data.
- However, how to use ICL for data annotation remains an open question.

## Conclusion
- The ICL ability emerges when the training data have examples appearing in clusters and have enough number of rare classes.
- Xie et al. found a positive correlation between the entropy metric and the ICL performance.
- With the entropy metric, they directly used the entropy metric to select the best ordering of examples.
- CoT prompting can learn complex reasoning by decomposing input-output mappings into many intermediate steps.
- Fu et al. proposed a complexity-based demonstration selection method.
