---
title: "Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts"
date: 2022-10-07T19:58:47.000Z
author: "Asahi Ushio, Leonardo Neves, Vitor Silva, Francesco Barbieri, Jose Camacho-Collados"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-03797v2.webp" # image path/url
    alt: "Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.03797)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.03797).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/named-entity-recognition-in-twitter-a-dataset).

# Abstract
- Recent progress in language model pre-training has led to important improvements in Named Entity Recognition (NER).
- However, this progress has been mainly tested in well-formatted documents such as news, Wikipedia, or scientific articles. In social media the landscape is different, in which it adds another layer of complexity due to its noisy and dynamic nature.
- In this paper, we focus on NER in Twitter, one of the largest social media platforms, and construct a new NER dataset, TweetNER7, which contains seven entity types annotated over 11,382 tweets from September 2019 to August 2021.
- The dataset was constructed by carefully distributing the tweets over time and taking representative trends as a basis.
- Along with the dataset, we provide a set of language model baselines and perform an analysis on the language model performance on the task, especially analyzing the impact of different time periods. In particular, we focus on three important temporal aspects in our analysis: short-term degradation of NER models over time, strategies to fine-tune a language model over different periods, and self-labeling as an alternative to lack of recently-labeled data.
- TweetNER7 is released publicly (https://huggingface.co/datasets/tner/tweetner7) along with the models fine-tuned on it.

# Paper Content

## Introduction
- Named Entity Recognition (NER) is a longstanding task that consists of identifying an entity in a sentence or document, and classifying it into an entity-type from a fixed typeset.
- One of the most common and successful types of NER system is achieved by fine-tuning pre-trained language models (LMs) on a human-annotated NER dataset with token-wise classification (Peters et al., 2018;Howard and Ruder, 2018;Radford et al., 2018Radford et al., , 2019;;Devlin et al., 2019).
- Remarkably, LM finetuning based NER models (Yamada et al., 2020;Li et al., 2020) already achieve over 90% F1 score in standard NER datasets such as CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) and OntoNotes5 (Hovy et al., 2006).
- However, NER is far from being solved, specialized domains such as financial news (Salinas Alvarado et al., 2015), biochemical (Collier and Kim, 2004), or biomedical (Wei et al., 2015;Li et al., 2016) still pose additional challenges (Ushio and Camacho-Collados, 2021).
- Lower performance in these domains may be attributed to various factors such as the usage specific terminologies within those domains, which LMs have not seen while pre-training (Lee et al., 2020).
- Among recent studies, social media has been acknowledged as one of the most challenging domains for NER (Derczynski et al., 2016(Derczynski et al., , 2017)). Social media texts are generally more noisy and less formal than conventional written languages in addition to its vocabulary specificity.
- In social media, there is another particular feature that needs to be addressed, which is the presence of (quick) temporal shifts in the text semantics (Rijhwani and Preotiuc-Pietro, 2020), where the meaning of words is constantly changing or evolving over time.
- This is a general issue with language models (Lazaridou et al., 2021), but it is especially relevant given the dynamic landscape and immediacy present in social media (Del Tredici et al., 2019).
- There have been a few specific approaches to deal with the temporal shifts in social media. For instance, Loureiro et al. (2022) addressed this issue by pre-training language models on a large tweet collection from different time period, highlighting the importance of having an up-to-date language model. Agarwal and Nenkova (2022) studied the temporal-shift in various NLP tasks including NER and analyzed methods to overcome the temporal-shift with strategies such as self-labeling.
- In this paper, we propose a new NER dataset for Twitter (TweetNER7 henceforth). TweetNER7 contains tweets from diverse topics that are distributed uniformly from September 2019 to August 2021. It contains 11,382 annotated tweets in total, spanning seven entity types (person, location, corporation, creative work, group, product, and event).
- To the best of our knowledge, Tweet-NER7 is the largest Twitter NER datasets with a high coverage of entity types.
- TTC (Rijhwani and Preotiuc-Pietro, 2020) contains about same amount of annotation yet with three entity types, while WNUT17 (Derczynski et al., 2017) has six entity types yet suffer from very small annotations.
- The tweets for TweetNER7 were collected by querying tweets with weekly trending keywords so that the tweet collection covers various topics within the period, and we further removed near-duplicated tweets and irrelevant tweets without any specific topics in order to improve the quality of tweets.
- We provide baseline results with language model finetuning that showcases the difficulty of TweetNER7, especially when dealing with time shifts.
- Finally, we provide a temporal analysis with different strategies including self-labeling, which does not prove highly beneficial in our context, and provide insights in the model inner working and potential biases.

## Related Work
- There is a large variety of NER datasets in the literature.
- CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) and OntoNotes5 (Hovy et al., 2006) are widely used common NER datasets in the literature, where the texts are collected from public news, blogs, and dialogues.
- WikiAnn (Pan et al., 2017) and MultiNERD (Tedeschi and Navigli, 2022) are both multilingual NER datasets where the training set is constructed by distantsupervision on Wikipedia and BabelNet.
- As far as domain-specific NER datasets are concerned, FIN (Salinas Alvarado et al., 2015) is a NER dataset of financial news, while BioNLP2004 (Collier and Kim, 2004) and BioCreative (Wei et al., 2015;Li et al., 2016) are both constructed from scientific documents of the biochemical and biomedical domains.
- However, none of these datasets address the same challenges posed by the social media domain.
- In the social media domain, the pioneering Broad Twitter Corpus (BTC) NER dataset (Derczynski et al., 2016) included users with different demographics with the aim to investigate spatial and temporal shift of semantics in NER.
- More recently, the test set of WNUT2017 (Derczynski et al., 2017) contained unseen entities in the training set from broader social media including Twitter, Reddit, YouTube, and StackExchange.
- The recent Twee-BankNER dataset (Jiang et al., 2022) annotated TweeBank (Liu et al., 2018) with entity labels to investigate the interaction between syntax and NER.
- The most similar dataset to ours is the Temporal Twitter Corpus (TTC) NER dataset. (Rijhwani and Preotiuc-Pietro, 2020), which was also aimed at analysing the temporal effects of NER in social media.
- For this dataset, 2,000 tweets every year from 2014 to 2019 were annotated.
- In general, however, these social media datasets suffer from limited data, non-uniform distribution over time, or limited entity types (see Subsection 3.3 for more details).
- In this paper, we contribute with a new NER dataset (TweetNER7) based on recent data until 2021, which is specifically designed to analyze temporal shifts in social media.

### Data Collection
- The NER dataset annotates a similar tweet collection used to construct TweetTopic (Antypas et al., 2022).
- The main data consists of tweets from September 2019 to August 2021 with roughly same amount of tweets in each month.
- This collection period makes it suitable for our purpose of evaluating short-term temporal-shift of NER on Twitter.
- The original tweets were filtered by leveraging weekly trending topics as well as by various other types of filtering see Antypas et al. (2022) for more details on the collection and filtering process).
- The collected tweets were then split into two periods: September 2019 to August 2020 (2020-set) and September 2020 to August 2021 (2021-set).

### Dataset Annotation
- Annotation: To attain named-entity annotations over the tweets, we conducted a manual annotation on Amazon Mechanical Turk with the interface shown in Figure 1.
- We split tweets into two periods: September 2019 to August 2020 (2020-set) and September 2020 to August 2021 (2021-set), and randomly sampled 6,000 tweets from each period, which were annotated by three annotators, collecting 36,000 annotations in total.
- As the entity types, we employed seven labels: person, location, corporation, creative work, group, product, and event.
- We followed Derczynski et al. (2017) for the selection of the first six labels, and additionally included event, as we found a large amount of entities for events in our collected tweets.
- Pre-processing. We pre-process tweets before the annotation to normalize some artifacts, converting URLs into a special token {{URL}} and nonverified usernames into {{USERNAME}}.
- For verified usernames, we replace its display name with symbols @.
- Quality Control. Since we have three annotations per tweet, we control the quality of the annotation by taking the agreement into account. We disregard the annotation if the agreement is 1/3, and manually validate the annotation if it is 2/3, which happens for roughly half of the instances.

### Statistics
- The TweetNER7 dataset has a high coverage of entity types and includes recent tweets from 2019 to 2021
- The TweetNER7 dataset has a uniform distribution over time, which is essential for temporal analysis
- The TweetNER7 dataset has a higher coverage of entity types than other Twitter NER datasets

### Baseline Results
- LM is trained on the 2020-set and the 2021-set
- LM is trained with the training and the validation set from the 2020-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2021-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2021-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2021-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the 2020-set and the 2021-set
- LM is evaluated on the 2021-set
- LM is trained with the training and the validation set from the 2020-set and the...

## Temporal Analysis
- The temporal-shift has an effect on the performance of a computer program
- The temporal-shift has an effect on the performance of a computer program when the program is split into multiple parts
- The temporal-shift has an effect on the performance of a computer program when the program is split into multiple parts and the parts are fine-tuned
- The temporal-shift has an effect on the performance of a computer program when the program is self-labeled

### Short-Term Temporal Effect
- If TweetNER7 does not suffer temporal-shift, the model performance is unchanged
- Temporal-shift usually occurs in a situation where the training and the validation sets do not contain any texts from the test period, so we keep the amount of the training/validation split as the same
- The increase of accuracy in 2021 is achieved with the inclusion of training/validation set from 2021, and the decrease of accuracy in 2020 is caused by the reduced number of the training/validation set from the same 2020 period
- This result further highlights the benefit of having a human annotated training set from the test period, even if the time period differs in a year only

### Continuous vs. Joint Fine-Tuning
- The main aim of this analysis is to explore different strategies to improve the original model.
- In addition to fine-tuning LMs on the combined set of the 2020-set and 2021-set as in Subsection 4.1, we employed a continuous finetuning scheme, where we first fine-tune LMs on the 2020-set and then continue fine-tuning on the 2021-set.
- Table 7 shows the results of all strategies for different language models.
- As can be observed, continuous fine-tuning provides the best results in terms of micro F1 and type-ignored F1 in the 2021 test sets in most cases, although the differences with respect to the concatenation of sets are not substantial.

### Self-Labeling
- The model can be improved by self-labeling, but it doesn't help to mitigate the temporal-shift
- In our setting, self-labeling does not help to overcome the temporal-shift
- We used a retrieval module to find relevant tweets given a target entity and see the portion of retrieved tweets containing the true prediction.
- Most frequent predictions are usually the same as the original predictions, which means that the original language model tends to output similar predictions for the same entities, irrespective of the context.
- As far as the time variable is concerned, the ratio is almost consistent over time, which suggests that the possible original bias of the model does not change over time.

## Conclusion
- In this paper, we have constructed TweetNER7, a new NER dataset for Twitter, in which we annotated 11,382 tweets with seven entity types.
- The collected tweets are distributed uniformly over time from September 2019 to August 2021, which facilitates temporal analysis in NER for social media.
- The dataset is diverse topic-wise, as we leveraged weekly trending topics to query tweets and nearduplicated and irrelevant tweets were dropped.
- To establish baselines on TweetNER7, we fine-tuned standard LMs including a few Twitter-specific LMs. Moreover, we performed a few targeted temporalrelated analyses in order to better understand the short-term temporal effect.
- Finally, we show that self-labeling is not enough to mitigate the temporalshift and had no noticeable improvement over the baseline vanilla fine-tuning, which further highlights the challenging nature of the dataset.
