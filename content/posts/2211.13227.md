---
title: "Paint by Example: Exemplar-based Image Editing with Diffusion Models"
date: 2022-11-23T18:59:52.000Z
author: "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2211-13227v1.webp" # image path/url
    alt: "Paint by Example: Exemplar-based Image Editing with Diffusion Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2211.13227)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2211.13227).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/paint-by-example-exemplar-based-image-editing).

# Abstract
- Language-guided image editing has achieved great success recently
- In this paper, for the first time, we investigate exemplar-guided image editing for more precise control
- We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar
- However, the naive approach will cause obvious fusing artifacts
- We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image
- Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image
- The whole framework involves a single forward of the diffusion model without any iterative optimization
- We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.

# Paper Content

## Introduction
- Creative editing for photos has become a ubiquitous need due to advances in social media platforms
- AI-based techniques significantly lower the barrier of fancy image editing that traditionally requires specialized software and labor-intensive manual operations
- Deep neural networks can now produce compelling results for various low-level image editing tasks, such as image inpainting, composition, colorization, and aesthetic enhancement, by learning from richly available paired data
- A more challenging scenario, on the other hand, is semantic image editing, which intends to manipulate the high-level semantics of im-age content while preserving image realism
- Tremendous efforts have been made along this way, mostly relying on the semantic latent space of generative models, e.g., GANs
- However, even the detailed textual description inevitably introduces ambiguity and may not accurately reflect the user-desired effects; indeed, many fine-grained object appearances can hardly be specified by the plain language
- Hence, it is crucial to develop a more intuitive approach to ease fine-grained image editing for novices and non-native speakers
- In this work, we propose an exemplar-based image editing approach that allows accurate semantic manipulation on the image content according to an exemplar image provided by users or retrieved from the database
- As the saying goes, "a picture is worth a thousand words". We believe images better convey the user's desired image customization in a more granular manner than words
- This task is completely different from image harmonization, which mainly focuses on color and lighting correction when compositing the foreground objects, whereas we aim for a much more complex job: semantically transforming the exemplar, e.g., producing a varied pose, deformation or viewpoint, such that the edited content can be seamlessly implanted according to the image context
- In fact, ours automates the traditional image editing workflow where artists perform tedious transformations upon image assets for coherent image blending
- To achieve our goal, we train a diffusion model conditioned on the exemplar image
- Different from textguided models, the core challenge is that it is infeasible to collect enough triplet training pairs comprising source image, exemplar and corresponding editing ground truth. One workaround is to randomly crop the objects from the input image, which serves as the reference when training the inpainting model
- The model trained from such a selfreference setting, however, cannot generalize to real exemplars, since the model simply learns to copy and paste the reference object into the final output
- We identify several key factors that circumvent this issue
- One is to utilize a generative prior. Specifically, a pretrained text-to-image model has the ability to generate high-quality desired results, we leverage it as initialization to avoid falling into the copy-and-paste trivial solution
- However, a long time of finetuning may still cause the model to deviate from the prior knowledge and ultimately degenerate again. Hence, we introduce the information bottleneck for self-reference conditioning in which we drop the spatial tokens and only regard the global image embedding as the condition
- In this way, we enforce the network to understand the high-level semantics of the exemplar image and the context from the source image, thus preventing trivial results during the selfsupervised training
- Moreover, we apply aggressive augmentation on the self-reference image which can effectively reduce the training-test gap
- We further improve the editability of our approach in two aspects. One is that our training uses irregular random masks so as to mimic the casual user brush used in practical editing. We also prove that classifier-free guidance is beneficial to boost both the image quality and the style resemblance to the reference
- To the best of our knowledge, we are the first to address this semantic image composition problem where the reference is semantically transformed and harmonized before blending into another image

## Related Work
- Many methods have been proposed focusing on image harmonization to make the composite look more realistic.
- Traditional methods extract handcrafted features to match the color distribution.
- Recent works leverage deep semantic features to improve the robustness.
- A more recent work DCCF proposes four human comprehensible neural filters in a pyramid manner and achieves a state-of-the-art color harmonization result.
- However, they all assume that the foreground and the background are semantically harmonious and only adjust the composite in the low-level color space while keeping the structure unchanged.
- In this paper, we target at semantic image composition, taking the challenging semantic inharmony into consideration.
- Semantic image editing, to edit the high-level semantics of an image, is of great interest in the vision and graphics community due to its potential applications.
- Early works leverage pretrained GAN generators and text encoders to progressively optimize the image according to the text prompt.
- However, these GAN-based manipulation approaches struggle on editing images of complex scenes or various objects due to the limited modeling capability of GANs.
- The rapid rise and development of diffusion models have shown powerful capability in synthesizing high-quality and diverse images.
- Many works exploit diffusion models for text-driven image editing.
- For example, DiffusionCLIP, dreambooth, and Imagic finetune the diffusion models case-specifically for different text prompts.
- Blended Diffusion proposes a multi-step blended process to perform local manipulation using a userprovided mask.
- While these methods achieve remarkably impressive results, we argue that the language guidance still lacks precise control, whereas images can better express one's concrete ideas.
- As such in this work we are interested in exemplar-based image editing.

## Method
- We target at exemplar-based image editing that automatically merges the reference image (either retrieved from a database or provided by users) into a source image in a way that the merged image looks plausible and photo-realistic.
- Despite the recent remarkable success of text-based image editing, it is still difficult to use mere verbal descriptions to express complex and multiple ideas.
- While images, on the other hand, could be a better alternative for conveying people's intentions, as the proverb says: "a picture is worth a thousand words".
- Formally, denote the source image as x s ∈ R H×W ×3 , with H and W being the width and height respectively.
- The edit region could be a rectangular or an irregular shape (at least connected) and is represented as a binary mask m ∈ {0, 1} H×W where value 1 specifies the editable positions in x s .
- Given a reference image x r ∈ R H ×W ×3 containing the desired object, our goal is to synthesize an image y from {x s , x r , m}, so that the region where m = 0 remains as same as possible to the source image x s , while the region where m = 1 depicts the object as similar to the reference image x r and fits harmoniously.
- This task is very challenging and complex because it implicitly involves several non-trivial procedures.
- Firstly, the model requires understanding the object in the reference image, capturing both its shape and texture while ignoring the noise from the background.
- Secondly, it is critical to enable synthesizing a transformed view of the object (different pose, different size, different illumination etc.) that fits in the source image nicely.
- Thirdly, the model needs to inpaint the area around the object to generate a realistic photo, showing a smooth transition across the merging boundary.
- Last, the resolution of the reference image may be lower than the edit region. The model should involve super-resolution in the process.

### Preliminaries

### Model Designs
- Compressed representation: We re-analyze the difference between text condition and image condition. For text condition, the model is naturally compelled to learn semantics as text is an intrinsically semantic signal. In regards to image condition, it is very easy to remember instead of forgetting.
- Image prior: To further avoid the trivial solution of directly remembering the reference image, we leverage a well-trained diffusion model for initialization as a strong image prior. Specifically, we adopt a text-to-image generation model, Stable Diffusion [52], in consideration of two main reasons. First, it has a strong capability to generate high-quality in-the-wild images, thanks to the property that given any vector lying in its latent space will lead to a plausible image. Second, a pretrained CLIP [45] model is utilized to extract the language information, which shares a similar representation to our adopted CLIP image embedding, making it a good initialization.
- Mask shape augmentation: On the other hand, the mask region m from the bounding box ensures that the reference image contains a whole object. As a result, the generator learns to fill an object as completely as possible. However, this may not hold in practical scenarios. To address this, we generate an arbitrarily shaped mask based on the bounding box and use it in training. Specifically, for each edge of the bounding box, we first construct a Bessel curve to fit it, then we sample 20 points on this curve uniformly, and randomly add 1 − 5 pixel offsets to their coordinates. Finally, we connect these points with straight lines sequentially to form an arbitrarily shaped mask. The random distortions D on the mask m break the inductive bias, reducing the gap between training and testing.
- i.e., Mask shape augmentation: On the other hand, the mask region m from the bounding box ensures that the reference image contains a whole object. As a result, the generator learns to fill an object as completely as possible. However, this may not hold in practical scenarios. To address this, we generate an arbitrarily shaped mask based on the bounding box and use it in training. Specifically, for each edge of the bounding box, we first construct a Bessel curve to fit it, then we sample 20 points on this curve uniformly, and randomly add 1 − 5 pixel offsets to their coordinates. Finally, we connect these points with straight lines sequentially to form an arbitrarily shaped mask. The random distortions D on the mask m break the inductive bias, reducing the gap between training and testing.

### Comparisons
- Uses CLIP model to provide gradients to guide the diffusion sampling process.
- Slightly modify the Blended Diffusion by using the reference image to calculate the CLIP loss, denoted as Blended Diffusion (image).
- Stable Diffusion. We use the text prompt as condition to represent the reference image, and inpaint the mask region.
- We also choose the state-of-the-art image harmonization method DCCF as baseline.
- Considering it can only fuse a foreground image to background, we first use an unconditional image inpainting model LAMA [61] to inpaint the whole mask region, then extract the foreground of reference image through an additional semantic mask, and finally harmonize it into the source image.

### Ablation Study
- Introduces four key techniques for achieving high-quality exemplar-based image editing-
- Uses image prior, strong augmentation, information bottleneck and classifier-free guidance to achieve better image quality-
- Results show that the image quality improves according to the lower FID score, but copy-and-paste issue still exists-
- By leveraging the information bottleneck technique, the boundary artifacts are eliminated-
- Adding augmentations also partially alleviates the copy-and-paste issue, while classifier-free guidance makes the generated region more similar to the reference input.

### From Language to Image Condition
- Language can be controlled to generate similar results to reference images
- There is a large gap between language and image-guided results

### In-the-wild Image Editing
- Our method can generate multiple outputs from the same input
- The generated outputs vary, but they all maintain the identity of the reference image

## Conclusion
- Our approach can handle a wide variety of reference images and source images.
- Our training pipeline is robust and achieves high performance on in-the-wild images.
- Our method can generate results that are semantically consistent with the input reference images in high perceptual quality.
- Our framework can synthesize realistic and diverse results from the same source image and exemplar image.
