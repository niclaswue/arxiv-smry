---
title: "Multi-Concept Customization of Text-to-Image Diffusion"
date: 2022-12-08T18:57:02.000Z
author: "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2212-04488v1_wiQYYWmfF.jpg" # image path/url
    alt: "Multi-Concept Customization of Text-to-Image Diffusion" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.04488)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.04488).


# Abstract
- Generative models can produce high-quality images of concepts learned from a large-scale database
- Can we teach a model to quickly acquire a new concept, given a few examples?
- Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization.
- Our fine-tuned model generates variations of multiple, new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms several baselines and concurrent works, regarding both qualitative and quantitative evaluations, while being memory and computationally efficient.

# Paper Content

## Introduction
- Recently released text-to-image models have represented a watershed year in image generation.
- By simply querying a text prompt, users are able to generate images of unprecedented quality.
- Such systems can generate a wide variety of objects, styles, and scenes -seemingly "anything and everything".
- However, despite the diverse, general capability of such models, users often wish to synthesize specific concepts from their own personal lives.
- For example, loved ones such as family, friends, pets, or personal objects and places, such as a new sofa or a recently visited garden, make for intriguing concepts.
- As these concepts are by nature personal, they are unseen during large-scale model training.
- Describing these concepts after the fact, through text, is unwieldy and unable to produce the personal concept with sufficient fidelity.
- This motivates a need for model customization.
- Given the few user-provided images, can we augment existing text-toimage diffusion models with the new concept (for example, their pet dog or a "moongate" as shown in Figure 1)?
- The fine-tuned model should be able to generalize and compose them with existing concepts to generate new variations.
- This poses a few challenges -first, the model tends to forget [14,38,55] or change [37,43] the meanings of existing concepts: e.g., the meaning of "moon" being lost when adding the "moongate" concept.
- Secondly, the model is prone to overfit the few training samples and reduce sampling variations.
- Moreover, we study a more challenging problem, compositional fine-tuning -the ability to extend beyond tuning for a single, individual concept and compose multiple concepts together, e.g., pet dog in front of moongate (Figure 1).
- Learning multiple new concepts poses additional challenges, such as concept mixing and concept omission.
- In this work, we propose a fine-tuning technique, Custom Diffusion for text-to-image diffusion models.
- Our method is computationally and memory efficient.
- To overcome the above-mentioned challenges, we identify a small subset of model weights, namely the key and value mapping from text to latent features in the cross-attention layers [5,73].
- Fine-tuning these is sufficient to update the model with the new concept.
- To prevent model forgetting, we use a small set of real images with similar captions as the target images.
- We also introduce augmentation during fine-tuning, which leads to faster convergence and improved results.
- To inject multiple concepts, our method supports training on both simultaneously or training them separately and then merging.
- We build our method on Stable Diffusion [1] and experiment on various datasets with as few as four training images.
- For adding single concepts, our method shows better text alignment and visual similarity to the target images than concurrent works and baselines.
- More importantly, our method can compose multiple new concepts efficiently, whereas concurrent methods struggle and often omit one.
- Finally, our method only requires storing a small subset of parameters (3% of the model weights) and reduces the fine-tuning time (6 minutes on 2 A100 GPUs, 2 − 4× faster compared to concurrent works).

## Related Work
- Generative models aim to synthesize samples from a data distribution, given a set of training examples
- To improve controllability, these models can be conditioned on a class, image, or text prompt
- Our work mainly relates to textconditioned synthesis
- While earlier works were limited to a few classes, recent text-to-image models have demonstrated remarkable generalization ability
- However, such models are by nature generalists and struggle to generate specific instances like personal toys or rare categories
- We aim to adapt these models to become specialists in new concepts
- Image and model editing: While generative models can sample random images, a user often wishes to edit a single, specific image
- Several works aim at leveraging the capabilities of generative models, such as GANs or diffusion models towards editing
- A challenge is representing the specific image in the pre-trained model, and such methods employ per-image or per-edit optimization
- A closely related line of work edits a generative model directly
- Whereas these methods aim to customize GANs, our focus is on text-to-image models
- Transfer learning: A method of efficiently producing a whole distribution of images is leveraging a pretrained model and then using transfer learning
- For example, one can adapt photorealistic faces into cartoons
- Different from these works, which focus on tuning whole models to single domains, we wish to acquire multiple new concepts without catastrophic forgetting
- By preserving the millions of concepts captured in large-scale pretraining, we can synthesize the new concepts in composition with these existing concepts
- Relatedly, several methods propose to train adapter modules for large-scale models in the discriminative setting
- In contrast, we adapt a small number of existing parameters and do not require additional parameters
- Adapting text-to-image models: Similar to our goals, two concurrent works, DreamBooth and Textual Inversion, adopt transfer learning to text-to-image diffusion models
- Our work differs in several aspects. First, our work tackles a challenging setting: compositional fine-tuning of multiple concepts, where concurrent works struggle. Second, we only fine-tune a subset of cross-attention layer parameters, which significantly reduces the fine-tuning time. We find these design choices lead to better results, validated by automatic metrics and human preference studies.

## Method
- Our proposed method for model fine-tuning only updates a small subset of weights in the cross-attention layers of the model.
- In addition, we use a regularization set of real images to prevent overfitting on the few training samples of the target concepts.
- In this section, we explain our design choices and final algorithm in detail.

### Single-Concept Fine-tuning
- Stable Diffusion is a model that is built on the Latent Diffusion Model
- LDM first encodes images into a latent representation, using hybrid objectives of VAE, Patch-GAN, and LPIPS
- Learning objective of diffusion models is to approximate the original data distribution q(x 0 ) with p θ (x 0 ): where x 1 to x T are latent variables of a forward Markov chain s.t.
- Diffusion models are a class of generative models that aim to approximate the original data distribution q(x 0 ) with p θ (x 0 ): where x 1 to x T are latent variables of a forward Markov chain s.t. and p θ (x 0 ) is the predicted distribution
- The rate of change of weights is analyzed following Li et al. and the results suggest that the cross-attention layer parameters have relatively higher ∆ compared to the rest of the parameters
- Model fine-tuning is done by cross-attention block modifies the latent features of the network according to the condition features, i.e., text features in the case of text-to-image diffusion models

### Multiple-Concept Compositional Fine-tuning
- Our method is faster than joint training
- We restrict the weight update to cross-attention key and value parameters
- Constrained optimization to merge concepts

## Experiments
- The method can generate concepts from multiple datasets
- The method can generate concepts from multiple datasets with different training samples
- The method can generate concepts from multiple datasets with different training samples and different prompts
- The method can generate concepts from multiple datasets with different prompts and different token embedding
- The method can generate concepts from multiple datasets with different token embedding and different concurrent works

### Single-Concept Fine-tuning Results
- We test each fine-tuned model on a set of challenging prompts
- Our method, Custom Diffusion, has lower KID than most baselines
- Computational requirements are low

### Multiple-Concept Fine-tuning Results
- Our method has higher visual similarity with the personal cat and chair images shown in the first column while following the text condition.
- DreamBooth omits the cat in 3 out of 4 images, whereas our method generates both cats and wooden pots.
- Our method generates the target flower in the wooden pot while maintaining the visual similarity to the target images.
- Generating the target table and chair together in a garden. For all settings, our optimization-based approach and joint training perform better than DreamBooth, and joint training performs better than the optimization-based method.

### Human Preference Study
- We perform the human preference study using Amazon Mechanical Turk.
- We do a paired test of our method with Figure 8.
- Compared to DreamBooth, Textual Inversion, and our w/ fine-tune all baseline, our method lies further along the upper right corner (with less variance).
- There exists a trade-off between high similarity to target images vs. text-alignment on new prompts.
- Keeping in consideration this trade-off, our method is on-par or better than the baselines.
- For multi-concept both joint training and optimization based method are better than other baselines.

### Ablation and Applications
- Our method can be used to learn new styles
- Fine-tuning on a style
- Can be used by existing image-editing methods

## Discussion and Limitations
- The method can generate novel variations of the fine-tuned concept in new contexts while preserving the visual similarity with the target images
- The method can coherently compose multiple new concepts in the same scene
- Composing increasing three or more concepts together is also challenging

## A. Multi-Concept Optimization Based Method
- To compose multiple concepts together, we solved a constrained least squares problem (introduced in Section 3.2, Eqn. 4).
- We solved the problem in closed form as shown in Eqn. 5 in the main paper.
- Here, we show the full derivation.
- First restate the objective below. Here, the matrix norms are the Frobenius norm, W ∈ R o×d is the matrix from the pretrained model, C ∈ R s×d is the text features of dimension d. These are compiled of s target words across all N concepts, with all captions for each concept flattened out and concatenated. Similarly, C reg ∈ R sreg×d consists of text features of ∼ 1000 randomly sampled captions for regularization.
- By using the method of Lagrange multipliers [8], we need to minimize the following objective: here v ∈ R s×o is the Lagrangian multiplier corresponding to the constraints.
- Differentiating the above objective and equating it to 0, we obtain:
- We assume C reg is non-degenerate.
- Using the above solution in Eqn. 6, W C = V , we obtain:
- In this section, we show more experiments, including ablation and analysis of the limitations of our method.
- Analysis of singular values of the difference of the key and value projection matrices in the cross-attention layers between pretrained and fine-tuned model.
- As shown in the plot, the singular values drop to 0, suggesting that we can approximate the difference matrix with a low-rank matrix.
- Figure 13 and 18 shows the quantitative and qualitative results with decreasing compression ratio.
- Even with an approximation using only the top 60% of singular values (5× compression in model storage), the performance is similar. But as we increase the compression to only include the top 1% of singular values, the image-alignment decreases.
- Top k% implies singular values till the rank where cumulative sum is k% of total sum of singular values.
- We also attempted to enforce a low-rank update to the key, value matrices during fine-tuning but observed the results to be sub-optimal [18].
- Choice of V * .
- In all experiments, we initialized the unique modifier token with the token-id 42170.
- During joint training with two concepts, we initialize the other with tokenid 47629 in the pretrained CLIP tokenizer used in Stable Diffusion.
- We ablate our choice of V * with -(1) Random initialization with the mean and standard deviation of existing token embeddings and then optimizing the modifier token, (2) not optimizing the modifier token, once initialized with the rarely occurring token.
- The quantitative results are shown in Table 4.
- We observe that not optimizing V * leads to significantly lower image-alignment. Compared to random initialization of V * , our method has higher image-alignment and lower text-alignment. But we observe that the generated samples with the prompt a {category} shift more towards target image distribution of V * category, compared to our final method, as shown in Figure 15.
- Limitations of our multi-concept fine-tuning.
- As shown in Figure 11 in the paper, our method fails at difficult compositions like generating personal cat and dog in the same scene.
- We observe that the pretrained model also struggles with such compositions and hypothesize that our model inherits these limitations.
- We analyze the attention map of each token on the latent image features in Figure 16.
- The "dog" and "cat" token attention maps are largely overlapping for both our and pretrained models, which might lead to worse composition.
- For example, fine-tuned model on V * dog generates images with saturation artifact for the prompt a dog as shown in Figure 15.

## D. Implementation and Experiment Details
- Our method is better than baselines when averaged across datasets
- We reduce the learning rate to 8 × 10 −6 for multi-concept, which works better than 8 × 10 −5 and train for 500 steps with a batch size of 8
- For single-concept, we train for 5000 steps but pick the best checkpoint at 3000 iterations
