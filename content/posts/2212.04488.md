---
title: "Multi-Concept Customization of Text-to-Image Diffusion"
date: 2022-12-08T18:57:02.000Z
author: "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-04488v1.webp" # image path/url
    alt: "Multi-Concept Customization of Text-to-Image Diffusion" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.04488)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.04488).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multi-concept-customization-of-text-to-image).

# Abstract
- While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items).
- Can we teach a model to quickly acquire a new concept, given a few examples?
- Furthermore, can we compose multiple new concepts together?
- We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models.
- We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~6 minutes).
- Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization.
- Our fine-tuned model generates variations of multiple, new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms several baselines and concurrent works, regarding both qualitative and quantitative evaluations, while being memory and computationally efficient.

# Paper Content

## Introduction
- Recently released text-to-image models can generate images of unprecedented quality.
- By simply querying a text prompt, users are able to generate images of unprecedented quality.
- However, despite the diverse, general capability of such models, users often wish to synthesize specific concepts from their own personal lives.
- For example, loved ones such as family, friends, pets, or personal objects and places, such as a new sofa or a recently visited garden, make for intriguing concepts.
- As these concepts are by nature personal, they are unseen during large-scale model training.
- Describing these concepts after the fact, through text, is unwieldy and unable to produce the personal concept with sufficient fidelity.
- This motivates a need for model customization.
- Given the few user-provided images, can we augment existing text-toimage diffusion models with the new concept (for example, their pet dog or a "moongate" as shown in Figure 1)?
- The fine-tuned model should be able to generalize and compose them with existing concepts to generate new variations.
- This poses a few challenges -first, the model tends to forget [14,38,55] or change [37,43] the meanings of existing concepts: e.g., the meaning of "moon" being lost when adding the "moongate" concept.
- Secondly, the model is prone to overfit the few training samples and reduce sampling variations.
- Moreover, we study a more challenging problem, compositional fine-tuning -the ability to extend beyond tuning for a single, individual concept and compose multiple concepts together, e.g., pet dog in front of moongate (Figure 1).
- Learning multiple new concepts poses additional challenges, such as concept mixing and concept omission.
- In this work, we propose a fine-tuning technique, Custom Diffusion for text-to-image diffusion models.
- Our method is computationally and memory efficient.
- To overcome the above-mentioned challenges, we identify a small subset of model weights, namely the key and value mapping from text to latent features in the cross-attention layers [5,73].
- Fine-tuning these is sufficient to update the model with the new concept.
- To prevent model forgetting, we use a small set of real images with similar captions as the target images.
- We also introduce augmentation during fine-tuning, which leads to faster convergence and improved results.
- To inject multiple concepts, our method supports training on both simultaneously or training them separately and then merging.
- We build our method on Stable Diffusion [1] and experiment on various datasets with as few as four training images.
- For adding single concepts, our method shows better text alignment and visual similarity to the target images than concurrent works and baselines.
- More importantly, our method can compose multiple new concepts efficiently, whereas concurrent methods struggle and often omit one.
- Finally, our method only requires storing a small subset of parameters (3% of the model weights) and reduces the fine-tuning time (6 minutes on 2 A100 GPUs, 2 − 4× faster compared to concurrent works).

## Related Work
- Generative models aim to synthesize samples from a data distribution, given a set of training examples.
- To improve controllability, these models can be conditioned on a class, image, or text prompt.
- Our work mainly relates to textconditioned synthesis.
- While earlier works were limited to a few classes, recent text-to-image models have demonstrated remarkable generalization ability.
- However, such models are by nature generalists and struggle to generate specific instances like personal toys or rare categories, e.g., "moongate".
- We aim to adapt these models to become specialists in new concepts.
- Image and model editing. While generative models can sample random images, a user often wishes to edit a single, specific image.
- Several works aim at leveraging the capabilities of generative models, such as GANs or diffusion models towards editing.
- A challenge is representing the specific image in the pre-trained model, and such methods employ per-image or per-edit optimization.
- A closely related line of work edits a generative model directly.
- Whereas these methods aim to customize GANs, our focus is on text-to-image models.
- Transfer learning. A method of efficiently producing a whole distribution of images is leveraging a pretrained model and then using transfer learning.
- For example, one can adapt photorealistic faces into cartoons.
- To adapt with just a few training images, efficient training techniques are often useful.
- Different from these works, which focus on tuning whole models to single domains, we wish to acquire multiple new concepts without catastrophic forgetting.
- By preserving the millions of concepts captured in large-scale pretraining, we can synthesize the new concepts in composition with these existing concepts.
- Relatedly, several methods propose to train adapter modules for large-scale models in the discriminative setting.
- In contrast, we adapt a small number of existing parameters and do not require additional parameters.

## Method
- Our proposed method for model fine-tuning only updates a small subset of weights in the cross-attention layers of the model
- In addition, we use a regularization set of real images to prevent overfitting on the few training samples of the target concepts

### Single-Concept Fine-tuning
- Stable Diffusion [1]
- Latent Diffusion Model (LDM) [60]
- VAE [34]
- Patch-GAN [30]
- LPIPS [85]
- Forward Markov Chain [28]
- Learning Objective of Diffusion Models
- Diffusion Models [28,68]
- Gaussian Image (or Latent) [69]
- Fine-tuning
- Change in Parameters for Layers in Fine-tuned Model
- Regularization Dataset
- High Similarity in Feature Space between Target Text Prompt and Selected Regularization Images

### Multiple-Concept Compositional Fine-tuning
- Joint training on multiple concepts
- For fine-tuning with multiple concepts, we combine the training datasets for each individual concept and train them jointly with our method
- To denote the target concepts, we use different modifier tokens, V * i , initialized with different rarely-occurring tokens and optimize them along with cross-attention key and value matrices for each layer
- As shown in Figure 7, restricting the weight update to cross-attention key and value parameters leads to significantly better results for composing two concepts compared to methods like DreamBooth, which fine-tune all the weights
- Constrained optimization to merge concepts
- Let set {W k 0,l , W v 0,l } L l=1 represent the key and value matrices for all L cross-attention layers in the pretrained model and {W k n,l , W v n,l } L l=1 represent the corresponding updated matrices for added concept n ∈ {1 • • • N }
- As our subsequent optimization applies to all layers and key-value matrices, we will omit superscripts {k, v} and layer l for notational clarity
- We formulate the composition objective as the following constrained least squares problem: Here, C ∈ R s×d is the text features of dimension d. These are compiled of s target words across all N concepts, with all captions for each concept flattened out and concatenated. Similarly, C reg ∈ R sreg×d consists of text features of ∼ 1000 randomly sampled captions for regularization
- Intuitively, the above formulation aims to update the matrices in the original model, such that the words in target captions in C are mapped consistently to the values obtained from finetuned concept matrices
- The above objective can be solved in closed form, assuming C reg is non-degenerate and the solution exists, by using the method of Lagrange multipliers
- We show full derivation in the Appendix A.
- Compared to joint training, our optimization-based method is faster ( ∼ 2 seconds) if each individual fine-tuned model exists
- Our proposed methods lead to the coherent generation of two new concepts in a single scene, as shown in Section 4.2.

## Experiments
- The method can generate concepts from multiple datasets
- The method can generate concepts from multiple datasets with different training samples
- The method can generate concepts from multiple datasets with different training samples and different prompts
- The method can generate concepts from multiple datasets with different prompts and different evaluation metrics
- The method can generate concepts from multiple datasets with different evaluation metrics and different baselines

### Single-Concept Fine-tuning Results
- We test each fine-tuned model on a set of challenging prompts.
- Our method, Custom Diffusion, has lower KID than most baselines, which suggests less overfitting to the target concept.
- Training time of our method is ∼ 6 minutes (2 A100 GPUs), compared to 20 minutes for Ours (w/ fine-tune all) (4 A100s), 20 minutes for Textual Inversion (2 A100s), and ∼ 1 hour for DreamBooth (4 A100s).
- Also, since we update only 75MB of weights, our method has low memory requirements for storing each concept model.
- We keep the batch size fixed at 8 across all experiments.

### Multiple-Concept Fine-tuning Results
- Our method has higher visual similarity with the personal cat and chair images shown in the first column while following the text condition.
- DreamBooth omits the cat in 3 out of 4 images, whereas our method generates both cats and wooden pots.
- Our method generates the target flower in the wooden pot while maintaining the visual similarity to the target images.
- Generating the target table and chair together in a garden.
- For all settings, our optimization-based approach and joint training perform better than DreamBooth, and joint training performs better than the optimization-based method.

### Human Preference Study
- We perform the human preference study using Amazon Mechanical Turk.
- We do a paired test of our method with Figure 8.
- Compared to DreamBooth, Textual Inversion, and our w/ fine-tune all baseline, our method lies further along the upper right corner (with less variance).
- There exists a trade-off between high similarity to target images vs. text-alignment on new prompts.
- Keeping in consideration this trade-off, our method is on-par or better than the baselines.
- For multi-concept both joint training and optimization based method are better than other baselines.

### Ablation and Applications
- Our method can be used to learn new styles
- Fine-tuning on a style
- Can be used by existing image-editing methods

## Discussion and Limitations
- Our method can generate novel variations of the fine-tuned concept in new contexts while preserving the visual similarity with the target images.
- We only need to save a small subset of model weights.
- Our method can coherently compose multiple new concepts in the same scene.
- As shown in Figure 11, difficult compositions, e.g., a pet dog and a pet cat, remain challenging. In this case, the pretrained model also faces a similar difficulty, and our model inherits these limitations.
- Comp composing increasing three or more concepts together is also challenging.
- We show more analysis and visualization in the Appendix B.
- In Section A, we show the derivation of our optimizationbased method for merging multiple concepts into a single model.
- In Section B, we show additional experiment results of our method.
- We then provide more evaluation results, for example, the trend with training iterations, in Section C and implementations details in Section D.
- Finally, we discuss the societal implications of our method in Section E.
