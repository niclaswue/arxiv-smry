---
title: "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images"
date: 2022-09-22T17:16:19.000Z
author: "Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, Sanja Fidler"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-11163v1.webp" # image path/url
    alt: "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.11163)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.11163).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/get3d-a-generative-model-of-high-quality-3d).

# Abstract
- As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident.
- In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications.
- Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial.
- In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high-fidelity textures.
- We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textures, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.

# Paper Content

## Introduction
- 3D content is becoming increasingly important for several industries
- Manual creation of 3D assets is very time-consuming and requires specific technical knowledge and artistic modeling skills
- One of the main challenges is thus scale - while one can find 3D models on 3D marketplaces, creating many 3D models to populate a game or movie with a crowd of characters that all look different still takes a significant amount of artist time
- To facilitate the content creation process and make it accessible to a variety of (novice) users, generative 3D networks that can produce high-quality and diverse 3D assets have recently become an active area of research
- However, to be practically useful for current real-world applications, 3D generative models should ideally fulfill the following requirements: (a) They should have the capacity to generate shapes with detailed geometry and arbitrary topology, (b) The output should be a textured mesh, which is a primary representation used by standard graphics software packages such as Blender and Maya, and (c) We should be able to leverage 2D images for supervision, as they are more widely available than explicit 3D shapes.
- Prior work on 3D generative modeling has focused on subsets of the above requirements, but no method to date fulfills all of them.
- GET3D, a Generative model for 3D shapes that directly outputs Explicit Textured 3D meshes with high geometric and texture detail and arbitrary mesh topology, is proposed to address this challenge.

## Related Work
- 2D generative models have achieved photorealistic quality in high-resolution image synthesis
- 3D Generative Models: early approaches aimed to directly extend the 2D CNN generators to 3D voxel grids, but the high memory footprint and computational complexity of 3D convolutions hinder the generation process at high resolution
- 3D-Aware Generative Image Synthesis: inspired by the success of neural volume rendering and implicit representations, recent work started tackling the problem of 3D-aware image synthesis
- GIRAFFE and StyleNerf improve the training and rendering efficiency by performing neural rendering at a lower resolution and then upsampling the results with a 2D CNN
- However, the performance gain comes at the cost of a reduced multi-view consistency
- By utilizing a dual discriminator, EG3D can partially mitigate this problem
- In contrast, GET3D directly outputs textured 3D meshes that can be readily used in standard graphics engines.

## Method
- GET3D framework for synthesizing textured 3D shapes
- Geometry branch outputs surface mesh of arbitrary topology
- Texture branch produces texture field that can be queried at surface points to produce colors
- Entire process is differentiable, allowing for adversarial training from images

### Generative Model of 3D Textured Meshes
- We aim to learn a 3D generator to map a sample from a Gaussian distribution to a mesh.
- Since the same geometry can have different textures, and the same texture can be applied to different geometries, we sample two random input vectors.
- Following StyleGAN [34,35,33], we then use non-linear mapping networks to map z 1 and z 2 to intermediate latent vectors which are further used to produce styles that control the generation of 3D shapes and texture, respectively.
- We formally introduce the generator for geometry in Sec. and the texture generator in Sec. Figure 2.

### Geometry Generator
- DMTet is a signed distance field representation of geometry
- This representation allows for explicit meshes to be generated with arbitrary topology and genus
- We first use 3D convolutional layers to generate a feature volume conditioned on w 1
- We then query the feature at each vertex v i ∈ V T using trilinear interpolation and feed it into MLPs that outputs the SDF value s i and the deformation ∆v i
- In cases where modeling at a high-resolution is required (e.g. motorbike with thin structures in the wheels), we further use volume subdivision following [60]

### Texture Generator
- Representing the texture field with a tri-plane representation
- Conditioning the mapping of the texture field on geometry
- Sampling the texture field at the locations of the surface points

### Differentiable Rendering and Training
- Uses a differentiable renderer to render extracted 3D mesh and texture field into 2D images
- Uses a discriminator to try to distinguish image from real object or rendered from generated object
- Uses a cross-entropy loss to remove internal floating faces that are not visible in any of the views

## Experiments
- We compare the quality of 3D textured meshes generated by GET3D to the existing methods
- We ablate our design choices in Section
- We demonstrate the flexibility of GET3D by adapting it to downstream applications

### Experiments on Synthetic Datasets
- Datasets of 7497, 6778, and 337 shapes are used for evaluation on ShapeNet.
- Each category is split into training (70%), validation (10%), and test (20%) sets.
- Shapes in the training set are randomly sampled to render the dataset.
- Metrics used to evaluate the quality of the generated shapes are CD and LFD.
- GET3D outperforms PointFlow and OccNet in terms of both geometry and texture quality.
