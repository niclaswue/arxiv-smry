---
title: "Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe"
date: 2022-09-12T15:29:13.000Z
author: "Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng, Hao Tian, Xizhou Zhu, Li Chen, Yulu Gao, Xiangwei Geng, Jia Zeng, Yang Li, Jiazhi Yang, Xiaosong Jia, Bohan Yu, Yu Qiao, Dahua Lin, Si Liu, Junchi Yan, Jianping Shi, Ping Luo"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-05324v2.webp" # image path/url
    alt: "Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.05324)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.05324).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/delving-into-the-devils-of-bird-s-eye-view).

# Abstract
- Learning powerful representations in bird's-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia.
- Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view.
- As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance.
- BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control.
- The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) how to adapt and generalize algorithms as sensor configurations vary across different scenarios.
- In this survey, we review the most recent work on BEV perception and provide an in-depth analysis of different solutions.
- Furthermore, several systematic designs of BEV approach from the industry are depicted as well. Furthermore, we introduce a full suite of practical guidebook to improve the performance of BEV perception tasks, including camera, LiDAR and fusion inputs. At last, we point out the future research directions in this area.

# Paper Content

## INTRODUCTION
- The recognition task in autonomous driving is essentially a 3D geometry reconstruction to the physical world
- Compared to its front-view or perspective view counterpart, which is broadly investigated [1,2] in 2D vision domains, BEV representation bears several inherent merits. First, it has no occlusion or scale problem as commonly existent in 2D tasks. Recognizing vehicles with occlusion or crosstraffic could be better resolved. Moreover, representing objects or road elements in such form would favorably make it convenient for subsequent modules (e.g. planning, control) to develop and deploy.
- In this survey, we term BEV Perception to indicate all vision algorithms in sense of the BEV view representation for autonomous driving.

## Big Picture at a Glance
- Based on the input data, BEV perception research is divided into three parts: vision-centric algorithms for 3D object detection or segmentation from multiple surrounding cameras, detection or segmentation task from point cloud input, and fusion mechanisms from multiple sensor inputs.
- The concept of BEV perception lies in the middle of these three levels, where BEV perception algorithms indicate accordingly based on different combinations from the sensor input layer, fundamental task, and product scenario.

## Motivation to BEV Perception Research
- The first aspect is the need for BEV to satisfy the needs of the drivers.
- The second aspect is the need for BEV to be perceived as safe.
- The third aspect is the need for BEV to be perceived as environmentally friendly.
- The first aspect is the need for BEV to satisfy the needs of the drivers.
- The second aspect is the need for BEV to be perceived as safe.
- The third aspect is the need for BEV to be perceived as environmentally friendly.

## Significance.
- BEV perception generates a real and meaningful impact on academia and/or society
- The gap between camera or vision based solutions over LiDAR or fusion based counterparts is over 20% in terms of NDS on nuScenes dataset
- The gap is even more than 30% on Waymo benchmark
- From the academic perspective, the very essence of designing a camera-based pipeline such that it outperforms LiDAR, is better to understand the view transformation process from a 2D, appearance input to a 3D, geometry output.
- How to transfer camera features into geometry representations as does in point cloud leaves a meaningful impact for the academic society.
- On the industrial consideration, the cost of a suite of LiDAR equipment into SDV is expensive; OEMs (Original Equipment Manufacturer, e.g., Ford, BMW, etc.) prefer a cheap as well as accurate deployment for software algorithms.
- Improving camera-only algorithms to LiDAR's naturally fall into this goal since the cost of a camera is usually 10x times cheaper than LiDAR.
- Moreover, camera based pipeline can recognize long-range distance objects and color-based road elements (e.g., traffic lights), both of which LiDAR approaches are incapable of.
- Although there are several different solutions for camera and LiDAR based perception, BEV representation is one of the best candidates for LiDAR based methods in terms of superior performance and industry-friendly deployment.
- Moreover, recent trends show that BEV representation also has huge progress for multi-camera inputs.

## Space.
- There are open problems or caveats in BEV perception that require substantial innovation
- The gist behind BEV perception is to learn a robust and generalizable feature representation from both camera and LiDAR inputs
- This is easy in LiDAR branch as the input (point cloud) bears such a 3D property. This is non-trivial otherwise in the camera branch, as learning 3D spatial information from monocular or multi-view settings is difficult
- While we see there are some attempt to learn better 2D-3D correspondence by pose estimation [9] or temporal motion [10], the core issue behind BEV perception requires a substantial innovation of depth estimation from raw sensor inputs, esp. for the camera branch
- Another key problem is how to fuse features in early or middle stage of the pipeline. Most sensor fusion algorithms treat the problem as a simple object-level fusion or naive feature concatenation along the blob channel. This might explain why some fusion algorithms behave inferior to LiDAR-only solutions, due to the misalignment or inaccurate depth prediction between camera and LiDAR
- How to align and integrate features from multi-modality input plays a vital role and thereby leaves extensive space to innovate

## Contributions
- Review the whole picture of BEV perception research in recent years
- Examine the core issues in BEV perception
- Provide a practical cookbook for improving BEV perception performance

## BACKGROUND IN 3D PERCEPTION
- Conventional approaches to perform perception tasks, including monocular camera based 3D object detection, LiDAR based 3D object detection and segmentation, and sensor fusion strategies are reviewed.
- Principal datasets in 3D perception are introduced, including KITTI dataset [11], nuScenes dataset [7] and Waymo Open Dataset [8].

## Task Definition and Related Work
- Monocular camera-based methods take an RGB image as input and attempt to predict the 3D location and category of each object
- Due to estimating depth from a single image being an ill-posed problem, typically monocular camear-based methods have inferior performance than LiDAR-based methods
- LiDAR Detection and Segmentation
- Sensor Fusion

## Datasets and Metrics
- The paper introduces some popular autonomous driving datasets.
- The paper summarizes the main statistics of prevailing benchmarks for BEV perception.
- Most of the datasets can be adopted in different tasks.
- A consensus reached that sensors with multiple modes and various annotations are required.
- Similar to the Kaggle and EvalAI leaderboards, we reveal the total number of submissions on each dataset to indicate the popularity of a certain dataset.

## Evaluation Metrics
- LET-3D-APL is used as the metric instead of 3D-AP
- Compared with the 3D intersection over union (IoU), the LET-3D-APL allows longitudinal localization errors of the predicted bounding boxes up to a given tolerance
- LET-3D-APL penalizes longitudinal localization errors by scaling the precision using the localization affinity
- The definition of LET-3D-APL is mathematically defined as: where p L (r) indicates the longitudinal affinity weighted precision value, the p(r) means the precision value at recall r, and the multiplier a l is the average longitudinal affinity of all matched predictions treated as T P (true positive).

## NDS.

## METHODOLOGY OF BEV PERCEPTION
- There is a trend of research being published in top-tiered venues, such as IEEE Transactions on Robotics and Automation
- There is a significant performance gain for 3D object detection and segmentation when compared to traditional methods
- There is a trend of research being published in top-tiered venues, such as IEEE Transactions on Robotics and Automation

## Method

## BEV Camera

## General Pipeline
- Inverse perspective mapping is proposed to map pixels onto the BEV plane
- Lift-splat-shoot is the first method to predict depth distribution of image features
- Other works develop different method to conduct view transformation
- Given the view transformation methods from perspective view to BEV, Fig. 5b shows the general pipeline for fusing image and point cloud data.

## Methods

## Modality
- Few attempts have been made to solve 3D pretraining
- Successful 2D vision Transformer techniques could be applied to 3D space
- Future investigation is needed in 3D perception

## View Transformation
- Recent research has focused on view transformation module [3,4,10,26,47,48,49,51,56,59], where 3D information is constructed from either 2D feature or 3D prior assumption.
- The 2D-3D method is introduced by LSS [57], where it predicts depth distribution per grid on 2D feature, then "lift" the 2D feature per grid via the corresponding depth to voxel space, and perform downstream tasks following LiDAR-based methods.
- CaDDN [46] employs a similar network to predict depth distribution (categorical depth distribution), squeezes the voxel-space feature down to BEV space, and performs 3D detection at the end.
- The second branch (3D to 2D) can be originated back to thirty years ago, when IPM [93] formulated the projection from 3D space to 2D space conditionally hypothesizing that the corresponding points in 3D space lie on a horizontal plane.
- Stereo methods such as LIGA Stereo [92] and DSGN [65] utilize this strong prior and perform on par with LiDAR-based alternatives on KITTI leaderboard [11].

## Discussion on BEV and perspective methods
- At the very beginning of camera-only 3D perception, the main focus is how to predict 3D object localization from perspective view, a.k.a. 2D space.
- Later, some research reached to BEV representation, since under this view, it is easy to tackle the problem where objects with the same size in 3D space have very different size on image plane due to the distance to camera.
- This series of work [43,46,65,89,92] either predict depth information or utilize 3D prior assumption to compensate the loss of 3D information in camera input.
- While recent BEV-based methods [3,4,5,47,49,91,101] has taken the 3D perception world by storm, it is worth noticing that this success has been beneficial from mainly three parts.
- The first reason is the trending nuScenes dataset [7], which has multi-camera setting and it is very suitable to apply multi-view feature aggregation under BEV.
- The second reason is that most camera-only BEV perception methods have gained a lot of help from LiDAR-based methods [44,45,67,84,85,102,103] in form of detection head and corresponding loss design.
- The third reason is that the long-term development of monocular methods [82,83,100] has flourished BEV-based methods a good starting point in form of handling feature representation in the perspective view.

## BEV LiDAR

## Pre-BEV Feature Extraction
- Voxel-based methods voxelize points into discrete grids, which provides a more efficient representation by discretizing the continuous 3D coordinate.
- Based on the discrete voxel representation, 3D convolution or 3D sparse convolution [116,117] can be used to extract point cloud features.
- We use Y j,c to represent the j-th voxel output Y at output channel c , and X i,c to represent the i-th voxel input X at input channel c.
- A normal 3D convolutional operation can be described as: where P (j) denotes a function for obtaining the input index i and the filter offset, and W k,c,c denotes filter weight with kernel offset k.
- For sparse input X and output Ỹ , we can rewrite Eqn. 9 into 3D sparse convolution: where R k,j denotes a matrix that specifies the input index i given the kernel offset k and the output index j.
- Most stateof-the-art methods normally utilize 3D sparse convolution to conduct feature extraction.
- The 3D voxel features can then be formatted as a 2D tensor in BEV by densifying and compressing the height axis.
- Given V = {p i = [x i , y i , z i , r i ] T } i=1...n as n ≤ N points inside a non-empty voxel, where x i , y i , z i are coordinates in 3D space, r i is reflectance, N is the maximal number of points, and the centroid of is the local mean of all points, the feature of each point is calculated by: FCN is the composition of a linear layer, a batch normalization, and an activation function.
- Feature of the voxel is the element-wise max-pooling of all f i of V.
- A 3D convolution is applied to further aggregate local voxel features.
- After merging the dimension of channel and height, the feature maps, which are transformed implicitly into BEV, are processed by a region proposal network (RPN) to generate object proposals.
- SECOND [85] introduces sparse convolution in processing voxel representation to reduce training and inference speed by a large margin.
- CenterPoint [67], which is a powerful center-based anchor-free 3D detector, also follows this detection pattern and becomes a baseline method for 3D object detection.
- PV-RCNN [66] combines point and voxel branches to learn more discriminative point cloud features. Specifically, high-quality 3D proposals are generated by the voxel branch, and the point branch provides extra information for proposal refinement.
- SA-SSD [104] designs an auxiliary network, which converts the voxel features in the backbone network back to point-level representations, to explicitly leverage the structure information of the 3D point cloud and ease the loss in downsampling.
- Voxel R-CNN [106] adopts 3D convolution backbone to extract point cloud feature. A 2D network is then applied on the BEV to provide object proposals, which are refined via extracted features.
- Object DGCNN [107] models the task of 3D object detection as message passing on a dynamic graph in BEV.
- VoTr [105] introduces Local Attention, Dilated Attention, and Fast Voxel Query to enable attention mechanism on numerous voxels for large context information.
- SST [68] treats extracted voxel features as tokens and then applies Sparse Regional Attention and Region Shif in the non-overlapping region to avoid downsampling for voxelbased networks.
- AFDetV2 [69] formulates a single-stage anchor-free network by introducing a keypoint auxiliary supervision and multi-task head.

## Post-BEV Feature Extraction
- Voxels in 3D space are sparse and irregular, so 3D convolution is inefficient
- MV3D is the first method to convert point cloud data into a BEV representation
- After discretizing points into the BEV grid, the features of height, intensity, and density are obtained according to points in the grid to represent grid features
- As there are many points in a BEV grid, in this processing, the loss of information is considerable

## Discussion
- As images are in perspective coordinate and point clouds are in 3D coordinate, spacial alignment between two modalities becomes a vital problem.
- Though it is easy to project point cloud data onto image coordinates using geometric projection relationships, the sparse nature of point cloud data makes extracting informative features difficult.
- Inversely, transforming images in perspective view into 3D space would be an ill-posed problem, due to the lack of depth information in perspective view.
- Based on prior knowledge, previous work such as IPM [120] and LSS [57] make it possible to transform information in the perspective view into BEV, providing a unified representation for multisensor and temporal fusion.
- Fusion in the BEV space for lidar and camera data provides satisfactory performance for the 3D detection task.
- Such a method also maintains the independence of different modalities, which provides the opportunity to build a more robust perception system.
- For temporal fusion, representations in different timestamps can be directly fused in the BEV space by concerning ego-motion information.
- Compensation for ego-motion is easy to obtain by monitoring control and motion information, as the BEV coordinate is consistent with the 3D coordinate.

## BEV Fusion

## LiDAR-camera Fusion
- Two papers explore fusion in BEV from different directions
- Camera-to-lidar projection throws away the semantic density of camera features, so BEVFusion designs an efficient camera-to-BEV transformation method
- BEV-Fusion regards the BEV fusion as a robustness topic to maintain the stability of the perception system
- It encodes camera and lidar features into the same BEV to ensure the independence of camera and lidar streams
- This design enables the perception system to maintain stability against sensor failures

## Temporal Fusion
- BEV provides a desirable bridge to connect scene representations in different timestamps
- MVFuseNet [123] utilizes both BEV and range view for temporal feature extraction
- Other works [53,63,64] use ego-motion to align the previous BEV features to the current coordinates, and then fuse the current BEV features to obtain the temporal features
- BEVDet4D [64] fuses the previous feature maps with the current frame using a spatial alignment operation followed by a concatenation of multiple feature maps
- BEV-Former [4] and UniFormer [124] adopt a soft way to fusion temporal information
- The attention module is utilized to fuse temporal information from previous BEV feature maps and previous frames

## Industrial Design of BEV Perception
- Recent years have witnessed trending popularity for BEV perception in the industry.
- In this section, we describe the architecture design for BEV perception on a system level.
- Fig. 5 depicts two typical paradigms for sensor fusion in the industrial applications.
- Prior to BEV perception research, most autonomous driving companies construct the perception system based on perspective view inputs.
- As shown in Fig. 5a, for the perspective view (PV) pipeline, LiDAR track generates 3D results directly.
- The 3D results from images are transformed from 2D results, based on geometry prior.
- Then we fuse the prediction from images and LiDAR, utilizing
- Fig. 6: BEV architecture comparison across industrial corporations.
- These paradigms follow similar workflow as illustrated in Fig. 5b unanimously (input sensors might vary).
- First they encode input data with backbone and perform BEV projection via transformer.
- Then, BEV features are fused temporally and spatially.
- At last, they decode BEV feature with different heads.
- Tesla takes camera image and IMU as input while Horizon and HAOMO embrace point clouds to the inputs additionally.
- Backbone varies in different architectures. some hand-crafted approaches which do not always perform well on realistic scenarios.
- On the contrary, BEV based methods, as illustrated in Fig. 5b, perform 2D to 3D transformation with neural networks and integrate features instead of the direct detection outputs from different modalities, leading to less hand-crafted design and more robustness.
- Fig. 6 summarizes various BEV perception architecture proposed by corporations around the globe.
- The detailed model/input options are described in Sec. D of Appendix.

## Input Data
- BEV based perception algorithms support different data modalities, including camera, LiDAR, Radar, IMU and GPS.
- Camera and LiDAR are the main perception sensors for autonomous driving.
- Some products use camera only as the input sensor, e.g., Tesla [6], PhiGent [127], Mobileye [128].
- The others adopt a suite of camera and LiDAR combination, e.g., Horizon [125], HAOMO [126].
- Note that IMU and GPS signals are often adopted for sensor fusion plans [6,125,126], as do in Tesla and Horizon, etc.

## Feature Extractor
- The feature extractor transforms raw data into appropriate feature representations
- There are different combinations for the feature extractor as backbone and neck
- For instance, ResNet [115] in HAOMO [126] and RegNet [129] in Tesla [6] can be employed as the image backbone
- The neck could be FPN [80] from HAOMO [126], BiFPN [130] as in Tesla [6], etc.

## PV to BEV Transformation
- There are mainly four approaches to perform view transformation in industry: (a) Fixed IPM. Based on the flat ground assumption, a fixed transformation can project PV feature to BEV space. Fixed IPM projection handles ground plane well.
- However, it is sensitive to vehicle jolting and road flatness. (b) Adaptive IPM utilizes the extrinsic parameters of SDV, which are obtained by some pose estimation approaches, and projects features to BEV accordingly. Although adaptive IPM is robust to vehicle pose, it still hypothesizes on the flat ground assumption.
- (c) Transformer based BEV transformation employs dense transformer to project PV feature into BEV space. This data driven transformation operates well without prior assumption, and are thereby widely adopted by Tesla, Horizon, HAOMO.
- (d) ViDAR is first proposed in early 2018 by Waymo and Mobileye in parallel at different venues [13,128], to indicate the practice of using pixel-level depth to project PV feature to BEV space based on camera or vision inputs, resembling the representation form as does in LiDAR. The term ViDAR is equivalent to the concept of pseudo-LiDAR presented in most academic literature.

## Fusion Module
- The alignment among different camera sources has been accomplished in the previous BEV transformation module.
- In the fusion unit, they step further to aggregate BEV features from camera and LiDAR.

## Temporal & Spatial Module
- By stacking BEV features temporally and spatially, one can construct a feature queue.
- The temporal stack pushes and pops a feature blob every fixed time, while the spatial stack does it every fixed distance.
- After fusing feature in these stacks into one form, they can obtain a spatial-temporal BEV feature, which is robust to occlusion.

## Prediction Head
- The multi-head design is widely adopted
- Since BEV feature aggregates information from all sensors, all 3D detection results are decoded from BEV feature space.
- In the meanwhile, PV results (which are still valuable for autonomous driving) are also decoded from the corresponding PV features in some design.
- The prediction results can be classified into three categories [125]: (a) Low level results are related to physics constrains, such as optical flow, depth, etc. (b) Entity level results include concepts of objects, i.e., vehicle detection, laneline detection, etc. (c) Structure level results represent relationship between objects, including object tracking, motion prediction, etc.

## EMPIRICAL EVALUATION AND RECIPE
- BEVFormer++ is a tool that is used to improve the performance of a self-driving car on various benchmarks
- BEVFormer is a tool that is used to improve the performance of a self-driving car
- Voxel-SPVCNN is a tool that is used to improve the performance of a self-driving car on LiDAR segmentation
- SPVCNN is a tool that is used to improve the performance of a self-driving car

## Data Augmentation

## BEV Camera (Camera-only) Detection
- Common data augmentations on images for 2D recognition tasks are applicable for the tasks of camera based BEV perception
- Color variation augmentation is directly applicable
- Spatial transformation moving pixels around is also applicable
- Augmentations based on color variation are directly applicable
- For augmentations involving spatial transformation, apart from ground truth transformed accordingly, calibration in camera parameter is also necessary
- Common augmentations adopted in recent work are color jitter, flip, multi-scale resize, rotation, crop and grid mask
- In BEVFormer++, color jitter, flip, multi-scale resize and grid mask are employed
- The input image is scaled by a factor between 0.5 and 1.2, flipped by a ratio of 0.5; the maximum 30% of total area is randomly masked with square masks.
- Notably, there two ways for flipping images in BEV perception. The first way is to simply flip image, ground truth and camera parameters accordingly. The second way also flips image orders to maintain coherence in overlapped area between images, which resembles to flip the whole 3D space symmetrically.
- The second way of flip is adopted in BEVFormer++.

## LiDAR Segmentaion
- Different from the task of detection, heavy data augmentation can be applied in the task of segmentation, including random rotation, scaling, flipping, and point translation.
- For random rotation, an angle is picked from the range of [0, 2π), rotation is applied to every point on the x-y plane.
- A scale factor is chosen from the range of [0.9, 1.1], and then multiplied on the point cloud coordinate.
- Random flipping is conducted along X axis, Y axis, or both X and Y axes.
- For random translation, offsets for each axis are sampled separately from a normal distribution with a mean of 0 and a standard deviation of 0.1.
- Besides coordinates and reflectance, extra information can be utilized to boost model performance.
- Paint [73,122] is a common technique to enhance point cloud data with image information.
- For unlabeled image data, by projecting point cloud labels onto corresponding images and densifying the sparse annotations, semantic labels on images are obtained from annotated point cloud data.
- An image model is trained to provide 2D semantic segmentation results.
- Then, predicted semantic labels are painted as one-hot vectors to point cloud data as additional channels to represent semantic information from images.
- Besides, temporal information can also be used, as datasets in autonomous driving are usually collected sequentially.
- Past consecutive frames are concatenated with the current frame.
- An additional channel is appended to represent the relative time information of different frames.
- To reduce the number of points, a small voxelization network is applied.
- Then, voxels treated as points are served as input to our models.

## BEV Encoder

## BEV Camera: BEVFormer++
- BEVFormer++ has multiple encoder layers, each of which follows the conventional structure of transformers [95], except for three tailored designs, namely BEV queries, spatial cross-attention, and temporal self-attention.
- BEV queries are grid-shaped learnable parameters, which is designed to query features in BEV space from multi-camera views via attention mechanisms.
- Spatial cross-attention and temporal self-attention are attention layers working with BEV queries, which are used to lookup and aggregate spatial features from multi-camera images as well as temporal features from history BEV, according to the BEV query.
- During inference, at timestamp t, we feed multi-camera images to the backbone network (e.g., ResNet-101 [115]), and obtain the features of different camera views, where F i t is the feature of the i-th view, N view is the total number of camera views.
- At the same time, we preserve BEV features B t−1 at the prior timestamp t − 1.
- In each encoder layer, we first use BEV queries Q to query the temporal information from the prior BEV features B t−1 via the temporal self-attention.
- We then employ BEV queries Q to inquire about the spatial information from multicamera features F t via the spatial cross-attention.
- After the feed-forward network [95], the encoder layer generates the refined BEV features, which is consequently the input of the next encoder layer.
- After six stacking encoder layers, unified BEV features B t at current timestamp t are generated.
- Taking the BEV features B t as input, the 3D detection head and map segmentation head predict the perception results such as 3D bounding boxes and semantic map.
- To improve the feature quality contributing from BEV encoder, three main aspects are to be discussed as follows. (a) 2D Feature Extractor. Techniques for improving backbone representation quality in 2D perception tasks are most likely to improve presentation quality for BEV tasks as well.
- In the image backbone we adopt feature pyramid that is widely used in most 2D perception tasks.
- As depicted in Tab. 4, the structural design of 2D feature extractor, e.g. state-of-the-art image feature extractor [133], global information interaction [134], multi-level feature fusion [80,135] etc. all contribute to better feature representation for BEV perception.
- Apart from the structural design, auxiliary tasks supervising backbones are also important for the performance of BEV perception, which will be discussed in Sec. (b)
- View transformation. The transformation takes in image features and reorganizes them into BEV space. Hyper-parameters, including the image feature's sampling range and frequency, as well as BEV resolution are of vital importance for the performance of BEV perception.
- The sampling range decides how much of the viewing frustum behind an image will be sampled into BEV space. By default this range is equal to the effective range of LiDAR annotation.
- When efficiency is of higher priority, the upper z-axis part of the viewing frustum can be compromised since it only contains unimportant information such as sky in most cases.
- The sampling frequency decides the utility of image features. Higher frequency ensures the model to accurately sample corresponding image features for each BEV location with the cost of higher computation.
- BEV resolution decides the representation granularity of the BEV feature, where each feature can be accurately traced back to a grid in the world coordinates. High resolution is required for better representation of small scale objects such as traffic lights and pedestrians.
- Related experiments are depicted in Tab. 4 ID 2&3.
- In view transformation, feature extraction operations, e.g. convolution block or Transformer block are also present in many BEV perception networks. Adding better feature extraction sub-networks in the BEV space can also improve the BEV perception performance.

## BEV LiDAR: Voxel-SPVCNN
- SPVCNN [84] does not work well with small instances due to the coarse voxelization and aggressive downsampling
- Voxel-SPVCNN [84] uses Minkowski U-Net [117] in the voxel-based branch to preserve point cloud resolution
- Features of the point-based and voxel-based branches would be propagated to each other at different stages of the network
- Voxel-SPVCNN is more efficient than SPVCNN

## 3D Detection Head in BEVFormer++
- BEVFormer++ uses three detection heads, each with a different design
- These heads cover three categories of the detector design, including anchor-free, anchor-based and center-based
- We choose various types of detection heads that differentiate in design as much as possible, so as to fully leverage detection frameworks for the potential ability in different scenarios
- The diversity of heads facilitates the ultimate ensemble results

## Test-time Augmentation (TTA)

## BEV Camera-only Detection
- While BEV detection removes the burden of multi-camera object level fusion, we observe distinguished fact that can benefit from further post-processing.
- By nature of BEV transformation, duplicate features are likely to be sampled on different BEV locations along a light ray to camera center. This results in duplicate false detection on one foreground objects, where every false detection have different depth but can all be projected back to the same foreground objects in the image space.
- To alleviate this issue, it is beneficial to leverage 2D detection results for duplicate removal on 3D detection results, where 2D bounding boxes and 3D bounding boxes are bipartite matched.
- In our experiments, the 3D detection performance can be improved when using ground truth 2D bounding boxes as the filter. However, when using predicted 2D bounding boxes from the 2D detection head trained with auxiliary supervision as mentioned in Sec. 4.5.1, we observe that improvements can be barely obtained.
- This could be caused by the insufficient training of the 2D detection. Therefore, further investigation of the joint 2D/3D redundant detection removal is in demand.

## LiDAR Segmentation
- Most misclassification occurs within similar classes
- Semantic classes can be divided into groups, in which classes are intensively confused compared to classes outside the group
- Post-processing techniques are conducted on foreground semantic groups separately, and bring an improvement of 0.9 mIoU in Tab. 5 ID 10.
- Existing segmentation methods perform per-point classification, without considering the consistency of a single object.
- For instance, some points labeled as foreground objects would be predicted as background.
- Object-level refinement is conducted to further improve object-level integrity based on the aforementioned hierarchical classification.
- By masking points in the same semantic group based on prediction and performing Euclidean clustering, points could be grouped into instances.
- Then the prediction of each instance is determined by majority voting.
- Besides, for each object, the justification of object-level classification is performed by a lightweight classification network to determine the ultimate predicted class of the object.

## Loss

## LiDAR segmentation
- Instead of a conventional cross-entropy loss, Geo loss and Lovász loss are used to train all models.
- To have a better boundary of different classes, Geo loss has a strong response to the voxels with rich details.
- Lovász loss serves as a differentiable intersection-over-union (IoU) loss to mitigate the class imbalance problem. It improves the model performance by 0.6 mIoU as shown in Tab. 5 ID 2.

## Ensemble

## Post-Processing

## CONLUSION
- In this survey, we conduct a thorough review on BEV perception in recent years and provide a practical recipe according to our analysis in BEV design pipeline.
- Grand challenges and future endeavors could be: (a) how to devise a more accurate depth estimator; (b) how to better align feature representations from multiple sensors in a novel fusion mechanism; (c) how to design a parameter-free network such that the algorithm performance is free to pose variation or sensor location, achieving better generalization ablity across various scenarios; and (d) how to incorporate the successful knowledge from foundation models to facilitate BEV perception.
- More detailed discussion could be found in the Appendix.
