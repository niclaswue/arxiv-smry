---
title: "KNIFE: Knowledge Distillation with Free-Text Rationales"
date: 2022-12-19T18:49:09.000Z
author: "Aaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi, Hanjie Chen, Xiang Ren"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09721v1.webp" # image path/url
    alt: "KNIFE: Knowledge Distillation with Free-Text Rationales" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09721)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09721).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/knife-knowledge-distillation-with-free-text).

# Abstract
- Free-text rationales (FTRs) follow how humans communicate by explaining reasoning processes via natural language
- A number of recent works have studied how to improve language model (LM) generalization by using FTRs to teach LMs the correct reasoning processes behind correct task outputs.
- These prior works aim to learn from FTRs by appending them to the LM input or target output, but this may introduce an input distribution shift or conflict with the task objective, respectively.
- We propose KNIFE, which distills FTR knowledge from an FTR-augmented teacher LM (takes both task input and FTR) to a student LM (takes only task input), which is used for inference.
- Crucially, the teacher LM's forward computation has a bottleneck stage in which all of its FTR states are masked out, which pushes knowledge from the FTR states into the task input/output states.
- Then, FTR knowledge is distilled to the student LM by training its task input/output states to align with the teacher LM's.

# Paper Content

## Introduction
- Pretrained language models (LMs) are typically finetuned on downstream tasks using only the task labels as supervision (Raffel et al., 2020;Liu et al., 2019;Devlin et al., 2018).
- In contrast to conventional supervised learning, explanation-based learning (EBL) aims to improve LM generalization by also explaining the correct reasoning process behind a given correct output (Hase and Bansal, 2021;Narang et al., 2020;Joshi et al., 2022).
- Most EBL works have focused on learning from extractive rationales, which explain how to solve a task by highlighting important features (e.g., tokens) in the task input (Sundararajan et al., 2017;Li et al., 2016;Chan et al., 2022b).
- Since extractive rationales assign an importance score to each input feature, there is a one-to-one correspondence between the features and scores.
- In light of this, many regularization methods have been proposed for aligning the LM's extractive rationales (i.e., how did the LM solve the task?) with humanannotated extractive rationales (i.e., how did the human solve the task?) (Joshi et al., 2022;Ross et al., 2017;Rieger et al., 2020;Liu and Avci, 2019)
- Meanwhile, there is growing interest in learning from free-text rationales (FTRs). Following how humans communicate, FTRs explain reasoning processes via natural language (Narang et al., 2020;Rajani et al., 2019;Camburu et al., 2018;Wei et al., 2022).
- Also, FTRs can reference things beyond the task input, while supporting high flexibility in content, style, and length (Rajani et al., 2019;Wiegreffe et al., 2021;Chan et al., 2022a).
- Hence, FTRs can potentially provide much richer knowledge than extractive rationales. However, this also means that, unlike extractive rationales, FTRs do not have a feature-score correspondence to leverage.
- Without this structure, it is unclear how to effectively incorporate FTRs into LM training.
- During task finetuning, existing works have attempted to learn from FTRs by appending FTRs to the LM's input (Sun et al., 2022;Wang et al., 2022;Wiegreffe et al., 2021;Hase et al., 2020) or jointly training the LM to generate FTRs (Narang et al., 2020;Rajani et al., 2019;Brahman et al., 2021;Li et al., 2022;Hase et al., 2020) (Fig. 1).
- Still, these paradigms have key limitations.
- Appending FTRs to the LM's input as additional context is problematic because it either assumes access to gold (or large-LM-generated) FTRs during inference, or introduces an input distribution shift between training and inference when FTRs are unavailable during inference.
- Although multitask learning enables the LM to explain its behavior via the FTR during inference, this paradigm can also be problematic since the task prediction and FTR generation objectives may conflict, especially if the generated FTR does not support the LM's predicted label.
- Thus, both paradigms may even perform worse than vanilla (i.e., non-FTR) finetuning baselines (Wiegreffe et al., 2021;Narang et al., 2020).
- In light of this, how can we more effectively inject FTR knowledge into LMs, in order to improve the LM's generalization ability?
- We propose KNowledge DIstillation with Free-...

## Background

### LM Designs
- The KNIFE learning framework consists of a teacher LM and a student LM.
- The teacher LM's forward computation includes a bottleneck stage, where all of its FTR states are masked out and inaccessible beyond this stage.
- To leverage FTR context for solving the task, knowledge must be transferred from the FTR states to the unmasked task input/output states.
- Therefore, KNIFE distills FTR knowledge to the student LM by training its task input/output states to align with the teacher LM's.

### Knowledge Distillation
- The teacher LM T is essentially trained to predict the compatibility of each (y i , r) pair.
- T 's FTR bottleneck design causes the FTR knowledge to be transferred to other parts of T .
- First, since T dec can only use the task input states as context, T enc must be trained to distill sufficient knowledge from r into the task input states.
- Second, since T dec is the component that actually performs the reasoning process to compute the final compatibility scores, T dec 's task output states must also contain useful knowledge. Thus, in KNIFE, KD is done by training the student LM S so that its task input states and/or task output states are aligned with the teacher LM's.
- Besides the KD losses, we can also train S with the same task loss used to train T .
- Now, we formally define these learning objectives.
- Given predicted label distribution P and target label distribution Q, we first define the task loss L task as cross-entropy loss:
- Next, let dist denote an arbitrary distance function, e.g., mean squared error (MSE).
- Let L KD-in denote KNIFE's task input states based KD loss, which pushes the student LM's task input states (e (j) Sx ) to be closer to the teacher LM's (e Tx ): dist(e Sx , e Tx )
- Similarly, let L KD-out denote KNIFE's task output states based KD loss, which pushes the student LM's task output states (e (j) Sy i ) to be closer to the teacher LM's (e (j) Ty i ): Sy i , e (j)
- Finally, let L denote the total loss, defined as: with loss weights λ task , λ KD-in , and λ KD-out , such that λ task + λ KD-in + λ KD-out > 0.

## Experiments
- Knife outperforms existing FTR-based EBL methods
- Can also outperform baselines in low-resource settings

### Evaluation Protocol
- Text classification tasks are commonly annotated with QA datasets
- OBQA and StrategyQA are two popular QA datasets
- The LM is trained to output y * given x as input for I→O, IR→O, and I→OR, and to output [y * , r] given x as input for I→RO

### Baselines
- Vanilla Finetuning refers to finetuning methods that do not use KD to learn from FTRs
- Vanilla Finetuning methods are distinguished by their input-Output mode (i.e., I→O, IR→O, I→OR, I→RO)
- Vanilla Finetuning + I→O and Vanilla Finetuning + IR→O are trained using the same task loss defined in §3.2.
- For fair comparison with other methods, we omit Vanilla Finetuning + IR→O's FTR input during inference, which naturally introduces an input distribution shift.
- Vanilla Finetuning + I→OR and Vanilla Finetuning + I→RO are trained using the typical greedy search decoding strategy described in §3.1 (Raffel et al., 2020;Vaswani et al., 2017;Narang et al., 2020)
- By default, all KNIFE variants are trained without task loss L task .
- As an upper bound, we also report the performance of the teacher LM, which has access to FTRs during both training and inference.

### Implementation Details
- Uses T5-Base and T5-Large as the LM architecture
- Default is to use the same architecture for the teacher and student LMs
- Consider a T5-Large → T5-Base setting, in which the teacher LM uses T5-Large while the student LM uses T5-Base

### Main Results
- Table 1: Main results on OBQA and Strat-egyQA
- Vanilla Finetuning + I→O and Vanilla Finetuning (KNIFE Teacher Init.) + I→O consistently outperform all other Vanilla Finetuning baselines
- This suggests that existing FTRbased methods (which involve R) are not effective
- By appending an FTR to the input only during training, Vanilla Finetuning + IR→O introduces an input distribution shift between training and inference
- By including an FTR generation loss, Vanilla Finetuning + I→OR and Vanilla Finetuning + I→RO create conflict with the task loss
- Plus, Vanilla Finetuning + I→OR and Vanilla Finetuning + I→RO are not trained/evaluated with labelspecific teacher-forcing, which means it is possible to generate task predictions that do not match any of the label candidates
- In particular, Vanilla Finetuning + I→RO performs quite poorly, especially on StrategyQA
- This is because training the LM to generate its variable-length FTR before its predicted label makes it more likely that the predicted label is never generated
- Second, we find that Vanilla Finetuning (KNIFE Teacher Init.) + I→O does not consistently outperform Vanilla Finetuning + I→O
- This indicates that simply initializing a student LM with an FTRaugmented teacher LM's parameters is not enough to transfer FTR knowledge to the student LM
- Third, across all three architecture settings, we observe that KNIFE Student Finetuning consistently achieves significant improvements over all baselines
- On average, KNIFE Student Finetuning (In+Out) performs best, although other KNIFE variants may perform better on certain settings
- This demonstrates the effectiveness of KNIFE 's KD training process
- Fourth, as a sanity check, we see that KNIFE Teacher Finetuning outperforms all other methods
- Note that KNIFE Teacher Finetuning is not directly comparable to other methods because it has access to FTRs during both training and inference
- Also, despite its FTR access, KNIFE Teacher Finetuning's performance is still relatively far from perfect
- This suggests that the FTRs are not labelleaking

### Low-Resource Learning
- Table 2 displays the results of low-resource learning experiments.
- All methods in the low-resource setting perform worse than in the fully-supervised setting.
- However, KNIFE's performance gains stem from transferring knowledge from the teacher LM's FTR states to its task input/output states.
- By default, KNIFE trains the student LM with only the KD losses, omitting the task loss.
- In this ablation study, we show that the FTR bottleneck is effective at distilling knowledge from the teacher LM's FTR states to its task input/output states, so that the teacher LM can still leverage FTR context for solving the task even when the FTR states are masked out.
- For Student Finetuning (In), we observe that having the FTR bottleneck always improves task performance, especially for StrategyQA.
