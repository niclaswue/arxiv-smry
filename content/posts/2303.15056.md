---
title: "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks"
date: 2023-03-27T09:59:48.000Z
author: "Fabrizio Gilardi, Meysam Alizadeh, MaÃ«l Kubli"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-15056v1.webp" # image path/url
    alt: "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.15056)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.15056).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/chatgpt-outperforms-crowd-workers-for-text).

# Abstract
- NLP applications require manual data annotations for various tasks.
- Tasks can be conducted by crowd-workers or trained annotators.
- Using a sample of 2,382 tweets, ChatGPT outperforms crowd-workers for annotation tasks.
- ChatGPT's accuracy and intercoder agreement exceed that of crowd-workers and trained annotators.
- ChatGPT is much cheaper than MTurk.

# Paper Content

## Introduction
- NLP applications require labeled data to train classifiers or evaluate unsupervised models.
- Labeled data is needed to build a training set or gold standard.
- Labeled data is sometimes available, but often researchers have to conduct original annotations.
- Two strategies for original annotations: trained coders or crowd-workers on platforms like MTurk.
- Recently, large language models (LLMs) have been explored for text annotation tasks.
- ChatGPT outperforms MTurk annotations at a fraction of the cost.
- LLMs have been shown to perform well for a range of purposes.
- ChatGPT's intercoder agreement exceeds that of both MTurk and trained annotators.
- ChatGPT is significantly cheaper than MTurk.
- LLMs have potential to transform how researchers conduct data annotations.

## Results
- Used a dataset of 2,382 tweets for a study on discourse around content moderation
- Annotated manually with trained annotators
- Five conceptual categories with different numbers of classes
- Performed same classifications with ChatGPT and crowdworkers
- Four sets of annotations with ChatGPT to explore effect of temperature parameter
- Crowdworkers filtered for "MTurk Masters"
- ChatGPT outperformed MTurk for four out of five tasks
- Accuracy of ChatGPT more than adequate
- Intercoder agreement of ChatGPT very high
- Positive but weak correlation between intercoder agreement and accuracy
- Test to which ChatGPT was subjected was hard

## Discussion
- LLMs have potential to transform text-annotation procedures
- LLMs may be superior to crowd-annotations
- Further study of LLMs is needed
- Performance of ChatGPT across multiple languages and types of text should be studied
- Few-shot learning should be compared with BERT and RoBERTa
- Semi-automated data labeling systems should be constructed
- Chain of thought prompting should be used to increase zero-shot reasoning
- Annotation tasks should be implemented with GPT-4

## Materials and methods

### Twitter data
- Collected tweets about content moderation for a previous study
- Dataset covers tweets with relevant keywords and hashtags from Jan 2020 to April 2021
- Subset of 2,382 tweets labeled by trained annotators

### Text classification tasks
- Implemented 5 text classification tasks
- Relevance: whether tweet is about content moderation
- Topic detection: whether tweet is about 6 pre-defined topics
- Stance detection: whether tweet is in favor/against/neutral about repealing Section 230
- General frame detection: whether tweet contains 2 opposing frames (problem/solution)
- Policy frame detection: whether tweet contains 14 policy frames

### Human annotation
- Trained two political science graduate students to annotate tweets
- Coders given instructions and asked to annotate tweets
- Accuracy of ChatGPT and MTurk calculated based on tweets both coders agreed on

### Crowd-workers annotation
- Used MTurk workers to annotate tasks
- Provided same instructions as for other annotators
- Restricted access to tasks to workers with high approval rate and located in US
- No worker could annotate more than 20% of tweets for a given task
- Each tweet was annotated by two different crowd-workers

### Chatgpt annotation
- Used ChatGPT API with 'gpt-3.5-turbo' version to classify tweets
- Annotation conducted between March 9 and March 20, 2023
- Prompted ChatGPT with annotation instruction text
- Avoided adding ChatGPT-specific prompts
- Fed tweets one by one to ChatGPT with prompt
- Temperature parameter set at 1 and 0.2
- Collected four ChatGPT responses for each tweet
- Created new chat session for every tweet

### Evaluation metrics
- Average accuracy of ChatGPT and MTurk crowd-workers compared to trained human annotations
- Intercoder agreement of ChatGPT, MTurk crowd-workers, and trained human annotators measured

## S1 annotation codebook
- Content moderation is the practice of screening and monitoring content posted on social media sites.
- Content moderation decisions are based on specific rules and guidelines.
- Content moderation can involve removing, flagging, labeling, or disputing content.
- Deciding what should be allowed on social media is not always easy.

### S1.3 task 2: problem/solution frames
- Content moderation can be seen as a problem
- Content moderation can be seen as a solution
- Negative effects of content moderation include restrictions to free speech and biases
- Positive effects of content moderation include protection from harmful content
- Content moderation can be linked to various topics such as health, crime, or equality
- Carefully read the text of the tweet and classify it into one of the topics
- Topics include economy, capacity and resources, morality, fairness and equality, constitutionality and jurisprudence, policy prescription and evaluation, law and order, crime and justice, security and defense, health and safety, quality of life, cultural identity, public opinion, political, external regulation and reputation, and other

### S1.5 task 4: stance detection
- Section 230 is a law in the US that protects websites from being held legally responsible for user content
- Websites can still choose to moderate content and remove anything that violates their own policies
- Texts can be classified according to topics such as Section 230, Trump Ban, Complaints, Twitter Support, Platform Policies, and Other
