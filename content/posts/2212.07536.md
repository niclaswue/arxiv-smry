---
title: "Robust Policy Optimization in Deep Reinforcement Learning"
date: 2022-12-14T22:43:56.000Z
author: "Md Masudur Rahman, Yexiang Xue"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07536v1.webp" # image path/url
    alt: "Robust Policy Optimization in Deep Reinforcement Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07536)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07536).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/robust-policy-optimization-in-deep).

# Abstract
- Policy gradient method enjoys the simplicity of the objective
- In the continuous action domain, parameterized distribution of action distribution allows easy control of exploration
- Entropy can play an essential role in policy optimization by selecting the stochastic policy
- However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory
- Additionally, certain parametric distributions might only work for some environments and require extensive hyperparameter tuning
- Robust Policy Optimization (RPO) leverages a perturbed distribution to encourage high-entropy actions and better represent the action space

# Paper Content

## INTRODUCTION
- The policy gradient method directly optimizes the expected return, making them easy to understand, stable to train, and effective.
- One advantage of this approach is that the exploration can be controlled by representing the action space with parameterized distribution (e.g., Gaussian).
- Due to this heavy dependency on action distribution, it is important that the parameterized distribution adequately represent the underlying action of the task.
- A popular and often effective choice is the Gaussian distribution to continuous action. However, one particular distribution might only be suitable for some tasks requiring the practitioner to tune for different distributions.
- Furthermore, the policy often wants to reduce the variance in selected action as training progress, which might collapse entropy and lead to sub-optimal exploration.
- As a significant source of exploration in the policy gradient method, this issue severely hampers the exploration, which results in sub-optimal policy learning.
- To mitigate these issues, we propose a simple solution that eventually results in better performance across different control tasks in several benchmarks.
- Our solution consists of adding a uniform random number to the mean of the representing action distribution.
- Exploration in a high-dimensional environment is challenging due to the online nature of the task.
- In a reinforcement learning (RL) setup, the agent is responsible for collecting high-quality data. The agent has to decide on taking action which maximizes future return.
- In deep reinforcement learning, Figure 1: [Left] Standard Gaussian and corresponding Perturb Gaussian distribution (ours). We observe that the probability density is less centered around the mean in our distribution. [Middle] Policy entropy at different timesteps of training in DeepMind Hopper Hop Environment. Similar patterns are observed for other evaluated environments. The PPO agent who uses Standard Gaussian starts from a particular entropy and becomes less stochastic as the training progresses and reduces the policy entropy. In contrast, our agent RPO uses our perturbed Gaussian distribution, increases the entropy at the initial timestep, and then maintains a certain level of entropy throughout the training. [Right] Our agent RPO shows over 3x performance improvement in normalized return compared to the base PPO agent on 12 DeepMind control environments. The results are averaged over 10 random seed runs.

## PRELIMINARIES AND PROBLEM SETTINGS
- An MDP is a tuple (S, A, P, R) where an agent interacts with the environment and takes an action a t ∈ A at a discrete timestep t when at state s t ∈ S.
- After that the environment moves to next state s t+1 ∈ S based on the dynamic transition probabilities P(s t+1 |s t , a t ) and the agent recieves a reward r t according to the reward function R.
- In a reinforcement learning framework, the agent interacts with the MDP, and the agent's goal is to learn a policy π ∈ Π that maximizes cumulative reward, where Π is the set of all possible policies.
- Given a state, the agent prescribes an action according to the policy π, and an optimal policy π * ∈ Π has the highest cumulative rewards.
- The policy gradient method is a class of reinforcement learning algorithms where the objective is formulated to optimize the cumulative future return directly.
- In a deep reinforcement learning setup, the policy can be represented as a neural network that takes the state as input and output actions.
- The objective of this policy network is to optimize for cumulative rewards.
- In this paper, we focus on a particular policy gradient method, Proximal Policy Optimization (PPO) (Schulman et al., 2017), which is similar to Natural Policy Gradient (Kakade, 2001) and Trust Region Policy Optimization (TRPO) (Schulman et al., 2015).
- The following is the objective of the PPO: , where π θ (a t |s t ) is the probability of choosing action a t given state s t at timestep t using the current policy parameterized by θ.
- On the other hand, the π θ old (a t |s t ) refer to the probabilities of using an old policy parameterized by previous parameter values θ old .
- The advantage A(s t , a t ) is an estimation which is the advantage of taking action a t at s t compared to average return from that state.
- In PPO, a surrogate objective is optimized by applying clipping on the current and old policy ratio on equation 1.
- In the case of the discrete action, the output is often used as the categorical distribution, where the dimension is the number of actions.
- On the other hand, in continuous action cases, the output can correspond to parameters of Gaussian distribution, where each action dimension is represented by mean and variance.
- Finally, the action is taken as the sample from this Gaussian distribution.
- In this paper, we mainly focus on the continuous action case and assume the Gaussian distribution for action.
- Moreover, we demonstrate our method on the other distributions, such as Laplace and Gumbel.
- Entropy Measure
- Entropy is a measure of uncertainty in the random variable.
- In the context of our discussion, we measure the entropy of a policy from the action distribution it produces given a state.
- The more entropic an action distribution is, the more stochastic the decision would be as the final action is eventually a sample from the output action distribution.
- This measure can be seen as the policy's stochasticity, and higher entropy distribution allows agents to explore the state space more.
- Thus, entropy can play a role in controlling the amount of exploration the agent performs during learning.
- In this paper, we focus on the entropy measure and refer to the stochasticity of policy by this measure.

## ROBUST POLICY OPTIMIZATION (RPO)
- Exploration at the start of training can help an agent to collect diverse and representative data.
- However, as time progresses, the agent might start to reduce the exploration and try to be more certain about a state.
- In standard PPO, we observe that the agent shows more randomness in action at the beginning (see Figure 1), and it becomes smaller as the training progresses.
- We measure this randomness in the form of entropy, which indicates the randomness of the actions taken.
- In the continuous case, a practical approach is to represent the action as a parameter of Gaussian distribution.
- The entropy of this distribution represents the amount of randomness in action, which also indicates how much the policy can explore new states.
- However, in Gaussian distribution, the samples are concentrated toward the mean; thus, it can limit the exploration in some RL environments.
- This paper presents an alternative to this Gaussian distribution and proposes a new distribution that accounts for higher entropy than the standard Gaussian.
- The goal is to keep the entropy higher or at some levels throughout the agent training.
- In particular, we propose to combine Gaussian Distribution and Uniform distribution.
- At each timestep of the algorithm training, we perturb the mean µ of Gaussian N (µ, σ) by adding a random number drawn from the Uniform U(−α, α) and get the perturbed mean µ .
- Then the sample for action is taken from the new distribution N (µ , σ).
- In this setup, the Gaussian parameters still depend on the state, similar to the standard setup.
- However, the Uniform distribution does not depend on the states. Thus as training progresses, the standard Gaussian distribution can be less entropic as the policy optimizes to be more confident in particular states.
- However, the state-independent perturbation using the Uniform distribution still enables the policy to maintain stochasticity.
- Figure 1 shows the diagram of the resulting Gaussian distributions.
- This diagram is generated by first sampling data from a Gaussian distribution with mean µ = 0.0 and standard deviation σ = 1.0 and then corresponding perturbed mean µ = µ + z and standard deviation σ = 1.0, where z ∼ U(−3.0, 3.0).
- We see that the values are less centered around the mean for our Perturb Gaussian than the standard.
- Therefore, samples from our proposed distribution should have more entropy than the standard Gaussian.
- Note that, our method of integrating uniform distribution can be applied to other types of parameterized distribution as well where the mean and standard deviation is used for example Laplace and Gumbel distribution.
- We show the results and compared different distributions in the experiment section.
- Due to the popular and effective choice of Gaussian distribution (we also observe this hold), we report our results with Gaussian distribution, unless otherwise mentioned.

## EXPERIMENTS

## SETUP
- Uses a Gaussian distribution to represent the action output from the policy network
- Uses data augmentation to increase the policy's entropy
- Uses the entropy coefficient 0.0, 0.01, 0.05, 0.5, 1.0, and 10.0 to compare their performance in entropy

## RESULTS
- Our method has a lower entropy than baselines
- The entropy of the policy is high

## COMPARISON WITH PPO
- Our method RPO consistently performs better than the PPO in the Cartpole environment.
- In the BallBalance environment, our method RPO achieves a slight performance improvement compared to PPO.

## COMPARISON WITH ENTROPY REGULARIZATION
- The entropy requirement might be different
- In the tested environments, we observe that sometimes the performance improves with a proper setup of tuned coefficient value and often worsens the performance (in Figure 6)
- However, our method RPO consistently performs better than PPO and the different entropy coefficient baselines
- Importantly, our method does not use these coefficient hyper-parameters and still consistently performs better in various environments

## COMPARISON WITH OTHER DISTRIBUTIONS
- The choice of action distribution is important in solving a reinforcement learning task
- In Figure 7, we see that the usual Gaussian distribution fails to learn any reasonable performance
- We further replace the Gaussian with Laplace and Gumbel distribution keeping all other implementations and hyperparameters the same
- We observe that the performance of the Laplace distribution is significantly better than the Gaussian
- On the other hand, Gumber distribution fails similarly to the Gaussian in this environment
- These results show the importance of choosing a suitable distribution for the task
- Furthermore, we combine our method of adding uniform distribution with the distributions (Gaussian, Laplace, and Gumbel)
- The results in Figure 7 and Figure 8 show that our method achieves overall improved performance compared to the base distributions

## COMPARISON WITH DATA AUGMENTATION

## ABLATION STUDY
- We conducted experiments on the α value ranges in the Uniform distribution.
- Figure 9 shows the return and policy comparison. We observe that the value of α affects the policy entropy and, thus, return performance.
- A smaller value of α (e.g., 0.001) seems to behave similarly to PPO, where policy entropy decreases over time, thus hampered performance.
- Higher entropy values, such as 1000.0, make the policy somewhat random as the uniform distribution dominates over the Gaussian distribution (as in Algorithm 1. This scenario keeps the entropy somewhat at a constant level; thus, the performance is hampered.
- Overall, a value between 0.1 to 3 often results in better performance. Due to overall performance advantage, in this paper, we report results with α = 0.5 for all 18 environments.

## RELATED WORK
- Since the inception of the PPO (TRPO and Natural Policy Gradient), several studies have investigated different algorithmic components
- Entropy regularization enjoys faster convergence and improves policy optimization
- However, some empirical findings observe the difficulty in setting proper entropy setup and observe no performance gain in many environments
- Gaussian distribution is commonly used to represent continuous action (Schulman et al., 2015;2017)
- Thus many follow-ups to implementation also use this Gaussian distribution as s default setup
- In contrast, in this paper, we propose perturbed Gaussian distribution to represent the continuous action to maintain policy entropy
- Furthermore, in this paper, we showed the empirical advantage of our method in several benchmark environments compared to the standard Gaussian, Laplace, and Gumbel distributions
- Another form of approach which adds entropy to the policy is data augmentation
- We observe that the policy entropy often increases when the data is augmented often by some form of random processes. This improvement in entropy can eventually lead to better exploration in some environments. However, the type of data augmentation might be environment specific and thus requires domain knowledge as to what kind of data augmentation can be applied.
- In contrast, to this method, our method maintains an entropy threshold during entire policy learning. Moreover, we evaluated our algorithm with two kinds of data augmentation-based methods RAD (Laskin et al., 2020) and DRAC (Raileanu et al., 2020)
- Finally, we evaluate with an effective data augmentation approach random amplitude modulation, which was found to be worked well in vector-based states set up.
