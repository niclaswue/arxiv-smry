---
title: "Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"
date: 2022-12-19T22:37:46.000Z
author: "Jing Huang, Zhengxuan Wu, Kyle Mahowald, Christopher Potts"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09897v1.webp" # image path/url
    alt: "Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09897)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09897).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/inducing-character-level-structure-in-subword).

# Abstract
- Language tasks involving character-level manipulations (e.g., spelling correction, many word games) are challenging for models based in subword tokenization
- To address this, we adapt the interchange intervention training method of Geiger et al. (2021) to operate on type-level variables over characters
- This allows us to encode robust, position-independent character-level information in the internal representations of subword-based models
- We additionally introduce a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context
- While simple character-level tokenization approaches still perform best on purely form-based tasks like string reversal, our method is superior for more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games

# Paper Content

## Introduction
- Many common natural language tasks can fruitfully be described in terms of character-level manipulations
- For some of these tasks, the best models may be ones that tokenize their inputs and outputs at the character level
- However, with only a few exceptions (Xue et al., 2022;Tay et al., 2022;Clark et al., 2022), our best general-purpose models at present do not tokenize their inputs into characters, but rather into words and subword units
- We develop a causal intervention-based framework for pushing subword-based models to encode character-level information in their internal representations
- The techniques are based on the interchange intervention training (IIT) method of Geiger et al. (2021b), which trains neural hidden representations to correspond to variables in a high-level causal model capturing aspects of the task domain
- We apply IIT at the level of variable types (Type-level IIT), which allows us to learn robust, position-independent representations of characters in the hidden states of subword-based models
- We compare against approaches that tokenize inputs and/or outputs at the character level
- We introduce a suite of character-level evaluation tasks, summarized in Figure 1
- All of these tasks depend heavily on character-level manipulation of forms, but they differ in terms of how much they (a) involve meaning and (b) depend on the full context of the input string (see Table 1)

## Related Work

### Subword and Character Modeling
- Subword-based models tokenize their inputs into words and word pieces, most of which are longer than individual characters.
- The most prominent subword tokenization methods are byte-pair encoding (Sennrich et al., 2016), word-piece tokenization (Schuster and Nakajima, 2012), and unigram language models (Kudo, 2018;Bostrom and Durrett, 2020).
- These methods have become standard for large pretrained language models (Brown et al., 2020;Raffel et al., 2020;Liu et al., 2019).
- Character-level models, by contrast, represent inputs as character sequences. These methods have generally not been as widely employed for large language models; the token sequences are much longer, which introduces significantly higher costs for both training and inference (Libovickỳ et al., 2021;Mielke et al., 2021;Pinter, 2021).
- However, it should be noted that a few recent character-level large language models have proven highly successful on standard benchmarks (Xue et al. 2022;Tay et al. 2022;Clark et al. 2022; see also Dos Santos and Zadrozny 2014; Belinkov and Bisk 2018;Rosales Núñez et al. 2021).
- Another line of research has sought to create hybrid character-level and subword (or word) models (Luong and Manning, 2016;Ma and Hovy, 2016;Pinter et al., 2017;Peters et al., 2018;Schick and Schütze, 2019;Aguilar et al., 2021).
- These methods typically modify the input layer, define additional weights to learn character embeddings, and construct character-to-word mappings.

### Character Manipulation Tasks
- Character manipulation tasks such as word scrambling and basic arithmetic are increasingly prominent in large language model evaluations
- In addition, a number of recent efforts have focused on linguistic phenomena that depend, at least in part, on characterlevel manipulations
- Examples include digit tokenization (Geva et al., 2020), creative blends like 'hangry' (Pinter et al., 2021), puns (Yu et al., 2020;Mittal et al., 2022), and the wordplay involve in crossword puzzle clues (Efrat et al., 2021;Rozner et al., 2021;Wallace et al., 2022)
- These studies tend to show that subword tokenization models do not fully encode information about the characters contained in their tokens

### Intervention-Based Training Methods
- Our core technique is based in the interchange intervention method (IIT) of Geiger et al. (2021b).
- With IIT, one can train a neural model to conform to a high-level causal model of some aspect of the task domain while still allowing it to learn from data as usual.
- IIT belongs to a family of causal abstraction techniques (Beckers and Halpern, 2019;Beckers et al., 2020) that have proven successful for obtaining human-interpretable explanations of complex neural networks (Geiger et al., 2021a;Wu et al., 2022a).
- The key innovation of IIT is to extend these explanation techniques to model training.

## Character-level Manipulation Tasks
- Tasks designed to test models of form, meaning, and context
- All character manipulation tasks involve aspects of form
- Variants of spelling correction that differ in the role of context are tested
- Tasks explore the role of context even more deeply

### Character Reversal
- The Character Reversal task takes in a string, and the task is to reverse the characters contained in that string (e.g., ⇒).
- The inputs and outputs do not need to be valid English words.
- Hence the task is form only, with no meaning or context involved.
- The dataset consists of 20K/2K train/dev examples, covering 1.6K and 9K unique subword pieces (of a pretrained T5 vocab) in source and target.
- To evaluate the generalization capacity of our models, we create two test splits with size 4K/1K: an In-Vocab (IV) split with source tokens in the training vocab (but unseen token sequences), and an Out-Of-Vocab (OOV) split with the source tokens out of the training vocab.

### Unit Conversion
- The Unit Conversion task takes a decimal number, a source unit, and a target unit, and applies decimal shifting (multiplication or division by power of 10), as in convert 1.23 m to cm ⇒ 123.
- The correct way to move the decimal point depends on the units, but the manipulation of digits itself is a mechanical, string-oriented process of moving a character. Hence we categorize the task as involving form and context, but not meaning.
- We acknowledge that it is in principle possible for a model to find a semantic (truly arithmetic) solution to this task, but this is not necessary to solve it.
- The dataset consists of 20K/2K train/dev examples, covering 300 and 500 unique subword pieces with digits in source and target.
- IV and OOV splits, as described in Section 3.1 above, are used.

### Unscramble
- The Unscramble task takes a random permutation of a word and outputs the unscrambled word (e.g., ⇒).
- Unlike Brown et al. (2020), we do not constrain the first or last letter of the permutations.
- Unscrambling involves meaning, as models need to recognize the sequence of characters in the output as valid English words.
- We construct the dataset from 30K English words.
- The dataset consists of 100K train examples with 3 random permutations of letters per word and 30K dev examples, covering 4K and 13K unique subword pieces in source and target.
- We again define IV and OOV splits.

### Single Word Spelling Correction
- The Single Word Spelling Correction task takes a word with a spelling error and outputs the correct word (e.g., misspellde ⇒ misspelled).
- We follow the setup in Belinkov and Bisk (2018) to introduce four types of synthetic errors: swapping two adjacent characters, substituting a character with its neighbors on the keyboard, deleting a character, and repeating a character.
- Similar to the Unscramble task, spelling correction involves semantics because the correction needs to create an attested English word.
- We construct the dataset from the same 30K English words used in the Unscramble task.
- The dataset consists of 100K training examples with 3 corruptions per word and 30K dev examples, covering 9K and 13K unique subword pieces in source and target.
- We again define IV and OOV splits. In addition, we also evaluate on the 6K real spelling errors collected by Belinkov and Bisk (2018).

### Spelling Correction with Context
- Spelling correction with context adds contextual aspects to the previous single-word spelling correction task
- Context can be critical in spelling correction as some spelling errors have multiple potential corrections
- For our train dataset, we sample 30K sentences containing the target word from the Wikipedia corpus as context
- For test sets, we have two variants. Our focus is the 'Dependent' condition, where there are multiple potential corrections and the correct one depends on the context, as in our example involving "actuall". We also evaluate an 'Independent' condition in which only one correction is valid.

### Word Search
- Adapted from the popular Word Search Puzzle
- The task involves understanding the meaning of the hidden letters and the relationship to the theme
- We generate synthetic puzzles with the structure definition: charstring, and the task is to find in charstring a substring that, when reversed, is defined by definition
- For defining words, we use the definition or the name of the corresponding WordNet Synset (Miller, 1995) and a set of at least 5 hyponyms in the Synset
- For our train dataset, we generate 30K examples where the charstring contains two valid English words, one matching the definition and the other not
- The two words are hidden at random positions, with the rest of charstring padded with valid words in the forward direction to a total length of 16 characters

## Character-level Interventions
- Character-level tokenization methods provide models with direct evidence for learning about characters as independent units or concepts.
- Subword tokenization methods do not provide such direct evidence, and it seems clear that these models do not reliably learn character-level concepts on their own.
- In the current section, we present a method that seeks to address this shortcoming of subword models by training models to internally represent characters and the subword-to-character mappings needed for character manipulations.
- Our core method for doing this interchange intervention training (IIT; Geiger et al. 2021b).
- IIT has three core steps. First, one defines a high-level causal model of some aspect of the problem domain. Second, one aligns a variable V in that causal model with a set of neurons N in one's target neural model. Third, one trains the neural model to make predictions according to the causal model using not only standard input-output pairs, but under interchange interventions: counterfactual instances creating by replacing the values of N in a target example with those obtained by processing a distinct input, with the counterfactual label provided by the causal model.
- The overall effect of this process is to train the model to represent the variable V in the neurons N , which leads to modular, internal structure for the network.
- In prior work using IIT (Geiger et al., 2021b;Wu et al., 2022b,a), the interchange interventions have targeted individual variables.
- For our tasks, these variables would correspond to the values of individual characters in strings, which would push the model to learn positionally-encoded character representations.
- We seek to learn position-independent character representations that support arbitrary operations on strings.
- To further this goal, we define Type-level IIT, which allows interchange interventions to target any two variables of the same type.

### Causal Models for Characters
- The first step for IIT is defining a high-level causal model.
- To illustrate this, we focus on the Character Reversal task and then sketch how the needed causal models can be defined for our tasks.
- Our causal model for Character Reversal is given in Figure 2a.
- The input to this model is a single string; for illustrative purposes only, we specify that the string has length 3.
- The model creates three intermediate variables V 1 , V 2 , and V 3 , one per character, and outputs the values of those variables in reverse order, concatenated back into a string.
- This causal model is fully deterministic, and so we know exactly what will happen if we intervene on one of the variables V i to change its value to another character.
- For IIT, we perform such interventions using pairs of examples, a base input (left) and a source input (right) as in Figure 2a.
- We then take the value created at our target variable V 3 in the source example and use it in place of the value of V 1 in the base example.
- In our example, this amounts to replacing c with d, leading to output dba.
- In prior work, these interventions targeted the same variable in the base and source. Such interventions are certainly useful, but they would instruct the model to learn both the character and its position, whereas our tasks depend on characters as unified concepts. Thus, we allow type-level interventions like the one described in Figure 2a: V 1 can take on the value of V 3 because both have the same type.
- A similarly simple model can be defined for our other purely form-based task, Unit Conversion, which simply moves decimal places around based on the unit specified in the input string.
- For tasks involving meaning, the programs are somewhat more complex due to their dependence on English.
- For example, the Unscramble causal model forms intermediate representations of the characters in its input, as in Figure 2a, but the mapping to an output depends on a lexicon.
- The spelling correction and word search tasks are similarly constrained by a lexicon. However, the important common theme of all these programs is that they create characterlevel intermediate variables as the basis for their final output behavior.

### Aligning the Causal and Neural Models
- The second step for IIT is to define an alignment of variables in the causal model with sets of neurons in the target neural model.
- We again illustrate with the Character Reversal task. Figure 2b summarizes our alignment: the character variables V 1 , . . . , V 3 are mapped to the first-layer hidden states of the Transformer Encoder.
- Each character in the subword is mapped sequentially to a 16d vector of the hidden state, at the same step as the subword.
- For form-only tasks such as Reversal, the choice of Encoder layer is less critical, as the Decoder alone is sufficient to handle the task logic.
- For semantic tasks, where the task logic is dependent on the character values, character variables are best mapped to early layers in the network.

### Training with Character Interventions
- IIT objective is to use available train data to create a model that conforms to a high-level causal model
- The first step is to intervene on the causal model, assigning values to variables in the source and base examples
- The second step is to copy the gradients from the causal model to the neural model, and the model will predict an output string

### Handling Out-of-Vocab Items
- Extract 16d character representations from a set of vocabulary items seen in training
- Compute an averaged 16d representation per character
- Substitute the unseen token with a sequence of seen tokens
- Populate each position with an averaged representation of each character in the unseen vocab

## Experiments
- Our intervention-based approach is better than the other tokenization approaches
- The other tokenization approaches don't generalize well

### Baselines
- For subword-based models, we fine-tune the pretrained T5-small model.
- We also experiment with in-context learning by prompting GPT-3 (davinci-003 engine, used in December 2022).
- For subword-based models with character-level input and/or output, we use the same set of models as above. Additionally, for T5-small, we set the tokenizer to split input and/or outputs into characters.
- For GPT-3, we explore a variant in which we insert a hyphen (or space) between input and/or output characters (GPT-3-C).

### Intervention-based Models
- We use a character intervention-based method to improve the performance of a pretrained T5-small model.
- The coefficients λ 1 and λ 2 for the base and IIT losses are set to 1.

### Evaluation
- We use the test sets described in Section 3.
- To summarize, tasks 1-4 are evaluated on In-Vocab (IV) and Out-Of-Vocab (OOV) splits based on whether source tokens are seen in training.
- Task 4 is also evaluated on a "Real" split with natural spelling errors.
- Task 5 is evaluated on "Independent" and "Dependent" splits based whether a spelling error has multiple potential corrections.
- Task 6 is evaluated on a "P" split with paraphrased definitions, a "O" split with overlapped words, and a "P+O" split with a combination of both.
- For metrics, we use the sequence-level accuracy, i.e., whether the output is an exact match of the label.
- For the Unscramble task, we also count as correct outputs that satisfy all of the following three criteria: (1) anagrams of the label, (2) valid English words in the 30K train words, and (3) not identical to the input.
- For Single Word Spelling Correction, we also count outputs that are valid English words and satisfy the synthetic error rules, which are the context-dependent cases discussed in the Spelling Correction with Context task.
- For decoding, all T5-based models use the greedy decoding algorithm.

### Results
- Table 2 presents the results of the experiments
- Character-level models (Char-ST) are superior on purely form-based tasks
- Subword models are not competitive on these tasks
- Adding meaning aspect to these tasks (tasks 3-4) improves performance
- Character-level outputs consistently underperform subword outputs
- Subword-based models have a clear advantage on splits where form alone is insufficient to determine the output
- For the word search task, Subword+IIT models further improve the accuracy on all splits by a small margin of 1-2%

## Analysis and Discussion

### Error Analysis on Word Search
- To further understand models' biases towards using form, meaning, and context for generalization, we analyze performance on the Word Search task.
- Specifically, we measure how well the predictions match characters in the charstring or the meaning of the definition.
- We define two new metrics: Character Match, which is the percentage of predictions that are a substring of the reversed charstring, and Synonym Match, which is the percentage of predictions that are semantically relevant to the definition based on WordNet Synset relationships.
- Both metrics would be 100% for a model with 100% sequence-level accuracy. However, they diverge when models make wrong predictions that only capture some aspects of form, meaning, or context.
- In particular, a model that made predictions primarily based on form would have high Character Match but low Synonym Match, whereas a model that made predictions primarily based on meaning and context would have a high Synonym Match but low Character Match.
- Table 3 shows the results of this analysis.
- Subword-based models are biased towards using and context for generalization and so have higher Synonym Match scores.
- Character-level models, on the other hand, are biased towards using form and so have higher Character Match scores.

### Visualizing Internal Character Representations
- The Subword+IIT models embed accurate, coherent representations of characters
- These representations are visible in the visualizations shown in Figure 4a

## Conclusion
- Character tasks have emerged as an Achilles heel for large language models that use subword tokenization.
- These models do not reliably represent the mapping from subword tokens to the characters they contain, and thus they stumble with characterlevel manipulations.
- Figure 2: Intervention for the Reversal task.
- Results (sequence-level accuracy). "Subword": T5 subword model baseline. "Char-T": T5 with characterlevel splitting on the target side. "Char-S": T5 with character-level splitting on the source side. "+IIT": Joint training with character-level interventions. 'Char-ST": T5 with character-level splitting on both source and target sides.
- "ByT5": ByT5 character model baseline.
- "GPT-3": GPT-3 davinci-003. "GPT-3-C": GPT-3 with hyphenseparated character in both input and output.
- * For GPT-3, the IV vs. OOV distinction is tricky, since the subword vocab is different from the T5 one.
- * For Unit Conversion, we use space instead of hyphen as the separator.
