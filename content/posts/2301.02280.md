---
title: "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"
date: 2023-01-05T19:48:01.000Z
author: "Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, Dhruv Mahajan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02280v1.webp" # image path/url
    alt: "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02280)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02280).


# Abstract
- dataset noise is reduced by using a filtering strategy called "Complexity, Action, and Text-spotting"
- concept distillation is used to leverage strong unimodal representations for contrastive training
- the traditional contrastive alignment objective is modified to up-sample the importance of hard-negatives

# Paper Content

## Introduction
- Contrastive pre-training is a popular paradigm in multimodal learning that involves training multimodal models on very largescale noisy datasets of image-text pairs sourced from the web
- It has been shown to be incredibly effective for a variety of vision-language tasks without any task-specific finetuning (i.e., zero-shot), such as image classification
- Few recent efforts have tried to clean the noise by filtering samples based on alignment scores from an existing model like CLIP
- On ImageNet1K, we show huge improvements over prior work for small k values

## Related work
- Dataset curation for contrastive pretraining: Largescale contrastive pretraining typically requires dataset sizes of the order of hundreds of millions to billions.
- Seminal works in this area, e.g., CLIP [61] and ALIGN [28], have largely relied on imagetext pairs crawled from the web.
- Subsequently, versions of large-scale image-text datasets have been created but not released publicly, including WIT-400M [61], ALIGN-1.8B [28], FILIP-340M [83], FLD-900M [85], BASIC-6.6B [57], PaLI-10B [10].
- These datasets often use unclear or primitive cleaning strategies, e.g., removing samples with short or non-English captions.
- Recently, LAION- 400M [66] used CLIP-based scores to filter down a large dataset.
- The authors later released an English-only LAION-2B and a LAION-5B unfiltered dataset sourced from Common Crawl1.
- Apart from LAION-400M and BLIP [39] which uses the bootstrapped image-grounded text encoder to filter out noisy captions, there has not been a significant investment in systematic curation strategies to improve zero-shot alignment performance on large-scale pretraining.
- In contrast to the previous work, we use quality-motivated filters that retain images whose captions are sufficiently complex, contain semantic concepts (actions), and do not contain text that can be spotted in the image [37].
- Knowledge distillation [25] aims to transfer knowledge from one model to another and has been used in many contexts across from improving performance and efficiency [6,7,41,63,72,79] to improving generalization capabilities [16,42,43].
- Several approaches use self-distillation to improve performance with lower computational overhead [23,80,86].
- For vision and language pre-training, [2,40] use soft-labels computed using embeddings from a moving average momentum model with the goal to reduce the adverse effects of noisy image-text pairs in the training data.
- Our concept distillation approach is a cheaper and more effective alternative, since it does not require us to run the expensive teacher model throughout the training2 while retaining the most useful information from the visual concepts.
- Another approach to take advantage of pre-trained visual models is to use them to initialize the image encoder, and continue pre-training either by locking the image encoder [57,87] or fine-tuning [57].
- Contrastive training with hard negatives: Noisecontrastive estimation (NCE) [22] is the typical objective for vision-text learning, with applications across large-scale multimodal alignment [11,28,42,61] and unsupervised visual representation learning [24,52].
- Several lines of work have studied the shortcomings of the original InfoNCE objective [54], specifically, the selection and importance of negative samples.
- Chuang et al. [12] present a debiasing approach to account for false negatives at very large batch sizes, typical in large-scale pretraining.
- Kalantidis et al. [30] present a MixUp approach to improve the quality of hard negative samples for unsupervised alignment.
- Using model-specific hard negatives in the training objective is proven to reduce the estimation bias of the model as well [88].
- Contrary to prior semi-supervised work, we extend the model-based hard negative objective, first proposed in Robinson et al. [62] to multimodal alignment.

## Method
- We consider the task of contrastive imagetext pretraining.
- Given a dataset D = {(I i , T i )} N i=1 of image-text pairs, we want to learn a dual encoder model φ = {φ image , φ text }, where φ image represents the image encoder, and φ text denotes the text encoder.
- We use the shorthand x = φ image (I) and t = φ text (T ) to denote the encoded images and texts, respectively, for an image-text pair (I, T ).
- We will now describe the three crucial components of our approach followed by the final training objective.

### Complexity, Action, and Text (CAT) filtering
- Our complexity, action, and text spotting (CAT) filtering is a combination of two filters: a caption complexity filter that removes image-caption pairs if a caption is not sufficiently complex, and an image filter that removes pairs if the image contains text matching the caption to prevent polysemy during alignment.
- We use the LAION-2B pre-cleaned obtained after using filters 3 in [67] as the base dataset.
- Filtering captions via complexity & actions.
- Noisy web-scale datasets do not have any semantic-based curation, and hence captions can be irrelevant, ungrammatical and unaligned.
- Our motivation is to decrease such noise by simply selecting captions that possess sufficient complexity, so that the training distribution matches the target tasks.
- To this end, we build a fast rule-based parser that extracts objects, attributes and action relations (see Figure 3 for an example) from text and we use the resulting semantic graph to estimate the complexity of the image captions.
- Specifically, we define the complexity of a caption as the maximum number of relations to any object present in the parse graph.
- For example, in the caption "A black cat is chasing a small 3 Not-suitable-for-view images and toxic captions. brown bird," the object "bird" has the attributes "small", "brown" and "A black cat is chasing", and hence, the complexity of the caption is C3.
- We only retain samples that at least have a C1 caption complexity.
- To further remove pairs likely containing products, we filter out captions if they do not contain at least one action (as obtained from the parser).
- Filtering images via text-spotting.
- Image-caption pairs in web-scale datasets often display part of the caption as text in the image (on visual inspection, we found up to ∼30% such examples for LAION [65]).
- Minimizing the objective, in these cases, can correspond to spotting text (e.g., optical character recognition) rather than the high-level visual semantics (e.g., objects, attributes) we would like the model to align to.
- This will reduce performance on object-centric and scene-centric downstream zero-shot tasks, and hence we remove such images from the training set using an offthe-shelf text spotter [37].
- We remove image-text pairs with a text spotting confidence of at least 0.8 and at least 5 predicted characters matching the caption in a sliding window.
- We observe (by inspection) that this approach is efficient at identifying images with text, and failure cases are primarily in non-English text.
- Filtering with multilingual text spotters trained can fix this issue, however, we leave this as future work.

### Concept distillation
- Recognizing visual concepts in images that correspond to objects and attributes in corresponding captions is crucial for alignment
- We propose to distill these concepts from a pre-trained teacher model to our image encoder
- Specifically, we add two auxiliary linear classifiers on top of the encoded image embeddings x to predict (i) objects and (ii) visual attributes and use the teacher model to generate the pseudo-labels for training them
- These classifiers are trained jointly with the contrastive loss
- We parse image captions using a semantic parser that extracts objects and attributes from text
- We then train the linear classifiers on the teacher model embeddings with a soft-target cross-entropy loss [20], after square-root upsampling lowfrequency concepts [48]
- It is important to freeze the backbone of the teacher model to make sure we retain the advantages of using a stronger model for distillation
- For each image, we then use these trained linear classifiers to generate two softmax probability vectors -p obj for objects, and p attr for attributes, respectively
- To minimize the storage overhead, we further sparsify them by retaining only the top-k predicted class values and re-normalizing them to generate the final pseudo-labels
- During multimodal training, we use the cross-entropy loss with these pseudo-label vectors as targets

### Multimodal alignment with hard negatives
- Contrastive learning is the defacto approach for multimodal alignment
- While this approach has enjoyed immense success in multimodal alignment, when learning from largescale noisy datasets, uniform sampling as applied in noisecontrastive estimation can often provide negative samples that are not necessarily discriminative, necessitating very large batch sizes.
- For the problem of contrastive self-supervised learning, Robinson et al. propose an importance-sampling approach to reweight negative sam-ples within a batch so that "harder" negatives are upsampled in proportion to their difficulty.
- We present a similar strategy for multimodal alignment. Specifically, for some α ∈ (0, 1], β ≥ 0, we propose the following hardnegative noise contrastive multimodal alignment objective: Where the weighing functions are given as 4 : The weights w β are designed such that difficult negative pairs (with higher similarity) are emphasized, and easier pairs are ignored. Furthermore, α rescales the normalization with the positive terms to account for the case when false negatives are present within the data.

### Training objective
- For any batch of n image-text pairs, we minimize the objective function
- The objective function is linear and takes into account the predicted objects and attributes from the teacher model
- The objective function is minimized using a classifier called a linear discriminant
- The objective function is minimized using a loss function called hardnegative contrastive alignment loss

## Experiments
- Evaluation across a broad range of vision and vision-language tasks
- Provides extensive ablations on 29 tasks
- Compare with state-of-the-art approaches on popular zero-shot benchmarks
- Alternate approach to do few-shot classification

### Experimental setup
- We use a 2.1B English caption subset of the LAION-5B dataset
- We filter out sample pairs with NSFW images, toxic words in the text, or images with a watermark probability larger than 0.5, following [67]
- This leaves us with 1.98B images, which we refer to throughout the paper as the LAION-2B dataset
- We explore training our models on a collection of Public Multimodal Datasets (PMD) from [68]
- After downloading 5 the data we are left with 63M (vs. 70M reported in [68]) image-text pairs due to missing samples and SBU Captions [55] (originally in PMD) going offline
- For our model architecture, we closely follow CLIP by Radford et al. [61]
- We utilize Vision Transformers (ViT) [17] for images and Text Transformers [73] for captions
- We experiment with 3 different architectures, denoted as B/32, B/16, and L/14, where 32, 16, and 14 denote the input image patch size
- See the supplementary for architecture details
- For distillation and fine-tuning experiments, we utilize the public SWAG-ViT models [69]
- We use the Adam [32] optimizer with a decoupled weight decay [47] and a cosine learning rate schedule [46]
- The input image size is 224×224 pixels
- To accelerate training and save memory, we use mixed-precision training [50]
- All hyperparameters are presented in the supplementary
- They are selected by training B/32 on a small scale setup, and reused for all architectures
- For ViT-L/14, we further train the model at a higher 336px resolution for 400M samples, denoting this model as L/14@336
- We evaluate our models on a zero-shot benchmark of 29 tasks: (i) 17 image classification, (ii) 10 cross-modal retrieval, (iii) 2 visual question answering.

### Ablations on zero-shot benchmarks
- dataset filtering
- distillation from objects and attributes predictions
- ablation of performance over zero-shot Accuracy@1 on the ImageNet1K (IN) validation set, text-to-image (T2I) and image-to-text (I2T) zero-shot Recall@1 on the COCO (COCO) and Flickr (FLICKR) test sets
- effect of dataset filtering
- effect of distillation approach
- robustness to the number of top-k predictions used

### Comparison with zero-shot state of the art
- Our models have same training and inference complexity as state-of-the-art dual-encoder models.
- Our models outperform models with same architecture by substantial margins.
- Our best models DiHT-L/14 and DiHT-L/14@336 trained at higher 336px resolution for additional 400M samples outperform models with significantly more complexity on popular text-image COCO and Flickr benchmarks.
- Compared to ALIGN [28] that has approximately twice the number of parameters compared to our DiHT-L/14 model and is trained on 4x bigger data, we improve the performance substantially for all the retrieval benchmarks.
- Our model also performs better than FILIP [83] which utilizes token-wise similarity to compute the final alignment, thus noticeably increasing the training speed and memory cost.
- We also outperform Florence [85] on all 4 retrieval benchmarks.

### Few-shot linear probing
- The ideal scenario for leveraging zero-shot recognition models is to warm start the task without training data and then improve the performance (by training a linear probe) via few-shot learning as more and more data is seen.
- However, in practice, few-shot models perform significantly worse than zero-shot models in the low-data regime.
- We present an alternate approach to do few-shot classification with prompt-based initialization. The key idea of our approach is to initialize the classifier with the zero-shot text prompts for each class, but to also ensure that the final weights do not drift much from the prompt using projected gradient descent (PGD).
- While few-shot models have been initialized with prompt priors in the past with naive L 2 penalties for weight to prevent catastrophic for-Table 5. Comparison with zero-shot state-of-the-art dual-encoder models.
- px: input image size; #P: model size; #D: training dataset size; #S: total samples processed at training.
- We evaluate CLIP [61] and OpenCLIP [27] using our codebase, other numbers are copied from respective papers.
- Grouped models (e.g., ViT-B/32) share same vision and language architecture as our model, following CLIP [61], others have different architectures and we outline the vision one.
- FILIP uses token-wise similarity, which is more expensive than global-token similarity and requires adapting the architecture, hence we put it in "Other".
- Getting [33], these approaches do not improve performance and the model simply ignores the supervision. In contrast, for any target dataset D target = {(x i , y i ) n i=1 }, where x i = φ image (I i ) denotes the image features from the trained image tower, we solve the following optimization problem, for some δ, δ b > 0:
- Here W 0 ∈ R d×nc denotes the prompt initialization from the text encoder. To optimize the objective, one can use projected gradient descent [5].
- We observe that our approach is able to bridge the gap between zero-shot and 1-shot classification, a common issue in prior linear probe evaluations.
- Figure 6 presents the full summary of results on the Ima-geNet1K [64] k-shot classification task.
- Hyperparameters δ and δ b for our approach, and weight decay for the baseline approach of training linear probes from scratch are found using grid search.
- Note that compared to the baseline, our method performs substantially better at very low values of k and maintains the performance continuum from zero-shot to 1-shot, and so on. At large k values, both approaches perform similarly, since there are sufficient data samples to render the zero-shot initialization ineffective.
- To further showcase the strength of our approach, we also compare our performance with linear probes trained on powerful SWAG [69] models that are especially suited for this task. Note that our approach outperforms the much larger SWAG ViT-H/14 model up to 25-shot classification.

## Conclusion and future work
- The paper demonstrates that it is possible to achieve substantial improvements in zero-shot performance on retrieval and classification tasks through large-scale pre-training
- CAT filtering approach can be applied generically to any large-scale dataset for improved performance with smaller training schedules
- Concept distillation approach presents a compute and storage efficient way of leveraging very large capacity pretrained image models for multimodal training
- Simple projected gradient approach covers the crucial performance gap between zero-shot and few-shot learning
