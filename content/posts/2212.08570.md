---
title: "Audio-based AI classifiers show no evidence of improved COVID-19 screening over simple symptoms checkers"
date: 2022-12-15T15:44:02.000Z
author: "Harry Coppock, George Nicholson, Ivan Kiskin, Vasiliki Koutra, Kieran Baker, Jobie Budd, Richard Payne, Emma Karoune, David Hurley, Alexander Titcomb, Sabrina Egglestone, Ana Tendero Cañadas, Lorraine Butler, Radka Jersakova, Jonathon Mellor, Selina Patel, Tracey Thornley, Peter Diggle, Sylvia Richardson, Josef Packham, Björn W. Schuller, Davide Pigoli, Steven Gilmour, Stephen Roberts, Chris Holmes"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08570v1.webp" # image path/url
    alt: "Audio-based AI classifiers show no evidence of improved COVID-19 screening over simple symptoms checkers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08570)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08570).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/audio-based-ai-classifiers-show-no-evidence).

# Abstract
- AI classifiers trained on audio recordings can accurately predict severe acute respiratory syndrome coronavirus 2 (SARSCoV2) infection status
- After matching on measured confounders, such as age, gender, and self reported symptoms, AI classifiers' performance is much weaker (ROC-AUC 0.619 [0.594, 0.644])

# Paper Content

## Results

### Study design
- volunteers were directed to the "Speak up and help beat coronavirus" web page
- volunteers were instructed to provide audio recordings of four respiratory audio modalities
- demographic and health metadata, along with a validated PCR test result, were transferred from existing T+T/REACT records
- additional audio-specific metadata were produced from the audio files after collection
- the final dataset consisted of 23,514 COVID + and 44,328 SARS-CoV-2 PCR-negative (COVID − ) individuals
- Figure 1 summarises the dataset, with a more detailed description in the Methods Section
- Figure 2(a) details the collected dataset pre and post data quality filtration checks

### Characterising and controlling recruitment bias
- The vast majority of individuals in Pillar 21 of the NHS T+T programme were PCR tested as a direct consequence of reporting symptoms.
- Figures 1(e) and 1(f) display our participants' symptom profiles, stratified by COVID-19 infection status.
- Figure 3(a) presents the joint distribution of COVID-19 status and binary symptoms status as "Symptomsbased enrolment", in contrast to Figure 3(b), which presents "General population enrolment", based on random sampling from a general population having 2% COVID + prevalence.
- Note that the dependence between binary symptoms status and COVID-19 is stronger under Symptoms-based enrolment (population correlation coefficient ρ=0.66) compared to General population enrolment (ρ=0.15).
- To clarify the source of enrolment-driven associations, we use a simplified causal model of symptoms-based recruitment (Figure 4(a)(i)).
- Enrolment is influenced jointly by COVID-19 status, self-reported symptoms, and factors such as age and gender (Figure S1 shows a detailed Bayesian knowledge graph of the recruitment process).
- Collecting data only from enrolled individuals is, in effect, conditioning on e i = 1 at the enrolment node in Figure 4(a)(i).
- Since the enrolment node has directed edges incoming from both COVID-19 status and self-reported symptoms, conditioning on it induces a non-causal dependence between its parent nodes (in addition to the causal dependence of symptoms on COVID-19 status).
- Figure 4(a)(ii) displays the moralised undirected graph implied by Figure 4(a)(i), conditional on enrolment, with the strong COVID-19-to-symptoms dependence represented by a thick line, illustratively labelled ρ=0.66 with reference to Figure 3(a).
- In contrast, Figure 4(b)(i) is conditional on random enrolment and does not introduce any additional non-causal association between COVID-19 status and self-reported symptoms.
- If a study's enrolment bias is unaddressed and shared across both training and held-out test sets, a classifier will appear to perform well but that performance may not generalise to other data sets.
- This is due to two effects. First, the classifier may learn to predict using confounding variables that are not causally related to COVID-19, but are associated because of their influence on enrolment; e.g., gender, age, or symptoms unrelated to COVID-19.
- Second, even symptoms that are truly causally related to COVID-19, such as a new continuous cough, may exhibit inflated association with COVID-19 in the enrolled cohort due to their influence on enrolment.
- As well as leading to poor generalisability, audible characteristics that are non-causally but strongly associated with COVID-19 can obscure any COVID-19 acoustic signature that may exist.
- This is illustrated in Figure 4(a)(iii), where the association between classifier prediction and SARS-CoV-2 status is mediated by symptoms instead of via the COVID acoustic signature of interest.
- Even in the case of randomised enrolment from the general population, a classifier may learn to predict SARS-CoV-2 status via self-reported symptoms, as opposed to via a latent COVID-19 acoustic signature, as illustrated in Figure 4

### Primary analyses
- Pre-specified analysis plan
- Detailed analyses to be conducted
- Generated test/validate/train data splits
- Audio-based COVID-19 prediction performance
- Table 1 presents our study's COVID-19 prediction performance
- Best prediction is achieved with the sentence modality
- When we control for enrolment bias based on age, gender and selfreported symptoms using matching, predictive accuracy drops to a consistently low level of ROC-AUC=0.619
- Exploring predictive accuracy within matched strata

### Confirmatory analyses and validation
- Audio-based classifiers can be useful in practice if they deliver improved performance relative to classifiers based on self-identifiable symptoms.
- It is beneficial to assess the performance of ABCS classifiers in test sets that reflect the application of the testing protocol in a real-life setting.
- We generate a "general population" test set, through balanced subsampling without replacement from our combined Standard and Longitudinal test sets, so as to capture the age/gender/symptoms/COVID-19 profile of the general population during the pandemic.
- Specifically, the proportion of symptomatic individuals is set at 65% in the COVID + subgroup 27 , compared to a setting of one of (10%, 20%, 30%) symptomatic individuals in the COVID − subgroup; the age distribution is constrained to be the same in both COVID + and COVID − subgroups; and with males/females balanced in 1:1 ratio in each COVID + /COVID − subgroup.
- We benchmark the COVID-19 predictive performance of the audio-based SSAST classifier against the performance attainable through random forest (RF) classifiers trained just on self-identifiable symptoms.
- We also include in the benchmarking an RF classifier taking as inputs the audio-based SSAST probabilistic outputs alongside self-identifiable symptoms ("Symptoms+Audio" RF classifier).
- Training for all three classifiers is performed in our Standard training set.
- The resulting ROC curves are shown in Figure 6(a)-(c).
- Focusing on the general population with 20% of COVID − symptomatic in Figure 6(b), the combined Symptoms+Audio RF classifier offers a significant (p=7.9 × 10 −9 , DeLong test) but small increase in predictive accuracy (ROC-AUC=0.782 over the Symptoms RF classifier (ROC-AUC=0.761 which in turn yields a less significant (p=0.0015) but larger increase in ROC-AUC compared to the Audio-only classifier (ROC-AUC=0.733 [0.717−0.748]).
- We replicate these findings using an external dataset.

## Discussion
- We have collected the largest PCR-validated dataset of its kind to date
- Before accounting for recruitment bias we observe high predictive accuracy in line with previous studies (ROC-AUC=0.85)
- However, upon controlling for recruitment bias via matching, only a small amount of residual predictive variation remains (ROC-AUC=0.62), some of which we attribute to unmeasured confounders.
- In separate analysis we find that classification based upon selfreported symptoms outperforms audio-based AI classifiers in simulated realistic settings, and replicate this finding on external studies.
- In exploring the basis for these high-level results we have provided new insights into recruitment bias in ABCS, particularly as effected by symptoms-based enrolment, and into the practical utility of such classifiers as compared with non-audio symptoms-based classifiers.
- COVID-19 is well known to be causally related to particular self-identifiable symptoms, such as a new continuous cough. This has allowed such symptoms to be used by governments during the pandemic as a basis for population intervention to control disease spread, e.g., a triage tool for individuals, via self-screening and without recourse to audio recording.
- It is therefore desirable to develop audio-based classifiers that can augment and complement the information provided by self-identifiable COVID-19-specific symptoms, i.e., to learn clinically valuable latent acoustic signatures caused by COVID-19.
- Problematically, enrolment based upon symptoms has the potential not only to artificially inflate the association between COVID-19 and its particular symptoms, but also to introduce association between COVID-19 and symptoms that are not COVID-19 specific. Furthermore, enrolment based upon other characteristics, such as gender and age, may additionally introduce non-causal COVID-19-to-gender or COVID-19-to-age associations in the enrolled subpopulation, possibly interacting with symptoms.
- Under such recruitment bias, classifiers trained to predict COVID-19 in enrolled subpopulations may learn to predict self-identifiable COVID-19-specific symptoms, thereby providing no additional utility beyond a classifier trained directly on those self-screened symptoms.
- Worse, the classifier may learn to predict age/gender/non-COVID-19-specific symptoms as proxies for COVID-19 in the enrolled subpopulation, in which case its performance will not generalise to subpopulations unaffected by the same recruitment bias.

## Methods

### Dataset collection and characteristics
- The first split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study.
- The second split, used for testing the models, consisted of the remaining individuals who enrolled in the REACT study and the NHS T+T channel.
- The third split, used for training the models, consisted of the first 3,000 individuals who enrolled in the NHS T+T channel.
- The fourth split, used for testing the models, consisted of the remaining individuals who enrolled in the NHS T+T channel.
- The fifth split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study and the NHS T+T channel.
- The first split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study.
- The second split, used for testing the models, consisted of the remaining individuals who enrolled in the REACT study and the NHS T+T channel.
- The third split, used for training the models, consisted of the first 3,000 individuals who enrolled in the NHS T+T channel.
- The fourth split, used for testing the models, consisted of the remaining individuals who enrolled in the NHS T+T channel.
- The fifth split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study and the NHS T+T channel.
- The first split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study.
- The second split, used for testing the models, consisted of the remaining individuals who enrolled in the REACT study and the NHS T+T channel.
- The third split, used for training the models, consisted of the first 3,000 individuals who enrolled in the NHS T+T channel.
- The fourth split, used for testing the models, consisted of the remaining individuals who enrolled in the NHS T+T channel.
- The fifth split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study and the NHS T+T channel.
- The first split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study.
- The second split, used for testing the models, consisted of the remaining individuals who enrolled in the REACT study and the NHS T+T channel.
- The third split, used for training the models, consisted of the first 3,000 individuals who enrolled in the NHS T+T channel.
- The fourth split, used for testing the models, consisted of the remaining individuals who enrolled in the NHS T+T channel.
- The fifth split, used for training the models, consisted of the first 3,000 individuals who enrolled in the REACT study and the NHS T+T channel.

### Machine Learning Models
- Three separate models were implemented to evaluate the task of COVID-19 detection from audio
- Each represents an independent machine learning pipeline
- Collectively the three methods cover the machine learning research space well
- These three approaches are summarised in Figure S6
- openSMILE-SVM was used as the baseline model
- The linear SVM was fitted to the space and tasked with binary classification
- The optimal SVM configuration was selected based on the validation set
- The features used to create an intermediate representation were Mel filterbank features
- The uncertainty analysis was performed over all audio modalities for a range of train-test partitions

### Matching methodology.
- The 1:1 Matched test set was constructed by exactly balancing the numbers of COVID+ and COVID-individuals in each stratum, where to be in the same stratum individuals must be Matched on all of (recruitment channel) x (10-year wide age bins) x (gender) x (all of six binary symptoms covariates).
- The six binary symptoms covariates are: 1) Cough, 2) Sore throat, 3) Asthma, 4) Shortness of breath, 5) Runny/blocked nose, and 6) "At least one symptom".
- In the resulting Matched test set we have 907 COVID+ and 907 COVID-participants.
- The Matched training set was constructed similarly to the Matched test set, though with slightly different strata based on the following metadata so as to increase sample size: (10-year wide age bins) x (gender) x (all of seven binary covariates).
- The seven binary covariates used for the match training set are: 1) Cough, 2) Sore throat, 3) Asthma, 4) Shortness of breath, 5) Runny/blocked nose, 6) COPD or Emphysema, and 7) Smoker status.
- The resulting Matched training comprises 2,599 COVID+ and 2,599 COVID-participants.
- We consider the action of applying a particular testing protocol to an individual randomly selected from a population. and use u ŷ,y to denote the combined utility of the consequences of outcome O ŷ,y .
- For a particular population prevalence proportion, π, the p ŷ,y are subject to the constraints leading to the following relationships, valid for π ∈ (0, 1), involving the sensitivity and specificity of the testing protocol:
- The expected utility is: where ( 4) and ( 5) are substituted in (8) to obtain (9), and ( 6) and ( 7) are substituted in (9) to obtain (10).

## Data Availability
- The dataset can be requested for access through the UK Data Service by registered or authenticated users
- Audio data are provided in .wav format, with four files (one for each recording) for each of the 72,999 participants (unless missing)
- Metadata including audio filenames are provided in three .csv files, linked by a participant identifier code
- Although the dataset is fully anonymised, and therefore does not contain any personal data, it has been deposited as safeguarded data in line with the privacy notice provided to participants
- Safeguarded data can be used for non-commercial, commercial and teaching projects
- Safeguarded data requires the data controllers to know who is using the data and for what purposes
- All potential users need to complete a registration and authentication process
- All users must also agree to the special conditions stated in the data access process

### Exploratory approaches to identify the influence of unmeasured confounders
- Matching can be used to control for bias attributable to confounders that are measured.
- The issue with unmeasured confounders is that they can be identified by audio models and lead to artificially inflated and non-generalisable classification performance.
- We introduce two exploratory approaches aimed at quantifying the effects of unmeasured confounding.
- Method 1: Weak-Robust approach. We take inspiration from work done in Natural Language Understanding, where weak models are used to propose samples in which bias is likely to be present.
- Consider a dataset of pairs of recordings x ∈ X and COVID-19 status/labels y ∈ Y . For pairs (x i , y i ), let the b(x i ) be biased features which might exist in x i and exhibit association with y i . We train a low capacity model, f b , designed to be insufficiently expressive to fit the targeted signal. Then, for correctly classified test cases, i.e., for f b (x i ) = y i , we hypothesise that the instance x i may contain confounding signal, b(x i ), because the model was able to predict the instance correctly despite not being sufficiently expressive to fit the targeted signal.
- Our approach is to remove individuals for whom the weak model correctly predicts COVID-19 status, the intention being to remove test set cases which may contain confounding signal. We create weak models through training SVM models on the first k Principal Components (PCs) of the openSMILE feature representions. The smaller the value of k, the weaker the model. Our approach requires us to design the weak model so as it lacks the capacity to fit the true targeted signal. To do this, we introduce two novel steps. First a calibration step, where we evaluate the weak model on a similar and known easier task than COVID-19 detection, namely classifying between coughing and laughing sounds. This calibration task was chosen as it was also a classification task in human respiratory space. We hypothesise that if the weak model cannot perform well at this calibration task it is too weak to identify COVID-19 audio features. Second, we project the COVID + signal to the null space through projecting both the COVID + and COVID − cases onto the first k PCs of only the COVID − cases. Good classification performance in this space is likely to be due to variation which exists in the COVID − population and not the COVID + population.
- As detailed in Figure S7, as we remove the cases which we hypothesise to contain confounding signal, the SSAST performance on the Matched test set quickly drops to a random prediction (UAR ≈ 0.5). We note that further removal of Matched test instances beyond the calibration threshold point leads to a drop in performance below 0.5. This is to be expected as we are removing the cases which the weak model is able to classify well, leaving behind only the miss-classified cases, cases the SSAST evidently also struggles to correctly classify.
- As k has exceeded the point at which the weak model performs well at the calibration task, this further drop in performance could either be due to the weak model identifying instances which truly contain the signal for COVID-19 or more bias.
