---
title: "There's Plenty of Room Right Here: Biological Systems as Evolved, Overloaded, Multi-scale Machines"
date: 2022-12-20T22:26:40.000Z
author: "Joshua Bongard, Michael Levin"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10675v1.webp" # image path/url
    alt: "There's Plenty of Room Right Here: Biological Systems as Evolved, Overloaded, Multi-scale Machines" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10675)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10675).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/there-s-plenty-of-room-right-here-biological).

# Abstract
- The applicability of computational models to the biological world is an active topic of debate
- We argue that a useful path forward results from abandoning hard boundaries between categories and adopting an observer-dependent, pragmatic view
- Such a view dissolves the contingent dichotomies driven by human cognitive biases (e.g., tendency to oversimplify) and prior technological limitations in favor of a more continuous, gradualist view necessitated by the study of evolution, developmental biology, and intelligent machines
- Efforts to re-shape living systems for biomedical or bioengineering purposes require prediction and control of their function at multiple scales
- This is challenging for many reasons, one of which is that living systems perform multiple functions in the same place at the same time
- We refer to this as "polycomputing" - the ability of the same substrate to simultaneously compute different things
- This ability is an important way in which living things are a kind of computer, but not the familiar, linear, deterministic kind; rather, living things are computers in the broad sense of computational materials as reported in the rapidly-growing physical computing literature
- Learning to hack existing polycomputing substrates, as well as evolve and design new ones, will have massive impacts on regenerative medicine, robotics, and computer engineering.

# Paper Content

## Introduction
- Biological systems that polycompute also contributes to an ongoing conceptual debate in interdisciplinary science ---the applicability of computer frameworks and metaphors to living systems [7] ---in three ways.

## Current Debates: dissolving dichotomous thinking
- Researchers look to nature for inspiration when technological progress slows in a particular domain
- In the spirit of this thesis, we acknowledge that there is no clear dividing line between these two "types" of machines
- As always, there is a continuum across machines capable of more or less computation

## Structure function mapping and polycomputing.
- An organism's structure can be compared to a machine's structure
- An organism's function can be compared to a machine's function
- It is difficult to tease apart structure and function for comparisons between organisms and machines

## Assumed distinction Counterexamples

## Software/Hardware
- Physical materials that compute and learn
- Tape/Machine Tape-less von Neumann self replicators
- Digital/analog Evolved digital circuits can exploit electromagnetic properties of the circuit's substrate
- Machine/Life form AI-designed organisms
- Automaton/Free agent
- The intentional stance
- Brain/Body Computational metamaterials

## Body/Environment
- Cells in a multicellular body are the environment for a cell
- AI technologies that seem to pass verbal, visual, or physical Turing tests are created/evolved
- Artefacts designed by human-created evolutionary algorithms are common
- There is a spectrum of complementarity between biology and technology

## Dichotomous thinking in the life sciences.
- Biological categories are not dichotomous, but rather a spectrum of mixed properties.
- Neither evolution nor developmental biology support clean bright lines between different categories.
- All of life is interoperable, which makes it impossible to objectively classify different types of chimeras.

## Dichotomous thinking in computer science.
- Physical computing is a conceptual advance that does similar work to that of morphological computation.
- In mechanical computing, computation is performed without recourse to electronics and instead relies on optical, mechanical, or quantum phenomena.
- Recent advances in mechanical computing show how inert bulk materials can be induced to perform non-trivial calculations, including error backpropagation, the algorithmic cornerstone of modern AI.
- The recent demonstration by one of the authors (Bongard) that propagation of acoustic waves through granular metamaterials can be exploited to perform multiple Boolean operations in the same place at the same time can be considered the first example of mechanical polycomputing.
- Morphological computation, a concept originating in the robotics literature, upholds that the body of an animal or robot can indeed compute and, moreover, it can "take over" some of the computation performed by a nervous system or robot control policy.

## Polycomputing in bodies and brains.
- If polycomputing is to be considered a kind of computation, one can then ask whether polycomputation can be shuttled back and forth between biological bodies and brains, or can be made to do so between machine bodies and brains.
- Traditional computation is assumed to be substrate agnostic: if configured appropriately, any physical material can compute.
- In contrast, only vibrational materials currently seem capable of polycomputing, as polycomputation requires storage of the results of multiple computations at the same place and at the same time, but at different peaks in the frequency spectrum.
- This would seem to preclude some materials, like digital circuitry and biological nervous systems, from performing polycomputation, since digital circuitry traffics in electrons, and nervous systems traffic in chemicals and ions; neither seems to traffic in vibration.
- At first glance, this seems poised to rescue the brain/body distinction via the surprising route of suggesting that bodies and brains are different things because bodies polycompute but brains do not.
- However, this odd-seeming distinction may be short-lived.
- It has been shown that neurons may communicate mechanically [90] in addition to electrically and chemically.
- If so, such mechanical neural communication may contain vibrational components, suggesting nervous systems may be polycomputing as well.
- If this turns out to be the case, it in turn opens the possibility that nervous tissues may have evolved incremental enrichments of non-neural cells' already proven ability to polycompute.
- This would once again frustrate our attempts to cleave body from brain, in this case by the claim that one polycomputes, while the other does not.
- Mechanical computing and morphological computation are closely related to another way in which computer science provides useful viewpoints for biology.
- In CS, the view that an algorithm drives (functionally determines) outcomes, even though implemented by the microphysics of electron flows through a CPU, is accepted and indeed essential to perform the useful and powerful activity of programming.
- This is in stark contrast to debates in biology and neuroscience about whether higher levels of description are merely epiphenomenal [91][92][93][94][95], supervening on biochemical microstates (reductionism).
- Computer science clearly shows how taking a causal stance at higher levels enables progress.
- Indeed, the recent advances in information theory around quantifying causal emergence [95,96] show how the same Boolean network can be computing different functions simultaneously (Figure 2), depending on the level of analysis chosen by an observer [95].
- This has interesting biological implications, since such networks are a popular model for understanding functional relationships between genes [97][98][99].
- Biological nervous systems -the human brain in particular -have attracted increasingly computational metaphors throughout the industrial revolution and information age.
- The application of computational concepts to brains has had unintended consequences, most of all the implicit assumption that tissues, cells, and other biological systems that are not brains do not compute.
- However, the brain:body dichotomy is increasingly being dismantled by studies of basal cognition (i.e., intelligence in unfamiliar embodiments) in plants [100,101], single somatic cells [102][103][104], microbes [105][106][107][108], and at the tissue level in organisms [109][110][111][112].
- Indeed, the bioelectric and neurotransmitter dynamics that implement predictive processing and other computations in brains are speed-optimized versions of extremely ancient bioelectrical computations that navigated spaces (such as anatomical morphospace, physiological space, etc.) long before brains and muscles appeared [58,113,114].
- The benefit of dissolving these arbitrary distinctions is that commonalities and fundamental design principles across substrates are beginning to emerge across evolved and designed constructs at all scales [32,103,119].
- Frameworks that are to survive the next decades, in which technological advancement will further enmesh biology and technology, must facilitate experimental progress at the expense of philosophical preconceptions.
- More than that, they must provide unifying insight by identifying symmetry and deep order across fields, to combat the ever-growing problems of big data and the interpretability crisis [120,121].

## Learning from superposed systems in engineering
- Multiple computations can be performed simultaneously
- Quantum computing has made clear that multiple computations can be performed
- But practical and general-purpose quantum computing remains a distant prospect
- Recently, one of the authors (Bongard) has shown that quantum effects are not necessary for polycomputing [8]
- Holographic data storage (HDS; [122]) stores and reads data that has been dispersed across the storage medium
- HDS stores and reads data that has been dispersed across the storage medium by etching a distributed representation of a datum across the storage medium, for example with laser light, from a particular direction
- That datum can then be retrieved by capturing the reflection of light cast from the same direction
- By storing data in this way from multiple directions, parts of multiple pieces of data are stored in the same place, but accessed at different times
- Exactly how this can be achieved in hardware such that it affords appreciable increases in storage density over current traditional approaches has yet to be resolved
- Efforts are also underway to design such materials to maximize the number of overlapping memories that they can store

## Biology is massively overloaded: polycomputing
- Reductionism: most analytic approaches are reductionist and "reduce" to characterizing one phenomenon that arises in one place, at one time, under one set of circumstances.
- Synthesis is difficult: polycomputable technologies seem, to date, resistant to traditional engineering design principles such as hierarchy and modularity.
- A fundamental problem is that typical human designs are highly constrained, such that changes made to optimize one function often interfere with another.
- Although humans struggle to manually design polycomputing technologies, it turns out that AI methods can do so, at least in one domain.
- We have recently applied an evolutionary algorithm-a type of AI search method-to automatically design a granular metamaterial that polycomputes.
- It does so by combining vibrations at different frequencies at its inputs, and providing different computations in the same place, at the same time, at different frequencies.
- Figure 3 illustrates this process.
- Many biological functions have been usefully analyzed as computations [56] (Table 2).
- These include molecular pathways [124,127], individual protein molecules [57], cytoskeletal elements [125,128,129], calcium signaling [130], and many others.
- Although differing from the most familiar, traditional algorithms, the massively parallel, stochastic (indeterministic), evolutionarily-shaped information processing of life is well within the broad umbrella of computations familiar to workers in the information sciences.
- Indeed, information science tools have been used to understand cell-and tissue-level decision-making, including estimations of uncertainty [104,[131][132][133][134][135][136][137][138][139][140][141], analog/digital dynamics [142], and distributed computation [100].
- Bioelectric networks within non-neural tissues, just like their neural counterparts, have shown properties very amenable to polycomputation, including ability to store diverse pattern memories that help execute morphogenesis on multiple scales simultaneously [20,22,112,[143][144][145][146][147][148][149], and enables the same genome to produce multiple diverse outcomes [150].
- A key aspect to recognizing unconventional computing in biology is that the notion of "what is this system really computing" has to be dropped (because of the multiple observers issue described above).
- Once we do this, biology is rife with polycomputing at all scales.
- Examples include the storage of a (possibly uncountable) number of memories in the same neuronal realestate of the brain [151,152], and many others summarized in Table 2.
- We do not yet know whether the prevalence of polycomputing is because of efficiency, robustness, or other gains that override the evolutionary difficulty of finding such solutions.
- Or, perhaps we overestimate the difficulty, and evolution has no problem identifying such solutions -they may indeed be the default.
- If so, it may be because of the generative, problem-solving nature of developmental physiology as the layer between the genotype and the phenotype [32,153].
- For these reasons, polycomputing may join degeneracy and redundancy [154] as one of the organizing principles that underlies the open-ended, robust nature of living systems.

## Table 2: Examples of biological polycomputing at diverse scales Multiple Computations in the Same Biological Hardware Reference
- Mitochondria also act as micro-lenses in photoreceptors
- Proteins acting in multiple (fluctuating) conformations
- Gene regulatory networks with multiple memories/behaviors
- Chemical networks performing neural network tasks
- RNA encoding enzyme and protein functions
- DNA with more than one active reading frame (overlapping genes)
- Ion channels that are also transcription factors
- DNA transcription factors working in DNA replication machinery
- Polysemanticity and superposition of meaning in neural networks and language understanding
- Cytoskeleton performing computations via simultaneous biomechanical, bioelectrical, and quantum-mechanical dynamics
- Electrophysiological networks performing memory functions while regulating heartbeat
- Bioelectric networks performing physiological functions while also regulating morphogenesis
- Spider webs as auditory sensors and structural elements
- Pleiotropy -most genes have multiple functions
- Holographic memory in the brain
- Multiple behaviors in the same neuronal circuit
- Multiple personalities in the same brain (dissociative identity disorder and split brain studies)
- Calcium dynamics performing as a hub in a huge bowtie network of diverse simultaneous processes
- 4.1. Evolutionary pivots: origins of polycomputing?
- Evolution is remarkably good at finding new uses for existing hardware because of its fundamental ability to generate novelty to exploit new niches while being conservative in terms of building upon what already exists. This ability to simultaneously innovate and conserve plays out across structural, regulatory, and computational domains. Moreover, re-use of the same conserved mechanisms in situ enabled evolution to pivot successful algorithms (policies) from solving problems in metabolic space to solving them in physiological, transcriptional, and anatomical (morphospace) spaces, and finally, once muscles and nerves came on the scene, to 3D behavioral spaces
- Multiscale competency architecture of life consists of molecular networks which make up cells which make up tissues which make up organs which make up organisms within swarms. Each layer is performing specific functions simultaneously; for example, the tissue layer is attempting to compute the correct attractor for the collective morphogenetic behavior of planarian fragment cells, which can build one of several head shapes). Each layer deforms the action landscape for the layer below it, providing incentives and shaping geodesics that force the lower-level components to use their behaviors in service of the higher level's goals.

## Polycomputing and the range of phenotypic possibility.
- The Xenobots are proto-organisms that result from a rebooting of multicellularity by frog skin cells [73,203].
- A key point is that Xenobots are not genetically modified -their novel functionality is implemented by perfectly standard frog cells.
- So, what did evolution learn [204][205][206][207][208] in crafting the Xenopus laevis genome and the frog egg's cytoplasmic complement?
- It was not just how to make a frog; it is how to make a system in which cells allow themselves to be coerced (by other cells) into a boring, 2-dimensional life as the animal's outer skin surface, or, when on their own, to assemble into a basal 3-dmensional creature that autonomously explores its environment and has many other capabilities (Figure 5).
- This capacity to do things that were not specifically selected for [209] and do not exist elsewhere in their (or others') phylogenetic history reveals that evolution can not only create seeds for machines that do multiple things, but ones that can do novel things.
- This is because genomic information is overloaded by physiological interpretation machinery (internal observer modules): the exact same DNA sequence can be used to build a tadpole or a Xenobot (and the same planarian genome can build the heads of several different species [210,211]).
- Thus, evolution teaches us about powerful polycomputing strategies because it does not make solutions for specific problems -it creates generic problem-solving machines, in which the competition and cooperation of overlapping, nested computational agents at all levels exploit the ability of existing hardware to carry out numerous functions simultaneously.
- This is closely tied to recent advances at the interface of the fields of developmental biology and primitive cognition, with the generation of models in which larger-scale Selves (in psychological, anatomical, etc. spaces) arise as composite systems made of smaller Selves, all of which are pursuing diverse agendas [199,212,213].
- As surprising as these examples are, we should have already seen this coming -biology had to work like that, and it could not work otherwise.

## Evolving polycomputing.
- The rate of evolution would be much slower without this multiscale competency architecture -the ability of parts to get their job accomplished even if circumstances change [216].
- In one remarkable example, the tadpole eye, placed in the wrong position on the head or even on the tail [217] still provides vision, because the eye primordia cells can make an eye in aberrant locations, move it if possible [218] and if not, connect to the spinal cord (rather than directly to the brain), providing visual signals that way [219,220].
- This competency of the substrate in regulative development and remodeling [32,200] can neutralize the lethal side effects of many mutations, enabling exploration of other possibly beneficial effects.
- For example, consider a mutation that causes displacement of the mouth and also another effect, E, elsewhere in the body. The potential benefits of E might never be explored in a monocomputational developmental architecture because the mouth defect would prevent the animal from eating and drive fitness to 0. Exploration of the effect of E would have to wait for another mutation to appear that produces the same effect without untoward side effects elsewhere -a very long wait, and often altogether impossible.
- In contrast, in a polycomputing architecture, structures solve morphological and physiological problems simultaneously: the mouth will move to the right location on its own [218], in parallel to all of the other developmental events, enabling evolution to explore the consequences of E. Thus, the overloaded competencies of cells and tissues allow evolution to simultaneously explore other effects of those mutations on phenotype (pleiotropy).
- Thus, polycomputing is a way for evolution to solve problems that monocomputing cannot.
- This occurs simultaneously at all scales of organization (Figure 4) and thus each level computes specific functions not only in its own problem space, but also participates in the higher level's space (as a component) and has an influence that deforms the action space of its lower levels' components [200].
- By behavior-shaping competent subunits as agential materials [221], evolution produces modules that make use of each other's outputs in parallel, virtually guaranteeing that the same processes are exploited as different "functions" by other components of the cell, the body, and the swarm.
- The evolutionary pressure to make existing materials do multiple duty is immense. However, much remains to be learned about how such pressure brings about polycomputing: how some materials can be overloaded with new functions without negatively impacting existing Figure 6:
- Morphogenesis plays the hand it is dealt The essence of biological morphogenesis is that it does not assume much about current circumstances and attempts to create a viable organism with whatever is at-hand. Thus, frog embryos in which eye primordia cells are moved from the head to the tail still make good eyes (A), try to connect to the spinal cord (A', red stain), and enable the organism to exhibit behavioral vision (A") despite a completely novel visual system-brain architecture which had no evolutionary prep time to get used to the new arrangement -nothing needed to be changed (at the DNA level) to make this new configuration workable. Similarly, tadpoles (B) which must rearrange their face to turn into a frog (B') can still do so even if everything is artificially placed in a scrambled configuration (B") because each organ is able to move as needed to get its job done (reach a specific region of morphospace). Finally, the cross-level nature of this overloading of basic mechanisms is seen in newt kidney tubules schematized here in cross-section. While they normally consist of 8-10 cells that communicate to make a tubule, the cells can be made experimentally very large -in that case, fewer cells will work together to make the same size tubule (C'). In...

## A new approach to identifying and harnessing computational capabilities in vivo and in silico.
- Existing neural network models can have novel computational functions
- This approach is an example of searching not for ways to re-wire the causal architecture of the system for a desired function, but searching instead for a functional perspective from which an un-modified system already embodies novel functions
- This illustrates an important principle of biological polycomputing: evolution prepared a computational affordance (the GRN) with multiple interfaces (different gene targets) through which engineers, neighboring cells, or parasites can manipulate the system to benefit from its computational capabilities.

## Conceptual transitions
- To develop better tools, we need to overcome human cognitive bias and resist the temptation to cleave phenomena apart in ways that feel comfortable.
- One approach is to look for phenomena that defy our attempted categories. Better yet is to seek gradients along which we can move from "obvious" approximations of phenomena to increasingly "non-obvious", but more accurate, reflections of reality.

## Directions of conceptual travel.
- Parallel computation is faster than serial computation
- The industrial revolution demonstrated the advantage of performing tasks in parallel
- Cognitive limitations may be a reason why people don't transition to parallelism quickly
- Modular processes are easier to understand than non-modular processes
- Polycomputing systems are not necessarily modular

## Practical implications for AI/robotics.
- learning how biological systems polycompute is important for creating more computationally dense AI technologies or robots
- technological components that polycompute may be more compatible with naturally polycomputing biological components
- polycomputing may provide a new solution to catastrophic interference
- a polycomputing agent might learn and store a new behavior at an underutilized place on the frequency spectrum of its metamaterial "brain" better than a polycomputing-incapable agent that must learn and incorporate the same behavior into its already-trained neural network controller
- what does the boot-up of a biologically-embodied intelligence consist of?
- one key aspect of this transition process is that it involves polycomputing
- during the first few microseconds when the power is first turned on, the system becomes increasingly more amenable to computational formalisms in addition to the electrodynamics lens
- maturation of the process consists of a dynamical mode which can profitably be modeled as "following an algorithm"
- one could observe externally supplied vibrations spreading through a metamaterial and consider when it makes sense to interpret the material's response as a computation
- in essence, the transition from an analog device to a computer is really just a shift in the relative payoffs for two different formalisms from the perspective of the observer

## Becoming a computer
- A system is a computer if it has an internal state, if it can read information from the environment in some way, update its behavior based on what it read and its current state, and (optionally) write information back out into the world.
- This theoretical construct has become known as a Turing Machine; any physical system that embodies it, including organisms, is formally referred to as a computer.
- This broad definition admits a wide range of actors that do not seem like computers, including consortia of crabs [234], slime molds [235], fluids [236], and even algorithms running inside other computers [237].
- With continuous dynamical systems such as these, observers may choose different views from which the system appears to be acting more or less like a physical instantiation of a Turing Machine.
- Even if an observed system seems to be behaving as if it is a Turing Machine, identifying the components of that machine, such as the tape or the read/write head, can be difficult.
- This is a common reason why it is often claimed that organisms are not machines/computers [67,238,239].
- Consider an example from the authors' own recent work [72]. We found that motile multicellular assemblies can "build" other motile assemblies from loose cells. This looks very much like von Neumann machines: theoretical machines that can construct copies of themselves from materials in their environment.
- von Neumann initially proved the possibility of such machines by constructing, mathematically, Turing Machines that built copies of themselves by referring to and altering an internal tape. However, in the biological Turing Machines we observed, there seems to be no tape.
- If there is one, it is not likely localized in space and time.
- This difficulty in identifying whether something is a computer, or at what point it becomes one, is further frustrated by the fact that biological and non-biological systems change over time: even if one holds one view constant, the system, as it changes, may seem to act more or less like a computer.
- Finally, a polycomputing system, because it can provide different computational results to different observers simultaneously, may at the same time present as different computers ---better or worse ones, more general or more specialized ones ---to those observers.
- Such behavior would not only foil the question "Is that a computer?", but would even foil any attempts to determine the time at which a system becomes a computer, or begins to act more like a computer.

## Conclusion
- Biological systems are not simply lenses through which diverse observers can all understand a given system in the same way, but indeed that several computational models can be true of a biological system at the same time.
- It is now seen that there is no one-to-one mapping between biological form and function: the high conservation of biological form and function across evolutionary instances implements a kind of multiple realizability.
- At the same time, biological components are massively overloaded toward polycomputing. Indeed, their competency, plasticity, and autonomy [2,[241][242][243] may enable a kind of second-order polycomputing where various body components attempt to model each other's computational behavior (in effect serving as observers) and act based on their expected reward.
- Thus, modern computer engineering offers metaphors much more suited to understand and predict life than prior (linear, absolute) computational frameworks.
- Not only are biological systems a kind of computer (an extremely powerful one), but they are amazing polycomputing devices of a depth which has not yet been achieved by technology.
- In this sense, biological systems are indeed different than today's computers, although there is no reason why future efforts at deep, multiscale, highly plastic synthetic devices cannot take advantage of the principles of biologic polycomputing.
- A key implication of our view is that that blanket pronouncements about what living, or non-living, machines can do are worthless: we are guaranteed to be surprised by outcomes that can only be achieved by formulating and testing hypotheses.
- It is already clear that synthetic, evolved, and hybrid systems far outstrip our ability to predict limits on their adaptive behavior; abandoning absolutist categories and objective views of computation is a first step towards expanding our predictive capabilities.
- At stake are numerous practical outcomes in addition to fundamental questions.
- For example, to make transformative advances in our ability to improve health in biomedical settings [244], we must be able to control multiple scales of biological organization which are heavily polycomputing -from cellular pathways to patient psychological state.
- The ability to construct and model a kind of computational superposition, in which diverse observers (scientists, users, the agent itself, and its various components) have their own model of their dynamic environment and optimize their behavior accordingly, will also dovetail with and advance efforts in synthetic bioengineering, biorobotics, smart materials, and AI.
