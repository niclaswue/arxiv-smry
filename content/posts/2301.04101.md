---
title: "Neural Radiance Field Codebooks"
date: 2023-01-10T18:03:48.000Z
author: "Matthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04101v1.webp" # image path/url
    alt: "Neural Radiance Field Codebooks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04101)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04101).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/neural-radiance-field-codebooks).

# Abstract
- Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks
- Learning such representations for complex scenes and tasks remains an open challenge
- Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction
- NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer
- This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks
- We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate
- We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% relative improvement)
- Finally, we show that NRC improves on the task of depth ordering by 5.5% accuracy in THOR.

# Paper Content

## INTRODUCTION
- Object-centric representations enable humans to infer attributes such as geometry, affordances, and physical properties of objects solely from perception
- Unsupervised decomposition of the visual world into objects has been a long-standing challenge
- More recent work focuses on reconstructing images from sparse encodings as an objective for learning object-centric representations
- The intuition is that object encodings which map closely to the underlying structure of the data should provide the most accurate reconstruction given a limited encoding size
- Such methods have shown to be effective at decomposing 2D games and simple synthetic scenes into their parts
- However, they rely solely on color cues and do not scale to more complex datasets
- Advances in neural rendering (Mildenhall et al., 2021;Yang et al., 2021) have enabled learning geometric representations of objects from 2D images
- Recent work has leveraged scene reconstruction from different views as a source of supervision for learning object-centric representations
- However, such methods have a few key limitations
- The computational cost of rendering scenes grows linearly with the number of objects which inhibits scaling to more complex datasets
- Additionally, the number of objects per scene is fixed and fails to consider variable scene complexity
- Objects are decomposed on a per scene basis, therefore semantic and geometric information is not shared across object categories
- With this in consideration we introduce, Neural Radiance Codebooks (NRC)
- NRC learns a codebook of object categories which are composed to explain the appearance of 3D scenes from multiple views
- By reconstructing scenes from different views NRC captures reoccurring geometric and visual patterns to form object categories
- This learned representation can be used for segmentation as well as geometry-based tasks such as object navigation and depth ordering
- Furthermore, NRC resolves the limitations of current 3D object-centric methods
- First, NRC's method for assigning object Codes to regions of the image enables constant rendering compute whereas that of other methods scales with number of objects
- Second, we introduce a novel mechanism for differentiably adding new categories which allows the codebook to scale with the complexity of the data
- Last, modeling intra-category variation in conjunction with the codebook enables sharing of semantic and geometric object information across scenes

## RELATED WORK
- Object-centric learning aims to build compositional models of the world from building blocks which share meaningful properties and regularities across scenes.
- Prior works such as MONet (Burgess et al., 2019), IODINE (Greff et al., 2019), Slot Attention (Locatello et al., 2020), andMonnier et al. (2021) have demonstrated the potential for disentangling objects from images.
- Other work has shown the ability to decompose videos (Kabra et al., 2021;Kipf et al., 2021).
- In particular, Marionette (Smirnov et al., 2021) learns a shared dictionary for decomposing scenes of 2D sprites.
- We draw inspiration from MarioNette for learning codebooks, but differ in that we model the image formation process and intra-code variation, and dynamically add codes to our dictionary.
- 3D Object-Centric Learning Recent work has shown novel view reconstruction to be a promising approach for disentangling object representations.
- uORF (Yu et al., 2021b) and ObSuRF (Stelzner et al., 2021) combine Slot Attention with Neural Radiance Fields (Mildenhall et al., 2021) to decompose scenes.
- COLF (Smith et al., 2022) replaces the volumetric renderer with light fields to improve computational efficiency.
- NeRF-SOS Fan et al. (2022) uses contrastive loss for both geometry and appearance to perform object segmentation.
- SRT (Sajjadi et al., 2022b) encodes scenes into a set of latent vectors which are used to condition a light field.
- OSRT (Sajjadi et al., 2022a) extends SRT by explicitly assigning regions of the image to latent vectors.
- NeSF Vora et al. (2021) learns to perform 3D object segmentation using NeRF with 2D supervision.
- Although great progress has been made, these methods are limited to synthetic and relatively simple scenes.
- Our work differs from previous 3D object-centric works in that we learn reoccurring object codes across scenes and explicitly localize the learned codes.
- Additionally, our method can model an unbounded number of objects per scene compared to prior work which fixes this hyper-parameter a priori.
- We show that our approach generalizes to more complex synthetic and real-world scenes.

## METHOD
- The image is processed through a convolutional network to obtain a spatial feature map.
- The feature map is projected to a novel view using the relative camera matrix.
- Feature vectors from each respective region of the image are assigned to categorical latent codes from the finite-size codebook.
- The object codes and feature vectors are passed to a convolutional network which transforms the categorical codes to fine-grained instance codes.
- A volumetric renderer is then conditioned on the instance code, view direction, and positional encodings to render each region of the scene from the novel view.
- The rendered image from the network is compared to the ground truth novel view for supervision.

## Input Image

## NRC Ground Truth
- The NRC encoder and codebook is used to replace the pretrained network in object navigation
- The depth ordering task consists of predicting which of two objects is closer to the camera
- To perform depth ordering with NRC we predict a segmentation mask and depth map
- To predict the depth map we condition the trained MLP on the instance codes and predict the density, Ïƒ, along a given ray
- We estimate the transmittance to predict the depth following the method of Yu et al. (2021a)

## EXPERIMENTS
- NRC representations are effective for downstream applications that require geometric and semantic understanding of scenes.
- NRC outperforms baseline methods on all three tasks.

## DATASETS
- ProcTHOR & RoboTHOR THOR (Kolve et al., 2017) consists of interactive home environments built in the Unity game engine
- We benchmark on the task of object navigation in RoboTHOR (Deitke et al., 2020), a variant of the THOR environment aimed at sim2real benchmarking
- Object navigation consists of an agent moving through different scenes to locate specified objects
- RoboTHOR consists of 89 indoor scenes split between train, validation, and test
- ProcTHOR (Deitke et al., 2022) consists of procedurally generated indoor scenes similar to RoboTHOR
- CLEVR-3D (Johnson et al., 2017) is a synthetic dataset consisting of geometric primitives from multiple views and is used for unsupervised segmentation
- Following the convention of Stelzner et al. (2021), we test on the first 500 scenes of the validation set and report foregroundadjusted random index (FG-ARI)
- Adjusted random index (ARI) Yeung & Ruzzo (2001) measures the agreement between two clusterings and is a standard metric for unsupervised segmentation
- In our case the two clusterings are the predicted and ground truth segmentations
- Foreground adjusted random index only measures the ARI for pixels belonging to foreground objects
- For comparison to prior works, we consider segmentations at both the class and instance level to be correct for CLEVR-3D, ProcTHOR, and NYU Depth.

## NYU Depth
- NYU Depth Dataset consists of images from real-world indoor scenes
- Methods are trained on the ProcThor dataset then evaluated on NYU Depth for segmentation
- We chose NYU Depth because it has object categories and scene layouts that are similar to THOR
- We report the adjusted random index (ARI).

## Results
- Table 2: Performance of NRC and baselines on RoboTHOR object navigation
- NRC outperforms the best baseline by 3% in success rate (SR) and by a 20% relative improvement in SPL.
- These performance gains indicate that the geometrically-aware NRC representation provide an advantage over traditional representations such as EmbCLIP and Video MoCo.
- Another key observation is that NRC has at least 18.8% improvement in SR and relative SPL over the recent object-centric learning methods, uORF and ObSuRF.
- In particular, NRC performs better than ObSuRF and uORF when navigating near furniture and other immovable objects (see supplemental for object navigation videos).

## OBJECT NAVIGATION
- We design the object navigation experiments in THOR to understand how well the learned representations transfer from observational data to embodied navigation
- For the representation learning component of the experiment we collect observational video data from a heuristic planner
- After training on ProcThor videos, we freeze the visual representations following standard practice
- We train a policy using DD-PPO (Wijmans et al., 2019) for 200M steps on the training set of RoboTHOR then evaluate on the test set
- We report success rate (SR) and success weighted by path length (SPL). Success is defined as the agent signaling the stop action within 1 meter of the goal object with it in the view. SPL is defined as pi, where l is the shortest possible path, p is the taken path, & S is the binary indicator of success for episode i.

## DEPTH ORDERING
- We evaluate NRC on the task of ordering objects based on their depth from the camera.
- Following the convention of Ehsani et al. (2018), we determine ground truth depth of each object by computing the mean depth of all pixels associated with its ground truth segmentation mask.
- For evaluation, we select pairs of objects in a scene and the goal is to predict which object is closer.
- We take the segmentation that has the largest IoU with the ground truth mask as the predicted object mask.
- To determine the predicted object depth, we compute the mean predicted depth of each pixel associated with the predicted object mask.
- All representations are trained on the ProcTHOR dataset and evaluated on the ProcTHOR test set.

## Method
- Depth ordering requires accurate segmentation and depth estimation
- Due to NRC's stronger segmentation performance and better depth estimation, we see 5.5% and 10.3% depth ordering accuracy compared to ObSuRF and uORF respectively
- The fine-grained localization of categorical latent codes in NRC allows for better depth ordering over existing object-centric methods
- In particular, the other object-centric methods tend to assign object instances to the background which leads to large errors in estimating the depth of objects

## ABLATION STUDY
- Learning the number of codes moderately improves performance
- However, if number of codes is found via hyper-parameter tuning, performance is matched
- Nonetheless, differentiably learning the codebook size avoids computationally expensive hyper-parameter tuning.

## LIMITATIONS
- Novel view reconstruction requires camera pose, which is not available for most images and videos.
- Some datasets such as Ego4D (Grauman et al., 2022) provide data from inertial measurement units that can be used to approximate camera pose, although this approach is prone to drift.
- An incorrect assumption that NRC and most object-centric prior works make is that scenes are static.
- However, it rarely is the case that scenes are free of movement due to the physical dynamics of our world.
- Recently, Kipf et al. (2021); Pumarola et al. (2021) made strides in learning representations from dynamic scenes.
- Although NRC is relatively efficient compared to the other NeRF based methods, the NeRF sampling procedure is compute and memory intensive.
- Sajjadi et al. (2022a) and Smith et al. (2022) leverage object-centric light fields to reduce memory and compute costs.
- The efficiency improvement from modeling scenes as light fields is orthogonal to NRC and can be combined.
- A final challenge inherent to novel view reconstruction is finding appropriate corresponding frames of videos.

## CONCLUSION
- Compositional, object-centric understanding of the world is a fundamental characteristic of human vision and such representations have the potential to enable high-level reasoning and efficient transfer to downstream tasks.
- We presented Neural Radiance Field Codebooks (NRC), a new approach for learning geometry-aware, object-centric representations through novel view reconstruction.
- By jointly learning a shared dictionary of object codes through a differentiable renderer and explicitly localizing object codes within the scene, NRC finds reoccurring geometric and visual similarities to form objects categories.
- Through experiments, we show that NRC representations improve performance on object navigation and depth ordering compared to strong baselines by 3.1% success rate and 5.5% accuracy respectively. Additionally, we find our method is capable of scaling to complex scenes with more objects and greater diversity.
- NRC shows relative ARI improvement over baselines for unsupervised segmentation by 29.4% on ProcTHOR and 29.0% on NYU Depth. Qualitatively, NRC representations trained on synthetic data from ProcTHOR show reasonable transfer to real-world scenes from NYU Depth.
