---
title: "Building a Culture of Reproducibility in Academic Research"
date: 2022-12-27T16:03:50.000Z
author: "Jimmy Lin"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13534v1.webp" # image path/url
    alt: "Building a Culture of Reproducibility in Academic Research" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13534)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13534).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/building-a-culture-of-reproducibility-in).

# Abstract
- Reproducibility is an ideal that no researcher would dispute "in the abstract", but when aspirations meet the cold hard reality of the academic grind, reproducibility often "loses out".
- In this essay, I share some personal experiences grappling with how to operationalize reproducibility while balancing its demands against other priorities. My research group has had some success building a "culture of reproducibility" over the past few years, which I attempt to distill into lessons learned and actionable advice, organized around answering three questions: why, what, and how.
- I believe that reproducibility efforts should yield easy-to-use, well-packaged, and self-contained software artifacts that allow others to reproduce and generalize research findings. At the core, my approach centers on self interest: I argue that the primary beneficiaries of reproducibility efforts are, in fact, those making the investments. I believe that (unashamedly) appealing to self interest, augmented with expectations of reciprocity, increases the chances of success.
- Building from repeatability, social processes and standardized tools comprise the two important additional ingredients that help achieve aspirational ideals. The dogfood principle nicely ties these ideas together.

# Paper Content

## Introduction

## How?
- Repeating from the introduction, my approach can be summarized in two high-level bits of advice: (1) motivate the importance of reproducibility by appealing to self interest instead of altruism, and (2) engineer social processes to promote virtuous cycles and build standardized tools to reduce technical barriers.
- In the context of a research group, it means that members of the group should be actively using the software artifacts developed by the group.
- Quite simply, software artifacts that are used tend to become refined over time, or at the very least, bugs get fixed, because otherwise research would grind to a standstill.
- My group uses Pyserini, Anserini, and a few other packages we've developed as the foundation for ongoing work.
- Many new research ideas build on, hook into, or otherwise depend on Pyserini. In turn, improved capabilities in Pyserini spur further advances.
- To provide an example, the two-click reproductions described in the previous section solve a number of problems for the community, including ourselves (invoking self interest again).
- Specifically, they provide competitive baselines for comparisons and a solid foundation for first-stage retrieval in a multi-stage ranking architecture.
- For example, students focused on building better rerankers need not waste time worrying about the proper setup of the first-stage ranker. They simply follow the prescriptions in our two-click reproductions as the starting point.
- Across the research group, this ensures consistency and reduces the possibilities of bugs: We can be confident that every reranker implementation is consuming exactly the same set of candidates and thus the comparisons are fair.
- More generally, Pyserini makes it easy to run experiments on standard IR test collections; the toolkit handles much of the boilerplate, such as bindings to query sets, relevance judgments, and evaluation scripts.
- Many capabilities come for "free", for example, general techniques such as rank fusion and pseudo-relevance feedback can be applied with little effort.
- This means that (a) the student writes less code (appealing to self interest again) and (b) the veracity of results increases due to greater consistency in experimental design.
- Thus, with the dogfood principle, it's clear to see that reproducibility is driven by self interest.

### Social Processes: From Repeatability to Reproducibility
- Documenting all the steps necessary to produce experimental results is motivated by self interest
- The social pressures of reciprocity can be an effective motivation
- Reproducibility guides are just markdown files in the docs/ directory of the GitHub repository that contains the code
- Getting another person to "try out" the guide is motivated by self interest and reciprocity
- A reproduction log is a record of individuals who have successfully reproduced the results and the commit id of the code version used

### Standardized Tools: From Reproducibility to Two-Click Reproductions
- Reproducibility can be divided into social and technical aspects.
- The first is more important, because any tool to support reproducibility will either be ignored or circumvented unless there are social processes to promote their usage.
- The previous section focused on exactly these, and that gets us from repeatability to reproducibility.
- Here, I discuss tooling to further lower barriers.
- If social processes stimulate "the will", standardized tools provide "the way".
- The core idea is to make investments in tooling to automate the process of "going through" the reproduction guides.
- In Anserini and Pyserini, we have built extensive regression tests with elaborate test harnesses (sometimes called jigs).
- These regressions are tightly integrated with the two-click reproductions discussed in the previous section: in fact, in many cases, the regression tests simply wrap the execution of the two-click reproduction commands and verify that the outputs match stored specifications.
- The script depends on having access to the raw corpus, which is stored on our group's servers at known file system locations. However, the corpus path is a configurable parameter, so anyone can run the same regression test if they have a copy of the corpus.
- There are currently around three hundred such tests, which take several days to run end to end (orchestrated by yet another script).
- The largest of these builds a 10 TB index on all 733 million pages of the ClueWeb12 collection.
- Although it is not practical to run these regression tests for each code change, we try to run them as often as possible, resources permitting, to catch new commits that break existing functionalities early so they are easier to debug.
- These regression tests are always run before a formal release of the toolkit, prior to publishing an artifact on Maven central, to ensure that released jars produce the expected experimental results.
- On top of the regression framework in Anserini, further tests in Pyserini compare its output against Anserini's output to verify that the Python interface does not introduce any bugs.
- These are written as Python unit tests and, for example, check different parameter settings from the command line, ensure that single-threaded and multi-threaded execution yield identical results, that pre-built indexes can be successfully downloaded.
- In Pyserini, experimental conditions are gathered together and organized into what we call a reproduction matrix, an example of which is shown in Figure 1 for the MS MARCO V2 passage corpus.
- Each row illustrates a particular experimental condition, while the columns show evaluation metrics with respect to different sets of queries.
- A row can be expanded to reveal the commands necessary to generate those evaluation figures.
- These are exactly the two-click reproductions described in Section 3, organized in an easy-to-consume format.
- Finally, the reproduction matrix is backed by another script that programmatically iterates through all rows (experimental conditions), performs retrieval with the specified invocations, and verifies that the evaluation scores are as expected (i.e., checks each cell in the table).

## Other Considerations
- The paper discusses a variety of topics that don't fit neatly into the "what", "why", and "how" narrative
- The paper discusses a smörgåsbord of issues
- The paper discusses a variety of topics

### Scoping and Timing of Reproducibility Efforts
- Reproduction guides and automated regression testing lie along a spectrum of "reproduction rigor", with different cost/benefit tradeoffs.
- We aim to have reproduction guides for every paper, but only a relatively small fraction of our group's research becomes "enshrined" in automated regressions that are maintained over time.
- We currently do not have clear-cut criteria as to which retrieval models or experimental results receive the regression treatment, but as a rough heuristic, we use the following question as a guide: Is this work we'd like to extend further?
- If so, then we would go about building an appropriate regression.
- Similarly, if we find a paper by others that we'd like to build on (as in the case of DPR), we would make the investment to replicate the work and to build regressions into our codebase.
- Consider the common case of a modeling advance that is described in a paper, i.e., we proposed a novel retrieval model that appears to be better than previous work, and the contribution represents a fruitful line of inquiry that the group hopes to push further.
- In this case, building an appropriate regression makes sense.
- However, to balance cost and reward, we do not construct regression tests for every experimental result reported in the paper. Instead, we are guided by the question: In a follow-up paper, which of the existing experimental conditions from this paper would serve as the baseline for comparison?
- That becomes the target for integration into our regression framework.
- We find that building tests for ineffective contrastive settings or ablation conditions provides little value relative to the amount of effort required.
- In some cases, a novel retrieval model does not neatly fit into the design of Pyserini. These model implementations (both training and inference) usually begin their lives in a separate repository, but as part of the reproducibility planning exercise, we debate whether it is worthwhile to import the model inference code into Pyserini so that end-to-end retrieval experiments can be conducted alongside all the other available models in a seamless manner (as part of a reproduction matrix, see Section 4.2).
- These decisions are made on a case-by-case basis.
- It is worth explicitly noting that any inclusion to the constantly growing test suites in Pyserini and Anserini represents an open-ended maintenance commitment for the life of the project.
- Any addition, in essence, incurs a permanent liability on the group (and as I'll discuss in Section 5.3, this burden usually falls on me).
- There's no point in adding a model to the regression framework unless there's the intention of keeping the code functional in the long term.
- Once added, we almost never abandon a regression test, except in very rare circumstances, for example, a failure due to changes in underlying code that we depend on but have no control over.
- Operationally, continual expansion of test suites means that the complete set of regressions takes longer and longer to run, which has the practical effect of slowing down release iterations (e.g., on PyPI).
- However, I don't think this has impacted the iteration speed of individual students since components in the codebase are largely decoupled.
- Nevertheless, servers continue to get faster and more powerful, so I think our current operations are sustainable.

### Bootstrapping Reproducibility
- The group has already established virtuous cycles
- Existing software artifacts are already functional and the benefits of using them are evident
- Processes and shared norms are in place, and tools to simplify the routine have been built
- Once again, motivated by self interest, the value that can be extracted by participating in, for example, our Pyserini reproducibility ecosystem is greater than the costs
- Second, look for opportunities where a long-term research agenda aligns with the construction of software artifacts that can be generalized into a toolkit, library, or platform
- Reproducibility efforts have substantial upfront costs that need to be amortized over several years to achieve a net gain
- The (planned) software toolkit should ideally provide the basis for several related research projects that yield multiple publications
- Without this long-term vision and commitment to a shared codebase, the group might never reap the rewards of the initial investment
- With a plan in place, it is possible to make progress incrementally. For example, the multi-layered regression frameworks in Anserini and Pyserini evolved over many years
- However, the commitment to build a toolkit for tackling the core ranking problem in information retrieval was made on day one

### The Critical Role of Leadership
- I am the overall architect of Pyserini and Anserini
- I am the person who usually runs the regression tests, shepherds the release of software artifacts, and generally keeps tabs on everything that is going on
- In software development terms, I am not only the engineering manager but also the tech lead
- Many of the retrieval models in Pyserini were built before the arrival of my most recent cohort of students, and some models will continue to be refined even after they graduate
- I have the most complete view of what everyone is working on, and this allows me to coordinate multiple overlapping research projects
- For example, I regularly introduce students to existing features in Anserini and Pyserini that can expedite their research
- Starting an academic research group has been analogized to running a startup, with the faculty member as the CEO
- In the context of the North American academic system, this analogy is apt, as faculty members generally lead their own groups
- However, as a faculty rises through the academic ranks and grows a research group, a common organizational pattern is to cede the role of the tech lead to a senior graduate student
- While certainly workable, this comes with associated risks
- Students (eventually) graduate (hopefully!) and the next stage of their careers may no longer have room for the role
- Arranging for succession "handoffs" become more difficult if the group leader is not intimately involved
- This is not unlike what happens in a corporate environment: When a tech lead departs, management needs to find a replacement, typically elevating another member from the same project
- In the academic context, this is likely another senior graduate student working on similar research problems, provided one exists.

## Conclusions
- Building a culture of reproducibility is hard work, but the rewards are worthwhile.
- Coming back to where I started: I am passionate about making research reproducible by building and sharing software artifacts that the community can use to recreate work by us and other researchers.
- One of the most flattering comments I've ever received from a colleague reflects exactly this passion: One of the most valuable contributions of Jimmy's work is the way he shares everything you need to reproduce it. He figures something out and he *shows* everyone how to do it.
- I cannot applaud his efforts on this front enough. It is enormously valuable to our community.
- I constantly learn things by looking at what Jimmy does -not just descriptions in writeups.
- I am a researcher because I love learning how things work.
- There are few people in our community doing a better job at teaching people how to learn what he has learned than Jimmy.
- Some of the best comments I've recently received from anonymous reviewers are in the same vein.
- The academic community has begun to recognize the importance of reproducibility and to reward such efforts with dedicated tracks and associated publications in top-tier venues.
- I think we're heading in the right direction, and I hope that this essay offers some insights on how we can continue building a culture of reproducibility everywhere.
