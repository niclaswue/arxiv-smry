---
title: "Objaverse: A Universe of Annotated 3D Objects"
date: 2022-12-15T18:56:53.000Z
author: "Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08051v1.webp" # image path/url
    alt: "Objaverse: A Universe of Annotated 3D Objects" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08051)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08051).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/objaverse-a-universe-of-annotated-3d-objects).

# Abstract
- Large data corpora like Wikipedia, Conceptual Captions, WebText, WebImageText, and LAION have propelled recent dramatic progress in AI
- A notable omission within this family of large-scale datasets is 3D data
- Objaverse 1.0 is a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations
- Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category
- We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models

# Paper Content

## Introduction
- Large scale datasets have enabled rapid progress in AI
- Massive datasets of language corpora, paired image and text datasets, YouTube video datasets, and multimodal datasets have led to models like CLIP and StableDiffusion
- The size of the datasets we are feeding to our data-hungry deep learning models in many other areas of research is simply not comparable
- OBJAVERSE is a large scale corpus of highquality, richly annotated, 3D objects
- Objects in OBJAVERSE are free to use and sourced from Sketchfab, a leading online platform for managing, viewing, and distributing 3D models
- In total, OBJAVERSE contains over 800K 3D assets designed by over 100K artists which makes this data large and diversely sourced
- With this remarkable increase in scale, OBJAVERSE has the potential to impact research progress across domains

## Related Work

## Objaverse

## Applications
- OBJAVERSE can be used for 3D generative modeling
- OBJAVERSE can be used for instance segmentation with CP3D
- OBJAVERSE can be used for open-vocabulary ObjectNav
- OBJAVERSE can be used for analyze robustness in computer vision models

### 3D Generative Modeling
- 3D generative modeling has shown much improvement recently with models such as GET3D [24] delivering impressive high quality results with rich geometric details.
- GET3D is trained to generate 3D textured meshes for a category and produces impressive 3D objects for categories like Car, Chair, and Motorcycle using data from ShapeNet [7].
- OBJAVERSE contains 3D models for many diverse categories including tail categories which are not represented in other datasets. It also contains diverse and realistic object instances per category.
- This scale and diversity can be used to train large vocabulary and high quality 3D generative models.
- In this work, we showcase the potential of this data as follows. We choose three categories of ob-Figure 5. Highlights of the visual diversity of objects that appear in OBJAVERSE, including animated objects, rigged (body-part annotated) characters, models separatable into parts, exterior environments, interior environments, and a wide range visual styles.
- For comparison, we also train a GET3D model on the set of 83 bags from the ShapeNet dataset.
- Fig. 6 shows a collection of 3D objects generated by our trained GET3D models.
- Qualitatively, the 3D-meshes generated by the OBJAVERSE-trained models are high-quality and diverse, especially when compared to the generations from the ShapeNet-trained model.
- To quantify this observation, we asked crowdworkers to rate the diversity of Bag generations produced by the OBJAVERSE and ShapeNet trained models.
- When shown collections of nine randomly sampled generations from both models, workers rated the collection generated from the OBJAVERSE trained model as more diverse in appearance 91% of the time.

### Instance Segmentation with CP3D
- Simulated data is cheaper to obtain expert annotations
- Annotated OBJAVERSE objects can be rendered into images, allowing them to serve as a rich source of additional data that can be used to enhance model performance on 2D computer vision tasks
- As a proof-of-concept demonstrating the effectiveness of this approach, we use segmented data from OBJAVERSE objects as auxiliary labels for training models on the LVIS dataset for Large Scale Instance Segmentation

### Open-Vocabulary ObjectNav
- introduces open-vocabulary Object-Nav, a new task propelled by the vast diversity of objects that appear in OBJAVERSE
- uses OBJAVERSE-LVIS subset and annotates placement constraints for each object category
- preprocesses images with visual branch of a frozen ResNet-50 CLIP model to generate a target description
- trains agent with DD-PPO and evaluates on houses with floor plans, objects, and descriptions unseen in training
- achieves success rate of 19.9%, for a random policy success of 5.1%.

### Analyzing Robustness
- The persistent bias present in many image datasets, e.g. ImageNet [65], is that the subjects of interest are generally photographed from a forward-facing, canonical, orientation.
- When, for example, taking a photograph of a television, few would choose to take this photograph crouched on the floor behind the television.
- This impact of this bias was studied by Alcorn et al. [2] who find that modern computer vision systems are highly susceptible to deviations from canonical poses.
- This is more than a theoretical problem: computer vision systems deployed in the real world will frequently encounter objects in non-canonical orientations and in many applications, e.g. autonomous driving, it will be safety critical that they behave well.
- Given the above, we adopt the experimental design of Alcorn et al. and design, using OBJAVERSE assets, a benchmark for evaluating the robustness of state-of-the-art computer vision classification models to orientation shifts.
- In particular, for each object in our OBJAVERSE-LVIS subset, we render 12 images of the object from random orientations rendered upon a background with RGB values equalling the mean RGB values from ImageNet; see Fig. 9 for examples.
- This ability to, at scale, render objects from random view-points is a practical impossibility in the real world but is made trivial when using 3D assets.
- We then evaluate several modern open-domain image-classification networks (constrained to the ≈1,200 LVIS categories) on these images and report 4 metrics for each model.
- These metrics include: • Top-1 Random Rotation -the frequency with which a model correctly classifies an image as belonging to the respective LVIS category. • Top-1 Any Rotation -the frequency with which a model classifies an image correctly from at least one of the 12 random orientations.
- We also have Top-5 variants of the above metric where the correct category need only be in the top 5 predictions from the model.
- We report our results in Tab. 2 in which we evaluate a variety of performant pretrained models.
- Comparing the gap in performance between the Top-k Random Rotation and Top-k Any Rotation metrics we find that model performance dramatically degrades when viewing objects from unusual orientations.

## Conclusion
- We present OBJAVERSE, a next-generation 3D asset library containing 818K high-quality, diverse, 3D models with paired text descriptions, titles, and tags.
- As a small glimpse of the potential uses of OBJAVERSE, we present four experimental studies showing how OBJAVERSE can be used to power (1) generative 3D models with clear future applications to text-to-3D generation, (2) improvements to classical computer vision tasks such as instance segmentation, (3) the creation of novel embodied AI tasks like Open Vocabulary Object Navigation, and (4) quantifying the rotational robustness of vision models on renderings of objects.
- We hope to see OBJAVERSE enable a new universe of new applications for computer vision.
