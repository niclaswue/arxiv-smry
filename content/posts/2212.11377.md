---
title: "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement"
date: 2022-12-21T21:36:52.000Z
author: "Wei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Donley, Yossi Adi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11377v1.webp" # image path/url
    alt: "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11377)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11377).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/revise-self-supervised-speech-resynthesis).

# Abstract
- The paper proposes to unify the study of speech quality with visual input and improve it by focusing on intelligibility, quality, and video synchronization.
- The proposed model is called ReVISE and it is the first high-quality model for in-the-wild video-to-speech synthesis.
- ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions.

# Paper Content

## Introduction
- Speech in-the-wild is often corrupted with all sorts of natural and non-natural sounds
- Speech recorded indoor often contains reverberation, mechanical noise, and overlapping speech from non-target speakers
- On top of those, recording devices and network may also introduce other types of distortion
- Distortion makes it hard for both human and machines to comprehend speech
- Improving the quality and the intelligibility of corrupted speech is essential for assistive listening and robust speech processing
- Generating clean speech signal based on its corrupted version is herein referred to as speech enhancement
- In speech enhancement, one line of research uses visual speech to provide auxiliary information
- Audio-visual speech (e.g., talking-head videos) can be seen as a multimodal view of the speech
- Since visual modality is immune to acoustic noise, combining both views enables more robust estimation of shared generating factors such as textual content
- Meanwhile, despite sharing the same goal of recovering corrupted speech, prior work often treats enhancement from each type of distortion as a separate problem
- Speech denoising and dereverberation addresses additive and convolutive non-speech noises
- Speech separation focuses on speech noises that exhibit similar characteristics to the target speech
- Speech inpainting aims to recover dropped audio frames
- Video-to-speech synthesis is the extreme case of inpainting where all the frames are dropped
- As a result, algorithms designed for one type of distortion may not be effective for another
- In this paper, we advocate a more holistic approach to audio-visual speech enhancement, where an algorithm should be evaluated on all types of distortion, and a sin-gle model should also be effective on all types of corrupted data
- Following [48], we coin the concept universal speech enhancement
- Exact reconstruction of the reference clean speech is not an appropriate objective especially when the level of distortion is high, many different clean samples could have generated the same observed sample after distortion
- To address the issue, we propose to relax the objective and solve the generalized speech enhancement (GSE) problem
- Instead of focusing on exact reconstruction and measuring metrics like signal-to-noise ratios (SNRs), the goal of GSE is to enhance a predefined set of attributes, such as content intelligibility, synchronization, and quality, which for example can be measured with word error rates (WERs), SyncNet scores [9], and mean opinion scores (MOS)
- This leaves the models with the freedom to decide what to render for other aspects, such as voice and pitch contour, recovery of which are not essential for many applications
- This paper focuses on recovering intelligibility, synchronicity and quality. The task of improving those could be broken down into two steps: predicting the frame-level content and synthesizing high quality audio from it. Inspired by the resemblance to audio-visual speech recognition and speech synthesis, we propose ReVISE, short for Resynthesis with Visual Input for Speech Enhancement
- ReVISE is composed of a pseudo audio-visual speech recognition model (P-AVSR) and a pseudo text-to-speech synthesis model (P-TTS); instead of using text as the output/input of the two models, self-supervised speech units that encode speech content [23,44] are adopted to bridge them, making the system free of text supervision
- Furthermore, observing the gain on speech recognition brought by self-supervised learning, we also initialize the P-AVSR with a self-supervised audio-visual speech model, AV-HuBERT [49], which significantly improves the performance, especially on low-resource setups
- To demonstrate the universality and compare with the literature, we construct four types of corrupted speech using Lip-reading Sentences 3 (LRS3) [2] and AudioSet [20], including audio-visual denoising, separation, inpainting, and video-to-speech
- Results show that ReVISE is the first model capable of high-quality in-the-wild video-tospeech synthesis, while prior models fail to produce intelligible content [21] or generate low-quality audio for inthe-wild...

## Background
- introduces the setup of audio-visual speech enhancement tasks
- approaches these problems separately or jointly
- contrasts their approach with the literature
- introduces self-supervised speech resynthesis

### Audio-visual speech enhancement tasks
- Audio-visual speech enhancement is the task of improving the quality of corrupted speech given its corresponding talking head video.
- During training, tuples of (x a , x v , x a ) are provided where x a is the reference clean speech.
- The tasks are further divided depending on the type of distortion applied to xa : speech denoising and dereverberation considers xa with impulse response h and additive noise n (t indexes discrete time steps).
- Source separation considers xa [t] = x a [t]+x a [t] where x a is an interfering speech sampled from the same marginal distribution as x a .
- For these two subtasks, masking-based methods (in magnitude spectrogram domain [16,41], complex spectrogram domain [14,19,55], or on learned basis [33,34]) are widely adopted where the enhancement model f predicts a mask M = f (x a , x v ).
- It is feasible because the target speech can be written as X = M • X where X and X are the clean and noisy speech in the target domain and M is the ideal ratio mask.
- On the other hand, speech inpainting and video-to-speech synthesis considers where m[t] ∈ {0, 1} for inpainting and m[t] = 0 for the latter.
- Masking-based methods are not feasible for these tasks. Hence, previous studies directly predict spectrogram [36,38] or waveform [7,37,39] as xa = f (x a , x v ) and optimize the model with regression and adversarial losses.
- However, previous models have not been able to generate realistic samples with in-the-wild datasets like LRS3, because these models are deterministic while the underlying mapping is highly stochastic, especially when there is a large portion of frames dropped.
- To accommodate general distortions, universal enhancement models have to adopt generation-based method [32,43,45,48,50], otherwise they cannot handle distortions like package loss (i.e., inpainting).
- For the same reason, our proposed model is also generation-based. However, it differs from previous works in two main aspects. First, prior universal enhancement models are audio-based, which do not leverage auxiliary input like visual speech.
- Second, regardless of the training objective (regression [40], adversarial [43,45,50], or score-matching [48]), previous universal models predict reference clean speech directly. We consider a different paradigm based on self-supervised speech resynthesis [44]: our model predicts a self-supervised representation z = k(x a ) of the reference clean speech. A separate model is used to convert z back to audio, which is described in the next section.

### Self-supervised speech resynthesis
- HuBERT is a pre-trained speech recognition model that is composed of a generator and a discriminator.
- The generator has a series of transposed convolution blocks with residual and dilated connections.
- The discriminator comprises of multi-period and multi-scale sub-discriminators.
- The entire model is optimized with a combination of regression and adversarial losses.
- HuBERT units are good candidates for replacing speech for text-free speech generative models because they encode less nuisance variation.

## Method
- Formalization of the generalized speech enhancement problem
- Proposed model
- Details

### Problem Formulation
- Without loss of generality, we assume that original speech x a and its auxiliary view x v are generated with a bijective mapping g m,Y as x m = g m,Y ({y i } i ), m ∈ {a, v} given a set of factors Y = {y i } i (e.g., pitch, speed, textual content) sampled from p Y .
- Corrupted speech xa is generated with a corruption function g d as xa = g d (x a , d) given x a and distortion parameter d sampled from p d .
- Audiovisual speech enhancement can be seen as estimating p(Y | g d (x a , d), x v ).
- When the level of distortion is low, there are fewer Y that can result in the same observed xa after corruption.
- Formulating it as a regression problem and measuring the performance of reconstructing all factors with metrics like SNR is reasonable.
- However, when the level of distortion is high, there exist multiple sets of Y rendering the same noisy speech xa .
- Take video-to-speech as an example: while the content and the timing of each word can be more accurately inferred, the exact pitch is hard to infer from the video. In other words, p(y i |x a , x v ) is more deterministic for some y i but not for the others.
- We argue that for these scenarios reconstructing the exact clean reference signal x a is an ill-posed problem. Instead, one should consider the generalized speech enhancement problem, where the goal is to generate an enhanced signal xa = f (x a , x v ) that is on the manifold of clean speech g a,Y (•) (high quality) and preserves the factors of interest (partial faithfulness).
- Let g −1 a,Y,i be the inverse mapping from speech to factor y i , and {y i } i∈I + be the list of factors of interest (which should be a subset of factors that can be inferred from corrupted speech). Faithfulness is measured by the discrepancy between g −1 a,Y,i (x a ) and g −1 a,Y,i (x a ) for i ∈ I + .
- In case of y i being the textual content, such discrepancy can be measured by the word error rate produced by an off-the-shelf speech recognizer for example.
- Quality can be measured by metrics commonly used for text-to-speech synthesis such as mean opinion scores, where human raters rate how realistic and clean the audio sounds regardless of the textual content.

### ReVISE
- Pseudo audio-visual speech recognition module predicts clean reference speech x a given (x a , x v )
- Pseudo text-to-speech synthesis module synthesizes speech conditioned on x a

### Model
- SSL speech tokenizer
- We use a BASE HuBERT model which is composed of a convolutional encoder and 12 Transformer layers.
- Each layer has embedding dimension of 768, feed-forward layer dimension of 3072, and 12 selfattention heads.
- The model is pre-trained for three iterations on 32 GPUs with 400K updates per iteration, using clusters of MFCC/6-th layer feature from iteration 1/9-th layer feature from iteration 2 as the target, with a codebook size of 100/500/1000, respectively, following the recipe of [30].
- A total of 221K hours of unlabeled speech data combining multilingual Librispeech [46], Common Voice [3], and Vox-Populi [53] in eight languages (En, Es, Fr, De, Nl, It, Pl, Pt) are used for pre-training.
- The final SSL units z used in Re-VISE are generated by clustering the third iteration feature at the last layer with a codebook size of 2000 with K-means.
- We use the unit HiFi-GAN described in Sec. for initialization.
- For Easycom, a u-HuBERT model [24] pretrained additionally with Librilight [26] is used for initialization, which improves upon using [49].

### Evaluation
- We evaluate the proposed model and baselines on the following axes: content, synchronization, quality, and lowlevel detail reconstruction.
- For content, we use the WER computed with a speech recognition model to measure the intelligibility quantitatively similar to [36,37].
- The public model from [56] is used, which reports a WER of 5.6% on LRS3 test split and 35.7% on the EasyCom close-talking validation set.
- For synchronization, following [21] we use SyncNet [9] metrics, the predicted temporal distance between audio and video (LSE-D) and the prediction's confidence (LSE-C) are averaged over the entire test set.
- For quality, we follow the tradition of text-to-speech synthesis evaluation and conduct subjective mean opinion score (MOS) studies with a scale from 1 to 5 and a 0.5 increment.
- We evaluate 50 randomly sampled files from the test set, where each sample was evaluated by at least 15 raters using the CrowdMOS package [47].
- Finally, to evaluate reconstruction of lowlevel details as typically done in speech denoising or source separation studies, we include ESTOI [51] and Mel cepstral distortion (MCD) [28].

## Results
- Quantitative results are presented
- We strongly recommend readers to watch the samples provided in the supplementary material for better understanding of the sample quality compared to the baselines.

### Ground truth and resynthesis performance
- The gap between the two is an approximation of how much information is lost when tokenizing speech into SSL units.
- It shows that intelligibility, synchronization, and quality are all slightly degraded.
- As ReVISE is trained to predict the tokens inferred from clean speech, if the ReVISE model have 100% prediction accuracy then the performance will be identical to the "Tgt. audio resynthesis" row here.

### Video-to-speech synthesis
- ReVISE is better than SVTS at synthesizing speech
- ReVISE is better because it has a better video-to-spectrogram predictor
- ReVISE is better because it generates higher quality audio

### Audio-visual speech inpainting
- We evaluate ReVISE on the audio-visual speech inpainting task with three test splits, where 30%, 40%, and 50% of the frames are dropped
- While there have been several works studying audio-based speech inpainting, [38] is the only work we are aware of studying the audio-visual setup. However, their model was only evaluated on the very constrained GRID dataset, showing a phone error rate of 13.7%.
- SVTS achieves a word error rate of 17.9%, hinting that [38] is likely worse than SVTS even when having partial audio as input. Accordingly, we decide not to compare with [38] but compare with a publicly available strong generation-based audio enhancement model, Demucs [12], which is trained on ∼650 hours of high quality clean speech mixed with noise from AudioSet [20] and FreeSound 4 .
- In addition, we consider resynthesis as another baseline, which encodes and decodes the corrupted speech with the same SSL tokenizer and P-TTS model.

### Audio-visual speech denoising
- The proposed model is compared with Visu-alVoice, a state-of-the-art model for audio-visual source separation and denoising.
- VisualVoice predicts a complex IRM given noisy audio, lip video, and speaker face image as input and the model is optimized with a combination of mask prediction loss, cross-modal matching loss (image and enhanced audio embeddings from the same speaker should be close) and consistency loss (embeddings of enhanced audio for the same speaker should be close).
- To have a fair comparison, we retrain VisualVoice models with the same dataset as ReVISE using a similar setup provided by the authors. In terms of intelligibility, we observe that the performance of resynthesizing corrupted audio is still bad.

### Audio-visual source separation
- Sec. 5.4: Four noisy test sets are created with different levels of SNR
- Tab. 4: Comparing the speech source separation results in Tab. 4 and denoising results in Tab. 3, we observe that audio model (Demucs) performs worse with speech noise than with non-speech noise at the same SNR level.
- In contrast, audio-visual models report similar performance on the two tasks at high SNR, and better performance on separation at low SNR.
- These results suggest that it is easier to remove speech noise if the model have auxiliary information that it can use to identify the target speech.
- Comparing VisualVoice and ReVISE, it reveals a similar trend where ReVISE is still substantially better at low SNR.

### Universal audio-visual speech enhancement
- The universal model is better than distortion-specific models on almost all tasks
- The universal model is 0.5% WER worse on video-to-speech synthesis

### AV enhancement on real data -EasyCom
- ReVISE outperforms other methods on EasyCom, which is much more challenging than LRS3 and has less hours of training data.
- ReVISE substantially enhances the intelligibility of noisy speech in both setups, while Demucs only manage to reduce the WER by 0.7% and 1.7%.
- The WER of the enhanced beamformed speech from ReVISE is only 4.4% away from resynthesized clean speech: the target ReVISE is trained to predict.
- One can expect the gap to clean speech (14.5% WER) will be reduced if the quality of the SSL units improves.
- Tab. 7 presents the subjective quality evaluation results for EasyCom. We first note that the quality of resynthesized target audio (4.13 MOS) is in fact higher than the target audio itself (3.76 MOS). This is because the P-TTS module is trained on the high-quality LJSpeech, which has better quality than the EasyCom audio recorded by the close-talking microphones, as the latter still contain mild background noise and overlapping speech.
- Similarly, the predicted audio from the proposed ReVISE model also delivers similar levels of audio quality (4.19).
- The results suggests a crucial advantage of the proposed ReVISE model compared to previous studies: for ReVISE, the audio quality is not bounded by the enhancement training data (LRS3 and EasyCom), but determined by the quality of P-TTS training data.
- In contrast, prior works such as SVTS or VisualVoice could only produce audio that is as good as the enhancement target.
- We conduct ablation studies to evaluate the importance of pre-training P-AVSR, P-AVSR model size, selection of audio SSL units, and visual input.
- WER are reported here because intelligibility exhibits the highest variation across models reported earlier.
