---
title: "Large Language Models Are Reasoning Teachers"
date: 2022-12-20T08:24:45.000Z
author: "Namgyu Ho, Laura Schmid, Se-Young Yun"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10071v1.webp" # image path/url
    alt: "Large Language Models Are Reasoning Teachers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10071)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10071).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/large-language-models-are-reasoning-teachers).

# Abstract
- Language models (LMs) have demonstrated remarkable performance on downstream tasks
- Recent works have shown that chain-of-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-by-step
- However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployment
- In this paper, we revisit the fine-tuning approach to enable complex reasoning in smaller LMs, optimized to efficiently perform a specific task
- We propose Fine-tune-CoT, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning
- We evaluate our method on publicly available LMs across a wide range of complex tasks and model sizes
- We find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance
- Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude
- We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models
- We also identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT and address them in our analysis.

# Paper Content

## Introduction
- Language models (LMs) have demonstrated remarkable performance in a wide range of downstream tasks, mainly attributed to their scalability enabled by the Transformer architecture (Vaswani et al., 2017) and availability of web-scale training data.
- Previous works on language models have followed the paradigm of pre-training on a large corpus and then fine-tuning on downstream tasks.
- Our code implementation is publicly available at https://github.com/itsnamgyu/reasoning-teacher.
- Large Teacher ? Questions Zero-shot-CoT Fine-tuning Prompt
- We consider a method consisting of multiple stages. First, a large teacher model is prompted to answer questions using multi-step reasoning, without relying on correct examples. That is, the teacher employs zero-shot chain-of-thought reasoning to generate output. We then use the resulting reasoning samples (consisting of the question and teacher output) for fine-tuning a much smaller student model.
- (Raffel et al., 2020;Devlin et al., 2018).
- Recently, large language models (LLMs) have demonstrated in-context generalization capabilities: performing downstream tasks simply by conditioning on few in-context exemplars or plain natural language task descriptions (Brown et al., 2020;Sun et al., 2021). LMs have also demonstrated the ability to solve complex tasks, when prompted to generate intermediate rationales.
- Standard prompting methods, which use few-shot exemplars of question-answer pairs or zero-shot instructions, have been shown to be insufficient for downstream tasks which require multiple reasoning steps (Chowdhery et al., 2022).
- However, recent works have demonstrated the possibility of eliciting complex reasoning abili-
- tities in small models for use in real-world applications. In this light, we propose an approach named Fine-tune-CoT, which aims to utilize the CoT reasoning capabilities of very large LMs to teach small models how to solve complex tasks.
- To elaborate, we apply existing zero-shot CoT prompting (Kojima et al., 2022) to generate rationales from very large teacher models, and use them to fine-tune smaller student models.
- Our approach, on the other hand, can be readily applied to novel downstream tasks, owing to the remarkable zero-shot reasoning ability of LM-based teachers (Kojima et al., 2022), without hand-crafted reasoning annotations or task-specific engineering.

## Related Work
- Much previous work established a "pre-train and fine-tune" paradigm for enhancing large language models' performance on downstream tasks (Radford et al., 2018;Raffel et al., 2020;Dong et al., 2019;Vaswani et al., 2017;Devlin et al., 2018).
- However, given that fine-tuning requires a very large dataset of task-specific labeled examples, and often does not generalize well to out-of-distribution settings, it is not always easily applicable (Liu et al., 2021;Hendrycks et al., 2020).
- More recent literature exhibits a paradigm shift towards "prompting" the model to predict the desired output (Liu et al., 2021;Raffel et al., 2020).
- Large LMs can exhibit strong performance in this setting (Brown et al., 2020).
- For smaller models to be able to perform similarly, additional engineering is usually required (Gao et al., 2021;Schick and Sch√ºtze, 2021b;Schick et al., 2020).
- In more complex tasks, the performance of very large LMs under prompting can be boosted with chain-ofthought (CoT) prompting (Wei et al., 2022b).
- This approach is preceded by the idea of using samples with explicit reasoning steps for fine-tuning a model, which however usually requires human reasoning annotation and task-specific training setups (Nye et al., 2021;Cobbe et al., 2021).
- In few-shot CoT prompting, the model is fed examples of step-by-step reasoning in natural language. It can then generate intermediate reasoning steps leading to a problem solution. This improves performance on a wide range of tasks (Wang et al., 2022b).
- Additionally, LLMs can perform well in an unsupervised task-agnostic setting, using zero-shot CoT (Kojima et al., 2022).
- This requires no fine-tuning or task specific conditioning, and substantially outperforms standard zero-shot learning and sometimes even few-shot learning on a wide number of tasks.
- Yet, prior work has shown that CoT requires extremely large models for optimal performance (Hoffmann et al., 2022;Chowdhery et al., 2022).
- In our work, we contrast this by showing how to utilize CoT reasoning methods for smaller models by fine-tuning them on rationales generated by a very large model.
- Using various LLMgenerated explanations for fine-tuning smaller models has been successfully used in prior work (Li et al., 2022a).
- Also, a similar approach to ours is mentioned in (Huang et al., 2022); however we note that the focus of this concurrent work lies on using few-shot CoT to self-generate fine-tuning examples by and for very large proprietary models.
- The authors provide a brief glimpse into using zero-shot CoT to generate reasoning examples for fine-tuning smaller distilled models, but the results are limited to one dataset and very large models that are also inaccessible to the general community.
- In contrast, we provide a rich set of results and qualitative/quantitative analysis on a wide range of datasets, using open-source models that are small and accessible to everyone.

## Chain of Thought Fine-Tuning
- We propose Fine-tune-CoT, a task-agnostic approach to enable chain-of-thought reasoning in small language models.
- The core idea is to generate reasoning samples from very large teacher models using prompt-based CoT methods and subsequently fine-tune small student models using the generated samples.
- This approach preserves the advantages of task-agnostic prompt-based CoT methods while overcoming their reliance on prohibitively large models.
- To maximize versatility, we use the most recent Zero-shot-CoT prompting method (Kojima et al., 2022) on teacher models, as it does not require any hand-annotated reasoning explanations.
- We note that our approach is not limited to this way of prompting the teacher model.

### Diverse reasoning
- To maximize the sample efficiency of Fine-tune-CoT, we can generate multiple reasoning explanations for each training sample, thereby augmenting the fine-tuning data.
- We refer to this as diverse reasoning.

## Experiments
- Evaluates on 12 datasets pertaining to 4 different categories of complex reasoning
- For arithmetic reasoning, evaluates on SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014) and MultiArith (Roy and Roth, 2016) from the Math World Problem Repository (Koncel-Kedziorski et al., 2016) as well as more recent datasets, GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017) and SVAMP (Patel et al., 2021)
- For commonsense reasoning, uses Com-monsenseQA (Talmor et al., 2018) and StrategyQA (Geva et al., 2021)
- Last Letters, Coin Flip are used to evaluate symbolic reasoning (Wei et al., 2022b) and Date Understanding and Tracking Shuffled Objects from BIG-bench (Srivastava et al., 2022)
- Models used in the study are GPT-3 family of models, as inference and fine-tuning operations are readily available via APIs provided by OpenAI
- For the teacher model, uses text-davinci-002, which is based on the largest InstructGPT model with 175B parameters (Ouyang et al., 2022)
- For student models, uses ada, babbage and curie, each based on GPT-3 (Brown et al., 2020)

### Results
- Fine-tuning elicits complex reasoning in small models
- Zero-shot-CoT fails to enable complex reasoning in smaller models
- Fine-tune-CoT elicits notable reasoning performance on the same tasks, demonstrating significant gains over Zero-shot-CoT using smaller models
- For complex arithmetic, Fine-tune-CoT achieves a notable 33% accuracy on Multi-Arith while Zero-shot-CoT only reaches 5%.
- For two commonsense reasoning tasks, Fine-tune-CoT is shown to outperform the near-random performance of Zero-shot-CoT by 37%p and 5%p, respectively.
- Fine-tuning performance is most notable in relatively simple tasks including other reasoning tasks (Date Understanding, Tracking Shuffled Objects) and symbolic reasoning (Last Letter Concatenation, Coin Flip), while Zero-shot CoT fails to overcome random-guess performance.
- Small models can outperform very large teachers in reasoning
- Diverse reasoning substantially improves Finetune-CoT performance

### Analysis
- Many datasets contain groups of samples which share common templates.
- This brings into question the validity of naive samplewise data split, as it has the potential to leak the same templates into the train and test sets.
- To investigate whether the student models are truly learning to reason rather than matching simple patterns, we manually group samples by template and evaluate Fine-tune-CoT using a template-wise data split.
- Table 5 shows the performance of Fine-tune-CoT when using sample-wise vs template-wise split, using the same train-test ratio of 70:30.
- While student performance is typically lower with a template-wise split, it still significantly outperforms random guess performance, as well as zero-shot and vanilla fine-tuning baselines shown in Table 3.
- This reaffirms that Fine-tune-CoT is able to elicit complex reasoning capabilities in small language models.
- It is possible for the teacher model to answer correctly despite incorrect reasoning, especially in multi-choice questions where the random-guess probability is significant.
- To investigate the potential impact of a better filtering scheme (as opposed to our baseline answer-based filtering) we manually annotate the correctness of rationales from the teacher model and evaluate student performance when fine-tuning on correctly reasoned samples.
- based on answer predictions vs golden samples, hand-picked based on correctness of rationales.
- We find that 28% of correct samples have incorrect rationales-significantly more than the randomguess performance of 17.12%, indicating the importance of filtering.
- Surprisingly, we however find that answer-based filtering outperforms the more stringent human-filtering by 5-11%, given the same initial samples.
- When we match the number of samples post-filtering (via undersampling), we do find that fine-tuning on golden samples outperforms that on correct samples by 5-8%.
- These results suggest that there is a tradeoff between the quality and quantity of reasoning samples which must be addressed when considering sample-filtering methods.
- Following the canonical setting for Zero-shot-CoT, we limit the max sequence length, or max tokens, allowed for the teachergenerated rationale and student reasoning predictions, denoted L r , L p , to 128 initially, following (Kojima et al., 2022).
- However, we find that this can be insufficient in many datasets.
- Allowing for longer inference, we observe that model performance improves significantly on AQUA and commonsense reasoning tasks (Appendix Table 9).
- Sample inspection shows that rationales with over ‚àº500 tokens are typically repetitive or too digressive.
- To investigate the effect of the max length L r of the teacher rationale on fine-tuning, we compare student performance using L e = {128, 512} (Table 7).
- The effect of L r on student performance varies across datasets, and increased L e does not necessarily improve student performance on tasks that require longer rationales, such as AQUA.
- Finally, we examine the length distribution of the generated rationales from the teacher model and student trained on short (L r = 128) and long (L r = 512) reasoning samples, respectively (Appendix Figure 4).
- We find that the distribution is different for each dataset. Notably, we find that while the distributions from the long students were similar to that of the teacher, the generated rationale from the short students were typically limited to less than ‚àº128 tokens.
- These findings are in line with the intuition that different tasks require different lengths of rationales, and suggest that careful consideration is needed in determining parameters related to sequence length.

## Discussion
- Versatility of the underlying prompt-based generation methods allows for easy application to any complex task
- Fine-tuning and inference on student models can also be performed on more accessible hardware, reducing long-term computational costs and environmental impact
- Optimizing for efficiency in single tasks leads to models that do not need to excel at everything, and allows for more task-specific optimization
- Vanilla fine-tuning outperforms Fine-tune-CoT on the majority of benchmarks, but Fine-tune-CoT gives intermediate, easy to trace reasoning steps instead of a complete black box
- Intuitively, student models using Fine-tune-CoT can generalize to previously unseen tasks

## Conclusion
- Large language models can be used to teach smaller student models how to reason step-by-step
- This method significantly improves the performance of small models on a range of different tasks with high sample efficiency
- By leveraging publicly available models with zero-shot prompting, we demonstrate a task-agnostic method to elicit reasoning performance in small models, accessible to the broader community
