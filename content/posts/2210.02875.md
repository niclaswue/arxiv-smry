---
title: "Binding Language Models in Symbolic Languages"
date: 2022-10-06T12:55:17.000Z
author: "Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-02875v1.webp" # image path/url
    alt: "Binding Language Models in Symbolic Languages" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.02875)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.02875).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/binding-language-models-in-symbolic-languages).

# Abstract
- Neural approaches have been dominating NLP tasks recently
- However, they lack interpretability and robustness
- We propose Binder, a training-free neural-symbolic framework that maps the task input to a program
- This allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python)
- It adopts an LM as both the program parser and the underlying model called by the API during execution
- Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging

# Paper Content

## INTRODUCTION
- Neural end-to-end systems are flexible and easy-to-use, but lack interpretability and robustness
- Symbolic approaches produce explicit intermediate representations such as logical forms, reasoning paths, or program code, which might then be executed to derive a final output
- Intermediate form produced by these symbolic methods is interpretable, and the resulting execution makes them more robust to input changes
- Most works proposing neural-symbolic systems require elaborate human design of the symbolic language and the calibration of corresponding neural modules to tackle problems in a specific domain with large training data
- We propose BINDER, a training-free neural-symbolic framework that maps task inputs to an executable program in a programming language (e.g., SQL, Python) bound with a unified API to call language models (LMs; Brown et al., 2020;Chen et al., 2021) to perform versatile functionalities, i.e. a BINDER program(e.g., Binder-SQL, Binder-Python in Fig. 1), with only a few input-BINDER program annotations as in-context exemplars
- Compared with end-to-end approaches, BINDER is more interpretable when debugging the model, more scalable to very large inputs, and more robust to noisy inputs

## APPROACH

## BINDER FRAMEWORK
- The BINDER framework is a way to solve NLP tasks
- The BINDER framework is defined as a set of rules
- The BINDER framework can be used to generate executables to solve NLP tasks
- The BINDER framework can be used to generate answers to NLP tasks

## BINDER Parsing
- The input natural language is parsed into a BINDER program
- A BINDER program is an expression in a symbolic language that optionally includes API calls
- We define the API call in the program as function f ( Q ; D) that accepts a question Q to be answered, and the context D to be queried on
- For example, "North America?" in Fig 1 the program is equivalent to solving the problem with an LM in end-to-end manner)
- The output of an API call f ( Q ; D) is the answer to Q, and it is represented as a variable compatible with the symbolic language grammar

## BINDER Execution
- In the execution stage, the program Z is executed by a BINDER interpreter to derive the answer A.
- The BINDER interpreter consists of a standard symbolic language interpreter and the model(s) realizing the API calls.
- In lexical and syntax analysis, f ( Q ; D) is added as a new identifier in the grammar, and the program is parsed as an abstract syntax tree (AST) based on this extended grammar.
- In program evaluation, the API calls are evaluated by calling the underlying neural models.
- The API call output is saved as a variable compatible with the standard symbolic language grammar, and thus the program can be finally executed by an off-the-shelf symbolic language interpreter to derive the output.

## IN-CONTEXT LEARNING FOR BINDER
- Large language models are being used for in-context learning
- Compared to fine-tuning, in-context learning only takes a few annotations/demonstrations as a prompt
- In-context learning is performed without training the model parameters
- BINDER is being used for code generation
- The model is used to generate candidate BINDER programs
- The programs are executed by the BINDER interpreter and the output answer is derived via a majority voting strategy.

## BINDER IMPLEMENTATION
- BINDER is a tool that can be used to solve questions in a domain by using a large language model to understand the data.
- The language model is able to understand the data and answer questions in the domain.
- BINDER can be used with SQL and Python.
- BINDER can be used to solve questions in a domain.
- BINDER can be used with a large language model to understand the data.
- BINDER can be used to solve questions in a domain.
- BINDER can be used with SQL and Python.

## EXPERIMENTS

## EXPERIMENT SETUP
- Datasets WIKITQ (Pasupat & Liang, 2015) and TABFACT (Chen et al., 2019a)
- Requires complex table reasoning skills to answer the questions
- According to SQUALL (Shi et al., 2020b), about 20% of WIKITQ questions are not answerable by pure SQL, either because of the need for extra knowledge or the limited coverage of the SQL grammar, both of which are issues that BINDER is designed to address
- Evaluation The evaluation metrics are execution accuracy (EA) for WIKITQ and TABFACT following common practice for these datasets. Program executions are likely to be semantically correct but fail to match the gold answer exactly -for example, SQL outputs 1/0 for yes/no questions. Though this is considered correct according to human evaluation, it is regarded as incorrect by the exact match evaluator. Thus, we add a pre-matching check for these semantically correct cases in WIKITQ to the official evaluator.

## Method
- Without finetuning, Codex end-to-end QA is 72.6
- Codex SQL is 80.7
- Codex BINDER is 85.1
- With few-shot retriever, TABFACT accuracy on the official small test set is 86.0
- Table 2: TABFACT accuracy on the official small test set.

## Implementation Details
- We use the OpenAI Codex (code-davinci-002) API1 model to generate programs and annotate them with BINDER programs.
- The BINDER programs are used to generate programs that are similar to the OpenAI Codex (code-davinci-002) API1 model.
- The BINDER programs are more interpretable and stable than other programs.

## ANALYSIS

## ABLATION STUDY
- BINDER outperforms Codex SQL by 10.1% on program-unsolvable questions
- According to SQUALL (Shi et al., 2020b), about 20% of WIKITQ questions cannot be annotated in SQL
- Codex BINDER significantly outperforms Codex SQL by 10.1% on program-unsolvable questions, aligned with the motivation of our design
- BINDER mitigates this phenomenon
- BINDER also achieves a higher score than SQL on program-solvable

## INTERPRETABILITY
- An important advantage BINDER provides is the improvement in interpretability over the end-to-end approaches
- We also use an example to show the advantages of using BINDER for debugging and interpreting results
- As shown in the example in Table 5, the result from the end-to-end system is 1967, which is incorrect but provides no clue to the reason behind this prediction
- The result from BINDER is incorrect, since the program uses the incorrect value and operator regarding the win_team column from the table (should be "LIKE "%kansas state%""), and it can be potentially fixed in the future by adding similar in-context exemplars or fuzzy match postprocessing

## ROBUSTNESS

## SCALABILITY
- BINDER is scalable because it parses and executes the symbolic language
- MMQA F1/EM on development set

## NOISY CONTENT
- End-to-end methods are more brittle to noisy inputs
- We build a noisy WIKITQ development subset (random 150 samples) with distractors
- Stability of BINDER is better than end-to-end QA when confronted with distractors

## BINDER WITH PYTHON
- We design BINDER to be easily extensible to various programming languages
- We explore using Python (with the Pandas package) as the BINDER language on WIKITQ
- Similar to SQL, BINDER with Python is implemented by incorporating the f (• ; •) neural API into it
- We evaluated it on the program-unsolvable subset of WIKITQ to test whether our method improves Python's capability
- Though this subset is split based on SQL, Python shares some similar weaknesses with SQL, such as in cases with external knowledge requirements and unsupported functionality
- As shown in Table 6, BINDER with Python effectively improves the Python coverage on the difficult subset
- The gap between Python and SQL performance on WIKITQ may lie in the properties of each programming language, as suggested by Guo et al. (2020)

## MULTIMODAL APPLICATION
- We explore applying BINDER on the multi-modal dataset MULTIMODALQA (MMQA) across text, tables, and images.
- To input images into the LM program generation, images are converted into textual image captions with a vision-text pretrained model OFA (Wang et al., 2022)
- As far as we know, we are the first to demonstrate that multi-modal QA across text, tables, and images can be handled with interpretable and executable programs.
- With the oracle retriever that assumes gold passages and images are given for each question, BINDER can achieve comparable performance with the state-of-the-art, showing the potential of BINDER approach in the future.

## RELATED WORK

## CONCLUSION
- Answer-biased majority vote selects the answer that appears most often in the candidate answer pool.
- The second variant is called answer-agnostic majority vote.
- Answer-agnostic majority vote selects the answer that appears most often in the input.
