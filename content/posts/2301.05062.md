---
title: "Tracr: Compiled Transformers as a Laboratory for Interpretability"
date: 2023-01-12T14:59:19.000Z
author: "David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, Vladimir Mikulik"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-05062v1.webp" # image path/url
    alt: "Tracr: Compiled Transformers as a Laboratory for Interpretability" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.05062)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.05062).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/tracr-compiled-transformers-as-a-laboratory).

# Abstract
- Interpretability research aims to build tools for understanding machine learning models
- In this work, we propose to build transformer models manually as a testbed for
- We introduce Tracr, a "compiler" for translating human-readable programs into weights of a
- Tracr takes code written in RASP, a domain-specific language (Weiss et al. 2021), and
- We use Tracr to create a range of ground truth transformers that implement programs

# Paper Content

## Introduction
- As deep learning models are becoming more capable and increasingly deployed in production, improving our ability to understand how they make decisions is crucial
- Mechanistic interpretability aims to achieve this by reverse engineering neural networks and producing mechanistic explanations of the algorithms a model implements
- This approach has achieved success in convolutional neural networks for image classification
- Cammarata et al. (2020) explain a range of specific circuits in InceptionV1 (Szegedy et al., 2015), including curve detectors, high-low frequency detectors, and neurons detecting more high-level concepts such as dogs or cars
- Elhage et al. (2021) and Wang et al. (2022) achieve early success in interpreting transformer language models using similar methods
- Despite this success, the toolbox of approaches for generating mechanistic explanations remains small and poorly understood
- Part of the difficulty is that evaluating mechanistic explanations requires creativity and effort by researchers
- It is difficult to evaluate how well an explanation tracks the actual mechanism used by the model when all our knowledge of the mechanism comes from the explanation itself
- Without access to ground truth about the proposed mechanism, we must verify the methods used to study it in some other way
- The standard approach for evaluating mechanistic explanations combines evidence from many ad-hoc experiments (e.g., Olah et al. (2020) and Olsson et al. (2022))
- However, since this is expensive to do, many methods are only evaluated in toy models (e.g., Elhage et al. (2022)) or on a handful of nontrivial circuits in real models (e.g., Chan et al. (2022))
- Systematic evaluation in nontrivial settings is usually intractable as it requires a lot of researcher time
- The situation is analogous to trying to invent a microscope lens without ever being able to point it at familiar, well-understood shapes
- Through careful reasoning and experimentation, we might notice regularities in the tiny world seen through the lens, and begin to trust findings made with it; but if we could look through the lens at something we already understand, we would recognise its optical properties and correct its flaws
- We propose to directly tackle the absence of ground truth explanations by "compiling" human readable code to weights of a neural network
- In this report, we present Tracr, a proof-of-concept implementation of such a compiler
- Using this approach, we can create models which perform nontrivial computation with a known implementation
- We can then evaluate interpretability tools by applying them to compiled models and comparing the resulting explanation to the ground truth
- Imagine we want to evaluate a method for locating specific knowledge in transformer models, such as "causal tracing" (Meng et al., 2022). In real language models, it can be challenging to check its correctness: the method might point out a location in the model, but we can't easily independently verify its claim, since no trusted procedure for establishing such facts about models in the wild exists yet
- With Tracr we can construct models that encode some information in a specific location and check if our method correctly locates it
- We can further explore special cases, such as information stored redundantly in different places
- In this work, we focus on transformer models (Vaswani et al., 2017) and use RASP, a domainspecific programming language for describing transformer computations
- We develop an approach to compile RASP programs to the weights of a transformer model by combining hand-coded and fully interpretable model components
- We further propose a method that uses gradient descent to compress the compiled models to make them more efficient and realistic
- More specifically, in this report, we:
- Describe a modified version of the RASP programming language better suited for being compiled to model weights (Section 3.2) and discuss some limitations of the RASP programming model
- Introduce Tracr, a "compiler" for translating RASP programs into transformer model weights (Section 3.4)
- To describe Tracr, we also introduce craft, its intermediate representation for expressing linear algebra operations using named basis directions (Section 3.3)
- Showcase several transformer models obtained by using Tracr (Section 4)
- Propose an optimization procedure to "compress" the compiled models and make them more efficient and realistic (Section 5)
- We analyse models compressed this way, demonstrating superposition (Elhage et al., 2022).
- Discuss potential applications and limitations of Tracr and how compiled models can help to accelerate interpretability research (Section 6)

## Background
- Tracr is a software that compiles a program into a transformer model
- The transformer model is a representation of the program that can be used to understand the program
- The residual stream is a representation of the output of the program
- The indicator variable is_x is used to indicate whether a particular input sequence is part of the program
- The select-aggregate operation is used to compute thefrac_prevs

### Transformer Models
- A transformer model consists of alternating multi-headed attention and multi-layer perceptron layers with residual connections.
- Multi-headed attention (Vaswani et al., 2017) computes attention maps on sequences of length .
- A single attention head first computes an attention pattern for some input  ∈ ℝ × , where    ,    ∈ ℝ ×  are learnable parameters.
- Usually, we call the entries of (   ) keys, and the entries of (   ) queries.
- Multi-headed attention combines attention heads heads by computing MHA() = Concat  1 ( 1  ), . . . ,   (  )    where    ∈ ℝ ×  and   ∈ ℝ   × are another set of learnable parameters.
- We commonly call the entries of (   ) values.
- The MLP layers in transformer models compute MLP() = ( 1 ) 2 where  1 ∈ ℝ ×ℎ ,  2 ∈ ℝ ℎ× are learnable weights, and  is a non-linear function, often the Gaussian Error Linear Unit (GeLU; Hendrycks and Gimpel, 2016).
- In this paper, we focus on decoder-only transformers with the popular GPT architecture (Radford et al., 2018), which consists of alternating blocks of MHA, MLP, and layer normalization (Ba et al., 2016).
- The input to the model is the sum of a learned embedding of a sequence of input tokens and a positional embedding.
- The model is trained to predict the next token using gradient descent.

### Transformer Circuits
- Transformers have residual connections at each attention and MLP layer.
- Elhage et al. (2021) consider the residual connections a core feature of the architecture and describe the model in terms of a residual stream that each layer reads from and writes to in sequence.
- The residual stream acts as a type of memory that earlier layers can use to pass information to later layers.
- Importantly, we can think of MHA as summing over the outputs of  independent attention heads, each parameterised by low-rank matrices   and   .
- acts as a bilinear operator reading from the residual stream, and   is a linear operator both reading from and writing to the residual stream.

### The RASP Programming Language
- RASP is a domain-specific language for expressing transformer computations
- RASP is a model to describe transformers and provides an interpreter for RASP code
- We are primarily interested in compiling actual transformer models
- In this section, we review the main features of RASP

## Tracr: A Transformer Compiler for RASP
- RASP maps to the transformer architecture
- propose a few modifications to RASP that make this mapping more straightforward
- introduce craft, our "assembly language" for transformer models
- describe how Tracr translates RASP programs to transformer weights

### Mapping RASP to Tranformers
- RASP provides a computational model of transformers
- For the most part, we can map RASP operations directly to the components of a transformer model
- The built-in s-ops tokens and indices correspond to a transformer's token and position embeddings
- For example, we can embed the tokens and positions as categorical variables in orthogonal subspaces of the embedding space
- MLP layers
- Any elementwise operation in RASP can be approximately computed by an MLP layer simply because MLPs can approximate any function with accuracy depending on the width and depth of the MLP
- Attention layers
- RASP's select-aggregate operations map to the attention layers in transformer models
- The post-softmax attention pattern needs to match the selection matrix for all inputs to implement a given selector
- So, given a large enough key/query-dimension, an attention head can implement an arbitrary binary attention pattern using its   matrix
- The   matrix of the attention head can then implement the aggregate operation

### Modifications to RASP
- RASP allows combining selectors using boolean operations; however, there is no natural analogue for this in real transformers.
- Combining selectors with different input variables is particularly problematic.
- For example, in RASP we can define a selector select (a , b , == ) and select (c , d , == ) using four s-ops a,b,c, and d. However, a real attention head only has two inputs.
- If RASP is the high-level language we compile, craft is our "assembly language", offering slightly more abstraction than operating on pure weight matrices.

### craft: An Assembly Language for Transformers
- Vector spaces with labelled basis dimensions and operations on them
- Allows us to define projections or other linear operations in terms of basis direction labels
- Craft abstracts away the need to keep track of padding in weight matrices
- We implement a transformer in craft that sticks closely to the transformer circuits view provided by Elhage et al. (2021)
- In particular, the residual stream is a vector space with a basis
- An attention head can be defined using a bilinear operator   :  ×  → ℝ and a linear operator   :  → , where , , ,  ⊂  are the vector spaces that reuse the same basis.
- craft then handles the projection of these operators up to  ×  → ℝ and  → , which corresponds to adding the requisite padding.

### Compiler Overview
- Tracr is a Python implementation of RASP that makes it easier to write annotations, such as variable encodings
- In Tracr, a RASP program is a data structure that is incrementally constructed by passing in dependencies to each operation
- For each s-op, we need to decide how to embed it in the residual stream
- In the second step, we traverse the graph and annotate each node with its possible outputs
- To construct a transformer model, we need to allocate all craft components in the computational graph to layers
- We can generally formulate this as a combinatorial optimization problem with several constraints: the transformer architecture has alternating attention and MLP layers, and all computations that depend on each other need to be in the correct order
- For scope reasons, we solve this with a heuristic
- First, we compute the longest path from the input to a given node
- Then we apply additional heuristics to combine layers with blocks that we can compute in parallel
- This approach returns a correct but sometimes suboptimal layer allocation
- We construct the residual stream space as the direct sum of all model components' input and output spaces
- In other words, we embed each s-op in its own orthogonal subspace, which is reserved for its sole use throughout the entire network
- Now, we can traverse the computational graph in the order determined by the layer allocation and stack the components to obtain a full transformer represented in craft
- Finally, we translate the craft representation of the model into concrete model weights

## Exploring Compiled Transformers
- Tracr can compile RASP programs for all the tasks described in Weiss et al. ( 2021)
- Tracr had to modify a few of the programs to only use features supported by Tracr.

### Example 1: Counting tokens
- The model uses a single input dimension that is fixed.
- This input dimension is used as a constant, for example, to add a bias in MLP layers.

### Example 2: Sorting
- The program computes the target position of each token by using the selector_width primitive in RASP
- The program can handle duplicates by adding a small multiple of indices to the keys

### More examples
- Tracr can compile a wide range of RASP programs
- In Appendix D, we discuss a few more examples
- Our open-source Tracr implementation contains a library of even more example programs

## Compressing Compiled Transformers
- Tracr models can be sparse and inefficient because they reserve an orthogonal subspace of the residual stream for each s-op.
- In this section, we propose an experimental approach for "compressing" the resulting models and making them more efficient.
- This feature is presented as preliminary work and is not yet provided in the Tracr library.
- Here, we present two case studies of compressing compiled models.
- In addition to making Tracr models more efficient, the compressed models allow us to study how real neural networks might compress  features into a representation space with fewer than  dimensions.

### Gradient Descent Based Compression
- We use a single linear projection to compress the disentangled residual stream with size to a smaller space with dimension
- We modify the model to apply whenever it reads from and whenever it writes to the residual stream (see Figure 6)
- We freeze the weights of all layers and train only using stochastic gradient descent (SGD)
- Since vanilla Tracr models are sparse and have orthogonal features, this process can be viewed as learning the projection from a "hypothetical disentangled model" to the "observed model" described by Elhage et al. (2022)
- We want the compressed model to minimise loss under the constraint that it implements the same computation as the original model. To achieve this, we train to minimise
- [L (, )], where L (, ) = L out (, ) + L layer (, ) where  () is the output of the compiled model for input , f () is the output of the compressed model, and ℎ  () and ĥ, () are the output vectors at layer  of the respective models.

### What does the compression learn?
- The model can be compressed from  = 14 to  = 6 without hurting performance
- To understand the compression better, we can study how  embeds the original  features in  <  dimensions
- The model can be compressed to  = 8 without changing performance

### Do the compressed models still implement the same computation?
- The compressed models achieve a low loss
- To this end, we evaluate the average cosine similarity between the output at each layer of the two models
- For the compressed frac_prevs model, the cosine similarity is close to 1, which implies that the compressed model is consistent with the compiled model (up to differences in norm)
- In categorical tasks the compressed model is encouraged to output vectors with a large norm due to the output softmax. We found that this can sometimes lead to the norm of the outputs at intermediate layers also changing even though the cosine similarity stays below 1
- However, in other cases, the cosine similarity stays below 1 even as the compressed model gets close to 100% in accuracy
- As an example, Figure 9 shows results from compressing the sort_unique model. Here, the compressed model achieves almost perfect accuracy on the task, but the average cosine similarity of the outputs at individual layers stays around 0.8. This suggests that the compressed model solves the tasks differently from the original compiled model.
- By inspecting the models' outputs at each layer, we can attribute the error to the target_pos variable. In the Tracr model, target_pos is encoded categorically, with a dimension allocated per position. However, the compiled model only uses one of these dimensions. This suggests that the compressed model moves the tokens to the target position with a numerical encoding of the target position rather than a categorical encoding.
- During training, this reduces the output loss at the cost of increasing the layer output regulariser. This case shows that even in this fairly restrictive compression setup, the compressed model can learn a different computation to be more efficient. This is both encouraging and problematic: it is evidence that we can achieve meaningful compression with a simple approach; however, even in this restrictive setting, the compressed model is not guaranteed to be faithful to the original RASP program, undermining the value provided by the compiler as a source of ground truth.

## Discussion
- We provide an open-source implementation of Tracr
- Tracr has potential applications in interpretability research
- The current limitations of Tracr can be addressed

### Applications of compiled models in interpretability research
- Compilers like Tracr allow researchers to set up controlled experiments that test specific hypotheses about the computational structure of transformers.
- In this way, it acts as a laboratory for research in interpretability, enabling research that might otherwise be intractable.
- Test cases for interpretability tools.
- Compiled models serve as a natural foundation for testing the faithfulness (Jacovi and Goldberg, 2020) of an explanation, and provide a way to falsify (Leavitt and Morcos, 2020) the explanations given by interpretability techniques.
- Ultimately, they could be used to build libraries of test cases for interpretability tools, which could in turn enable quantitative evaluation metrics.
- For example, Meng et al. (2022) propose a method to locate factual knowledge in transformers.
- Tracr could allow us to test what this or similar methods can locate in a range of models implementing different algorithms, contextualising its result in real models.
- Another way to evaluate our understanding of how a model works is to replace parts of the model with hand-coded components.
- For example, Nanda and Lieberum (2022) test their understanding of how a transformer implements modular addition by replacing components of the model with their own idealised implementation and find that this can increase downstream performance, which is strong evidence that the proposed explanation is correct.
- While Tracr compiles an algorithm into a full transformer model, it could be adapted to only compile part cosine similarity is 1. of a model to replace part of a trained model. This could make it easier to evaluate our understanding of a large model.
- Beyond evaluation, compiled models can be used as a testbed for studying circuits-level phenomena and developing new approaches for interpreting transformer models.
- For example, in Section 5 we successfully induced superposition in compressed Tracr models.
- Future work could analyse superposition in Tracr models, extending previous work in toy models (Elhage et al., 2022;Scherlis et al., 2022).
- In particular, Tracr allows studying how the structure of computation implemented by a model affects which features will be stored in superposition.
- One goal for this line of research could be to predict how a specific Tracr model will be compressed, which features will be stored in superposition and how.
- A complementary approach is to try reversing the superposition induced by a compression procedure, e.g., using ideas from compressed sensing and dictionary learning (Aharon et al., 2006;Donoho, 2006).

### Limitations of RASP and Tracr
- RASP is limited in terms of expressivity, efficiency and realism
- Many of these limitations could be overcome in future versions of Tracr
- Expressivity. RASP is designed for algorithmic tasks that map an input sequence to a discrete output sequence. However, current language models usually map a sequence of input tokens to a probability distribution over the next token. Circuits in real models often consist of components that increase or decrease the probability of some tokens based on previous tokens (Wang et al., 2022).
- RASP, and hence Tracr, cannot model such "probabilistic" computation, but could potentially be extended to support it.
- RASP only uses binary attention patterns, which inherently limits the range of algorithms it can implement (Merrill et al., 2022).
- A way to extend RASP to support numeric attention patterns is discussed in Weiss et al. (2021).
- Tracr models store all variables in orthogonal subspaces of the residual stream. Even if a variable is only used in part of the computation, Tracr reserves a subspace of the residual stream for it in all layers of the model.
- Real models use a more compressed representation and likely reuse dimensions for multiple features.
- Improved versions of the compression procedure discussed in Section 5 could address this limitation, as would using a constraint optimisation solver instead of a heuristic for layer allocation.
- Realism. Tracr constructs layers from hand-coded parameter matrices. This is both unrealistic and inefficient, but could be addressed by learning the layers in isolation, then assembling them into a full model manually. Similarly, instead of manually splitting the   and   matrices, matrix factorisation could be used to get more efficient solutions.
- Also, Tracr models align their features with the computational basis. This is unrealistic, and makes the resulting models easy to interpret just by inspecting the residual stream activations.
- Rotating the basis of the compiled model is a straightforward way to address this if obfuscation is needed; compression would be an even more comprehensive approach.
- While all of these issues could be overcome in a more sophisticated compiler, there are fundamental limitations on the role compiled models can play.
- Compiled models are an intermediate step between very simple toy models and real learned models. They help us understand ideas and methods, but results in compiled models do not necessarily generalise to real models.
- Compared with real models, compiled models will always be simpler. For example, we will likely never compile full-fledged language models.
- Compiled models will be more likely to be intepretable (e.g., the axis-aligned orthogonal residual stream bases in Tracr), and more likely to fit into existing paradigms for thinking about transformers.
- When using them to evaluate interpretability tools, we should be careful to make sure that the tools do not exploit this, treating such evaluations as a minimum bar rather than a full validation of a technique.
- Conversely, some methods might conceivably rely on features present in real models but not in compiled models.

## Conclusion
- Tracr is a tool for compiling human-readable code to the weights of a transformer model
- To this end, we developed Tracr, a tool for compiling human-readable code to the weights of a transformer model
- We outlined our vision for the use of compiled models in interpretability, and there may other potential applications of Tracr within and beyond interpretability research
- We are looking forward to seeing other researchers use it, and we hope studying compiled models will help to increase our understanding of neural networks
- This section highlights a few more implementation details of Tracr
- We describe how we construct MLP and attention blocks, how we implement the selector width primitive, and how we extend RASP and Tracr to use causal attention
- For the full implementation and documentation, refer to the code repository at https://github.com/deepmind/tracr
- Most of the compilation process does not change, and we only need to ensure to compile selectors to causal attention heads
- We implemented the compression described in Section 5 in Jax on top of the Haiku transformer implementation that comes with Tracr
