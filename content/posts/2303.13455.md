---
title: "CoBIT: A Contrastive Bi-directional Image-Text Generation Model"
date: 2023-03-23T17:24:31.000Z
author: "Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge and 1 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-13455v1.webp" # image path/url
    alt: "CoBIT: A Contrastive Bi-directional Image-Text Generation Model" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.13455)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.13455).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/cobit-a-contrastive-bi-directional-image-text).

# Abstract
- Field of vision and language has seen an increase in pre-trained foundation models.
- Existing methods use contrastive, image-to-text, and text-to-image objectives.
- CoBIT unifies the three objectives in one framework.
- CoBIT has superior performance in image understanding, image-text understanding, and text-based content creation.
- Examples of performance include 82.7% in zero-shot ImageNet classification, 9.37 FID score in zero-shot text-to-image generation, and 44.8 CIDEr in zero-shot captioning.

# Paper Content

## Introduction
- Foundation model development for vision-and-language learning has been dedicated to 3 directions
- 1st direction: guiding visual representation pre-training using correlated textual descriptions
- 2nd direction: pre-training image-encoder & text-decoder models with Image-to-Text token generation loss
- 3rd direction: pre-training text-encoder & image-decoder models with Text-to-Image token generation loss
- Most models are trained independently with only one of these three objectives
- CoBIT unifies the three learning objectives: cross-modal contrastive learning, image-to-text generation and text-to-image generation
- CoBIT is trained on large-scale noisy web-crawled image-text data and image annotation data

## Related work
- Pre-training a visual backbone with paired text data
- CLIP and ALIGN use contrastive learning to pull together paired image-text embeddings
- Florence, BASIC and LiT scale datasets and models
- FILIP uses local token features from image and text
- MS-CLIP and CLIPPO share model parameters between vision and text
- Pre-training with mask-reconstruction loss and text autoregression
- Diffusion-based and token-based methods for text-to-image generation
- Unifying contrastive loss, image-to-text generation and text-to-image loss in one pre-training framework

### Input
- Model supports 3 types of input: text tokens, discrete image tokens, and raw images
- Text tokens are tokenized using a SentencePiece model with a 64k vocabulary
- Discrete image tokens are tokenized into a 32x32 grid of image tokens with 8192 image token classes
- Raw images are divided into non-overlapped patches with a resolution of 288x288 and patch size of 18x18

### Architecture
- CoBIT is composed of one image unicoder, one text unicoder and one cross-attention decoder
- Image unicoder has two working modes: encoding and decoding
- Text unicoder also has two working modes: encoding and decoding
- Cross-modal decoder is a fusion-and-generation module
- Different attention masks used for text and image generation

### Pre-training
- Pre-training of CoBIT has 3 objectives: image-text contrastive loss, I2T generation loss, T2I generation loss
- Contrastive loss uses raw image and text to get encoded image and text features
- I2T and T2I generation loss are token generation problems
- Classifier-Free Guidance is used in T2I generation
- Final loss is a combination of the 3 losses
- CoBIT is scaled up in terms of layers and model dimension
- Text unicoder is initialized with pre-trained text unimodal decoder

## Experiments
- Pre-training data and optimization process described
- Zero-shot and fine-tuning evaluation of 3 capacities: visual understanding, image captioning and multimodal understanding, text-to-image content creation
- Components of CoBIT ablated to justify design

### Pre-training details
- CoBIT is pre-trained with image-text data from ALIGN, JFT-4B and WebLI datasets
- De-duplication is used to remove examples close to downstream tasks
- Optimization is done using Adafactor optimizer with a weight decay of 0.045
- Models are pre-trained for 1M steps and take 12 days on CloudTPUv4 chips
- Models are further pre-trained for 50k steps with 576x576 high-resolution raw images as input

### Zero-shot evaluation on downstream tasks
- CoBIT is capable of versatile zero-shot abilities
- Evaluated on 5 representative tasks
- CoBIT performs image understanding, imagetext understanding and text-guided content creation in the zero-shot manner
- CoBIT-Large outperforms strong baselines on ImageNet
- CoBIT-Large outperforms CoCa-Large on 5 out of 8 metrics
- CoBIT-Large achieves impressive FID of 9.37, outperforming models with larger scale

### Fine-tuning on downstream tasks
- Demonstrate transferability of CoBIT on image understanding, image-text understanding and text-guided content creation
- Conduct linear probing or finetuning on multiple downstream tasks
- CoBIT-Large outperforms CLIP and ALIGN by around 1%
- Categorize VQA, SNLI-VE and image captioning into tasks requiring image-text understanding
- CoBIT computes caption predictions in the same way as zero-shot image captioning
- Propose unicoder to replace encoder, which can both encode and decode unimodal representations with shared parameters
- Ablate Unicoder vs. Encoder and demonstrate effectiveness of proposed unicoders
- Train CoBIT all from scratch without hurting much performance
- Visualize good and failed generated images of CoBIT-Large
- Unify contrastive loss, text-to-image generation and image-to-text generation loss
- Ablate different weights for three objectives and select best one as default configuration
- Models create risks with respect to misinformation and introduce challenges and opportunities for creativity and art
