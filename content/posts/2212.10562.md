---
title: "Character-Aware Models Improve Visual Text Rendering"
date: 2022-12-20T18:59:23.000Z
author: "Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, Noah Constant"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10562v1.webp" # image path/url
    alt: "Character-Aware Models Improve Visual Text Rendering" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10562)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10562).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/character-aware-models-improve-visual-text).

# Abstract
- Current image generation models struggle to reliably produce well-formed visual text
- In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word's visual makeup as a series of glyphs
- To quantify the extent of this effect, we conduct a series of controlled experiments comparing character-aware vs. character-blind text encoders
- In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell)
- Transferring these learnings onto the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark)
- Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.

# Paper Content

## Introduction
- Over the last year, image generation models have made impressive quality gains (Rombach et al., 2021;Ramesh et al., 2022;Saharia et al., 2022;Yu et al., 2022), and are increasingly visible in the public sphere.
- While many practical use cases are already within reach, rendering reliable visual text in images remains a challenge.
- For example, Ramesh et al. (2022) observe that DALL•E-2 "struggles at producing coherent text," and the latest release of Stable Diffusion lists "cannot render legible text" as a known limitation.
- In this paper, we seek to understand and improve the ability of image generation models to render high-quality visual text.
- To do so, we first investigate the spelling ability of text encoders in isolation. We find that despite their popularity, characterblind text encoders-which receive no direct signal as to the character-level makeup of their inputshave limited spelling ability.
- Building on Itzhak and Levy (2022), we test the spelling ability of text encoders across scales, architectures, input representations, languages, and tuning methods. We document for the first time the miraculous ability of character-blind models to induce robust spelling knowledge (>99% accuracy) through web pretraining, but show that this does not generalize well beyond English, and is only achieved at scales over 100B parameters, making it infeasible for most applications.
- We find that character-aware text encoders, on the other hand, are able to achieve robust spelling abilities at far smaller scales.
- Applying these findings to image generation, we train a range of character-aware text-to-image models and demonstrate that they significantly outperform character-blind models on existing and novel evaluations of text rendering.
- For models that are purely character-level, this improved text rendering comes at a cost-decreasing image-text alignment for prompts that don't involve visual text. To alleviate this, we propose combining character-level and token-level input representations, and find that this delivers the best of both worlds.

## The spelling miracle
- Language models can be categorized as to whether they have direct access to the characters making up their text input ("character-aware") or do not ("character-blind").
- Many early neural language models operated directly on characters, with no notion of multi-character "tokens" (Sutskever et al., 2011;Graves, 2013).
- Later models moved to vocabulary-based tokenization, with some like ELMo (Peters et al., 2018) retaining characterawareness, and others like BERT (Devlin et al., 2019) abandoning it in favor of more efficient pretraining.
- At present, most widely used language models are character-blind, relying on data-driven subword segmentation algorithms like Byte Pair Encoding (BPE) (Gage, 1994;Sennrich et al., 2016) to induce a vocabulary of subword pieces.
- While these methods back off gracefully to character-level representations for sufficiently uncommon sequences, they compress common character sequences into unbreakable units by design.
- This is illustrated in Figure 2.
- Recent work on "token-free" modeling has pointed to advantages of character-aware input representations.
- Xue et al. (2022) show that ByT5a character-aware multilingual language model trained directly on UTF-8 bytes-outperforms parameter-matched character-blind models on tasks related to spelling and pronunciation.
- While operating at the byte or character level comes at the cost of training and inference speed, additional work suggests that this can be overcome through downsampling (Clark et al., 2022;Tay et al., 2021).
- See Mielke et al. (2021) for a recent overview of tokenization methods and character awareness.
- Surprisingly, despite lacking direct access to a token's spelling, character-blind models are, to varying degree, able to infer the character-level makeup of their tokens.
- Itzhak and Levy (2022) observe that, after fine-tuning for spelling, RoBERTa and GPT-2 can achieve 32% and 33% accuracy at spelling held-out tokens.
- Kaushal and Mahowald (2022) confirm this ability and probe it further; however it remains unclear where in pretraining this knowledge is coming from, and how to improve it.
- For image-text models, another key source of knowledge is supervised image-caption data.
- Even if its text encoder is character-blind, could a model learn to spell by observing the makeup of words within images?
- While possible, we suspect this is an inefficient paradigm for learning, as each token would need to be learned separately, and would need to appear within an image-caption pair seen in training.
- In section §5 we find that, indeed, this "late-stage" learning of spelling is inferior to using a pretrained character-aware text encoder.

## Measuring text encoder spelling ability
- Text encoders can be used to produce representations for decoding
- Text encoders can be used to produce representations for spell checking
- Text encoders can be used to produce representations for machine translation

### The WikiSpell benchmark
- We create the WikiSpell benchmark by sampling words from Wiktionary.
- For each example in the dataset, the input to the model is a single word, and the expected output is its spelling, generated by inserting spaces between each Unicode character: elephant → e l e p h a n t
- We group the Wiktionary words into buckets based on how frequently they occur in the mC4 corpus (Xue et al., 2021).
- We then create a test set (as well as an analogous development set) from each bucket by sampling 1k words uniformly from it.
- The five (non-overlapping) buckets we use are: the top 1% most frequent words, the 1-10% most frequent, 10-20%, 20-30%, and the bottom 50% (which includes the words that were never seen in the corpus).
- Finally, we build a training set of 10,000 words by combining two parts: 5,000 were sampled uniformly from the bottom 50% bucket (the rarest words), and the other 5,000 were sampled proportional to their frequencies in mC4 (thus biasing this half of the training set toward frequent words).
- We exclude any words that were selected for any of the development or test sets from being selected for the training set, so evaluation is always on held out words.
- We repeat this process for each of the languages we evaluate on.

### Text generation experiments
- We use the WikiSpell benchmark to evaluate multiple pretrained text-only models across a variety of scales.
- In particular, we experiment with the following models: T5 (Raffel et al., 2020), a characterblind encoder-decoder model pretrained on English data; mT5 (Xue et al., 2021), which is similar to T5, but pretrained on >100 languages; ByT5 (Xue et al., 2022), a character-aware version of mT5 that operates directly on UTF-8 byte sequences; and PaLM (Chowdhery et al., 2022), a decoderonly model of much larger scale, pretrained predominantly on English.
- Experimental results from English-only evaluation are shown in Table 1, and multilingual evaluation in Table 2.
- The first notable finding is that character-blind models T5 and mT5 perform much worse on the bucket containing the Top-1% most frequent words.
- This result may seem counter-intuitive since models typically perform best on examples that appear frequently in the data, but due to the way subword vocabularies are trained, frequent words are typically represented as a single atomic token (or a small number of tokens), and indeed this is the case: for example, 87% of words in the English Top 1% bucket are represented as a single subword token by T5's vocabulary.
- Scores are a bit higher in the middle-frequency buckets, where words are typically broken into a few commonly occurring subword tokens, and lower again in the lowestfrequency bucket, where even the subword tokens may be less frequent. Thus, the low spelling accuracy scores indicate that T5's encoder does not retain sufficient information about the spelling of subwords in its vocabulary.
- Secondly, our experiments show that for character-blind models, scale is a significant factor in spelling ability.
- Both T5 and mT5 get progressively better as scale increases, but even at XXL scale, these models do not exhibit particularly strong spelling abilities; for example, T5-XXL's performance on common English words is only 66%.
- It's only when character-blind models reach PaLM's scale that we start to see nearperfect spelling ability: the 540B-parameter PaLM model achieves accuracies >99% across all frequency buckets in English, despite the fact that it sees only 20 examples in its prompt (as opposed to the 1,000 fine-tuning examples shown to T5).
- However, PaLM performs less well on other languages, likely due to there being considerably less pretraining data for them.
- Our experiments with ByT5 show that characteraware models, on the other hand, exhibit far greater spelling abilities. ByT5's performance at Base and Large sizes is only slightly behind XL and XXL (though still in at least the mid-90% range), and the frequency of a word does not seem to have much effect on ByT5's ability to spell it.
- These results far exceed those of (m)T5, and are in fact comparable to the English performance of PaLM, which has >100× more parameters, and exceed PaLM's performance on other languages.
- These findings indicate that substantially more characterlevel information is retained by the ByT5 encoder, and in such a way that it can be retrieved from those frozen parameters as needed for the decoding task.

## The DrawText benchmark
- Evaluating text-to-image models has been an ongoing topic of research
- The development of standard benchmarks from COCO (Lin et al., 2014) to DrawBench (Saharia et al., 2022), and metrics including FID (Heusel et al., 2017), CLIP score (Hessel et al., 2021), and human preferences (Saharia et al., 2022)
- However, there has been a lack of work on text rendering and spelling evaluation
- To that end, we present a new benchmark, DrawText, which is designed to comprehensively measure the text rendering quality of text-to-image models
- The DrawText benchmark consists of two parts, which measure different axes of model capabilities: 1) DrawText Spelling, which evaluates via plain word rendering with a sizable collection of English words; and 2) DrawText Creative, which evaluates via text rendering with visual effects

### DrawText Spelling
- 500 English WikiSpell prompts are sampled
- For each prompt, 4 images are sampled
- The DrawText Spelling prompt tends to generate a prominently positioned sign with text, which is then relatively simple for the off-the-shelf OCR system to identify
- Text is sometimes rendered across multiple lines, so it is post-processed
- Case is ignored when computing spelling accuracy

### DrawText Creative
- Visual text can appear in many forms
- If image generation models support flexible and accurate text rendering, this will enable designers to use these models to develop creative fonts, logos, layouts, and more
- To test the ability of image generation models to support these use cases, we worked with a pro- fessional graphic designer to construct 175 diverse prompts that require rendering text in a range of creative styles and settings
- The prompts vary in how much text is specified, ranging from a single letter to an entire sentence

## Image generation experiments
- State-of-the-art text-to-image generative models consist of a text encoder plus a cascade of either diffusion models (Saharia et al., 2022) or autoregressive models (Yu et al., 2022).
- In section §3 we saw that character-aware text encoders greatly outperform character-blind models on spelling in a text-only setting; in this section, we investigate whether making the text encoder character-aware improves the text rendering ability of text-to-image models.
- We evaluate the spelling ability of text-to-image generative models with the proposed DrawText benchmark.
- Character-aware text encoders greatly outperform character-blind models on spelling in a text-only setting.
- Making the text encoder character-aware improves the text rendering ability of text-to-image models.

### Models
- For an apples-to-apples comparison, we train two character-blind and three character-aware image generation models.
- Our training closely follows the procedure of Saharia et al. (2022), with the following modifications. First, our models train for 500,000 steps, which is 5.6× fewer steps than Imagen. Second, we only train the initial 64 × 64 model, as text rendering ability can already be assessed at this scale. This allows us to forgo the training of super-resolution models.
- Third, rather than a mixture of datasets, we train exclusively on the publicly available Laion-400M (Schuhmann et al., 2021). This improves reproducibility and also increases the amount of visual text seen during training.
- Inspecting a random sample of 100 images, we found that a relatively high proportion (around 71%) of Laion images contain text, and many (around 60%) exhibit correspondence between caption text and visual text.
- Fourth, to prevent models from clipping text, we train on uncropped images with arbitrary aspect ratios.
- Among these, Imagen is most similar to our experimental models, using the same T5-XXL encoder, but trained much longer and with a larger scale of data.

### DrawText Spelling Results
- Across all word frequencies, character-aware models (ByT5 and Concat) outperform the rest, with 15+ point accuracy gains over Imagen-AR on the most frequent words, and 30+ point gains on the least frequent words.
- To assess the approximate rate of false positives and false negatives due to OCR errors, we sample 32 examples labeled correct and 32 labeled incorrect for each of T5-XXL and ByT5-XXL, and perform a manual human validation. In our sample, we find no false positives-that is, when OCR detects the correct word, it is always correct. However we do observe false negatives for both models. These include cases where the text is not detected (e.g., due to being too small or too blurry), or where OCR misreads or drops a character, or confuses punctuation with arbitrary lines or dots in the images.
- For ByT5-XXL, we find that 34% of examples labeled by OCR as incorrect are actually correct. For T5-XXL, this error rate is lower at 9%. This asymmetry suggests that the benefit of character-aware modeling may be even greater than implied by our results in Figure 4.
- To gain a better qualitative understanding of different models' failure modes, we manually inspect the generations of our T5 and ByT5 models. Table 3 illustrates common error types. Several categories of error are only observed in T5 models, suggesting that they stem from the encoder's lack of core spelling knowledge.
- In severe errors, the model is off by more than just a few characters. In semantic errors, the model makes a plausible morpheme substitution, as in demonstrated → demonstrafied. In homophone errors, the model produces an incorrect spelling that could be pronounced similarly to the target word. This suggests that some of the T5 encoders' "miraculous" spelling ability may derive from phonetic pronunciation guides found online.
- In add glyph errors, the model inserts a letter that was absent from the target, again reflecting the model's uncertainty about a token's internal character makeup.
- Other error categories are found across all model types; these include dropped, repeated, merged, or misshapen glyphs.

### DrawText Creative Results
- To test our models in a more realistic user-facing setting, we sample 8 images from each of our T5 and ByT5 models on our 175 DrawText Creative prompts in Appendix C.
- These prompts are more diverse and challenging, with the majority targeting three or more words of rendered text.
- Focusing on text rendering ability, 6 we find once again that character-aware models have a clear advantage.
- Figure 8 shows representative samples on two prompts where T5-XXL consistently misspells one or more words; see Figures 12 and 13 for non-cherrypicked samples.
- On prompts targeting longer (e.g. sentencelength) text spans, all our models struggle, as seen 6 We note our models' overall image quality and alignment fall short of a state-of-art model like Imagen (see Figure 3). This is expected, given that our models train exclusively on the lightly curated Laion-400M dataset (Schuhmann et al., 2021).
- We suspect that the problem of arranging words plausibly in a fixed frame is particularly challenging for diffusion models-which render all positions in parallel-and that progress may require larger models, longer training, and/or improvements to the image generation module. Nevertheless, we observe that character-aware text encoders provide a clear lift on these prompts, reducing the misspellings of words like refrain, arguing, and chimpanzees.

### DrawBench Results
- Character-aware text encoders excel at spelling, in both text and visual domains
- However, this ability comes at a cost-can these models maintain the high image quality and strong text-image alignment of character-blind models?
- To shed light on this question, we run several side-by-side comparisons using the Draw-Bench evaluation of Saharia et al. (2022)
- Figure 9 shows DrawBench results of three side-by-side comparisons of character-aware models vs. T5-XXL
- While image quality ("fidelity") is similar across the board, we find that purely character-level models (ByT5-XL and ByT5-XXL) score worse on image-text alignment, with raters preferring T5-XXL on 60% of prompts
- By contrast, our Concat(T5-XXL, ByT5-Small) model closes this alignment gap to within error bars.
- Thus, this "hybrid" character-aware model is able to greatly improve text rendering (Figure 4), without significantly hurting performance elsewhere.
- To understand the alignment scores in more detail, we report per-category preference scores in Figure 10
- In line with our DrawText Spelling results, the character-aware models are always preferred in the text category-21 prompts testing the ability to render 7 short phrases in 3 visual styles
- The ByT5 models are also preferred in the count category, which tests prompts like Four dogs on the street. However, they are dispreferred in nearly all other cases, and perform particularly poorly on the color category.
- Through manual inspection, we find that in this category, the ByT5 models are more prone to ignore information in the prompt, for example leaving out a mentioned object, or choosing a canonical color over a requested one (e.g. a yellow banana instead of a red one).
- One possible explanation for this behavior is that we did not tune the guidance weight parameter used at inference time (Saharia et al., 2022), using a fixed value of 30 throughout.
- Increasing this parameter is known to boost image-text alignment, but at the cost of diversity.
- It may be that characterlevel models benefit from higher guidance values than token-based models.
- Another possibility is that the ByT5 models have a shallower understanding of English language due to their multilingual nature-as ByT5 was exposed to roughly 70× less English than T5 during pretraining.

### Multilingual Results
- The ByT5-XXL model exhibits understanding across all 11 languages
- The ByT5 encoder embeds similar prompts into a shared space that factors out the contribution of language
- To learn the glyph shapes, variants and fonts used for a given script, we should expect to need to train models on a large source of visual text in that script

## Conclusion
- Characterawareness improves spelling ability in the text and visual domains
- Character-aware text encoders provide large gains on spelling
- When used within an image generation model, these gains translate directly into improved visual text rendering
- However, using exclusively character-level representations deteriorates overall text-image alignment
- To resolve this, a hybrid model combining token-level and character-level signals is used
- While we saw substantial improvements on DrawText Spelling accuracy, some failure modes remain unaddressed
- Even our strongest models were observed to occasionally drop, repeat, or merge letters within a word, or words within a phrase
- Our results strongly suggest that resolving these issues will require orthogonal improvements outside the text encoder
