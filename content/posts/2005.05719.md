---
title: "Smooth Exploration for Robotic Reinforcement Learning"
date: 2020-05-12T12:28:25.000Z
author: "Antonin Raffin, Jens Kober, Freek Stulp"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2005-05719v2.webp" # image path/url
    alt: "Smooth Exploration for Robotic Reinforcement Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2005.05719)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2005.05719).


# Abstract
- Reinforcement learning enables robots to learn skills from interactions with the real world
- In practice, the unstructured step-based exploration used in Deep RL leads to jerky motion
- Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot
- We address these issues by adapting state-dependent exploration to current Deep RL algorithms
- To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE)
- We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car
- The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance

# Paper Content

## Introduction
- Shakey was the first robot to use artificial intelligence methods
- Shaking has now become quite prevalent in robotics, but for a different reason
- Unstructured exploration has also been applied to robotics, but for experiments on real robots it has many drawbacks
- SDE was proposed as a compromise between exploring in parameter and action space
- SDE replaces the sampled noise with a state-dependent exploration function
- To the best of our knowledge, no Deep RL algorithm has yet been successfully combined with SDE
- We surmise that this is because the problem that it solves -shaky, jerky movement -is not as noticeable in simulation

## Background
- Reinforcement learning is a learning algorithm that uses feedback to improve its performance
- In reinforcement learning, an agent interacts with its environment, usually modeled as a Markov Decision Process (MDP) (S, A, p, r) where S is the state space, A the action space and p(s |s, a) the transition function. At every step t, the agent performs an action a in state s following its policy π : S → A. It then receives a feedback signal in the next state s : the reward r(s, a).
- The objective of the agent is to maximize the long-term reward.
- More formally, the goal is to maximize the expectation of the sum of discounted reward, over the trajectories ρ π generated using its policy π: where γ ∈ [0, 1) is the discount factor and represents a trade-off between maximizing short-term and long-term rewards.
- The agent-environment interactions are often broken down into sequences called episodes, that end when the agent reaches a terminal state.

### Exploration in Action or Policy Parameter Space
- In continuous actions, the exploration is commonly done in the action space [21,22,23,24,25,5].
- At each time-step, a noise vector t is independently sampled from a Gaussian distribution and then added to the controller output: where µ(s t ) is the deterministic policy and π(a t |s t ) ∼ N (µ(s t ), σ 2 ) is the resulting stochastic policy, used for exploration.
- θ µ denotes the parameters of the deterministic policy.
- For simplicity, throughout the paper, we will only consider Gaussian distributions with diagonal covariance matrices. Hence, here, σ is a vector with the same dimension as the action space A. Alternatively, the exploration can also be done in the parameter space [11,19,20]: at the beginning of an episode, the perturbation is sampled and added to the policy parameters θ µ . This usually results in more consistent exploration but becomes challenging with an increasing number of parameters [19].

### State-Dependent Exploration
- State-dependent exploration (SDE) is an intermediate solution that consists in adding noise as a function of the state, to the deterministic action.
- At the beginning of an episode, the parameters θ of that exploration function are drawn from a Gaussian distribution.
- The resulting action is as follows: a for a given state is the same.
- SDE should not be confused with unstructured noise where the variance can be state-dependent but the noise is still sampled at every step, as it is the case for SAC.
- In the remainder of this paper, to avoid overloading notation, we drop the time subscript, i. e. we now write instead of.
- s or a now refer to an element of the state or action vector.
- In the case of a linear exploration function (s; θ ) = θ, by operation on Gaussian distributions, Rückstieß et al. [1] show that the action element a is normally distributed: where σ is a diagonal matrix with elements σj = i (σ ij s i ) 2 .
- Because we know the policy distribution, we can obtain the derivative of the log-likelihood log π(a|s) with respect to the variance σ: This can be easily plugged into the likelihood ratio gradient estimator [26], which allows to adapt σ during training.
- SDE is therefore compatible with standard policy gradient methods, while addressing most shortcomings of the unstructured exploration.
- For a non-linear exploration function, the resulting distribution π(a|s) is most of the time unknown. Thus, computing the exact derivative w.r.t. the variance is not trivial and may require approximate inference.
- As we focus on simplicity, we leave this extension for future work.
- Considering Eqs. ( 5) and (7), some limitations of the original formulation are apparent:
- i The noise does not change during one episode, which is problematic if the episode length is long, because the exploration will be limited [27].
- ii The variance of the policy σj = i (σ ij s i ) 2 depends on the state space dimension (it grows with it), which means that the initial σ must be tuned for each problem.
- iii There is only a linear dependency between the state and the exploration noise, which limits the possibilities.
- iv The state must be normalized, as the gradient and the noise magnitude depend on the state magnitude.
- To mitigate the mentioned issues and adapt it to Deep RL algorithms, we propose two improvements:
- 1. We sample the parameters θ of the exploration function every n steps instead of every episode.
- 2. Instead of the state s, we can in fact use any features. We chose policy features z µ (s; θ zµ ) (last layer before the deterministic output µ(s) = θ µ z µ (s; θ zµ )) as input to the noise function (s; θ ) = θ z µ (s). Sampling the parameters θ every n steps tackles the issue i. and yields a unifying framework [27] which encompasses both unstructured exploration (n = 1) and original SDE (n = episode length).
- Although this formulation follows the description of Deep RL algorithms that update their parameters every m steps, the influence of this crucial parameter on smoothness and performance was until now overlooked. Using policy features allows mitigating issues ii, iii and iv: the relationship between the state s and the noise is non-linear and the variance of the policy only depends on the network architecture, allowing for instance to use images as input.

## Experiments
- gSDE is compared to the original SDE
- gSDE is compared to other type of exploration noise
- gSDE performs well on a real system

### Compromise Between Smoothness and Performance
- Experiment setup: 4 locomotion tasks from the PyBullet environments HALFCHEETAH, ANT, HOPPER and WALKER2D
- SAC algorithm: used on the real robot
- Continuity cost: used to compare performance between different setups
- Results: without any noise, SAC has the highest variance in the results, while with correlated and parameter noise, gSDE is able to achieve a good compromise between unstructured exploration and correlated noise
- gSDE-8 (sampling the noise every 8 steps) even achieves better performance with a lower continuity cost at train time

### Comparison to the Original SDE
- 2.5
- 2.5
- 3.5
- 3.5
- 5.5
- 5.5

## Related Work
- Exploration is a key topic in reinforcement learning
- Most recent papers still focus on discrete actions
- Several works tackle the issues of unstructured exploration for continuous control by replacing it with correlated noise
- Korenkevych et al. [15] use an autoregressive process and introduce two variables that allows to control the smoothness of the exploration
- In the same vein, van Hoof et al. [27] rely on a temporal coherence parameter to interpolate between the step-or episode-based exploration, making use of a Markov chain to correlate the noise
- This smoothed noise comes at a cost: it requires an history, which changes the problem definition
- Exploring in parameter space [10,39,11,12,40] is an orthogonal approach that also solves some issues of the unstructured exploration
- Plappert et al. [19] adapt parameter exploration to Deep RL by defining a distance in the action space and applying layer normalization to handle high-dimensional space
- Population based algorithms, such as Evolution strategies (ES) or Genetic Algorithms (GA), also explore in parameter space
- Thanks to massive parallelization, they were shown to be competitive [42] with RL in terms of training time, at the cost of being sample inefficient
- To address this problem, recent works [20] proposed to combine ES exploration with RL gradient update
- This combination, although powerful, unfortunately adds numerous hyperparameters and a non-negligible computational overhead
- Obtaining smooth control is essential for real robot, but it is usually overlooked by the DeepRL community
- Recently, Mysore et al. [14] integrated a continuity and smoothing loss inside RL algorithms
- Their approach is effective to obtain a smooth controller that reduces the energy used at test time on the real robot

## Conclusion
- Unstructured exploration in Deep RL algorithms for continuous control leads to issues that cannot be addressed by the algorithms themselves
- To address these issues, we adapt State-Dependent Exploration to Deep RL algorithms by extending the original formulation
- This generalized version (gSDE), provides a simple and efficient alternative to unstructured Gaussian exploration
- gSDE achieves very competitive results on several continuous control benchmarks, while reducing wear-and-tear at train time
- We also investigate the contribution of each modification by performing an ablation study
- Our proposed exploration strategy, combined with SAC, is robust to hyperparameter choice

## A Supplementary Material

### A.1 State Dependent Exploration
- Linear case: policy and noise matrix are equivalent
- SDE is compatible with standard policy gradient methods
- In this section, we shortly present the algorithms used in this paper. They correspond to state-ofthe-art methods in model-free RL for continuous control, either in terms of sample efficiency or wall-clock time
- A2C is an actor-critic method that uses parallel rollouts of n-steps to update the policy
- PPO introduces a trust-region in the policy parameter space, formulated as a constrained optimization problem
- TD3 Deep Deterministic Policy Gradient combines the deterministic policy gradient algorithm with the improvements from Deep Q-Network
- SAC learns a stochastic policy, using a squashed Gaussian distribution
- TD3 and SAC are very similar, but SAC embeds the exploration directly in its objective function

### A.3 Real Robot Experiments
- Augment input with previous observation and last action taken
- Decompose reward function into primary and secondary components
- Normalize each component
- Use same algorithm as used for simulated tasks
- Train directly with real robot over several days
- Less than 30 minutes of interaction

### A.4 Implementation Details
- We used a PyTorch version of Stable-Baselines library
- The algorithm for SAC with gSDE is described in Algorithm 1
- Compared to the original SDE paper, we did not have to use the expln trick to avoid exploding variance for PyBullet tasks
- However, we found it useful on specific environment like BipedalWalkerHardcore-v2

### A.6 Ablation Study: Additional Plots
- Fig. 11 displays the ablation study on remaining PyBullet tasks.
- It shows that SAC is robust against initial exploration variance, and PPO results highly depend on the noise sampling interval.
- Parallel Sampling The effect of sampling a set of noise parameters per worker is shown for PPO in Fig. 12a. This modification improves the performance for each task, as it allows a more diverse exploration.
- Although less significant, we observe the same outcome for A2C on PyBullet environments (cf. Fig. 12b). Thus, making use of parallel workers improves both exploration and the final performance.
