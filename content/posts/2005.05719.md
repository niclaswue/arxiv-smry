---
title: "Smooth Exploration for Robotic Reinforcement Learning"
date: 2020-05-12T12:28:25.000Z
author: "Antonin Raffin, Jens Kober, Freek Stulp"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2005-05719v2.webp" # image path/url
    alt: "Smooth Exploration for Robotic Reinforcement Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2005.05719)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2005.05719).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/generalized-state-dependent-exploration-for).

# Abstract
- Reinforcement learning (RL) enables robots to learn skills from interactions with the real world.
- Deep RL often leads to jerky motion patterns on real robots, resulting in poor exploration or damage to the robot.
- We propose a new exploration method, generalized state-dependent exploration (gSDE), to address these issues.
- We evaluate gSDE in simulation and on three different real robots.
- gSDE allows training directly on the real robots without loss of performance.

# Paper Content

## Introduction

## Background
- Reinforcement learning involves an agent interacting with an environment modeled as a Markov Decision Process.
- The agent performs an action in a state and receives a reward in the next state.
- The goal is to maximize the long-term reward by maximizing the expectation of the sum of discounted rewards.
- Interactions are broken down into sequences called episodes, which end when the agent reaches a terminal state.

### Exploration in action or policy parameter space
- Exploration is done in the action space or parameter space.
- Noise vector t is sampled from a Gaussian distribution and added to the controller output.
- Gaussian distributions have diagonal covariance matrices.
- Exploration in the parameter space is more consistent but challenging with increasing number of parameters.

### State-dependent exploration
- State-Dependent Exploration (SDE) is an intermediate solution that adds noise to the deterministic action.
- Parameters of the exploration function are drawn from a Gaussian distribution at the beginning of an episode.
- SDE is smoother and more consistent than unstructured step-based exploration.
- SDE should not be confused with unstructured noise.
- Derivative of the log-likelihood can be obtained.
- Limitations of the original formulation include: noise does not change during one episode, variance depends on state space dimension, linear dependency between state and noise, and state must be normalized.
- Two improvements proposed: sample parameters every n steps and use policy features as input to the noise function.
- Integrating updated version of SDE into Deep RL algorithms is straightforward.

## Experiments
- gSDE is compared to the original SDE and other types of exploration noise
- gSDE is evaluated in terms of smoothness and performance
- gSDE is tested on a real system

### Compromise between smoothness and performance
- 4 locomotion tasks from PyBullet and OpenAI Gym used to compare gSDE to other types of exploration
- SAC algorithm used, PPO results reported in appendix
- Continuity cost defined to measure smoothness
- 5 configurations compared: no exploration noise, unstructured Gaussian noise, correlated noise, adaptive parameter noise, gSDE
- 1 million steps budget, 10 runs, 20 evaluation episodes every 10000 steps
- Results show good compromise between unstructured exploration and correlated noise with gSDE

### Comparison to the original sde
- Sampling the exploration function parameters every n steps and using policy features as input to the noise function improves performance and smoothness.
- Sampling interval gSDE is a n-step version of SDE, allowing a compromise between performance and smoothness.
- Using policy features as input to the exploration function is usually beneficial, especially for PPO.
- Sampling interval n has the most impact on performance.
- Applied SAC with gSDE to a real system to control a tendon-driven elastic continuum neck.
- Compared SAC with gSDE to a model-based controller and a policy trained with a low-pass filter.
- Applied SAC with gSDE to an elastic quadruped robot and an RC car.

## Related work
- Exploration is a key topic in reinforcement learning
- Discrete actions have been extensively studied
- Correlated noise can be used to replace unstructured exploration
- Parameter space exploration is an orthogonal approach
- Population based algorithms can be used to explore parameter space
- Combining ES exploration with RL gradient update can be powerful but adds hyperparameters
- Obtaining smooth control is essential for real robots
- Mysore et al. integrated a continuity and smoothing loss inside RL algorithms

## Conclusion
- Issues arise from unstructured exploration in Deep RL algorithms for continuous control
- Cannot be directly applied to learning on real-world robots
- Adapt State-Dependent Exploration to Deep RL algorithms by extending the original formulation
- Sample the noise every n steps and replace the exploration function input by learned features
- Generalized version (gSDE) provides a simple and efficient alternative to unstructured Gaussian exploration
- Achieves competitive results on several continuous control benchmarks
- Reduces wear-and-tear at train time
- Ablation study shows noise sampling interval has the most impact
- Robust to hyperparameter choice, suitable for robotics applications
- Demonstrated successfully on three different robots
- Error in position and orientation on evaluation trajectory comparable for model-based and learned controller
- Used PyTorch version of Stable-Baselines library and RL Zoo training framework
- Algorithm for SAC with gSDE described in Algorithm 1
- Clipped mean in range [-2, 2] to ensure numerical stability
- Parallel sampling improves performance for both on-policy and off-policy algorithms
