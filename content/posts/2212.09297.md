---
title: "Transferring General Multimodal Pretrained Models to Text Recognition"
date: 2022-12-19T08:30:42.000Z
author: "Junyang Lin, Xuancheng Ren, Yichang Zhang, Gao Liu, Peng Wang, An Yang, Chang Zhou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09297v1.webp" # image path/url
    alt: "Transferring General Multimodal Pretrained Models to Text Recognition" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09297)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09297).


# Abstract
- OFA-OCR is a new method to transfer multimodal pretrained models to text recognition
- OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark
- The code (https://github.com/OFA-Sys/OFA) and demo (https://modelscope.cn/studios/damo/ofa_ocr_pipeline/summary) are publicly available

# Paper Content

## Introduction
- Optical character recognition (OCR) plays an important role in the real-world applications
- In practice, building a tool for OCR needs a pipeline consisting of a text localization module and a text recognition module
- In this work, we focus on improving the accuracy of text recognition
- Text recognition has often been regarded as a key challenge owing to the room for improvements in recognition accuracy
- In the deep learning era, the classical methods are mostly based on CNN and RNN, which are responsible for visual feature extraction and sequence modeling, respectively
- Recently, with the rise of Transformer (Vaswani et al., 2017), researchers applied the Transformer encoder-decoder framework to text recognition and achieved outperforming results over the baselines
- However, most methods are based on largescale pretraining on human-annotated or synthetic OCR data
- It is hard for other researchers to collect or create such data for reproduction
- Furthermore, the methods often include complex model or objective designs, like DETR-like decoder (Carion et al., 2020), CTC loss (Graves et al., 2006), etc.
- These components also might hinder reproduction as they increase the difficulty in training
- Therefore, we naturally raise a question: Is there any way to achieve high recognition accuracy without complex designs on data and model?
- Inspired by the recent progress in multimodal pretraining, we argue that the transfer of a unified multimodal pretrained model is a possible solution
- Multimodal pretraining has proved significant to the performance of downstream tasks, and thanks to the rise of unified multimodal pretrained models, they can perform both cross-modal understanding and generation and achieve state-of-theart performance
- We therefore propose to transfer the unified multimodal pretrained model by finetuning the pretrained model on the text recognition datasets with the task of image captioning, which is essentially a simple sequence-to-sequence learning task with maximum likelihood estimation for optimization
- To support the effectiveness of the proposed method, we have conducted extensive experiments on the Chinese text recognition benchmark (Chen et al., 2021b) covering multiple scenarios, including scene, web, document, and handwriting.
- Specifically, we finetune the open-source Chinese multimodal pretrained model OFA (Wang et al., 2022a) on text recognition, and we name the model OFA-OCR.
- Figure 1 demonstrates the results of methods with or without general-domain pretraining. It shows that multimodal pretraining on generaldomain vision-language data can effectively boost downstream performance in text recognition.
- To achieve the best performance, we apply the multitask + single-task finetuning to OFA-OCR, and it outperforms the previous state-of-the-art methods on the benchmark.
- Furthermore, through the ablation studies, we demonstrate the effectiveness of our method designs, including multitask + singletask finetuning, data augmentation, etc.
- Furthermore, to enable deployment for real-world applications, we construct a pipeline with both OFA-OCR and a simple text localization module. We find that this simple pipeline can provide high-quality OCR performance, competitive with a productlevel API.
- To leverage the capability of the multimodal pretrained model for image captioning, we employ the unified multimodal pretrained model architecture. Specifically, we implement our models on OFA (Wang et al., 2022a), an open-source state-ofthe-art unified multimodal pretrained model with the release of Chinese models. The model is mainly based on the Transformer encoder-decoder framework (Vaswani et al., 2017).

### Finetuning with Image Captioning
- It is natural to recast text recognition as image captioning
- We finetune the model with maximum likelihood estimation for optimization
- To better alleviate the discrepancy between upstream and downstream data, we apply a transformation to the input images to make them square

### Multitask Finetuning
- Multiple subtasks in text recognition
- Implementing multitask finetuning and single-task finetuning
- Boosting performance with multitask finetuning and single-task finetuning

### Datasets and Metrics
- We implemented OFA-OCR on the Chinese text recognition benchmark
- This benchmark consists of multiple subtasks of text recognition, which are text recognition in different scenarios, including scene, web, document, and handwriting
- The details of the datasets are provided in Sec. A.1
- The evaluation metric includes accuracy, which refers to the ratio of exact match.

### Experimental Results
- The experimental results are demonstrated in Table 1
- We compare our method with baseline models of OCR, including the previous state-of-the-art MaskOCR (Lyu et al., 2022)
- It can be found that with no regard to the scale of models, the base-size OFA-OCR, which is finetuned from the pretrained Chinese OFA Base, can outperform both the basesize and large-size MaskOCR models
- Specifically, it shows the advantages of 9.0, 6.9, and 5.3 absolute improvements in the scenarios of scene, web, and handwriting
- On average, the base-size OFA-OCR outperforms the base-size MaksOCR by 5.2 and the large-size MaskOCR by 3.4
- Scaling up the model size can consistently bring steady improvement in the downstream performance
- On average, OFA Large reaches the best results of 86.3.

### Ablation Study of Training Strategies
- The multitask learning influences the final performance
- The addition of the initialization of the pretrained OFA model significantly boosts the performance on the datasets
- Multitask finetuning alone can outperform single-task finetuning on all 4 tasks
- The combination of multitask finetuning and single-task finetuning is the best solution

### Ablation Study of Data Augmentation
- The preprocessing of images can boost the performance
- The preprocessing of images can be done in a way that doesn't impact the performance
- The preprocessing of images can be done in a way that is effective
- The preprocessing of images can be done in a way that is simple

### Deployment
- To construct an OCR system applicable in realworld scenarios, a strong text recognition model is not sufficient, and we need to build a pipeline with both the text detection and text recognition module.
- While the former one is not the focus of this research, we directly use a light-weight model from EasyOCR3 for detection.
- After detecting all the bounding boxes which possibly contain texts, we crop them with boxes to create a batch of new images.
- The final step is to process the images with OFA-OCR for the generation of text recognition results.

## Conclusion
- Computer science is the study of how computers work
- Computers can be programmed to do specific tasks
- Computer science is divided into several subfields, including software engineering, artificial intelligence, and computer science theory
- Computer science is a growing field, with new research being conducted all the time
- Computer science is used in a variety of industries, including business, healthcare, and technology
- Computer science is a valuable skill to have, as it can be used in a variety of fields

## Limitations
- OFA-OCR is more accurate than other baselines, but has higher costs
- In future work, we will find ways to make OFA-OCR more efficient

## Ethics Statement
- The method is based on a generation model
- The generated results should be aligned with the input
- There is a low risk of deliberate harmful contents being generated
- However, the model might still be triggered if the input contains harmful contents
- The model can be improved by focusing on improving downstream performance
