---
title: "Transferring General Multimodal Pretrained Models to Text Recognition"
date: 2022-12-19T08:30:42.000Z
author: "Junyang Lin, Xuancheng Ren, Yichang Zhang, Gao Liu, Peng Wang, An Yang, Chang Zhou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09297v1.webp" # image path/url
    alt: "Transferring General Multimodal Pretrained Models to Text Recognition" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09297)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09297).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/transferring-general-multimodal-pretrained).

# Abstract
- OFA-OCR recasts text recognition as image captioning and directly transfers a unified vision-language pretrained model to the end task
- Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark
- Additionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate that it can achieve competitive performance with the product-level API

# Paper Content

## Introduction
- Optical character recognition (OCR) plays an important role in the real-world applications
- In practice, building a tool for OCR needs a pipeline consisting of a text localization module and a text recognition module
- In this work, we focus on improving the accuracy of text recognition
- Text recognition has often been regarded as a key challenge owing to the room for improvements in recognition accuracy
- In the deep learning era, the classical methods are mostly based on CNN and RNN, which are responsible for visual feature extraction and sequence modeling, respectively
- Recently, with the rise of Transformer (Vaswani et al., 2017), researchers applied the Transformer encoder-decoder framework to text recognition and achieved outperforming results over the baselines
- However, most methods are based on largescale pretraining on human-annotated or synthetic OCR data
- It is hard for other researchers to collect or create such data for reproduction
- Furthermore, the methods often include complex model or objective designs, like DETR-like decoder (Carion et al., 2020), CTC loss (Graves et al., 2006), etc.
- These components also might hinder reproduction as they increase the difficulty in training. Therefore, we naturally raise a question: Is there any way to achieve high recognition accuracy without complex designs on data and model?
- Inspired by the recent progress in multimodal pretraining, we argue that the transfer of a unified multimodal pretrained model is a possible solution.
- Multimodal pretraining has proved significant to the performance of downstream tasks, and thanks to the rise of unified multimodal pretrained models, they can perform both cross-modal understanding and generation and achieve state-of-theart performance
- We therefore propose to transfer the unified multimodal pretrained model by finetuning the pretrained model on the text recognition datasets with the task of image captioning, which is essentially a simple sequence-to-sequence learning task with maximum likelihood estimation for optimization.
- To support the effectiveness of the proposed method, we have conducted extensive experiments on the Chinese text recognition benchmark (Chen et al., 2021b) covering multiple scenarios, including scene, web, document, and handwriting.
- Specifically, we finetune the open-source Chinese multimodal pretrained model OFA (Wang et al., 2022a) on text recognition, and we name the model OFA-OCR.
- Figure 1 demonstrates the results of methods with or without general-domain pretraining. It shows that multimodal pretraining on generaldomain vision-language data can effectively boost downstream performance in text recognition.
- To achieve the best performance, we apply the multitask + single-task finetuning to OFA-OCR, and it outperforms the previous state-of-the-art methods on the benchmark.
- Furthermore, through the ablation studies, we demonstrate the effectiveness of our method designs, including multitask + singletask finetuning, data augmentation, etc.
- Furthermore, to enable deployment for real-world applications, we construct a pipeline with both OFA-OCR and a simple text localization module. We find that this simple pipeline can provide high-quality OCR performance, competitive with a productlevel API.
- To leverage the capability of the multimodal pretrained model for image captioning, we employ the unified multimodal pretrained model architecture. Specifically, we implement our models on OFA (Wang et al., 2022a), an open-source state-ofthe-art unified multimodal pretrained model with the release of Chinese models. The model is mainly based on the Transformer encoder-decoder framework (Vaswani et al., 2017).
- We show that through pretraining on generaldomain data, the model can obtain the potential of text recognition by finetuning on small datasets.

### Finetuning with Image Captioning
- It is natural to recast text recognition as image captioning
- We finetune the model with maximum likelihood estimation
- To better alleviate the discrepancy between upstream and downstream data, we apply a transformation to the input images

### Multitask Finetuning
- implemented on Chinese text recognition benchmark consisting of 4 subtasks
- built a mixture of datasets for multitask finetuning
- found that directly applying multitask finetuning can help OFA-OCR achieve outstanding performance on all datasets
- additionally apply single-task finetuning after Metrics Scene Web Document Handwriting Average CRNN (Shi et al., 2017a) 53.4 54.5 97.5 46.4 67.0 ASTER (Shi et al., 2019) 54.5 52.3 93.1 38.9 64.7 MORAN (Luo et al., 2019) 51.8 49.9 95.8 39.7 64.3 SAR (Li et al., 2019) 62

### Datasets and Metrics
- We implement OFA-OCR on the Chinese text recognition benchmark
- This benchmark consists of multiple subtasks of text recognition, which are text recognition in different scenarios, including scene, web, document, and handwriting
- The details of the datasets are provided in Sec. A.1
- The evaluation metric includes accuracy, which refers to the ratio of exact match.

### Experimental Results
- The experimental results are demonstrated in Table 1
- We compare our method with baseline models of OCR, including the previous state-of-the-art MaskOCR (Lyu et al., 2022)
- It can be found that with no regard to the scale of models, the base-size OFA-OCR, which is finetuned from the pretrained Chinese OFA Base, can outperform both the basesize and large-size MaskOCR models
- Specifically, it shows the advantages of 9.0, 6.9, and 5.3 absolute improvements in the scenarios of scene, web, and handwriting
- On average, the base-size OFA-OCR outperforms the base-size MaksOCR by 5.2 and the large-size MaskOCR by 3.4
- Scaling up the model size can consistently bring steady improvement in the downstream performance
- On average, OFA Large reaches the best results of 86.3.

### Ablation Study of Training Strategies
- The multitask learning influences the final performance
- On average, the addition of the initialization of the pretrained OFA model significantly boosts the performance on the datasets
- Multitask finetuning alone can outperform single-task finetuning on all 4 tasks
- The combination of multitask finetuning and single-task finetuning is the best solution

### Ablation Study of Data Augmentation
- The preprocessing of images can boost the performance
- The preprocessing can be done using a variety of methods
- The preprocessing can be done before or after the training
- The preprocessing can be done with or without weight initialization

### Deployment
- To construct an OCR system applicable in realworld scenarios, a strong text recognition model is not sufficient, and we need to build a pipeline with both the text detection and text recognition module.
- While the former one is not the focus of this research, we directly use a light-weight model from EasyOCR3 for detection.
- After detecting all the bounding boxes which possibly contain texts, we crop them with boxes to create a batch of new images.
- The final step is to process the images with OFA-OCR for the generation of text recognition results.

## Conclusion
- OFA-OCR achieves high accuracy on multiple text recognition datasets
- OFA-OCR costs are larger than the non-Transformer baselines
- It is difficult to deploy such large models
- Thus in our future work, we will discover how to distill or compress OFA-OCR to a light-weight model with high efficiency
- Our method is essentially based on a generation model, and thus the OCR results should be taken as AI-generated contents
- However, the model maintains such ability and it might be triggered
- We also have to note that due to the application context, the input may contain harmful contents itself
- In such scenario, we believe the model should faithfully reflect its input, which is valuable to measures against such contents
- Although we believe that after finetuning on the public datasets the risk of such phenomena is extremely low, we still take it into account
