---
title: "NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions"
date: 2023-03-22T18:59:48.000Z
author: "Mohamad Shahbazi, Evangelos Ntavelis, Alessio Tonioni, Edo Collins, Danda Pani Paudel and 2 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-12865v1.webp" # image path/url
    alt: "NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.12865)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.12865).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/nerf-gan-distillation-for-efficient-3d-aware).

# Abstract
- Pose-conditioned convolutional generative models have difficulty creating high-quality 3D-consistent images from single-view datasets.
- NeRF-GANs use 3D neural representations and volumetric rendering, but are computationally complex.
- This study proposes a method to efficiently generate 3D-consistent images using a pre-trained NeRF-GAN and a pose-conditioned convolutional network.
- Experiments show that the proposed method is comparable to volumetric rendering in terms of quality and 3D consistency, but is more computationally efficient.

# Paper Content

## Introduction
- Generative Adversarial Networks (GANs) have been used for photo-realistic image generation and manipulation.
- There is an increasing interest in extending GANs to 3D-aware generation from single-view image datasets.
- Image GAN models have been historically based on convolutional architectures, but pose-conditioned convolutional GANs (pcGANs) struggle with 3D-consistent image generation.
- Neural Radiance Fields (NeRFs) have been used to transform the neural 3D representation and the task of novel-view synthesis.
- NeRFs have been integrated with GANs to achieve promising results in 3D-aware generation.
- Volumetric rendering is computationally demanding, so recent works have proposed different approaches to improve the computational efficiency of NeRF-GANs.
- This study investigates the capacity of convolutional generators to achieve 3D-consistent rendering with explicit pose control.
- A convolutional generator is proposed to distill a pretrained NeRF-GAN into a pose-conditioned fully convolutional generator.
- Experiments show that the generator trained by the distillation method preserves the 3D consistency, image quality, and semantics of the pretrained NeRF-GAN.

## Related works
- Prior works have attempted to create 3D awareness in 2D GANs using explicit 3D supervision
- Later works aimed at unsupervised methods by introducing 3D inductive biases in GANs
- NeRF-GANs have shown potential in compactly representing 3D scenes for novel view synthesis
- Later studies used convolutional superresolution networks for higher resolutions
- Epi-GRAF adopted an efficient multi-scale patch training protocol
- Recent advances in efficiency of NeRFS have been applied to NeRF-GANs
- Exploiting tri-planes and enforcing 3D consistency improves image quality
- Revisiting convolutional generators for efficient rendering during inference
- Exploiting underlying 3D knowledge and establishing correspondence between convolutional generator and NeRF-GAN's 3D representations

## Method
- Overview of NeRF-GANs
- Explanation of proposed formulation

### Preliminaries
- NeRF-GANs consists of a 3D-representation generator which maps a latent variable to a 3D representation of an object
- EG3D is used to represent 3D scenes using tri-planes
- A lightweight MLP decodes the feature vector to a density and color value
- EG3D is trained in an adversarial fashion with a viewpoint-conditioned dual discriminator

### Convolutional rendering of pretrained nerf-gans
- Aim of proposed method is to distill pre-trained 3D representation into 2D
- Exploit well-disentangled style space of 3D generator to distill into 2D
- Architecture of EG3D and convolutional generator visualized in 2
- Convolutional generator based on StyleGAN3
- Mapping network transforms style code of 3D and target viewpoint to style code of 2D
- Low-resolution feature predictor estimates EG3D features and low-resolution image
- Super-resolution network maps estimated features and images to high-resolution output

### Training
- Training regime uses teacher-student framework
- Volumetric rendering supervises convolutional renderer
- Training sample consists of randomly sampled z and c, style code, low-resolution image, feature maps, and high-resolution image
- Loss function composed of three parts: low-resolution reconstruction, high-resolution reconstruction, and adversarial
- 2-stage training curriculum used to counter 3D inconsistency

## Experiments
- Described experimental setup for evaluation of method
- Compared proposed method with baselines
- Evaluated visual quality, 3D consistency, and computational efficiency
- Provided ablation study and discussion on benefits and trade-offs

### Datasets
- Evaluated method on three datasets: FFHQ, AFHQ Cats, ShapeNet Cars
- FFHQ contains 70k high-quality images of real-world human faces
- AFHQ Cats contains 5k high-quality images of cat faces
- ShapeNet Cars contains synthetic images of cars

### Baselines
- EG3D and SURF-GAN are the main baselines for the study
- EG3D serves as the upper bound for 3D consistency
- PC-GAN is a standard 2D GAN conditioned on pose annotations
- SURF is a 2D StyleGAN based on the proposed method in [28]
- LiftGAN is a method predating EG3D and SURF baselines

### Implementation details
- Pretrained NeRF-GAN used official models from EG3D
- Batch size of 16 used for training
- Rendering resolution and final resolution for FFHQ, AFHQ, and Shapenet are (64, 512), (128, 512), and (64, 128) respectively
- Experiments conducted using NVidia RTX 3090 GPUs
- Weights of reconstruction loss terms set to 1, weight of adversarial loss set to 0.1

### Metrics
- Evaluated method quantitatively in terms of visual quality, 3D consistency, and correspondence with NeRF-GAN output
- Used Fr√©chet Inception Distance (FID) and Kernel Inception Distance (KID) to assess quality and diversity of generation
- Used Pose Accuracy (PA) to measure ability of model in generating images of query poses
- Used Identity Preservation (ID) and 3D Landmark Consistency to measure 3D consistency
- Used Peak Signal-to-Noise Ratio (PSNR) to measure correspondence between NeRF-GAN and convolutional generator

### Quantitative comparison
- Quantitatively compare proposed method with baselines in terms of inference efficiency, visual quality, and 3D consistency
- Example of generation speed and memory consumption of two methods using different batch sizes on a fixed GPU budget
- Fix objects across batches and only change viewpoints
- EG3D restricted to small batch sizes due to costly memory consumption
- Convolutional generator achieves enormous speed-up compared to volumetric rendering
- Table 1 shows FID and KID scores for proposed method and baseline methods on different datasets
- Table 2 shows pose accuracy, 3D landmark consistency, and face identity preservation
- Pose-wise comparison of methods in terms of face identity preservation in Fig. 3
- Visual comparison of proposed method with baselines in Fig. 4, 5, and 6
- Ablation on loss functions in Table 3
- Ablation on importance of low-resolution loss terms in Table 5
- Comparison of identity preservation across different camera poses in Fig. 8
- Examples of inversion, latent space interpolation, and style mixing in Fig. 8
- Comparison of computational efficiency for inference in Fig. 9
