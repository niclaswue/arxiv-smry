---
title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
date: 2022-05-27T17:53:09.000Z
author: "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2205-14135v2.webp" # image path/url
    alt: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.14135)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.14135).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient).

# Abstract
- Transformers are slow and memory-hungry
- Approximate attention methods have attempted to address this problem by trading
- We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory
- We propose FlashAttention, an IO-aware exact attention algorithm that
- We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes
- We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method

# Paper Content

## Introduction
- IO-aware attention algorithms reduce the number of memory accesses required for the computation.
- FlashAttention is an IO-aware attention algorithm that does not require reading and writing the large  ×  attention matrix to and from HBM.
- FlashAttention speeds up model training and improves model quality by modeling longer context.
- FlashAttention is faster than standard attention implementations by up to 7.6x.

## Background
- GPUs are more efficient than CPUs for deep learning
- The standard attention implementation is efficient
- There are many different deep learning models and each has its own performance characteristics
- It is important to understand the performance characteristics of a given model before using it in a application

### Hardware Performance
- GPUs have a massive number of threads to execute an operation (called a kernel).
- Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.
- Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound.
- This is commonly measured by the arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.
- Kernel fusion is the most common approach to accelerate memory-bound operations.
- Compilers can automatically fuse many elementwise operations [53,65,75].
- However, in the context of model training, the intermediate values still need to be written to HBM to save for the backward pass, reducing the effectiveness of naive kernel fusion.

### Standard Attention Implementation
- standard attention implementation materializes matrices S and P to HBM, which takes  memory
- often  (e.g., for GPT2,  = 1024 and  = 64)
- the large number of memory accesses translates to slow wall-clock time
- this problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P
- as a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax

### An Efficient Attention Algorithm With Tiling and Recomputation

## Experiments
- FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and speeds up GPT-2 up to 3× over HuggingFace [87] and 1.8× over Megatron [77] over standard Transformers.
- FlashAttention speeds up the long-range arena (LRA) benchmark 2.4×.
- Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality.
- FlashAttention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity.
- Modeling longer sequences yields 6.4 points of lift on two longdocument classification tasks.
- Finally, FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K).
- We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length.
- We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3× faster than standard attention for common seq. lengths (up to 2K).
- We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines.
- Additional experiment details are in Appendix E.

### Better Models with Longer Sequences
- The runtime and memory-efficiency of FlashAttention allows us to increase the context length of GPT-2 by 4× while still running faster than the optimized implementation from Megatron-LM.
- Table 4 shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity.
- Transformer models are able to solve Path-X and Path-256 with FlashAttention.
- FlashAttention achieves 61.4 accuracy on Path-X.

### Benchmarking Attention
- We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM
- We compare against reference implementations for exact attention, approximate attention, and sparse attention
- We report a subset of baselines in the main body; Appendix E contains more baselines and full details

## Limitations and Future Directions
- Our approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation.
- This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires significant engineering effort.
- Implementations may also not be transferrable across GPU architectures.
- These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDA-similar to efforts such as Halide in image processing [70].
- We believe that the IO-aware approach can extend beyond attention.
- Attention is the most memory-intensive computation in Transformers, but every layer in a deep network touches GPU HBM.
- We hope our work inspires IO-aware implementations of additional modules.
- We discuss these potential extensions in Appendix D.
- Multi-GPU IO-Aware Methods.
- Our IO-aware implementation of attention is optimal within constants for computing attention on a single GPU.
- However, the attention computation may be parallelizable across multiple GPUs [72].
- Using multiple GPUs adds an additional layer to IO analysis-accounting for data transfer between GPUs.
- We hope our work inspires future work in this direction.
