---
title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
date: 2022-05-27T17:53:09.000Z
author: "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2205-14135v2.webp" # image path/url
    alt: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.14135)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.14135).


# Abstract
- Transformers are slow and memory-hungry
- Approximate attention methods have attempted to address this problem by trading model quality to reduce the compute complexity, but often do not achieve wall-clock speedup
- We argue that a missing principle is making attention algorithms IO-aware--accounting for reads and writes between levels of GPU memory
- We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM
- We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes
- We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method

# Paper Content

## Introduction
- Transformer models have emerged as the most widely used architecture in applications such as natural language processing and image classification.
- Transformers have grown larger and deeper, but equipping them with longer context remains difficult, since the self-attention module at their heart has time and memory complexity quadratic in sequence length.
- An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences.
- Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation to low-rank approximation, and their combinations, but many of them do not display wall-clock speedup against standard attention and have not gained wide adoption.
- One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).
- In this paper, we argue that a missing principle is making attention algorithms IO-aware-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM, Figure 1).
- We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM. This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass.
- We apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.
- We implement FlashAttention in CUDA to achieve fine-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6x on GPT-2, Figure 1 right) and uses less memory-linear in sequence length-than standard attention, thanks to the massively reduced amount of HBM access.
- We analyze the IO complexity [1] of FlashAttention, proving that it requires (2^2-1) HBM accesses where  is the head dimension and  is the size of SRAM, as compared to Ω(+2) of standard attention. For typical values of  and , FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9× fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes.
- We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead.

## Background
- GPUs are faster than CPUs for deep learning
- The standard attention implementation is fast
- There are many deep learning libraries and frameworks available
- 

### Hardware Performance
- GPUs have a massive number of threads to execute an operation (called a kernel).
- Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.
- Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound.
- This is commonly measured by the arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.
- Kernel fusion is the most common approach to accelerate memory-bound operations.
- Compilers can automatically fuse many elementwise operations [53,65,75].

### Standard Attention Implementation
- standard attention implementation materializes the matrices S and P to HBM, which takes  ( 2 ) memory
- often   (e.g., for GPT2,  = 1024 and  = 64)
- the large number of memory accesses translates to slow wall-clock time
- this problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P
- as a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax [77]
- in Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length

### An Efficient Attention Algorithm With Tiling and Recomputation
- Given the inputs Q, K, V ∈ R  × in HBM, we aim to compute the attention output O ∈ R  × and write it to HBM.
- Our goal is to reduce the amount of HBM accesses (to sub-quadratic in ).
- We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses.
- We describe this in Algorithm 1.
- By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end.
- Tiling. We compute attention by blocks.
- Softmax couples columns of K, so we decompose the large softmax with scaling [51,60,66].
- For numerical stability, the softmax of vector  ∈ R  is computed as: .
- For vectors  (1) ,  (2) ∈ R  , we can decompose the softmax of the concatenated  =  (1)  (2) ∈ R 2 as: () = (  (1)  (2) ) = max(( (1) ), ( (2) )),  () =  (  (1) )−( )  ( (1) )  (  (2) )−( )  ( (2) ) , ℓ() = ℓ(  (1)  (2) ) =  (  (1) )−( ) ℓ( (1) ) +  (  (2) )−( ) ℓ( (2) ), softmax() = .
- One of our goals is to not store  ( 2 ) intermediate values for the backward pass.
- The backward pass typically requires the matrices S, P ∈ R  × to compute the gradients with respect to Q, K, V. However, by storing the output O and the softmax normalization statistics (, ℓ), we can recompute the attention matrix S and P easily in the backward pass from blocks of Q, K, V in SRAM.
- This can be seen as a form of selective gradient checkpointing [10,34].
- While gradient checkpointing has been suggested to reduce the maximum amount of memory required [66], all implementations (that we know off) have to trade speed for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses (Fig. 2).
- The full backward pass description is in Appendix B.
- Implementation details: Kernel fusion. Tiling enables us to implement our algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout in Appendix B).
- Require: Load K  , V  from HBM to on-chip SRAM. Load Q  , O  , ℓ  ,   from HBM to on-chip SRAM.
- On chip, compute 12: to HBM. end for 15: end for 16: Return O.

## Experiments
- FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and speeds up GPT-2 up to 3× over HuggingFace [87] and 1.8× over Megatron [77] over standard Transformers.
- FlashAttention speeds up the long-range arena (LRA) benchmark 2.4×.
- Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality.
- FlashAttention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity.
- Modeling longer sequences yields 6.4 points of lift on two longdocument classification tasks.
- Finally, FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K).
- We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3× faster than standard attention for common seq. lengths (up to 2K). We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines.
- Additional experiment details are in Appendix E.
- BERT. FlashAttention yields the fastest single-node BERT training speed that we know of.
- We train a BERT-large [22] model with FlashAttention on Wikipedia.
- Table 1 compares our training time to the implementation from Nvidia that set the training speed record for MLPerf 1.1 [58]. Our implementation is 15% faster.
- Table 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs.
- Training time (minutes) Nvidia MLPerf 1.1 [58] 20.0 ± 1.5 FlashAttention (ours) 17.4 ± 1.4 GPT-2.
- FlashAttention yields faster training times for GPT-2 [67] on the large OpenWebtext dataset [32] than the widely used HuggingFace [87] and Megatron-LM [77] implementations.
- Table 2 shows up to 3× endto-end speedup compared to Huggingface and 1.7× speedup compared to Megatron-LM.
- FlashAttention achieves the same perplexity as the other two implementations, as we do not change the model definition.
- Appendix E includes plots of the validation perplexity throughout training, confirming that FlashAttention is as numerically stable as the baselines and produces the same training / validation curves.

### Better Models with Longer Sequences
- The runtime and memory-efficiency of FlashAttention allows us to increase the context length of GPT-2 by 4× while still running faster than the optimized implementation from Megatron-LM.
- Table 4 shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity.
- Transformer models are able to solve Path-X and Path-256 (Table 6) with 61.4 accuracy using FlashAttention.

### Benchmarking Attention
- We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM
- We compare against reference implementations for exact attention, approximate attention, and sparse attention
- We report a subset of baselines in the main body; Appendix E contains more baselines and full details

## Limitations and Future Directions
- Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation.
- This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires significant engineering effort.
- Implementations may also not be transferrable across GPU architectures.
- These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDA-similar to efforts such as Halide in image processing [70].

## A Related Work
- IO-Aware Runtime Optimization: The broad concept of optimizing for reading and writing to fast/slow memory has a long history in computer science and has been known by many names, including analyzing I/O complexity.
- Efficient ML Models with Structured Matrices: Matrix multiply is the core computational bottleneck of most machine learning models, and to reduce the computational complexity, there have been numerous approaches to learn over a more efficient set of matrices. These matrices are called structured matrices, which have subquadratic number of parameters and runtime.
- Sparse Training: Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices.
- Transformer-based Models: Transformer-based models have become the most widely-used architecture in natural language processing and computer vision, but one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length.
- HiPPO and its extensions, most notably S4 projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models.
- LambdaNetworks, AFT, and FLASH are other attempts at replacing attention in the context of image classification and language modeling.

## B Algorithm Details
- We first derive the forward and backward passes of attention
- They can be computed in a memory-efficient manner (requiring extra memory linear instead of quadratic in the sequence length)
- Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed
- We describe the FlashAttention algorithm to implement both the forward and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller memory footprint

### B.1 Memory-efficient forward pass
- The main challenge in making attention memory-efficient is the softmax that couples the columns of K (and columns of V).
- Our approach is to compute the softmax normalization constant separately to decouple the columns.
- This technique [60] has been used in the literature [51,66] to show that attention computation does not need quadratic extra memory (though the number of HBM accesses is still quadratic, resulting in slow run-time).
- For simplicity, we omit here the max-shifting step during softmax.
- The full algorithm in Appendix B.3 contains all the steps.
- Recall that given input sequences Q, K, V ∈ R  × , we want to compute the attention output O ∈ R  × :
- We have that    =      where   and   are the -th and -th columns of Q and K respectively.
- Define the normalization constants of softmax:
- Let   be the -th column of V, then the -th columns of the output is We see that once   is computed, we can compute   without extra memory by repeatedly summing . Therefore the forward pass can be computed with  () extra memory.

### B.2 Memory-efficient backward pass
- The backward pass can be computed without quadratic extra memory by applying gradient checkpointing to the memory-efficient forward pass.
- The backward pass can also be computed with  () extra memory.

## E Full Experimental Results
- We use the LAMB optimizer with a learning rate of 3.75e-3
- We use the LAMB optimizer with a learning rate of 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium
- We use the AdamW optimizer with a learning rate of 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium
- We use the Openwebtext dataset
- We randomly select 0.5% of the dataset as the validation set
- We train the model on 8×A100-40GB GPUs
- We measure the wall-clock training time

### E.4 Comparison with Apex FMHA
- FlashAttention is faster than FMHA for short sequences
- FMHA is faster for short sequences than FlashAttention, but FlashAttention is faster for long sequences
- FlashAttention does not store the attention matrix in the forward pass and recomputes it in the backward pass, which saves memory

### E.5 Speedup On Different Hardware and Configurations
- Speedup varies between different types of GPU types and generations depending on HBM bandwidth and SRAM size.
- In this section, we profile FlashAttention speedup on different GPUs and configurations.
- A100 Figure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention heads, across different sequence lengths. We generally see 2-4× speedup, and we see more speedup when using dropout and masking due to kernel fusion.
- A100, Head Dimension 128 Speedup also changes when we increase the head dimension. Each block requires more memory, so we need to use smaller block sizes to fit into SRAM.
- Figure 6 shows speedup with head dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overall-but we can still see significant speedup (up to 3×) with a causal mask, where half the blocks are masked out.
- RTX 3090 Figure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention heads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5×), since the memory bandwidth on an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s).
- T4 Figure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block sizes smaller in FlashAttention. As a result, we observe less speedup on T4, which matches the IO complexity analysis in Section 3.2.
- RTX 3090 GPUs are commonly used for inference, so we also report speedup on the forward pass only.

### E.6 Full Benchmarking Results
- We compare against reference implementations for exact attention, approximate attention, and sparse attention.
- For approximate attention, we compare against reference implementations of Reformer [51], Local Attention [68], Linformer Attention [84], Smyrf [19], and LongShortFormer (LSFormer) [94].
- For sparse attention, we compare against reference implementations of Block-Sparse Attention form OpenAI [11], Longformer [3], and BigBird Attention [92].
- We measure runtime and memory usage of the attention computation with 8 heads of dimension 64, and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM.
- We vary sequence length in our experiments.
- We compute attention on random vectors for Q, K, and V (we do not measure the projection from the hidden layer).
- For dropout, we use dropout 0.1; for masking, we use a padding mask with uniformly-random mask lengths between the total sequence length and the total sequence length minus 20.
- To measure runtime, we take the average of 100 measurements of the attention call.
- We only measure memory footprint once, since it does not vary between runs.
- We report timing results on the forward pass, backward pass, and combined forward + backward pass.
- We measure each method with and without dropout, masking, or both-except for Block Sparse, Longformer, and BigBird. These methods did not successfully run the backward pass with masking due to a bug in external libraries, so we measured them without masking to be generous.
- We use FP16 for all measurements, except for Local Attention, whose implementation only supports FP32.
- For each baseline, we increase sequence length until it runs out of memory on the GPU, except for the following exceptions: The Megatron implementation does not support sequence lengths longer than 2048. Block-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not support sequence lengths longer than 8092.
- We measure memory usage on the combined forward + backward pass, without dropout or masking.
- Results Table 8 summarizes all the experimental configurations and contains pointers to the results tables.
- Figure1: Left: FlashAttention uses tiling to prevent materialization of the large  ×  attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large  ×  attention matrix to HBM, resulting in an 7.6× speedup on the attention computation.
- Load P and V by blocks from HBM, compute O = PV, write O to HBM.
- Return O.
