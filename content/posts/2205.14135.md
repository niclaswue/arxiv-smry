---
title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
date: 2022-05-27T17:53:09.000Z
author: "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2205-14135v2_nLCIi15T0.jpg" # image path/url
    alt: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.14135)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.14135).


# Abstract
- Transformers are slow and memory-hungry
- Approximate attention methods have attempted to address this problem by trading model quality to reduce the compute complexity, but often do not achieve wall-clock speedup
- We argue that a missing principle is making attention algorithms IO-aware--accounting for reads and writes between levels of GPU memory
- We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM
- We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes
- We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method

# Paper Content

## Introduction
- Transformer models have emerged as the most widely used architecture in applications such as natural language processing and image classification
- Transformers have grown larger and deeper, but equipping them with longer context remains difficult, since the self-attention module at their heart has time and memory complexity quadratic in sequence length
- Many approximate attention methods have aimed to reduce the compute and memory requirements of attention, but many of them do not display wall-clock speedup against standard attention and have not gained wide adoption
- One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO)
- In this paper, we argue that a missing principle is making attention algorithms IO-aware-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM, Figure 1)
- We implement FlashAttention in CUDA to achieve fine-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6x on GPT-2 [67], Figure 1 right) and uses less memory-linear in sequence length-than standard attention, thanks to the massively reduced amount of HBM access.
- We analyze the IO complexity [1] of FlashAttention, proving that it requires 𝑂 (𝑁 2 𝑑 2 𝑀 −1 ) HBM accesses where 𝑑 is the head dimension and 𝑀 is the size of SRAM, as compared to Ω(𝑁 𝑑 + 𝑁 2 ) of standard attention. For typical values of 𝑑 and 𝑀, FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9× fewer, as shown in Fig. 2).
- Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes.
- We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead.

## Background
- GPUs are faster than CPUs for deep learning
- The standard implementation of attention is slow
- There are faster ways to do deep learning

### Hardware Performance
- GPUs have a massive number of threads to execute an operation (called a kernel).
- Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.
- Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound.
- This is commonly measured by the arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.
- Kernel fusion is the most common approach to accelerate memory-bound operations.
- Compilers can automatically fuse many elementwise operations [53,65,75].

### Standard Attention Implementation
- standard attention implementation materializes matrices S and P to HBM, which takes 𝑂 (𝑁 2 ) memory
- often 𝑁 𝑑 (e.g., for GPT2, 𝑁 = 1024 and 𝑑 = 64)
- the large number of memory accesses translates to slow wall-clock time
- this problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P
- as a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax [77]
- in Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length 𝑁

### An Efficient Attention Algorithm With Tiling and Recomputation
- We aim to compute the attention output O in sub-quadratic HBM accesses.
- We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses.
- We describe this in Algorithm 1.
- By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end.
- Tiling. We compute attention by blocks.
- Softmax couples columns of K, so we decompose the large softmax with scaling [51,60,66].
- For numerical stability, the softmax of vector 𝑥 ∈ R 𝐵 is computed as: .
- For vectors 𝑥 (1) , 𝑥 (2) ∈ R 𝐵 , we can decompose the softmax of the concatenated 𝑥 = 𝑥 (1) 𝑥 (2) ∈ R 2𝐵 as: 𝑚(𝑥) = 𝑚( 𝑥 (1) 𝑥 (2) ) = max(𝑚(𝑥 (1) ), 𝑚(𝑥 (2) )), 𝑓 (𝑥) = 𝑒 𝑚( 𝑥 (1) )−𝑚( 𝑥) 𝑓 (𝑥 (1) ) 𝑒 𝑚( 𝑥 (2) )−𝑚( 𝑥) 𝑓 (𝑥 (2) ) , ℓ(𝑥) = ℓ( 𝑥 (1) 𝑥 (2) ) = 𝑒 𝑚( 𝑥 (1) )−𝑚( 𝑥) ℓ(𝑥 (1) ) + 𝑒 𝑚( 𝑥 (2) )−𝑚( 𝑥) ℓ(𝑥 (2) ), softmax(𝑥) = .
- Therefore if we keep track of some extra statistics (𝑚(𝑥), ℓ(𝑥)), we can compute softmax one block at a time.
- One of our goals is to not store 𝑂 (𝑁 2 ) intermediate values for the backward pass.
- The backward pass typically requires the matrices S, P ∈ R 𝑁 ×𝑁 to compute the gradients with respect to Q, K, V. However, by storing the output O and the softmax normalization statistics (𝑚, ℓ), we can recompute the attention matrix S and P easily in the backward pass from blocks of Q, K, V in SRAM.
- This can be seen as a form of selective gradient checkpointing [10,34].

## Experiments
- FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and speeds up GPT-2 up to 3× over HuggingFace [87] and 1.8× over Megatron [77] over standard Transformers.
- FlashAttention speeds up the long-range arena (LRA) benchmark 2.4×.
- Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality.
- FlashAttention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity.
- Modeling longer sequences yields 6.4 points of lift on two longdocument classification tasks.
- Finally, FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K).
- We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3× faster than standard attention for common seq. lengths (up to 2K). We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines.
- Additional experiment details are in Appendix E.
- BERT. FlashAttention yields the fastest single-node BERT training speed that we know of.
- We train a BERT-large [22] model with FlashAttention on Wikipedia.
- Table 1 compares our training time to the implementation from Nvidia that set the training speed record for MLPerf 1.1 [58]. Our implementation is 15% faster.
- Table 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs.
- Training time (minutes) Nvidia MLPerf 1.1 [58] 20.0 ± 1.5 FlashAttention (ours) 17.4 ± 1.4 GPT-2.
- FlashAttention yields faster training times for GPT-2 [67] on the large OpenWebtext dataset [32] than the widely used HuggingFace [87] and Megatron-LM [77] implementations.
- Table 2 shows up to 3× endto-end speedup compared to Huggingface and 1.7× speedup compared to Megatron-LM.
- FlashAttention achieves the same perplexity as the other two implementations, as we do not change the model definition.
- Appendix E includes plots of the validation perplexity throughout training, confirming that FlashAttention is as numerically stable as the baselines and produces the same training / validation curves.

### Better Models with Longer Sequences
- The runtime and memory-efficiency of FlashAttention allows us to increase the context length of GPT-2 by 4× while still running faster than the optimized implementation from Megatron-LM.
- Table 4 shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity.
- Transformer models are able to solve Path-X and Path-256 (Table 6) with 61.4 accuracy using FlashAttention.

### Benchmarking Attention
- We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM
- We compare against reference implementations for exact attention, approximate attention, and sparse attention
- We report a subset of baselines in the main body; Appendix E contains more baselines and full details

## Limitations and Future Directions
- Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation.
- This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires significant engineering effort.
- Implementations may also not be transferrable across GPU architectures.
- These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDA-similar to efforts such as Halide in image processing [70].

## A Related Work
- IO-Aware Runtime Optimization: This paper discusses how the community can adopt ideas from literature on analyzing I/O complexity to optimize the deep learning stack.
- Efficient ML Models with Structured Matrices: Structured matrices have been shown to be efficient in theory, but haven't seen wide adoption since it is hard to translate their efficiency to wall-clock speedup.
- Sparse Training: Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices.
- Transformer-based Models: Transformer-based models have become the most widely-used architecture in natural language processing and computer vision, but one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length.
- HiPPO: HiPPO is an approach to overcome the computational bottleneck of transformer-based models by projecting the history on a polynomial basis.
- LambdaNetworks: LambdaNetworks is an attempt at replacing attention in the context of image classification and language modeling.

## B Algorithm Details
- We first derive the forward and backward passes of attention
- They can be computed in a memory-efficient manner (requiring extra memory linear instead of quadratic in the sequence length)
- Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed
- We describe the FlashAttention algorithm to implement both the forward and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller memory footprint

### B.1 Memory-efficient forward pass
- The main challenge in making attention memory-efficient is the softmax that couples the columns of K (and columns of V).
- Our approach is to compute the softmax normalization constant separately to decouple the columns.
- This technique [60] has been used in the literature [51,66] to show that attention computation does not need quadratic extra memory (though the number of HBM accesses is still quadratic, resulting in slow run-time).
- For simplicity, we omit here the max-shifting step during softmax.
- The full algorithm in Appendix B.3 contains all the steps.
- Recall that given input sequences Q, K, V ∈ R 𝑁 ×𝑑 , we want to compute the attention output O ∈ R 𝑁 ×𝑑 :
- We have that 𝑆 𝑖 𝑗 = 𝑞 𝑇 𝑖 𝑘 𝑗 where 𝑞 𝑖 and 𝑘 𝑗 are the 𝑖-th and 𝑗-th columns of Q and K respectively.
- Define the normalization constants of softmax:
- Let 𝑣 𝑗 be the 𝑗-th column of V, then the 𝑖-th columns of the output is We see that once 𝐿 𝑖 is computed, we can compute 𝑜 𝑖 without extra memory by repeatedly summing 𝐿 𝑖 𝑣 𝑗 . Therefore the forward pass can be computed with 𝑂 (𝑛) extra memory:

### B.2 Memory-efficient backward pass
- We derive the backward pass of attention and show that it can also be computed with linear memory.
- Rabe and Staats [66] suggests that the backward pass can be done without quadratic extra memory by applying gradient checkpointing to the memory-efficient forward pass.
- We instead derive the backward pass explicitly and show how it can be computed in a memory-efficient manner.
- Suppose that there is a scalar loss function 𝜙, and let the output gradient be dO ∈ R 𝑛×𝑑 (where dO denotes 𝜕𝜙 𝜕O ).
- We want to compute the input gradients dQ, dK, dV ∈ R 𝑛×𝑑 (where dQ, dK, dV denote 𝜕𝜙 𝜕Q , 𝜕𝜙 𝜕K , 𝜕𝜙 𝜕V respectively).
- The gradient dV is easy to see. Applying reverse-mode autodiff by hand (aka the chain rule), we obtain (in matrix notation) dV = P 𝑇 dO.
- Thus:
- Since we already computed 𝐿 𝑖 , 𝑑𝑣 𝑗 can be computed without extra memory by repeated summing.
- The gradients dQ and dK are a little more complicated.
- We go through the gradients dP and dS first.
- From Eq. (2), we have that dP = dOV 𝑇 , and so:
- Similarly,
- Therefore the backward pass can also be computed with 𝑂 (𝑛) extra memory:
- 1. Compute 𝑑𝑣 𝑗 for all 𝑗 according to Eq. ( 3), which takes 𝑂 (𝑛) extra memory.
- 2. Compute 𝐷 𝑖 for all 𝑖 according to Eq. ( 4), which takes 𝑂 (𝑛) extra memory.
- 3. Compute 𝑑𝑞 𝑖 for all 𝑖 according to Eq. ( 5), which takes 𝑂 (𝑑) extra memory.
- 4. Compute 𝑑𝑘 𝑗 for all 𝑗 according to Eq. ( 6), which takes 𝑂 (𝑑) extra memory.
- We describe the full details of FlashAttention forward pass.
- Given input sequences Q, K, V ∈ R 𝑁 ×𝑑 , we want to compute the attention output O ∈ R 𝑁 ×𝑑 :
- where 𝜏 ∈ R is some softmax scaling (typically 1 ), mask is some masking function that sets some entries of the input to −∞ and keep other entries the same (e.g., key padding mask when sequences in the batch don't have the same lengths and are padded), and dropout(𝑥, 𝑝) applies dropout to 𝑥 elementwise (i.e., output 𝑥 1− 𝑝 with probability 1 − 𝑝 and output 0 with probability 𝑝 for each element 𝑥).
- The full algorithm is in Algorithm 2.

## E Full Experimental Results
- We use the LAMB optimizer with a learning rate of 3.75e-3
- We use the Apex AMP for FP16 precision
- We compare our results with the reported training speed from Nvidia
- We use the same train / validation data split provided by MLPerf 1.1 reference implementation
- We train the model on 8×A100-80GB GPUs
- Each training run takes between 16 and 19 minutes
- We average the results of 10 runs
- We use the standard implementations of GPT-2 from Huggingface transformers library and from Nvidia's Megatron-LM repo
- We follow the training recipe of the Megatron-LM repo
- We use an effective batch size of 512
- We use gradient accumulation to fit into available GPU memory
- We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and weight decay of 0.1
- All models are trained with the same hyperparameters for 400K steps
- We run all implementations with mixed-precision training (PyTorch AMP)
- We use the Openwebtext dataset, with the GPT-2 BPE tokenizer
- We randomly select 0.5% of the dataset as the validation set, with the rest being used as training set
- We train the model on 8×A100-40GB GPUs
- Each training run takes between 16 and 19 minutes
- We average the results of 10 runs
- We use the geometric mean of the wallclock-time speedup of each of the five tasks to calculate the overall wallclock-time speedup

### E.4 Comparison with Apex FMHA
- FlashAttention is faster than Apex FMHA for short sequences
- FlashAttention does not store the attention matrix in the forward pass and recomputes it in the backward pass

### E.5 Speedup On Different Hardware and Configurations
- Speedup varies between different types of GPU types and generations depending on HBM bandwidth and SRAM size.
- In this section, we profile FlashAttention speedup on different GPUs and configurations.
- A100 Figure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention heads, across different sequence lengths. We generally see 2-4× speedup, and we see more speedup when using dropout and masking due to kernel fusion.
- A100, Head Dimension 128 Speedup also changes when we increase the head dimension. Each block requires more memory, so we need to use smaller block sizes to fit into SRAM.
- Figure 6 shows speedup with head dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overall-but we can still see significant speedup (up to 3×) with a causal mask, where half the blocks are masked out.
- RTX 3090 Figure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention heads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5×), since the memory bandwidth on an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s).
- T4 Figure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block sizes smaller in FlashAttention. As a result, we observe less speedup on T4, which matches the IO complexity analysis in Section 3.2.
- RTX 3090 GPUs are commonly used for inference, so we also report speedup on the forward pass only.

### E.6 Full Benchmarking Results
- We compare against reference implementations for exact attention, approximate attention, and sparse attention
- For approximate attention, we compare against reference implementations of Reformer [51], Local Attention [68], Linformer Attention [84], Smyrf [19], and LongShortFormer (LSFormer) [94]
- For sparse attention, we compare against reference implementations of Block-Sparse Attention form OpenAI [11], Longformer [3], and BigBird Attention [92]
- We measure runtime and memory usage of the attention computation with 8 heads of dimension 64, and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM
- We vary sequence length in our experiments
- We compute attention on random vectors for Q, K, and V (we do not measure the projection from the hidden layer)
- For dropout, we use dropout 0.1; for masking, we use a padding mask with uniformly-random mask lengths between the total sequence length and the total sequence length minus 20
- To measure runtime, we take the average of 100 measurements of the attention call
- We only measure memory footprint once, since it does not vary between runs
- Table 8 summarizes all the experimental configurations and contains pointers to the results tables
