---
title: "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"
date: 2022-12-22T09:43:36.000Z
author: "Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11565v1.webp" # image path/url
    alt: "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11565)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11565).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/tune-a-video-one-shot-tuning-of-image).

# Abstract
- Large-scale text-video dataset is used for fine-tuning
- Humans have amazing ability to learn new visual concepts from just one single exemplar
- We propose to adapt the T2I diffusion model pretrained on massive image data for T2V generation
- Intuitively, we propose to generate images that align well with the verb terms

# Paper Content

## Introduction
- Large-scale multimodal dataset has enabled breakthrough in open-domain Text-to-Image (T2I) generation
- To replicate this success in Text-to-Video (T2V) generation, recent works extend the spatial-only T2I generation models to the spatiotemporal domain
- The key to video generation is to keep the continuous motion of consistent objects
- State-of-the-art T2I diffusion models are able to generate images that align well with the text, including the verb terms
- Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames
- Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous
- By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths

## Related Work

### Text-to-Image Generation
- DALL-E considers the text-toimage (T2I) generation task as a sequence-to-sequence translation problem
- To further improve the quality of gen-erated images, Parti employs a more advanced image tokenizer, i.e., ViT-VQGAN, together with an encoder-decoder architecture
- CogView2 achieves competitive results with hierarchical transformers and local parallel auto-regressive generation
- Make-A-Scene focuses on improving scene generation controllability by employing domain-specific knowledge on scene regions
- Since recently, Denoising Diffusion Probabilistic Models (DDPMs) have been widely used for T2I generation
- GLIDE proposes the classifier-free guidance in the diffusion model to largely improve the image quality during cascade generation
- DALLE-2 further improves the text-image alignments using the CLIP feature space
- Imagen employs cascaded diffusion models for high definition video generation
- To improve the training efficiency, subsequent works such as VQ-diffusion and Latent Diffusion Models (LDM) operate in the latent space of an autoencoder

### Text-to-Video Generation
- GODIVA is the first work to extend VQ-VAE to T2V generation by mapping text tokens to video tokens.
- N ÜWA proposes a unified auto-regressive framework to support both T2I and T2V generation tasks.
- To improve the video generation quality, CogVideo extends CogView-2 to T2V generation using temporal attention modules and pre-trained text to images models.
- Video Diffusion Models (VDM) proposes a factorized space-time U-Net to perform the diffusion process directly on pixels.
- More recently, Imagen Video improves VDM with cascaded diffusion models and v-prediction parameterization to generate high definition videos.
- Phenaki is the first work to generate videos from time variable prompts.
- To achieve this, they compresses videos to small representations of discrete tokens with causal attention in time, and thus can handle variable-length videos.
- To address the lack of video-text pair data, they joint train on a large scale of image-text pairs and a smaller number of video-text pairs, achieving better generalization results than available video datasets.
- Make-A-Video shares similar motivation and aims to transfer the significant progress in T2I generation to T2V generation.

### Single Video Generative Models
- Single-video GANs generate novel videos of similar appearance and dynamics to the input video
- Patch nearest-neighbour methods perform video generation of higher quality while reducing computation expense by orders of magnitude
- Lately, SinFusion adapts diffusion models to single-video tasks, and enables autoregressive video generation with improved motion generalization capabilities
- However, it is still incapable of producing videos of different semantic contexts

## Method
- Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) are used to model the diffusion of information
- Tune-A-Video is a method that adapts pre-trained T2I models to one-shot video generation
- The method is based on DDPMs and LDM

### Preliminary: Diffusion Models
- Denoising Diffusion Probabilistic Models (DDPMs) are latent generative models trained to recreate a fixed forward Markov chain x 1 , . . . , x T .
- The Markov transition q(x t |x t−1 ) is defined as a Gaussian distribution with a variance schedule β t ∈ (0, 1), that is,
- By the Bayes' rules and Markov property, one can explicitly express the conditional probabilities q(x t |x 0 ) and q(x t−1 |x t , x 0 ) as
- To generate the Markov chain x 1 , . . . , x T , DDPMs leverage the reverse process with a prior distribution p(x T ) = N (x T ; 0, I) and Gaussian transitions
- Learnable parameters θ are trained to guarantee that the generated reverse process is close to the forward process.
- To this end, DDPMs follow the variational inference principle by maximizing the variational lower bound of the negative log-likelihood, which has a closed-form given the KL divergence among Guassian distributions.
- Empirically, these models can be interpreted as a sequence of weightsharing denoising autoencoders θ (x t , t), which are trained to predict a denoised variant of their input x t .
- The objective can be simplified as E x, ∼N (0,1),t − θ (x t , t) 2 2 .
- Latent Diffusion Models (LDMs) [31] are newly introduced variants of DDPMs that operate in the latent space of an autoencoder.
- LDMs consist of two key components. First, an autoencoder [5,38] is trained with patch-wise losses on a large collection of images, where an encoder E learns to compress images x into latent representations z = E(x), and a decoder D learns to reconstruct the latent back to pixel space, such that D(E(x)) ≈ x.
- The second component is a DDPM that is trained to remove the noise added to a latent representation of an image.
- This diffusion model can be conditioned on encoded embeddings of class labels, texts, or segmentation masks.
- Let c be the conditioning vector, the LDM loss is then given by: E x, ∼N (0,1),t,c − θ (x t , t, c) 2 2 .

### Comparison with VDM Baselines
- The VDM baselines factorize space and time by appending an additional temporal attention block after each spatial attention block in T2I diffusion models.
- For a fair comparison, we adopt the same training pipeline in Fig. 4 to fine-tune the VDM baselines for One-Shot Video Generation.
- As shown in Fig. 8, the VDM baselines with factorized space-time attention fail to generate consistent content (compare the ap-A woman is running on the lawn. CogVideo Ours A cat is running on the single-plank bridge, comic style. CogVideo Ours Figure 9. Videos generated by CogVideo [19] and Tune-A-Video (Ours). Our Tune-A-Video is fine-tuned on the training video of "a man is running on the beach" in Fig. 7. pearance of the subjects across frames), whereas our Tune-A-Video with spatio-temporal cross-frame attention maintains better temporal consistency.
- We provide quantitative and qualitative comparisons with CogVideo [19] (the only public 3 T2V generation model). CogVideo is based on a pre-trained T2I model CogView2 [4] and consists of 9.4 billion parameters (around 6× larger than our Tune-A-Video). It is extensively trained on a large-scale dataset of 5.4 million captioned videos.
- Quantitative results. 1) CLIP score: We follow [14] to evaluate Tune-A-Video on CLIP score [13,27] for videotext alignment. We calculate the frame-wise CLIP scores and take the average score over all frames. The final CLIP score is computed on 1024 video samples. As shown in Tab. 1, our Tune-A-Video produces higher CLIP score, demonstrating better video-text alignment compared Our method (4th row) produces videos with strongest temporal consistency and language alignment. See Fig. 1 for training video.
- 2) Human preference study: Following [36], we perform human evaluation on video quality and text-video faithfulness over a test set of 32 videos. For video quality, we present two videos in random order and ask the raters which one is of higher quality. For faithfulness, we additionally show the text and ask raters which video aligns better with the text. We observe that raters prefer videos generated by Tune-A-Video in both video quality and textvideo faithfulness compared to CogVideo.
- Qualitative results. As shown in Fig. 9, the resultant video of "a woman is running on the lawn" generated by our Tune-A-Video trained on one text-video pair of "a man running on the beach", is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., "a cat is running on the single-plank bridge, comic style", our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.

### Ablation Study
- The models without SC-Attn use the spatial attention in original T2I models.
- We observe that models without cross-frame attention (the 1st row of Fig. 10) produce inconsistent results across frames (compare the generated pandas across frames).
- In contrast, the model using SC-Attn (the 3rd row of Fig. 10) yields consecutive images that are coherent in content (i.e., the similar panda and background can be observed).
- Even with tuning, the model without attention (the 2nd row of Fig. 10) fails to generate results with reasonable motion, while our method (the 4th row of Fig. 10) using SC-Attn for finetuning achieves the best results both spatially and temporally.
- This demonstrates that the SC-Attn can effectively capture the spato-temporal information when generating videos.

## Conclusion
- To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single textvideo pair is provided for training an open-domain T2V generator.
- To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models. Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatiotemporal domain with a tailored Sparse-Causal Attention.
- To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.
