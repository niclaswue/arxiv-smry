---
title: "ReCode: Robustness Evaluation of Code Generation Models"
date: 2022-12-20T14:11:31.000Z
author: "Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, Bing Xiang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10264v1.webp" # image path/url
    alt: "ReCode: Robustness Evaluation of Code Generation Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10264)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10264).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/recode-robustness-evaluation-of-code).

# Abstract
- Code generation models have achieved impressive performance.
- However, they tend to be brittle as slight edits to a prompt could lead to very different
- Most existing works on robustness in text or code tasks have focused on classification, while
- In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code
- We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They
- With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original
- In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking
- We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well
- Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most

# Paper Content

## Introduction
- Multiple models have been proposed to generate code using a natural-language description
- These models can offer real-life help to software engineers and enhance their productivity
- However, one important aspect, robustness of the code generation models, is commonly overlooked
- Anecdotally, people know that these models are sensitive to perturbations over prompts
- In Fig. 1 and Fig. 2, we show two failure cases on InCoder-6B and CodeGen-16B-mono where they perform correctly on regular prompts but fail on our perturbed ones after docstring paraphrasing and function camel case renaming in our ReCode benchmark
- The perturbed prompts are natural and retain the original meaning, indicating weakness of these models if deployed in real-life applications
- There exists no comprehensive and quantitative robustness benchmark for code generation models
- Li et al. (2022) includes a brief study on robustness but it has limited perturbation types and is in a setting with massive numbers of samples, unrealistic in practice
- Other existing works on robustness in text or code tasks have focused on classification and are not directly applicable to code generation
- In this paper, we present ReCode, a Robustness Evaluation framework for Code, aiming to provide comprehensive assessment for robustness of code generation models
- ReCode includes only transformations that (1) appear naturally in practice and (2) preserve the semantic meaning of the original inputs
- We carefully collect and customize a comprehensive list of natural transformations on docstrings, function and variable names, code syntax, and code format
- We verify the quality of the perturbed data using both human evaluation and objective similarity scores
- We take advantage of the fact that executing the generated code can serve as objective evaluation and define three robustness evaluation metrics that aggregate a model's correctness across randomized transformations and transformation types
- These metrics quantify a model's accuracy on perturbed prompts, its relative accuracy drop from original prompts, as well as its general instability
- We summarize our contributions below:
- We present the first robustness evaluation benchmark ReCode for code generation tasks. Our evaluation framework is general and can be easily extended to any code generation datasets and models.

## Related Work
- Recent research have identified the severe robustness problem in Pretrained Language Models (PLMs) using adversarial examples.
- For example, PLMs can be easily fooled by synonym replacement (Jin et al., 2020;Zang et al., 2020).
- To better illustrate the severity of adversarial robustness problems for NLP models, and encourage people to explore more to build robust and trustworthy models, existing works (Nie et al., 2020;Gardner et al., 2020;Kiela et al., 2021;Wang et al., 2021a)
- Code generation.
- Code generation, also known as program synthesis, is a task of generating code based on natural language statements or code from context.
- Researchers have adapted transformerbased large language models to the code generation field.
- Various architectures have been explored: For example, CodeBERT (Feng et al., 2020), PLBART (Ahmad et al., 2021), CodeGPT (Lu et al., 2021) explores BERT, BART and GPT architectures for language models pretrained on code corpus.
- There are also works that propose to incorporate code structures for models to better understand the semantic information, including Graph-CodeBERT (Guo et al., 2021) andCodeT5 (Wang et al., 2021b).
- Most recently, models with much larger size (i.e., billion-scale parameter numbers) are shown to significantly improve the performance on code generation benchmarks.
- Codex-12B (Chen et al., 2021) and CodeGen-16B (Nijkamp et al., 2022) are two representative very large pretrained code generation models and have established new state of the arts.
- However, few works have systematically explored robustness in code generation.

## Methodology
- Introduce transformations to perturb prompts to both text (docstring) and code
- Proposed new robust evaluation metrics

### Problem Formulation
- We consider the end-to-end model-based code generation task
- The input prompt can include natural language statements that describe the functionality, signature of the function to generate, helper functions, and possibly a half-written function
- The goal is left-to-right generation that creates or completes the function
- This setting is agnostic to model architectures and is applicable to encoderdecoder or decoder-only models
- We perturb the input prompt with transformations
- We focus on natural transformations that preserve the semantic meaning of the original prompt and that are likely to appear in practice, e.g., frequent typos in docstrings, tab to four spaces, function name style changes, and many more
- We do not consider adversarial attacks that require model feedbacks in this paper because it is non-trivial to control the naturalness of adversarial attacks and they often require higher computational cost
- Instead, we randomly generate perturbed prompts based on the restrictions for each type of perturbations and propose new metrics to evaluate model robustness based on these prompts
- We leave adversarial attacks for future work

### Natural Transformations on Docstrings
- Docstrings can vary greatly when written by different users, so robustness against changes in docstrings is critical for usability in applications.
- For docstrings, we use the NL-Augmenter (Dhole et al., 2021) library which is designed for data augmentation and robustness evaluation on text.
- We carefully select ten transformations, including character-level, wordlevel and sentence-level ones, that are likely to preserve semantic similarity.
- The selected perturbations include CharCaseChange, where random characters are replaced with their upper cases, SynonymSubstitution, where random words are replaced with their synonyms, and more.
- To perform perturbations, we extract docstring sentences from the input prompt and then put the perturbed version back to the prompt.
- See Appendix A for details.
- We observe that directly applying NL-Augmenter to docstrings without constraints can potentially lead to low quality due to keywords in the programming languages. For example, "Create a list a[][]" could be perturbed by "Create a list [a][]" by character case swap, which is not natural.
- Therefore, to guarantee naturalness of perturbations, we use tree-sitter to parse the whole code snippet (the prompt & the canonical solution) to extract any existing function names, variable names ("a"), and type names ("list").
- We then exclude them from being perturbed by the transformations.

### Natural Transformations on Code Syntax
- Code generation models are often used on function completion task
- In such scenarios, the partial code in prompt is work in progress and can be subject to frequent editing
- ideally, a model should be robust with respect to perturbations in the partial code
- for this evaluation, we derive new customized datasets from HumanEval and MBPP
- by adding half 3 of the canonical solutions to the prompts
- then we perturb such partial code inside prompts
- transformations on partial code must be syntactically correct and must not alter semantic meaning
- the next section will address code format, and let us first focus on code refactoring

### Natural Transformations on Code Format
- We consider three methods of new line insertions: (1) empty lines at randomly selected positions, (2) an empty line inserted between docstring and partial code, and (3) an empty line inserted after partial code.
- We randomly replace any space indent with tab or replace tab with 4 spaces for indent-sensitive languages like Python.
- We select the longest line of code and split it into two lines in the middle.
- We convert docstrings to comments (e.g., """ docstring """ to # docstring for Python).

### Evaluation Metrics
- Many proposed transformations are randomized operations.
- For each transformation and each prompt, we create s randomly perturbed prompts.
- The model under evaluation generates outputs for each of them.
- We measure the worst-case performance across each group of s perturbed prompts: the model is considered robust on a prompt if and only if it generates a correct solution for all s perturbed prompts, where correctness is measured by executing associated unit tests.
- Based on such worst-case measurements, we propose three new metrics for robustness evaluation.
- Robust Pass s @k (RP s @k): Pass@k is a widely used metric for measuring the performance of code generation tasks (Chen et al., 2021).
- We extend its definition to Robust Pass s @k (RP s @k) with s random perturbations.
- For an original prompt x and for each transformation, let the perturbed prompts be x 1 , • • • , x s .
- We sample n generations by the model for each prompt, and in total there are n • s generations f i (x j ), where 1 ≤ i ≤ n and 1 ≤ j ≤ s.
- Instead of regular pass@k, we first consider the worst-case correctness across .
- Following definition of pass@k, we define the RP s @k metric as Eq. ( 1).
- Robust Drop s @k (RD s @k): RP s @k directly measure worst-case robustness in absolute values.
- It provides a worst-case estimation for models under certain perturbation.
- But in some applications, users may care more about relative performance change to compare worst-case performance and average-case performance.
- We propose Robust Drop s @k defined in Eq. ( 2) as another important robustness metric to quantify relative changes.
- Robust Relative s @k (RR s @k): Lastly, there are cases where models generate incorrect code on original prompts yet predict correctly on perturbed ones.
- This can (arguably) be considered as nonrobust behavior that we should include when reporting model robustness.
- We advocate to report all of them to provide a comprehensive estimation of model robustness.

## Evaluation
- Uses execution-based code generation benchmarks Hu-manEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to demonstrate ReCode's robustness
- Conducts a comprehensive study of robustness evaluation on popular public models including CodeGen (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and GPT-J (Wang and Komatsuzaki, 2021)
- Uses general and applicable perturbations and metrics

### Code Generation Robustness Evaluation
- Tab. 3 and Tab. 4 show the general perturbation performances on all the models in terms of the four general perturbation categories including transformations on docstrings, function names, code syntax, and code format.
- The nominal baselines are the pass@k on nonperturbed datasets for docstrings and function name perturbations.
- For perturbations on code syntax and format, the nominal baseline is the pass@k on nonperturbed customized datasets with partial code (see Sect. 3.4).
- We use greedy sampling for all the models to eliminate randomness effect and enable fair comparisons.
- We consider s = 5, i.e., we generate five different datasets with different random seeds for each type of perturbation and evaluate worst-case robustness performance according to the robustness evaluation metric defined in Sect.
- In specific, we say the model is robust on an input under docstring perturbations only when the model predicts correctly on all the s perturbed datasets for each transformation listed in Tab. 1.
- We present detailed numbers for each perturbation in Appendix E, Tab. (1)
- Diverse pretraining corpus helps with both generalization and worst-case robustness.
- Comparing all code generation models with the same size 6B, CodeGen models have much better nominal performance, and have better robustness on RP 5 @1, a very strict worst-case robustness metric.
- That is possibly because CodeGen models are pretrained over a more diverse corpus than In-Coder and GPT-J and thus have more capacity to deal with unseen instances and perturbations.
- Although CodeGen models have worse performance on RD 5 @1 and RR 5 @1, two robustness metrics relative to nominal performance, indicating that CodeGen models cannot generalize in a robust way (e.g., may learn to use spurious features in data).
- In general, we observe higher RP 5 @1 for larger models within the same model family (e.g., improved from 0.174 to 0.217 for CodeGen-mono 2B  to 16B on average across all perturbations), indicating larger model helps improve worst-case robustness.
- Similarly, we observe that larger models usually have larger RR 5 @1 (e.g., increased from 27.90% to 35.91% for CodeGen-mono 2B to 16B on average), indicating that larger models may risk overfitting as the relative performance drops under perturbations are significant.

### Ablation Study
- As described in Section 3.6, our robustness metrics consider worst-case performance across s perturbed datasets for each perturbations.
- Larger s leads to stronger perturbations evaluated, larger performance drops, and more extensive coverage to practical failures.
- The performance drops will start converging when large enough s evaluated.
- We can clearly see such trends in Figure 4 where we evaluate CodeGen-16Bmono RD s @1 and RR s @1 under greedy sampling with s = 1, ..., 10.
- Perturbation categories like docstring and syntax that involve larger searching space and more randomness tend to benefit more with larger s (see Appendix A for details).
- As a trade-off, evaluation cost linearly increase with s.
- Thus, we recommend s = 5 as a good balance between cost and evaluation strength.

## Perturbation Sample Quality
- Human evaluation: To verify the naturalness of the perturbations in ReCode, we randomly sample and shuffle 100 and 50 perturbed and nonperturbed MBPP and HumanEval data points and create a shuffle mix of 300 samples.
- Sentence cosine similarity: We measure the sentence cosine similarity between perturbed and non-perturbed docstrings and function names. We obtain the embeddings by sentence transformers using model all-mpnet-base-v25 (Song et al., 2020).
- Code similarity: In Tab. 7, we also measure the code similarity using CodeBLEU scores (Lu et al., 2021) for perturbed and nonperturbed data involving code syntax/format transformations.
- On average, we have score 0.96 and 0.97 for CodeBLEU syntax and dataflow, showing good quality of perturbed datasets.

## Conclusion
- Collects and customizes over 30 natural transformations under categories of docstrings, function names, code syntax, and code format perturbations
- These transformations are carefully selected and designed to be natural in practice and preserve the semantic meaning after perturbations
- Proposed general worst-case robustness metrics to give a unified overview of the model robustness performance
- Empirically demonstrate ReCode benchmark on popular models including CodeGen, InCoder, and GPT-J using HumanEval and MBPP datasets and function completion tasks derived from them
- With human evaluation, over 90% of perturbed data are confirmed to preserve the original semantic meaning
- Sentence similarity and CodeBLEU scores additionally support the quality of perturbations in ReCode
