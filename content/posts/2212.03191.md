---
title: "InternVideo: General Video Foundation Models via Generative and Discriminative Learning"
date: 2022-12-06T18:09:49.000Z
author: "Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-03191v2.webp" # image path/url
    alt: "InternVideo: General Video Foundation Models via Generative and Discriminative Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.03191)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.03191).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/internvideo-general-video-foundation-models).

# Abstract
- The foundation models have recently shown excellent performance on a variety of downstream tasks in computer vision
- However, most existing vision foundation models simply focus on image-level pretraining and adpation, which are limited for dynamic and complex video-level understanding tasks. To fill the gap, we present general video foundation models, InternVideo, by taking advantage of both generative and discriminative self-supervised video learning.
- Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications.
- Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Especially, our methods can obtain 91.1% and 77.2% top-1 accuracy on the challenging Kinetics-400 and Something-Something V2 benchmarks, respectively. All of these results effectively show the generality of our InternVideo for video understanding.

# Paper Content

## Introduction
- Foundations models have been gaining increasing attention in the research community
- Through simple adaption or zero/few-shot learning, foundation models greatly reduce downstream design and training costs using generic representations learned from web-scale data with a strong backbone of high capacity
- It is expected that developing foundation models can cultivate cognition from perception, obtaining general vision capability
- Though a line of vision foundation models is proposed, video understanding and the corresponding tasks are less explored compared with image ones, mainly used for validating that the visual features from these models are also beneficial to spatiotemporal representations
- We conjecture this relatively scarce focus from the academic community is caused by 1) a high computing burden from video processing, and 2) quite a few current video benchmarks can be handled by exploiting appearance features from image backbones with accordingly temporal modeling
- For efficiency, the additional time dimension in video processing makes at least one order of magnitude higher complexity than image processing when their spatial resolutions are close and the temporal sampling ratio is usually 16
- For some current video datasets, image features alone or with lateral temporal modules are sufficient to give decent results, especially with the rise of the multimodal model CLIP
- Its various temporal variants yield competitive or state-of-the-art performance in several core tasks
- Regarding this, a simultaneous spatiotemporal learner does not seem like a sweet spot between research & development cost and payback. Moreover, the transferability of current vision foundation models is somewhat narrow considering the wide spectrum of video applications
- These models either concentrate on action understanding tasks (e.g. action recognition, spatiotemporal action localization, etc) or video-language alignment ones (e.g. video retrieval, video question answering, etc)
- We suppose this results from their learning schemes, as well as the lack of a comprehensive benchmark for measuring video understanding capabilities
- Thus, these works focalize a few specific tasks to demonstrate their spatiotemporal perceptions
- The community desires a general foundation model that enables a broader application domain
- In this paper, we advance video foundation model research with a cost-effective and versatile model InternVideo
- To establish a feasible and effective spatiotemporal representation, we study both popular video masked modeling [23,25] and multimodal contrastive learning [13,26]
- UVR not only empirically shows video representation outperforms image one with temporal capturing significantly on core video tasks, but also is training-efficient. Its MAE exploits high redundancies in videos and trains with only a few visible tokens
- Meanwhile, multimodal learning in InternVideo extends existing image-pretrained backbones for video contrastive training. After supervised training these two video encoders, we craft cross-model attention to conduct feature alignments between these two almost frozen encoders
- More than a unified video representation learning paradigm, we also make practices and guidelines for training large-scale video foundation models in a tractable and efficient manner

## Related Work
- CLIP and ALIGN prepare web-scale noisy image-text pairs to train dual-encoder models with contrastive learning
- INTERN expands the self-supervised pretraining into multiple learning stages, which use a large quantity of image-text pairs as well as manually annotated images
- INTERN achieves a better linear probe performance compared with CLIP, and improves data efficiency in the downstream image tasks
- Florence extends them with unified contrastive learning and elaborate adaptation models, which support a wide range of vision tasks in different transfer settings
- SimVLM and OFA train encoder-decoder models with generative targets and show competitive performances on a series of multimodal tasks
- Besides, CoCa unifies contrastive learning as CLIP and generative learning as SimVLM
- Recently, BeiT-3 introduces Multiway Transformers with unified BeiT pretraining, achieving state-of-the-art transfer results on several vision and image-language tasks
- Video Foundation Models. Previous image foundation models only show promising performance for video recognition (especially on Kinetics)
- In contrast, MERLOT Reserve collects 20M video-text-audio pairs to train the joint video representations with contrastive span matching
- Compared with image foundation models, current video foundation models support limited video and video-language tasks, especially for those fine-grained temporal discrimination tasks such as temporal localization
- Self-supervised Pretraining. Self-supervised learning has developed rapidly recently. It focuses on designing different pretext tasks for pretraining [33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51]
- The seminal methods [52,53] use pretrained visual and language encoders to extract the offline video and text features, while the recent methods [9,24,26,46,54,55] have demonstrated the feasibility of end-to-end training. Besides, the popular methods often include two or three pretraining tasks, e.g. masked language modeling [31], video-text matching [24], video-text contrastive learning [47] and video-text masked modeling [30]

## InternVideo

### Self-Supervised Video Pretraining
- InternVideo conducts both masked and contrastive training without supervision
- According to [13,23], video masked modeling produces features that excel at action discrimination, e.g., action recognition and temporal action localization, and video-language contrastive learning is able to understand videos with semantics from text without annotations
- We employ two transformers with different structures for better leveraging these two optimization targets
- The final representation is constructed by adaptively aggregating these two types of representations
- We follow most rituals from our proposed VideoMAE [23] work to train a vanilla Vision Transformer (ViT) as a video encoder for spatiotemporal modeling
- VideoMAE conducts a video reconstruction task with highly masked video inputs, using an asymmetric encoder-decoder architecture
- To characterize spatiotemporal interaction globally, we employ joint space-time attention [58,59] in ViT
- It is computationally tractable as only a few tokens are preserved for calculation
- We conduct both video/image-text contrastive learning and video captioning tasks for pretraining
- For training efficiency, we build our multimodal structure based on the pretrained CLIP [13]

### Supervised Video Post-Pretraining

### Cross-Model Interaction
- Cross-representation learning with added cross-model attention modules
- To learn a unified video representation based on both video masked modeling and video-language contrastive learning
- Conduct cross-representation learning with added cross-model attention modules
- Add elaborate learnable modules (cross-model attention) for aligning representations learned in different approaches
- Cross-model attention (CMA) is formed by standard Multi-Head Cross Attention (MHCA) along with Feed-Forward Network (FFN)
- It employs intermediate tokens from a multimodal video encoder as keys and values while using these from a masked video encoder as queries
- The new tokens computed from CMA are treated as a gradually aligned representation with that from the multimodal video encoder
- One design exception is that for the last CMA module, its keys and values are from the tokens of the masked video encoder and the query is from the class token of the multimodal video encoder
- Thus, the class token is updated based on tokens from the masked encoder
- It transfers single-modal knowledge to CMA in the multimodal video encoder

## Experiments
- We detail our experimental configurations first
- We present the downstream performance of Intern-Video on the proposed video understanding benchmark with three types of tasks
- We detail our experimental configurations first

### Implementations
- With the initialization from CLIP, we post-pretrain our multi-modal model with WebVid2M, WebVid10M, and HowTo100M.
- Since the training corpus of video-text datasets is not as rich as CLIP-400M [13], we co-train the video model with image-text datasets, a subset of LAION-400M [71] containing 100M image-text pairs.
- We alternate images and videos for each iteration.
- The batch size of video-text is 14,336, and the batch size of image-text is 86,016.
- We train for 400k steps on 128 NVIDIA A100 GPUs in 2 weeks, with a learning rate of 8 × 10 −5 , weight decay of 0.2, cosine annealing schedule, and 4k warm-up steps.
- We train the VideoMAE-Huge for 1200 epochs on the UnlabeledHybrid dataset with 64 80G-A100 GPUs.
- The model adapts the cosine annealing learning rate schedule and warmup 10% total epochs.
- The learning rate is set to 2.5e − 4.
- Besides, we use a cosine annealing schedule for 5 epochs with 1 warmup epoch.
- All used data augmentations are the same as in UniFormerV2 [56].

### Downstream Tasks
- We conduct extensive experiments on a spectrum of downstream tasks to evaluate InternVideo.
- The employed tasks are of three categories that consider action understanding, video-language alignment, and open understanding.
- InternVideo contains masked video encoder specializing in spatiotemporal variation characterization and fused multimodality video encoder, which can improve action understanding (Section 4.3.1) and video-language alignment (Section 4.3.2) tasks significantly.
- Its generalization brought by large-scale training data also enables its impressive zero-shot and open-set capabilities on the related tasks (Section 4.3.3).
- Even transferred to ego-centric tasks, InternVideo still gives an overwhelmingly favorable performance with simple heads.

## Concluding Remarks
- The paper proposes a versatile and training-efficient video foundation model InternVideo
- It achieves state-of-the-art performance on nearly 40 datasets covering 10 different tasks
- Compared with previous related work, it greatly lifts the generality of video foundation models
- With simple ViT and its corresponding variants, the model achieves generalized video representations with 64.5K GPU hours (A100-80G)
- Even for zero-shot and open-set settings, the model spectrum still gives consistent and non-trivial performance increases

### Limitations
- Video foundation models are effective and feasible
- Video perception tasks are handled well
- The devise can hardly process long-term video tasks

### Future Work
- To further extend the generality of the video foundation models, we suppose embracing model coordination and cognition is necessary for its studies.
- There are multiple technical routes to address it, e.g. model distillation, unifying different pretraining objectives, and feature alignment, to name a few.
- By exploiting previously learned knowledge, we can accelerate video foundation model development sustainably.
- In the long run, foundation models are expected to master cognitive capabilities more than perceivable ones.
- Considering its feasibility, we suppose one of its research trends is to achieve large-scale spatiotemporal analysis (long-term & big scene) from the foundational dynamic perception in the open world, leading to essential cognitive understanding.
- Additionally, it has raised a tide that combining foundation models with decision-making to form intelligent agents to explore new tasks. In this interaction, data collection and model training are also automated.
- Our initial experiments (Section 4.3.2) on vision-language navigation demonstrate the promising future of integrating video foundation models into Embodied AI.

## Broader Impact
- The InternVideo spectrum can recognize actions on around 40 different datasets
- The InternVideo spectrum is able to deliver the state-of-the-art performance on these datasets
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The InternVideo spectrum is able to deliver the state-of-the-art performance on around 40 datasets, capable of action discrimination, video-language alignment, and open understanding
- The Intern...
