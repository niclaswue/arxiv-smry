---
title: "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"
date: 2022-12-19T18:21:00.000Z
author: "Or Honovich, Thomas Scialom, Omer Levy, Timo Schick"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09689v1.webp" # image path/url
    alt: "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09689)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09689).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/unnatural-instructions-tuning-language-models).

# Abstract
- Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions.
- These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions.
- In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor.
- We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth.
- This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs.
- Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks.

# Paper Content

## Introduction
- Instruction tuning enables pretrained language models to generalize to unseen tasks in a zero-shot setting
- One way to collect examples of instructions and their execution is to reformulate existing NLP datasets in an explicit instruction-input-output format via prompt engineering
- However, the resulting data is limited to existing academic benchmarks, even though the instruction paradigm can describe any text-based task
- Alternatively, Ouyang et al. (2022) collect user-generated prompts and fourth
- Figure 1) to automatically produce 64,000 diverse triplets of instructions, inputs, and outputs
- We further diversify the dataset's format by generating additional natural language paraphrases of each instruction, while preserving the contents of any input arguments and outputs, expanding the dataset to approximately 240,000 examples
- Although the dataset contains noise, our analysis reveals that more than 50% of generated examples are indeed correct, and that even incorrect examples typically contain valuable information for instruction tuning
- At the same time, we find that Unnatural Instructions contains highly creative tasks -some of which are very different from "classic" NLP tasks -and has a more diverse set of instructions than Super-Natural Instructions
- Experiments show that fine-tuning an 11Bparameter T5 model (Raffel et al., 2020) on Unnatural Instructions can outperform both T0++ (Sanh et al., 2021) and Tk-Instruct (Wang et al., 2022) across several benchmarks, including Super-Natural Instructions (Wang et al., 2022), BIGbench Hard (Suzgun et al., 2022), and LMentry (Efrat et al., 2022)
- When controlling for all variables besides the data, we find that a model trained on Unnatural Instructions performs competitively with a baseline model trained on Super-Natural Instructions
- In particular, we observe an 18-point gain on BIG-bench Hard (original task formulation) and a 16-point gain on LMentry, suggesting that Unnatural Instructions is particularly useful for generalizing to instructions that deviate from the distribution of classic NLP tasks
- These improvements become even more pronounced when the cost of generating examples is amortized; in this case, training on Unnatural Instructions substantially outperforms our baseline on all benchmarks
- We observe a log-linear relationship between the number of generated examples and downstream task performance, suggesting that performance of models trained on Unnatural Instructions can further be improved simply by increasing its size

## Data Collection
- Unnatural Instructions is a dataset of 240,670 natural language instructions for a wide variety of natural language tasks
- Each example contains a natural language instruction as input and its expected execution as output
- Table 2 displays examples from the dataset
- Unnatural Instructions is collected in a completely automatic process, requiring a seed of only 15 manually-constructed examples

### Core Dataset Generation
- The core dataset consists of examples in a structured format, making it easier for the generating model M to predict and for us to filter automatically.
- We use stochastic decoding to generate example inputs (to promote creativity), followed by deterministic decoding to generate their outputs (for accuracy).
- Figure 3 illustrates the process.
- Each example in the core dataset contains four fields: (1) An instruction describing the task. The instruction can be a generic template (e.g. "Write whether the following review is positive or negative") that can be instantiated by a particular input argument (e.g. the review itself).
- Example 1 Instruction: You are given a science question (easy-level) and four answer options (associated with "A", "B", "C", "D"). Your task is to find the correct answer based on scientific facts, knowledge, and reasoning.
- We show here one of our 5 in-context seeds.
- Pink: One of the model's generations for the given prompt. The generated example includes an instruction, input, and constraints.
- We use a seed of three in-context demonstrations x 1 , x 2 , x 3 to create a large dataset of NLP tasks with instructions, inputs and outputs. As a first step, we sample instructions, inputs, and constraints from a language model M . In the next step, we use M to deterministically generate the corresponding outputs. Finally, the data can be used for instruction tuning.
- The input argument that instantiates the instruction, creating a specific example of the task.
- Output space constraints, which detail the restrictions on the task's output space. Constraints are mainly relevant for classification tasks; for tasks with no specific output space constraints, this field is "None."
- A textual output reflecting a correct execution of the instruction given the input arguments and output space constraints.
- The first three fields (instruction, input argument, constraints) are the model's input, and the output field acts as the reference for training and/or evaluation.
- In §6 we provide data-driven evidence for selecting this particular format.
- Input Generation The first step in the data generation pipeline is to generate examples of instruction-input-constraints. We do so by prompting a model with three task demonstrations x 1 , x 2 , x 3 , each presented in the structured instruction-input-constraint format (without outputs). These demonstrations are wrapped by a simple meta-prompt that incentivizes the model to create a fourth example x 4 , which we collect.
- This process is illustrated in Figure 2.
- We use 5 different seeds of 3 demonstrations each to generate the entire core dataset. In other words, the whole process requires only 15 manually-constructed examples.
- All demonstrations are taken from the Super-Natural Instructions (Wang et al., 2022) train set. To obtain various examples using the same prompt, decoding is done by nucleus sampling (top p) with p = 0.99 (Holtzman et al., 2020).
- Filtering We apply three automatic filters to the generated examples to remove: (1) model generations that do not include the three input fields (instruction, input argument, and constraints), (2) instructions and inputs that are identical to those demonstrated in the prompt, (3) duplicate examples, i.e. two different examples that have the same instruction and input argument.
- Output Generation Given a generated example x, we generate the corresponding output y by conditioning a pretrained language model with the instruction, input argument, and constraints (if not none), followed by an "Output:" prompt. Here we apply greedy decoding to prioritize correctness over creativity. We ignore examples for which the generated output is an empty string.

### Template Expansion
- The Unnatural Instructions core dataset has a strict instruction-input-output format.
- To increase the format diversity and obtain tasks phrased in free-form natural language, we collect alternative formulations that preserve the content of the original instructions.
- Specifically, we prompt a language model to reformulate the tasks in the core dataset and collect two alternative formulations for each generated task.
- The alternative formulations are often shorter and less formal than the original instructions.
- The rephrasing prompt contains two examples of instructions and their alternative formulation.
- We do not include inputs, constraints, and outputs in the rephrasing prompt; instead, we utilize the already-generated inputs and outputs to complement the rephrased instruction.
- Unlike the examples in the core dataset, in some alternative formulations, the input is embedded into the task description rather than following it.
- We achieve that by adding an "{INPUT}" placeholder, which marks the position for input insertion.
- In some cases, the model generates two identical additional formulations, while in others, it generates more than two paraphrases.
- By cross-referencing each instruction's alternative phrasings with all of its input arguments, we can extend the data even further and arrive at a total of 240,670 examples without additional cost.

## Data Analysis
- The task creativity of crowd workers is a major challenge
- Crowd workers may collapse into predictable heuristics to form annotation artifacts
- Unnatural Instructions provides a highly informative training signal
- The inputs of Unnatural Instructions tend to be less similar to each other than the inputs of Super-Natural Instructions

## Experimental Setup
- We use Unnatural Instructions to fine-tune models
- We fine-tune T5-LM, the language-model-adapted variant of T5-11B
- We follow standard practice for fine-tuning

### Baselines
- We measure the relative utility of Unnatural Instructions by comparing it to a variety of models, all based on T5-11B, which were fine-tuned with different types and quantities of manually-annotated instruction data.
- T0++ (Sanh et al., 2021) is an instruction-tuned variant of T5-LM, trained on tasks in the Prompt-Source (Bach et al., 2022) prompt formats.
- T5-LM on Natural Instructions Our main point of comparison is the utility of the original manually-curated instructions in Super-Natural Instructions.
- We therefore train a model which is identical to ours in all aspects but data. Specifically, we fine-tune the LM-adapted variant of T5-11B on a subsample of 64,000 examples from Super-Natural Instructions training set, excluding exam-ples from any task that participates in the validation set.
- This model differs from Tk-Instruct along three aspects: the dataset subsample, the base model (T5-LM), and some training hyperparameters (batch size 16 for 3 epochs).

### Evaluation
- We evaluate models on the test set of Super-Natural Instructions (Mishra et al., 2022;Wang et al., 2022).
- As in the original papers, outputs are generated using greedy decoding, and performance is measured using Rouge-L.
- We evaluate models on the heldout set of T0 (Sanh et al., 2021), using rank classification for decoding and accuracy as a metric.
- For fair comparison, we remove tasks supersets of which are present in the Tk-Instruct training set. The final set contains six tasks: ANLI R1-R3, CB, COPA and RTE.
- We refer to this evaluation set as T0: Zero-Shot.
- Unlike Super-Natural Instructions, T0: Zero-Shot tasks do not have a strict format and are phrased in a rather free-form manner, including inputs that can be embedded into the task description.
- We therefore expect models trained on our core dataset (without instruction paraphrases) to perform poorly under these conditions, while adding the task reformulation data should boost performance on T0: Zero-Shot.

## Results
- Table 4: Performance of different models on different benchmarks
- T5-LM finetuned on Unnatural Instructions clearly outperforms several strong instruction-tuned baselines such as T0++ and Tk-Instruct; the only exception to this is BIG-bench: Hard (Orig), where T0++ performs better.
- Retraining a model on Super-Natural Instructions using our exact setup reveals that a much stronger performance than that of Tk-Instruct can be achieved using this dataset.
- However, even in this direct comparison setup, Unnatural Instructions leads to stronger or equal performance for every dataset except Super-Natural Instructions itself.

### Performance with Template Expansion
- Paraphrasing improves performance on T0: Zero-Shot (+3.3), Big-bench: Hard in its original format (+12.1) and LMentry (+8.7)
- Paraphrasing is helpful even when controlling for dataset size

### Performance Scaling by Dataset Size
- As all of our data is generated from the same model using the same set of prompts, scaling up the amount of generated examples might lead to numerous repetitions and, as a consequence, diminishing returns in terms of downstream task performance.
- To investigate whether this is an issue, we analyze how the amount of training examples affects the performance of our finetuned models.
- To this end, we train models on subsets of both Super-Natural Instructions and Unnatural Instructions, ranging from 250 to 64,000 examples.
- As shown in Figure 6 (top row), our core and full data as well as Super-Natural Instructions all exhibit loglinear scaling laws, suggesting that even for subsets of Unnatural Instructions containing thousands of examples, simply generating more examples still adds a valuable signal to our training data.
- Results for LMentry (Figure 6, top right) show that our template expansion process is still beneficial when controlling for dataset size.

### Performance Scaling by Cost
- The cost of obtaining training data is more relevant than the number of required examples
- Unnatural Instructions is more cost-efficient than manually curated data
- This is true even for the Super-Natural Instructions test set

## Data Collection Ablations
- We explore the effect of the different components of our data collection pipeline by conducting struc- tural prompt ablations.
- Throughout this section, we train models for 1,500 steps using 2,000 examples and evaluate the Super-Natural Instructions validation set performance, averaged across three different random seeds.

### Generative Model
- Text-davinci-002 is an instruction-tuned variant of GPT-3
- Replacing an instruction-tuned model with a vanilla model does not affect the quality of generated data
- In other words, informative and diverse instructions can be generated by untuned language models. However, generating outputs does seem to require some level of instruction tuning.

### Meta-Prompts
- Language models are known to be sensitive to the meta-prompt -i.e., the text wrapping the in-context demonstrations, which can include task description or additional guidance regarding the desired output.
- We therefore experiment with three different metaprompt styles: minimal, enumeration, and verbose (Figure 7).
- Table 6 presents the results obtained from fine- tuning on datasets generated with different metaprompts.
- We observe that the simple enumeration approach elicits more informative examples than either the minimalistic or verbose approaches.
- Perhaps surprisingly, the verbose meta-prompt performs worse than the minimalistic one, possibly because the last line (the command) interrupts the pattern, and does not align well with patterns in the pretraining corpus.

### In-Context Examples
- Models such as GPT-3 are known to be sensitive to slight variations in prompt content, resulting in performance differences when provided with different demonstrations sampled from the same dataset
- To account for the effect of the provided demonstrations on the quality of the generated data, we experiment with each of our five demonstration sets separately
- Table 7 shows that the data generation pipeline is largely robust to variations in the in-context demonstrations, with one outlier (seed 4)
- Inspecting the differences between these groups, we find that seed 4 led to less constrained instructions: 1,376 out of 2,000 examples do not have constraints, whereas that number is between 28 and 880 for all other sets.

### Constraints
- The constraints field is associated with better quality outputs and inputs
- The constraints field is associated with better performance

### Two-Step Process
- One-step generation of instruction-input-output triplets obtains a score that is lower by 1.7 than the default two-step process.
- When generating outputs in a separate phase, we can use deterministic decoding algorithms to maximize accuracy.

## Related Work
- Instruction tuning
- Efrat and Levy (2020) propose the Instruction Paradigm
- Dagan et al. (2006)
- Bowman et al. (2015)
- Liu et al. (2022a)
- Schick and Schütze (2021b)
- Agrawal et al. (2022)
- Unnatural Instructions is the first work to go beyond a particular task and automatically generate a large-scale general-purpose dataset

## Conclusion
- We introduce Unnatural Instructions, an automatically generated dataset of natural language instructions and their corresponding inputs and outputs.
- To the best of our knowledge, this is the first general-purpose NLP dataset that was automatically generated.
- Our experiments show that models trained on Unnatural Instructions can outperform models trained on manually annotated datasets across several benchmarks.
- Unnatural Instructions is not only very cost-effective, we also provide evidence of enhanced diversity in the instructions produced and a high level of creativity in the tasks devised, a trait difficult to obtain with crowd workers.
- Ablations show that even weaker models without instruction tuning can generate use-ful instructions, though they may struggle with producing the corresponding outputs. However, coming up with interesting tasks and writing diverse instructions for them is arguably the main challenge of the data collection process, whereas given instructions and inputs, outputs are often far easier to annotate through crowdsourcing.
- Our findings incentivize utilizing models for general-purpose data generation, which we view as an intriguing direction for future research.
