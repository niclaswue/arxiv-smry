---
title: "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning"
date: 2022-12-19T07:40:02.000Z
author: "Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu, Xiang Ren"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09282v1.webp" # image path/url
    alt: "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09282)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09282).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/apollo-a-simple-approach-for-adaptive).

# Abstract
- Logical reasoning ability is important
- Requires understanding the information present in the text, their interconnections, and then reasoning through them to infer new conclusions
- Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills
- In this work, we propose APOLLO, an adaptively pretrained language model that has improved logical reasoning abilities
- We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model
- We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences
- The proposed training paradigm is both simple and independent of task formats
- We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.

# Paper Content

## Introduction
- Logical reasoning is an important ability of humans that helps us in making rational decisions based on known information
- Logical reasoning abilities are important for text understanding across various downstream tasks, e.g., in open-domain question answering (Yang et al., 2018;Zhu et al., 2021), machine reading comprehension (MRC) (Baradaran et al.,
- Recently, there has been an increasing focus on evaluating the logical reasoning abilities of language models by using MRC tasks that specifically require a significant amount of logical reasoning to obtain the correct answer (Yu et al., 2020;Liu et al., 2021).
- In these datasets, the model needs to understand a given context, reason logically about a question to infer new conclusions, and then select the correct answer from a set of options.
- With the advent of large pre-trained language models (PLMs) in NLP (Devlin et al., 2019;Radford et al., 2019;Raffel et al., 2020), understanding and improving the logical reasoning abilities of these models has become even more important as these are increasingly being used across a wide variety of real-world tasks.
- There have been some recent works on improving the logical reasoning abilities of PLMs (Wang et al., 2022;Ouyang et al., 2022;Jiao et al., 2022).
- These works typically generate a dataset containing symbolic structures such as logical graphs from text, logical contrast sets, etc., and then train the LM using custom loss objectives to learn logical reasoning abilities.
- While the performance improvements achieved by these methods are encouraging, the proposed solutions generally require complex data processing to generate the additional structural information (graphs, contrast data, etc.) required for logical reasoning.
- Further, the loss functions proposed in these works are very specifically designed in accordance with their respective data augmentation technique and widely differs from the typical masked language modeling loss used for LM pretraining (Devlin et al., 2019).
- These complex processing steps usually require task-specific design choices, which are not necessarily learning generalizable logical reasoning ability that is reusable across different task formats.
- For example, Wang et al. (2022) parses symbolic logical structures from the training data of a specific dataset, which might not generalize to a new dataset or task.
- Jiao et al. (2022) constructs synthetic context and options from entities in Wikipedia, which is specific to the multiplechoice MRC setting.
- Overall, it is unclear if these highly specific inductive biases are indeed essential for improving the logical reasoning abilities in language models, or if a simple approach is sufficient.
- Prior works (Gururangan et al., 2020) have shown that continued domain-adaptive pretraining of PLMs leads to performance gains on downstream tasks.
- Inspired by this, we propose APOLLO, a continued pretraining-based approach to inject logical reasoning abilities in language models that requires minimal data processing and loss function modifications.
- Firstly, we use a set of logical inference keywords to select a subset of sentences involving logical reasoning from a large text corpus, such that every sentence in the subset contains at least one of the keywords.
- These keywords are chosen such that the sentences containing the keywords are more likely to elicit reasoning when the PLMs fill out the masked tokens during pretraining.
- We note that in contrast to previous works (Gu-rurangan et al., 2020), our method only requires selecting sentences from a text corpus, eliminating the need for any domain-specific corpus.
- Secondly, we find that selectively masking specific types of words in masked language modeling (MLM) can benefit the model's logical reasoning skills, in line with previous works on task-guided finetuning (Lad et al., 2022).
- More...

## Method
- In APOLLO, we use a keyword-based dataset selection strategy to collect a dataset of reasoning-related sentences called IMPLICA-TION.
- This dataset is then used for continued pretraining of a model using two loss objectives: selective masked language modeling (S-MLM) loss and entailment classification (E-CLS) loss.
- Please refer to Section 2 for more details.

### Dataset Selection
- PLMs are typically trained on the data from the internet
- Here, instead of focusing on a specific task, we want to teach the PLM generalizable logical reasoning abilities
- We hypothesize that using training data that contains more logical sentences, rather than generic internet data, should help in improving the reasoning ability of the PLM
- Although creating such a dataset automatically is a challenging task in itself, in APOLLO, we explore a simple and intuitive way to create such a dataset
- First, we select specific keywords that are typically encountered in sentences with some logical implication
- Broadly, we categorize these keywords into two types: • Positive implication (Entailment): These keywords are present in sentences where the reason generally entails the inference. Examples of such keywords would be "therefore", "accordingly", etc. • Negative implication (Contradiction): The keywords in this category are usually present in sentences where the reason contradicts the inference. For example, keywords such as "but", "although", etc., come under this category
- Next, we select sentences from Wikipedia such that they contain at least one of the keywords
- We name this filtered version of Wikipedia as the IM-PLICATION dataset
- While this keyword-based filtering does not necessarily ensure that the sentence has a logical implication, it increases the probability of logically rich sentences being present in the training data
- This helps in teaching the required logical reasoning skills to the PLM

### Loss Function Design
- Selective masked language modeling loss (S-MLM) is a modified version of the masked language modeling (MLM) loss used in BERT (Devlin et al., 2019).
- In the MLM loss, tokens in a sentence are masked at random and the model learns to predict the masked tokens.
- While this helps in learning a good language model, not all masked tokens require a similar degree of reasoning to predict them.
- For example, for the sentence shown in Figure 3, words such as "were", "the", etc. are decided more by the structure of the English language than any form of reasoning.
- For example, in Figure 3, predicting words such as "more", "drop", etc., would require logical reasoning.
- Similarly, the connector words such as "and", "hence", etc. define the logical structure of the sentence.
- These types of logically relevant words are highlighted in blue.
- Thus, we hypothesize that masking these logical tokens would likely teach the model to perform reasoning more effectively than masking any random token.
- While finding these exact logical words for a given sentence is again a hard problem, in APOLLO we simplify this using a heuristic approach to consider tokens that belong to a specific set of parts-ofspeech (POS) tags.
- We select candidate tokens from the following SpaCy POS tags (Honnibal and Montani, 2017): ADJ, ADV, CONJ, CCONJ, PART, SCONJ, and VERB.
- Please refer to Section 4.3 for more empirical results that further justify this choice.
- If Earth were frozen entirely and hence be more reflective, the temperature would drop below.
- Entailment classification loss (E-CLS) is a loss function that predicts whether a sentence contains some reasoning aspects that portray a sense of entailment or contradiction within the sentence.
- For example, in Figure 3, the sentence is classified as "Entailment", because the phrase "more reflective" is entailed by the phrase "frozen entirely".
- A model would ideally require strong logical reasoning abilities to understand the sentence and then predict if it refers to an entailment or contradiction.
- The labels for this loss are bootstrapped using the heuristic of checking the type of implication keyword present in the sentence (refer to Section 2.1 for details).
- We note that although the keyword is a correlated feature that can be used to predict the label, on average the keyword would be masked out due to our selective masking policy, forcing the model to learn some logical semantics to minimize the loss.
- Additionally, the classification loss adds a strong inductive bias specifically about the positive and negative implication words than the S-MLM loss.

### Continued Pretraining
- The loss function used for continued pretraining in APOLLO is a multi-task loss with both the S-MLM loss and E-CLS loss weighted equally.
- Unlike prior works, we don't need to add MLM loss to avoid catastrophic forgetting.

### Finetuning
- Adds a randomly initialized MLP layer on top of the pretrained model
- Fines the combined model on the downstream dataset using the cross-entropy loss

## Experimental Setup
- The datasets used for the evaluation are from a variety of sources, including a large-scale natural language processing task and a large-scale image recognition task.
- The APOLLO algorithm is evaluated on a variety of datasets, including a large-scale natural language processing task and a large-scale image recognition task.
- The APOLLO algorithm is compared with a number of baselines, including a well-known deep learning algorithm and a more traditional machine learning algorithm.
- The implementation details of the APOLLO algorithm are described in detail.

### Datasets
- APOLLO is a machine learning algorithm that is designed to improve the performance of a computer system in logical reasoning tasks
- The algorithm was evaluated on two datasets: ReClor (created from the logical reasoning questions from standardized graduate admission examinations) and LogiQA (created from publicly available logical examination papers for reading comprehension)
- The algorithm was found to be more effective on the ReClor dataset than on the LogiQA dataset

### Baselines
- APOLLO is more accurate than LRReasoner, FOCAL REASONER, and MERIt.
- APOLLO uses additional data to improve logical reasoning abilities.

### Implementation Details
- For creating IMPLICATION dataset, we use the Wikipedia version provided under HuggingFace Datasets (Wolf et al., 2020) as the main corpus.
- The list of keywords we use for filtering sentences from Wikipedia are listed in Appendix A.
- We experiment with RoBERTa-Large (Liu et al., 2019a), DeBERTa-v3 (He et al., 2021), and DeBERTa-v2-xxlarge (He et al., 2020) as the base models for APOLLO.
- We pretrain the last two layers of the Transformer (Vaswani et al., 2017) layer for 3 epochs, using a batch size of 4096.

## Results

### Overall Results
- APOLLO outperforms all baselines on LogiQA
- APOLLO has lower performance on ReClor than two baselines
- Our pretraining strategy is effective and can be implemented easily

### Performance on GLUE Benchmark
- In APOLLO pretraining, we do not include the standard MLM loss (Devlin et al., 2019), which might question the general language modeling abilities of the model.
- To understand this, we finetune APOLLO on each dataset of the GLUE benchmark (Wang et al., 2019) and evaluate the finetuned checkpoint on the Dev set.
- The results are compared with the Dev set results for RoBERTa model (Liu et al., 2019b) in Table 3.
- Following Devlin et al. (2019), we omit the evaluation on the problematic WNLI set.
- Overall, we observe that APOLLO can slightly improve the overall performance on the GLUE benchmark.
- This demonstrates that our proposed continued pretraining strategy is able to learn better logical reasoning abilities without any catastrophic forgetting of general-purpose language modeling skills, and these logical reasoning capabilities are also beneficial for general natural language understanding.
- In this section, we ablate various design choices in constructing the IMPLICATION dataset, and our proposed method.
- For the ablations involving APOLLO, we use RoBERTa-Large as the base model and the IMPLICATION dataset, if not mentioned separately.
- All the reported numbers are on the validation set of the downstream task.
- To study the effect of using IMPLICATION for continued pretraining along with the proposed loss functions, we first create RANDOM, a random subset of Wikipedia of similar size as that of IMPLICATION, and also consider using the standard masked language modeling (MLM) loss (Devlin et al., 2019), where any token can be masked at random.
- The results of the ablation are shown in Table 4.
- We observe that using the IMPLICATION dataset leads to consistent improvements on both datasets when compared to the RANDOM dataset.
- Additionally, we find that both the S-MLM and E-CLS loss lead to improvements over MLM loss. Thus, this empirically justifies our choice of the dataset and loss functions proposed here.
- In this ablation, we study the effect of the keyword categories that we use for filtering Wikipedia.
- For this, we create two different pretraining datasets IMPLICATION-Positive and IMPLICATION-Negative using the positive and negative implication keywords, respectively (refer to Section 2.1).
- The total number of sentences in these datasets is 7.5M and 11.3M, respectively.
- Our complete dataset IMPLICATION thus has a total of 18.3M sentences.
- The results of the ablation are shown in Table 5, under the section "Keyword Category".
- We observe that IMPLICA-TION-Positive, although smaller in size, leads to better performance on both downstream tasks, compared to IMPLICATION-Negative.
- One reason for this is that the sentences with positive keywords are more likely related to reasoning than the negative counterparts because the negative keywords are used in many diverse scenarios in the English language.
- Overall, we observe that the combined IM-PLICATION dataset leads to the best performance, demonstrating that both the positive and negative implication keywords are essential to improve logical reasoning.
