---
title: "oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation"
date: 2023-01-03T19:52:17.000Z
author: "Jianhui Li, Zhennan Qin, Yijie Mei, Jingze Cui, Yunfei Song, Ciyong Chen, Yifei Zhang, Longsheng Du, Xianhang Cheng, Baihui Jin, Jason Ye, Eric Lin, Dan Lavery"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-01333v1_rRLQ_0y_R.jpg" # image path/url
    alt: "oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01333)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01333).


# Abstract
- With the rapid development of deep learning models and hardware support for dense computing, the deep learning workload characteristics changed significantly from a few hot spots on compute-intensive operations to a broad range of operations scattered across the models.
- Accelerating a few compute-intensive operations using the expert-tuned implementation of primitives does not fully exploit the performance potential of AI hardware.
- Various efforts are made to compile a full deep neural network (DNN) graph. One of the biggest challenges is to achieve end-to-end compilation by generating expert-level performance code for the dense compute-intensive operations and applying compilation optimization at the scope of DNN computation graph across multiple compute-intensive operations. We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid approach of using techniques from both compiler optimization and expert-tuned kernels for high-performance code generation of the deep neural network graph. oneDNN Graph Compiler addresses unique optimization challenges in the deep learning domain, such as low-precision computation, aggressive fusion, optimization for static tensor shapes and memory layout, constant weight optimization, and memory buffer reuse. Experimental results demonstrate up to 2x performance gains over primitives-based optimization for performance-critical DNN computation graph patterns on Intel Xeon Scalable Processors.

# Paper Content

## Introduction
- DL frameworks provide a rich set of deep neural network (DNN) operations for developers to describe a DNN model and use primitives libraries by default to offload the most performance-critical operations to CPU and GPU
- Most of the execution time of DL applications is spent on the DNN model.
- DL frameworks represent the DNN models internally as a computation graph of DNN operations. After performing high-level graph optimizations, the graph is executed operation by operation.
- On top of their own implementation of DNN ops, DL frameworks use third-party primitives libraries to offload the most performance-critical DNN operations.
- Primitives library offers a simple and effective way to offload deep learning computation. However, its performance benefit doesn't scale to new AI workloads and hardware due to its limited optimization scope.
- With the fast evolution of AI software and hardware, the performance characteristics of deep learning workload have been shifted from a few hot spots of concentrated compute-intensive operations to many scattered DNN operations including memory-bound operations.
- There are multiple sources that contribute to the increasing time percentage on memory-bound operations.
- The deep neural network used for natural language processing [19] and recommendation systems [18] has smaller input data and overall lower compute intensity compared to computer vision models [22] [23].
- Instead of supporting each innovative activation function with a complex DNN operation, DL Frameworks tend to compose multiple existing fine-grain operations, to maintain a balance of scalability and usability.
- Lastly, hardware acceleration usually focuses on accelerating the dense computation of low-precision data types and relies on the software to optimize memory-bound operations.

## High-level Design
- The principal of oneDNN Graph Compiler is performance, simplicity, and modularity.
- oneDNN Graph Compiler has two levels of intermediate representations: Graph IR, and Tensor IR.
- The input DNN computation graph is internally represented as Graph IR.
- Graph IR optimization module performs a number of transformations that optimize and group the computation graph as a sequence of fused operations.
- Graph IR is further lowered to Tensor IR.
- The Tensor IR doesn't preserve DNN operation semantics and is close to the C program semantics.
- Tensor IR is then further lowered to LLVM IR and intrinsic calls to Microkernels.
- The Graph IR keeps the DNN OP semantics, so most domain-specific optimizations are done at this level.
- Instead of lowering the DNN OP to Tensor IR and performing sophisticated loop analysis to achieve the best loop schedule and fusion, oneDNN Graph Compiler uses expert-developed templates, microkernels, and heuristics to guide the code generation of compute-intensive operations and the fusion process.
- The decisions of parallel task decomposition, loop scheduling and tiling, tensor memory layout, and whether to fuse with neighbor operations are based on the Graph IR with DNN OP semantics.
- This simplifies Tensor IR design.

## Figure 1. oneDNN Graph Compiler IR and Optimization
- Graph IR uses graph, logical tensor, and OP to describe a computation graph
- A graph contains a set of OPs and logical tensors
- Each OP represents an operation in a computation graph
- A logical tensor represents the tensor's metadata, like the element's data type, shape, and memory layout
- OP has kind, category, attributes, and logical tensors for inputs and outputs
- Graph IR optimization module first decomposes complex OPs into basic DNN OPs
- The complex DNN OPs are OPs with complex semantics which could be composed of simple fundamental operations like addition and multiplication
- They are introduced by DL frameworks to support high-level DNN OP semantics for ease of programming, such as batchnorm, quantize, gelu, and many activation operations
- The basic DNN OPs are categorized to be either Tunable OP or Fusible OP
- Tunable OPs describe DNN operations that use tunable parameters to instantiate a pre-defined template to generate the best-performing code
- Fusible OP refers to operations that can be fused to Tunable OPs, such as element-wise operations, broadcast, reduction, and data movement operations
- This paper describes the most effective techniques used in the oneDNN Graph Compiler

## Microkernel-Based Template for Tunable OP Lowering
- Automating the high-performance code generation for Tunable OPs is the foundation of a tensor compiler.
- oneDNN Graph Compiler took an approach inherited from Tensor is described with a Tensor name followed by index and size for each dimension.
- Tensor A[0:M, 0:K] refers to 2 dimensions tensor starting from the position [0,0] with size [M, K].
- A[0:MB, 0:KB] refers to a tensor slice containing a subset of A tensor elements, starting from position 0 to MB-1 along the m dimension, and 0 to NB-1 along the n dimension.
- The pseudo-code uses a blocked layout for A, B, and C.
- C[0:MPSN, 0:NPSN, 0:MB, 0:NB] denotes the full C tensor C[0:M, 0:N] reordered with a blocked layout.
- C[mps:1, np:NSN, 0:MB, 0:NB] denotes a tensor slice which "slice" the C tensor in the first 2 dimensions starting from position "mps" and "np" with size "1" and "NSN".
- A_addr[0..BS] denotes an array with BS elements from A_addr[0] to A_addr[BS-1].
- A[mps:1, ks:0..BS, 0:MB, 0:KB] denotes an array of BS tensor slices from A[mps:1, ks:0, 0:MB, 0:KB] to A[mps:1, ks:BS-1, 0:MB, 0:KB].
- from the performance library development, which first creates the code templates for a given Tunable OP and then instantiates it with parameters decided by a heuristic.
- The parameters are decided based on the input data tensor shape and hardware sizes of the microarchitecture.
- The template shown above is for a matmul op that does matrix multiplication over A[M, K] and B[K, N] and produces C[M, N].
- The template is applied to a common deep learning use case where the computation uses multiple cores, and the size of input and output tensor fits within the cache system.
- The outer parallel loops divide the kernel into multiple subtasks for multi-cores.
- Each subtask is assigned to one single core, named single-core kernel, which is represented by the inner loops which call a microkernel in the innermost loop body.
- The microkernel and the single-core kernel operate on a tensor slice that represents a subset of tensor elements.
- For example, the original tensor is represented as A[0:M, 0:N], where the subscription represents starting offset and size for each dimension.
- The tensor slice is represented as A[0:MB, 0:NB], where MB and NB refer to the tile size of the tensor slice along m and n dimensions.
- A submatrix is a special case of a 2-dimension tensor slice.
- In the template above, the microkernel produces a small submatrix C[0:MB, 0:NB], and the single-core kernel outputs a larger submatrix C[0:MSN, 0:NSN].
- The microkernel is an important element for the oneDNN Graph Compiler to achieve comparable performance to expert-tuned primitives.
- oneDNN Graph Compiler uses the microkernel named batch-reduce GEMM [8] [24].
- The microkernel has two inputs, both representing a batch of 2D matrices.
- It first applies matrix multiplication with each batch element to produce a batch of immediate 2D matrices and then sums them to a final 2D matrix output.
- This interface can be used for many variants of matmul op in both inference and training use cases and was adopted by both oneDNN primitives and oneDNN Graph Compiler.
- The microkernel is fine-tuned to maximize the compute efficiency by fully utilizing the compute function unit and the high bandwidth provided by registers and the L1 cache.
- It abstracts the ISA difference so oneDNN Graph Compiler doesn't need to deal with different vector or matrix instructions provided by different CPUs.
- However, the oneDNN Graph Compiler needs to choose the input submatrix sizes for the microkernel so that they are usually multiples of register sizes used by the vector and matrix function units.
- Also, it needs to choose the batch size for the microkernel so that the whole input and output submatrices fit within the L1 cache.

## Template with Anchors for Fused OP Lowering
- Fusion is a technique that allows multiple operations to be combined into a single operation that is more efficient to execute.
- Fusion is supported by the template of a Tunable OP, which contains placeholders for the input and output tensors.
- The Graph IR fusion optimization decides whether it is profitable to fuse a Fusible OP to a Tunable OP and which anchor point is assigned to the Fusible OP.
- The Fused OP lowering pass retrieves anchors for Fusible OPs and directly inserted its corresponding Tensor IR at the anchor.
- The main benefit of fusion is that the operation being fused only needs to access tensor slices associated with the anchor.
- The anchors before the microkernel are called pre-op anchors, which work on the input tensors' tensor slices. The anchors after the microkernel are called post-op anchors, which work on the output tensor's tensor slice.
- The fusion optimization evaluates total memory access and computation cost for each anchor point and selects one.
- There are multiple choices of commit anchors to insert a pre-op or post-op fusion, depending on the different levels of the loop nest.
- The fusion optimization uses a heuristic to decide which anchor to choose.

## Graph IR Optimization
- The Tunable OP template provides the foundation for automating the process of building high-performance compute-intensive primitives and fusing neighbor memory-bound operations
- On top of that, oneDNN Graph Compiler also exploits optimization opportunities only available when the computation is offloaded as a computation graph instead of individual operations
- The Graph IR is first decomposed into a graph of basic DNN operations, applied a number of optimizations, fused to a number of Fused OPs, and then lowered to Tensor IR using the templates for Tunable OPs and Fused OPs
- This section introduces a few graph-level optimizations specific to the deep learning domain: lowprecision conversion, constant weight preprocess, layout propagation, and fusion
- Figure . 5 illustrates these important optimizations with a quantized multilayer perceptron (MLP) example
- The low-precision computation brings significant speedup as it reduces both the computation and memory bandwidth required to compute a deep learning model
- The low-level precision computation graph preserves the compute-intensive operations in the FP32 data type with surrounding type conversion operations inserted by quantization tools
- Low-precision conversion transforms the input DNN computation graph and converts the compute-intensive operation to low-precision
- The example shows an asymmetric dynamic quantization case, so the first dequantize op scales A input tensor by a_s and then offset by a_z to adjust the zero point, and the other dequantize op just scales the B matrix with b_s. B matrix is the weight matrix.
- Low-precision conversion optimization first breakdown the quantize and dequantize op to be simple addition and multiply ops and transform the graph to be a mathematically equivalent form that uses Int8 matmul op.
- The transformation can be illustrated by the following mathematic equation. Matrix multiplication is denoted by "X", and elementwise multiply and addition are denoted by "*" and "+", the broadcast and type conversion needed for Uint8 or Int8 data type processing are omitted.
- The transformed equation looks more complex, but it lowers the matmul precision from FP32 to Int8, which is the main goal of the optimization
- The const weight preprocessing optimization is to exploit the optimization opportunity that some of the input tensors are constant at the execution time
- For the static quantization inference use case, the weight tensors and quantization parameters are constant, so a portion of the post-transformation equation contains computation over constant weight, scale, and zero point can be avoided completely at runtime
- The challenge is that the weight data buffer might not be available during the compilation, so the compiled code needs to preprocess the constant weight at the execution time when it first arrives
- As a_s, b_s, c_s, and c_z are constants passed as dequantize op's attribute, these constants can be folded in the compiletime
- The constant weight preprocess optimization recognizes the constant tensor and builds a special initial function that preprocesses the constant weight and reuses the processed weight at the runtime
- It recognizes the weight matrix B is passed as a constant logical tensor in the input graph, meaning that the weight buffer holds a constant value and won't change since the first execution of the compiled partition
- If a DNN op's inputs are runtime constant or compile-time constant, the output tensor is runtime constant as well
- The optimization propagates and marks all the runtime constants throughout the graph
- Later the lowering generates special code for runtime constants, to make sure these runtime constants only be executed once in the first execution, and all future execution will reuse the processed result

## Tensor IR optimization
- Tensor IR is the lowest intermediate representation in the oneDNN Graph Compiler
- At the Tensor IR level, the DNN computation graph is lowered to a C-like program, which includes function, statement, expression, and intrinsic functions
- The Fused OP is lowered as a function, which contains nested loops
- A complex statement describes a structure like a loop, and a simple statement does computation
- Var and Tensor represent scalar variables and multi-dimension arrays respectively
- Tensor IR supports Graph IR optimization by merging loops as instructed by Graph IR
- Figure 6 shows the example of Tensor IR for the pseudo-code in figure 4
- In the Tensor IR, the computation on the tensor slices is represented by either a nested loop or a function call to the microkernel
- The inserted pre-op and post-op are lowered to nested loops
- The two post-op, ReLU and reorder ops, are merged as one nested loop using the hint passed by Graph IR
- The main optimizations on Tensor IR are tensor size optimization and memory buffer optimization
- Tensor size optimization tries to reduce the tensor size of each temporary tensor
- The temporary tensors are introduced in the pre-op and post-op fusion process
- In the example code of figure 6, the postops are fused into one loop nest in the Tensor IR
- Since the accesses of the temporary tensors, C'' and C''', are local to the innermost loop body, the temporary tensor could be replaced by a scalar variable. Compared to accessing global and larger original tensors, the result code has a much smaller memory footprint and better cache locality.
- The temporary tensor introduced by pre-op fusion can be reduced similarly by analyzing the scope of the tensor usage.
- For example, A'[MSN, BS, MB, KB] be reduced to A' [BS, NB, KB], since the producer of A' and consume are within the "msi" loop, so there is no need to save the result along the 2 nd dimension of A'.
- After tensor size optimization, the multiple-dimension tensor representation is flattened to a one-dimensional array to represent the memory buffer
- The memory buffer optimization tries to reuse the memory buffer of temporary tensors to have a minimum overall temporary buffer size for the compiled code and tries to improve the locality of the temporary buffer use.
- The main target of memory buffer optimization is to reuse the memory buffer created for the temporary tensors between fused op.
- In the inference use case, the output tensor is only consumed by the next fused op, and so the buffer could be reclaimed once the next fused op completed execution.
- Since the input tensor size is known to the compilation process, the memory buffer usage can be tracked at the compile time and optimized to improve efficiency.
- Memory buffer optimization uses life span analysis like traditional compiler analysis for register allocation based on the def-use chain.
- Memory buffer optimization has extra consideration for buffer reuse since the memory buffer size used is not uniform as registers.
- It scans the Tensor IR, tracks all the memory buffers alive, and then computes the peak size of the total memory buffers needed for the entire compiled graph.
- The algorithm considers both reusing the hot memory and reducing the overall peak memory.
- At each point, when an intermediate buffer is needed, it tries to reuse the free intermediate buffers, which are already allocated but not used anymore.
- Among multiple choices of reusable memory buffers, it chooses the one that was used most recently, so likely the data is still in the cache system.

## Experimental results
- The oneDNN Graph Compiler targets performance-critical DNN computation graph, which is usually a subgraph of the whole DNN model graph.
- We selected two DNN computation subgraphs as target workloads to evaluate the performance.
- The Multilayer Perceptron (MLP) workload contains multiple matmul ops intermixed with activation ops like ReLU.
- The MLP subgraph is the basic building block for many deep learning models, including recommendation systems and natural language processing.
- The Multi-Head Attention (MHA) subgraph is the key element to Transformer based deep learning models like Bert for natural language processing.
- The MHA workload focuses on the scaled dot-product attention portion of the MHA graph, which contains two batch matmul ops and a softmax as well as other binary ops between them.
- Depending on the use case, the MLP and MHA subgraphs tend to account for more than half of total model execution time, especially for DLRM or Bert Large models.
- We choose the inference use case for the evaluation and measure the performance for both FP32 and Int8 data types.
- We choose several representative data shapes for weights and input tensors and select a wide range of batch sizes.
- The weight sizes for MLP are from the MLPerf Compiler demonstrates an average of 1.73x speed up on Int8 data type and 1.22x speed up on FP32.
- The five MLP_1 tests for Int8 data type show the highest speedup at an average of 2.72x.
- Among a total of 2.72x speed up, coarse-grain fusion is the main contributor and accounts for 1.95x.
- It merges 3 parallel loops, lowered from 3 matmul ops, into one parallel loop.
- The coarse-grain fusion greatly reduces the synchronization overhead and permits the activation data to be in the fastest cache for the next matmul op.
- For the MLP_1 Int8 tests, the entire activation and weight tensor fit in the L2 cache, so the coarse-grain fusion performs very well for these cases.
- When disabling the coarse-grain fusion, the remaining optimization accounts for about 1.4x.
- There are mainly three reasons. First, although the baseline implements the same fusion, oneDNN Graph Compiler has better performance for each individual matmul op in MLP_1, as shown in Figure 7.
- Second, the layout propagation allows all three matmul ops to run with the same blocked layout without extra reordering.
- Last, due to the MLP_1 tests being relatively short, the total API call overhead takes up to 10% of the execution time for the baselines, which is reduced by about 3 times since the compiled code needs only to be called one time.
- Compared to MLP_1 Int8 tests, the MLP_1 FP32 tests show an overall 1.47x performance gain, with 1.15x from coarse-grain fusion and 1.28x from rest optimizations.
- For MLP_2 test cases, the oneDNN Graph Compiler shows an overall 10% better performance Int8 data type and 1% on FP32.
- When the coarse-grain fusion is disabled, oneDNN Graph Compiler is 1% slower compared to the baseline with FP32 and Int8 test cases combined.
- As we learned from the individual matmul op analysis, the initial layer of MLP_2 (k=479) has lower performance for oneDNN Graph Compiler and negatively impacts the overall performance.
- MLP_2 tests also get benefit from the coarse-grain fusion but to a lesser extent.
- The coarse-grain fusion is not able to merge all the loop nests due to the current heuristic limitation.
- We believe that the performance for MLP_2 test cases can be further improved with more heuristics tuning.

## Conclusion
- We proposed a hybrid approach to address the unique challenges of deep learning compilation
- The template uses an expert-developed microkernel, algorithm, and heuristic, to ensure compiler-generated code achieves comparable performance to expert-tuned primitives
- Performance evaluation shows up to 2x performance gain for performance critical DNN computation graph in CPU inference usage

## Unnamed Section 221
- Microkernel based template for Tunable OP
- OP can be tuned to improve performance
- OP can be tuned to improve security
- OP can be tuned to improve reliability
- OP can be tuned to improve scalability

## Unnamed Section 222
- The figure shows a fused OP template with anchors and a cost table.
- The anchors help to keep the template organized and make it easier to understand.
- The cost table is a helpful tool that can help you to understand the costs associated with a particular operation.

## Unnamed Section 223
- Figure 4: Pseudo code for Fused OP Parallel loop
- Mp i=0, MPN, 1
- Parallel loop npi = 0, NPN, 1
- Loop msi = 0, MSN, 1
- mpsi = mpi * M/MPN + ms
- C'[0:NSN, 0:MB, 0:NB] = 0
- Loop ksi = 0, KSN, BS
- Reorder(A, [1, 1], A'[mpsi:1, ksi:BS, 0:MB, 0:KB], [MB, KB], from=[mpsi, ksi])
- Loop nsi = 0, NSN, 1
- npsi = npi * N/NPN + nsi
- A'_addr[0:BS] = &A'[mpsi, ksi:BS, 0, 0]
- B_addr[0:BS] = &B[ksi:BS, npsi, 0, 0]
- C'[nsi:1,0:MB, 0:NB] += Batch_reduce_gemm (A'_addr[0:BS], B_addr[0:BS],Batch = BS)
- C''[mpsi:1, npsi:NSN, 0:MB, 0:NB] = C'[0:NSN, 0:MB, 0:NB]
- C'''[mpsi:1, npi:NSN, 0:MB, 0:NB]) = Relu(C''[mpsi:1, npi:NSN, 0:MB, 0:NB])
- Reorder( C'''[mpsi:1, npi:NSN, 0:MB, 0:NB], [MB, NB], C, [MB2, NB2], to=[mpsi, npi])

## Unnamed Section 224
- Quantize a matrix of floating point values
- Dequantize a matrix of floating point values
- Calculate the product of two matrices using an XFP32 floating point format
- Convert the product to an integer value

## Unnamed Section 225
- The optimization passes are done on a graph of IR
- The optimization passes are done using a genetic algorithm
- The optimization passes are done with a population of IR nodes
- The optimization passes are done with a fitness function
- The optimization passes are done with a crossover operator
- The optimization passes are done with a mutation operator
- The optimization passes are done on a graph of IR
- The optimization passes are done using a genetic algorithm
- The optimization passes are done with a population of IR nodes
- The optimization passes are done with a fitness function
- The optimization passes are done with a crossover operator
- The optimization passes are done with a mutation operator

## Unnamed Section 226
- Lowering to Tensor IR is a way to reduce the size of a neural network
- Lowering to Tensor IR can be done using a variety of methods
- Lowering to Tensor IR can be used to improve the performance of a neural network

## Unnamed Section 227
- DLRM model
- The sequence length and hidden size choices for MHA are from the Bert models
- Performance data is collected on an Intel® Xeon® Platinum 8358 processor with 32 cores
- We first study the fine-grain fusion performance for individual layers since this is the foundation of the oneDNN Graph Compiler
- The tests evaluate all the problem sizes used in the MLP tests
- The baseline is heavily optimized and uses oneDNN primitives, which is the industry standard best-performant expert-tuned implementation
- Both the baseline and oneDNN Graph Compiler assume weight being pre-packed, compensated, and preprocessed
- Figure 7. shows that the performance of oneDNN Graph Compiler's automated kernel generated using the template approach is 6% better than the experttuned primitives for the given test cases
- oneDNNGraphCompileroutperformsthe expert-tuned primitives in many smaller problem sizes and falls behind on certain cases, particularly with k=479
- Figure 8. shows performance comparisons for MLP and MHA tests between oneDNN Graph Compiler and oneDNN Primitives
- The left side shows performance data for FP32 data type and the right side is Int8.
- Each test is named with workload category, batch size, and data type as shown in Table 1.
- For each test, we measure the performance of the baseline, oneDNN Graph Compiler, and the middle setting which disables the coarse-grain fusion and evaluates the rest optimizations including the fine-grain fusion for oneDNN Graph Compiler. The baseline uses expert-tuned oneDNN primitive with fusion support and has been integrated into multiple DL frameworks to accelerate deep learning on the CPU. Specifically, it uses oneDNN primitives post-op fusion to fuse matmul op with ReLU for the MLP tests and with division and addition ops for the MHA tests. For Int8 tests, before calling the oneDNN primitives, the baseline applies similar low-precision graph transformation and maps the graph to low-precision matmul and post-ops. Besides that, the baseline also performs weight prepacking, compensated weight preprocessing, and caches the result to avoid re-computation at runtime.
