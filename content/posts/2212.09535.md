---
title: "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting"
date: 2022-12-19T15:24:45.000Z
author: "Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman, Dragomir Radev, Vassilina Nikoulina"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09535v1.webp" # image path/url
    alt: "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09535)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09535).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/bloom-1-adding-language-support-to-bloom-for).

# Abstract
- The BLOOM model is a large open-source multilingual language model capable of zero-shot learning
- However, its pretraining was limited to 46 languages
- To improve its zero-shot performance on unseen languages, it is desirable to adapt BLOOM,
- In this work, we apply existing language adaptation strategies to BLOOM and benchmark
- We find language adaptation to be effective at improving zero-shot performance in new languages
- Surprisingly, adapter-based finetuning is more effective than continued pretraining for large models
- In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system
- It is primarily determined by the size of the language adaptation data
- We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot
- We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language
- We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at

# Paper Content

## Introduction
- Current multilingual language models have limited coverage of languages in their pretraining data.
- For instance, BLOOM (Scao et al., 2022), which is the largest multilingual open-source model to date created by the BigScience community, only covers 46 natural languages.
- It even excludes high-resource languages such as Korean and Russian which has tens of millions of speakers.
- This is due to only considering languages for which the community had enough expertise to manually inspect the data quality, deduplicate, and remove personally identifiable information (Laurençon et al., 2022;Kreutzer et al., 2022).
- Other reasons include the limited availability of unlabeled text (Joshi et al., 2020) and the consideration of the curse of multilinguality (Conneau et al., 2020), where a trade-off exists between the number of languages in pretraining data, model capacity, and the downstream performance for each individual language.
- Finally, it is challenging to anticipate any potential usage of language models in advance. Therefore, it is important to study a posteriori language adaptation to new languages.
- While previous work has investigated different language adaptation strategies (Pfeiffer et al., 2020;Ansell et al., 2022;Parović et al., 2022), most work focuses on small language models such as mBERT (Devlin et al., 2019;Pires et al., 2019) and XLM-Roberta (Conneau et al., 2020). Our work shows that some previous findings (Ebrahimi and Kann, 2021) do not necessarily generalize when language models scale up.
- Furthermore, thus far little work has explored the effects of language adaptation on prompting, which has gained recent popularity as prompting allows language models to generalize to a wide range of tasks with significantly less training cost and data than full finetuning (Liu et al., 2021;Le Scao and Rush, 2021).
- Studying the relationship between language adaptation and prompting can hugely benefit many languages that lack large amounts of labeled data.
- We focus on language adaptation of BLOOM models to support eight new languages (German, Russian, Bulgarian, Thai, Turkish, Greek, Korean, and Guarani) and evaluate their zero-shot prompting on various NLU tasks after adaptation. The new languages cover both seen and unseen scripts in the pretraining data, and they differ in their language families and word orders.
- We benchmark existing language adaptation methods, such as continued pertaining and MAD-X (Pfeiffer et al., 2020), as well as a state-of-the-art parameter-efficient transfer learning method, (IA) 3 (Liu et al., 2022). Our work is the first to explore the scaling effects of language adaptation strategies for language models with billions of parameters.
- Contrary to previous work on small multilingual masked language models (Ebrahimi and Kann, 2021), we recommend finetuning adapters instead of continued pretraining for BLOOM with at least 3 billion parameters for better prompting performance.
- We further connect this recommendation to the way the quality of language independent representation scales with model parameters.
- We also demonstrate the positive effects of monolingual language adaptation on the prompting performance of BLOOM on various datasets.
- BLOOMZ is a variant of BLOOM that is produced by finetuning BLOOM on a multitask mixture in the same languages seen during pretraining. We find that simply adding a new language in the multitask finetuning is effective in improving performance in the new language.

## Related Work
- Continued pretraining of the model (restricted to the embedding layer training only in some cases)
- Training of language-specific adapters (Pfeiffer et al., 2020(Pfeiffer et al., 2021a,b;,b;Philip et al., 2020;Üstün et al., 2021;Berard, 2021;Faisal and Anastasopoulos, 2022;Parović et al., 2022) for the target language; and
- Training of a sparse subset of model parameters (Ansell et al., 2022).

### BLOOM pretrained models
- We focus on adding language support to the BLOOM language model
- BLOOM has a decoder-only Transformer architecture that uses AliBi positional embeddings and layer normalization after embedding layers
- Its tokenizer is trained with byte-level Byte Pair Encoding (BPE) algorithm
- BLOOM is pretrained for around 350 billion tokens on the ROOTS corpus

### New Languages
- We consider all six languages of XNLI (Conneau et al., 2018) that are currently unsupported by BLOOM
- Table 1 summarizes the unseen languages used in our experiments.

### Language Adaptation Strategies
- Carries out three language adaptation strategies to analyze their effects on zero-shot prompting
- Continued Pretraining refers to continually training the BLOOM model with its causal language modeling pretraining objective on monolingual text of the new language
- MAD-X uses the language adapter and the invertible adapter of the MAD-X configuration (Pfeiffer et al., 2020) to adapt BLOOM to new languages
- Language adapter refers to the bottleneck adapter with down-and up-projection feedforward layers (Houlsby et al., 2019;Pfeiffer et al., 2021a) that are inserted into each Transformer block. The invertible adapter is used in the embedding layers to mitigate the mismatch between the original and new language vocabularies.
- (IA) 3 is a parameter-efficient finetuning method that performs element-wise rescaling of inner Transformer block activations through learnable vectors (Liu et al., 2022). These vectors can be merged with the original pretrained weights of a model at inference to reduce latency by avoiding passing the activations through additional adapter modules.

### Language Adaptation Setting
- We randomly sample 100K samples from the deduplicated OSCAR subcorpora (Ortiz Suárez et al., 2019) of the respective languages for language adaptation.
- Since Guarani only has around 100 samples in OSCAR, we use Jojajovai parallel corpora (Chiruzzo et al., 2022), which contains 30K Guarani sentences.
- We perform 25K language adaptation training steps using a batch size of 8 and the sequence length of 1,024.
- We do not retrain the tokenizer as BLOOM uses byte-level BPE tokenization, which never produces unknown tokens; therefore, we can perform language adaptation without extending the vocabulary.
- We adapt the embedding layer in two different fashions. For continued pretraining, we make the embedding layer trainable. This follows prior work on language adaptation (Pfeiffer et al., 2020;Chau et al., 2020;Ebrahimi and Kann, 2021;Fujinuma et al., 2022).

### Tasks and Prompt Templates
- The models are evaluated on five multilingual NLU tasks
- The models are reused without any task-specific finetuning
- The translations are done using automatic translation APIs

### Baselines
- We compare the adapted BLOOM model against generative multilingual language models which have reported state-of-the-art prompting performance.
- We report the prompting performance of the original BLOOM models without any adaptation.
- Among the baselines, XGLM, mGPT, and mT0 have seen all the new languages in Table 1 except Guarani during model pretraining.

## Results and Discussion

### Zero-shot Prompting Performance
- Figure 1: Language adaptation improves the original BLOOM's zero-shot prompting for unseen languages.
- In general, language adaptation follows the scaling law which dictates that performance gains correlate with model sizes.
- When the BLOOM transformer model becomes wider (from 560M to 1.7B parameters), certain tasks such as German XNLI and PAWSX experience performance drops.
- For the smallest BLOOM model with 560 million parameters, we see that continued pretraining yields the best prompting performance.
- Our result supports Ebrahimi and Kann's (2021) findings that continued pretraining of masked language models of similar size, such as mBERT and XLM-Roberta, gives better NER and POS tagging performance than adapters.
- However, when model sizes increases beyond 3 billion parameters, adapter-based language adaptation methods outperform continued pretraining despite having fewer trainable parameters.
- Furthermore, contrary to previous findings (Yong and Nikoulina, 2022), BLOOM adapts well to new languages regardless of their language family, word order, and whether they share the same script system with languages in pretraining data (Figure 2).
- We note that there are many differences in Yong and Nikoulina's (2022) setting. Yong and Nikoulina (2022) trained a multilingual model that uses learned positional embeddings instead of Alibi (Press et al., 2022).
- We find that the adapted BLOOM matches mGPT's performance in several XNLI tasks and even outperforms XGLM and mT0 on the German PAWS-X and Russian XWinograd tasks.
- Nonetheless, mT0, which has seen the languages during pretraining and is trained on a multilingual task prompts mixture, exhibits the best zero-shot prompting performance when model parameters are increased.
- We find the adapted BLOOM performs poorly on Guarani, which is a truly low-resource language.

### Perplexity
- Perplexity can be viewed as a measure of uncertainty when predicting the next token in a sequence
- Better language modeling ability means lower perplexity
- Figure 4 shows that evaluation perplexity on Russian texts for continued pretraining and MAD-X language adapters
- We find that perplexity during language adaptation training does not necessarily correlate with prompting performance
- While perplexity becomes lower for larger models, there is a drop in XWinograd per-  formance for both language adaptation strategies when the model capacity increases from 1.1 billion to 1.7 billion parameters
- Even though continued pretraining has a lower perplexity than MAD-X language adapters, which suggests that continually-pretrained models better model the Russian OSCAR data, continually-pretrained BLOOM underperform their counterparts for larger model sizes in both XWinograd and XNLI tasks

### Connection to Language Independent Representation
- Figure 5 reports sentence retrieval (SR) accuracy for Russian for non-adapted models, as well as models adapted via MAD-X adapters or continued pretraining.
- For BLOOM adapted with MAD-X, SR accuracy improves as model grows in parameters.
- The reason is that adapters' trainable parameters grow in size so they represent Russian sentences better and larger model start from better representations of both languages.
- Interestingly, for continued pretraining, the best SR accuracy result is achieved with the smallest BLOOM model with 560 million parameters, while larger models achieve much lower SR accuracy.
- This phenomenon goes against the scaling law and is opposite to what has been observed for MAD-X.
- Previous works (Dufter and Schütze, 2020) suggest that smaller model would emerge better language-independent representations as it is forced to reuse the same parameters for different languages.
- However, when model grows it has more freedom to partition its' parameters between languages.
- Note that this observation has been made in the synthetic settings and to the best of our knowledge has not been confirmed in real multilingual models.
- Our results in Figure 5 could be seen as an additional support to that initial hypothesis.
- When doing continued pretraining with relatively small set of the language adaptation data, there are many ways for the model to optimize it's performance (cf Lottery ticket hypothesis (Frankle and Carbin, 2019)).
- If the model had more freedom to partition its' parameters between different languages, there is no guarantee that the continued pretraining would leverage English-related parameters and therefore could diverge its representation space further away from English.
- We hypothesize that this could be a possible explanation of degradation of continued pretraining sentence retrieval accuracy for larger models.

### Adapters' Capacity
- We investigate the effect of the size of adapters' capacity by varying the reduction factor (Rücklé et al., 2021) in the adapter's bottleneck layer.
- Contrary to Yong and Nikoulina (2022), we observe a positive correlation between the amount of adapters' parameters and prompting performance (see Figure 7).

### Placement of Adapters
- In this experiment, we examine how adapters' placement impacts the overall performance.
- For this, we keep a single adapter at different layers of the model, where we increase the bottleneck size in a way to match the same parameter count of the model with a full set of adapters.
- Figure 8 compares adapter placement results on XNLI task. We note that layers in the middle benefit less from the language adaptation, and the last layers benefit most from the language adaptation.
- Invertible Adapters We analyze the performance of MAD-X with and without invertible adapters, which is used to adapt the embedding layer of BLOOM, on prompting for natural language inference tasks.
- Figure 9 shows that invertible adapters only improve performance for German, Bulgarian, and Turkish. This implies that the prompting performance gain from language adaptation mainly results from adapting the Transformer blocks.
- We also perform language adaptation with continued pretraining and MAD-X language adapters on a randomly initialized BLOOM.
- Figure 10 shows that, without pretraining, the adapted BLOOM model behaves like a random classifier on the XNLI task.
- Our results confirm that knowledge transfer takes place during language adaptation of pretrained models.
- We also investigate language adaptation strategies for BLOOMZ, which is BLOOM finetuned on many different task prompts to achieve better crosslingual and cross-task generalization (Muennighoff et al., 2022).
- Similar to adapting BLOOM, we train MAD-X language adapters for BLOOMZ using the same experimental setting on monolingual OSCAR data.
- In Figure 11, we show that BLOOMZ-560M has a median accuracy of around 38.5% for the German XNLI tasks (left bar), but after language adaptation, it performs the worst with an accuracy as poor as a random classifier at 33% (right bar). However, when equipped with BLOOM's language adapters (this is possible because BLOOM and BLOOMZ share the same architecture), BLOOMZ retains its prompting ability (middle bar).
- The result suggests that BLOOMZ loses its prompting capability gained from multitask instruction tuning after language adaptation on the free-form text of monolingual OSCAR corpora.
- We experiment with learning a new language during instruction tuning using the same recipe as BLOOMZ (Muennighoff et al., 2022).
- We use Russian, which BLOOM models have not intentionally seen during pretraining.
- We collect supervised natural language task data in Russian and finetune the pretrained 7.1 billion parameter BLOOM model to create two variants: (a) BLOOMZ-7.1B-RU, which is finetuned only on the Russian task data, and (b) BLOOMZ-7.1B-xP3RU, which is finetuned on the full xP3 dataset (Muennighoff et al., 2022) with Russian data added to it.
- We compare the two models with .
- We find that finetuning on only Russian (BLOOMZ-7.1B-RU) without the other languages and tasks in the xP3 mixture shows only tiny improvements over the pretrained baseline on XSto-ryCloze. This is likely due to the lack of diversity in the finetuning of BLOOMZ-7.1B-RU (Chung et al., 2022), as the Russian-only split contains fewer tasks and prompts than the full xP3 dataset.
- When adding Russian to the instruction tuning mixture (BLOOMZ-7.1B-xP3RU), the performance...
