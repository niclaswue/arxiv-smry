---
title: "GPT Takes the Bar Exam"
date: 2022-12-29T18:19:43.000Z
author: "Michael Bommarito II, Daniel Martin Katz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2212-14402v1_4q6GpY1NDl.jpg" # image path/url
    alt: "GPT Takes the Bar Exam" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14402)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14402).


# Abstract
- Nearly all jurisdictions in the United States require a professional license
- To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including
- three years at an accredited law school
- In addition, most test-takers also undergo weeks to months of further, exam-specific preparation
- Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try
- In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in "AI?"
- In this research, we document our experimental evaluation of the performance of OpenAI's `text-davinci-003` model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam
- While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance
- For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that an LLM will pass the MBE component of the Bar Exam in the near future.

# Paper Content

## Introduction
- The legal system is becoming increasingly complex
- Artificial intelligence and process engineering have promised help for decades to both non-professional and professional users of legal systems
- Significant research and development has gone into use cases like search and legal aid for laypeople, automated argumentation or brief construction, pre-and post-execution contract processes, due diligence and e-discovery, and judicial analysis
- However, the complexity of legal language and vastness of legal knowledge has made it historically difficult to develop systems that understand the nuances of legal tasks
- As noted in [9], law is a field which is heavily reliant on the use of language, producing massive volumes of textual data
- Documents such as briefs, memos, statutes, regulations, contracts, patents, and judicial decisions are continuously authored by lawyers, judges, and regulators
- To make matters even more difficult, legal language is notoriously complex; lawyers and other legal professionals undertake nearly a decade of education and professional training to understand and generate it
- The age of neural NLP can be traced to [13], which has been followed by successive waves of embedding [14] [15] and transformer-based large language models (LLMs) [16] [17] [18] [19] [20]
- In particular, transformer architectures, first introduced in [25], have revolutionized machine learning research, and have been most successfully applied to text and image modalities
- As of this publication, OpenAI's APIs offers text completion, code completion, image generation, and embedding generation endpoints
- In recent weeks, OpenAI has also released a public-facing chatbot version of GPT-3.5 known as ChatGPT, which reportedly resulted in over 1M user signups within six days of release

## Data
- The Bar Exam is a standardized test used by many jurisdictions in the United States to assess a law applicant's knowledge and ability to answer exam-specific questions.
- The Bar Exam is composed of three components: a multiple choice test, an essay test, and a performance test.
- The multiple choice component, the Multistate Bar Examination or MBE, is typically worth 50% of an overall bar exam score.
- The MBE is also scaled by jurisdictions and the NCBE after each exam window; for example, a raw score of roughly âˆ¼ 60% may yield an approximate scaled score of 133, which would be enough to pass in a significant number of jurisdictions, including New York, Illinois, and the District of Columbia.
- Questions on the MBE are designed to test both legal knowledge and reading comprehension skills, requiring above-average semantic and syntactic command of the English language.
- In order to pass the Bar Exam, the typical applicant is required to complete at least seven years of post-secondary education, including a four-year bachelors degree and successful completion of three years of study at an ABA-accredited law school.
- Following graduation from law school, most applicants also invest substantial amounts of time and money into post-graduation Bar preparation training.
- The NCBE also maintains statistical information regarding exam performance.
- For comparison, we show their reported average accuracy of students by question category in Table 1.

## Methods
- The text completion API was used to provide prompts to a machine learning model
- Prompts were tested with different modes, including single choice, top two choices, and rank order
- Prompts that improved model performance were found to be those that combined non-entailment performance with probabilistic entailment and recall

### (Hyper)parameters for GPT-3
- Machine learning and computational research are highly sensitive to model parameters or hyperparameters
- In this research, we evaluated how hyperparameters like model "temperature" impacted the performance of the model
- We tested values in {0.0, 0.25, 0.5, 0.75, 1.0} for temperature
- We tested values in {0.75, 1.0} for top p
- We tested values in {1, 2, 4} for best of
- We tested values in {16, 32} for max tokens

### Fine-tuning
- LLMs like GPT-3.5 have received so much interest in part because their zero-shot or few-shot performance is so good.
- OpenAI does make some retraining or "fine-tuning" capabilities available through its API, and these API endpoints do allow for some control of the training process like learning rates or batch sizes.
- We did attempt to fine tune text-davinci-003 by providing it with 200 unseen, simulated MBE bar exam questions with correct and incorrect explanations.
- We provided the training samples both with and without explanatory text from the answer guide.
- In total, we trained six fine-tuned models, altering training prompts, training responses, batch size, learning rate, and prompt weighting.
- However, in all cases, the fine-tuned model significantly underperformed text-davinci-003 itself.

## Results
- In total, 107 exams were executed, with the best performing prompt being prompt style #7 -rank-ordering of the top three choices -performed best
- This performance gap may be attributable to at least two issues: first, it is possible that GPT's poor performance corresponds to bodies of knowledge that were absent from its training data or removed during subsequent model compression or fine-tuning, or second, that the design of the questions may be responsible for poor performance
- GPT's second best answer is highly correlated with correctness, and in all categories, the top two answers exceed the baseline random chance rate of 50%

## Conclusion and Future Work
- GPT-3.5 outperforms the baseline rate of random guessing on the MBE portion of NCBE's model Bar Exam
- Its rank-ordering of possible choices is strongly correlated with correctness in excess of random chance, confirming its general understanding of the legal domain
- Overall, we find that GPT-3.5 significantly exceeds our expectations for performance on this task
