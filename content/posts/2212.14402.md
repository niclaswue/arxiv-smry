---
title: "GPT Takes the Bar Exam"
date: 2022-12-29T18:19:43.000Z
author: "Michael Bommarito II, Daniel Martin Katz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14402v1.webp" # image path/url
    alt: "GPT Takes the Bar Exam" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14402)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14402).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/gpt-takes-the-bar-exam).

# Abstract
- Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as "the Bar Exam," as a precondition for law practice.
- To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including
- In addition, most test-takers also undergo weeks to months of further, exam-specific preparation. Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try.
- In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in "AI?" In this research, we document our experimental evaluation of the performance of OpenAI's `text-davinci-003` model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam.
- While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance.
- For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that an LLM will pass the MBE component of the Bar Exam in the near future.

# Paper Content

## Introduction
- The legal system is becoming increasingly complex, leading to a need for technology to assist with the quantity, quality, and accessibility of legal services demanded by society.
- Artificial intelligence and process engineering have promised help for decades to both non-professional and professional users of legal systems.
- Significant research and development has gone into use cases like search and legal aid for laypeople, automated argumentation or brief construction, pre-and post-execution contract processes, due diligence and e-discovery, and judicial analysis.
- However, the complexity of legal language and vastness of legal knowledge has made it historically difficult to develop systems that understand the nuances of legal tasks, and many systems have failed to deliver desired results or reach adoption.
- As noted in [9], law is a field which is heavily reliant on the use of language, producing massive volumes of textual data.
- Documents such as briefs, memos, statutes, regulations, contracts, patents, and judicial decisions are continuously authored by lawyers, judges, and regulators.
- To make matters even more difficult, legal language is notoriously complex; lawyers and other legal professionals undertake nearly a decade of education and professional training to understand and generate it.
- The answer is likely two-fold. First, for both technical and cultural reasons, the grammar of legal language is significantly different than the grammar of normal language, featuring both highly-stylized customs and pedantically-precise phrasing.
- Second, by the very nature of common law and precedent, legal language is full of semantic nuance and history. Words like "security" that have common meaning in normal language often have different, context-specific meanings in legal language. Many words that do not occur at all in normal language, like "estoppel" or "indemnitor," occur regularly in legal corpora.
- This semantic depth and breadth traditionally required systems that interact with legal text to embed a large amount of domain-specific knowledge.
- As of this publication, OpenAI's APIs offers text completion, code completion, image generation, and embedding generation endpoints.
- In recent weeks, OpenAI has also released a public-facing chatbot version of GPT-3.5 known as ChatGPT, which reportedly resulted in over 1M user signups within six days of release.

## Data
- Professional licensure exams, like the Bar Exam, are common across professional fields
- The National Conference of Bar Examiners (NCBE) is the organization responsible for designing most of the bar examination materials used across the United States
- In order to be eligible, the typical applicant is required to complete at least seven years of post-secondary education, including a four-year bachelors degree and successful completion of three years of study at an ABA-accredited law school
- Following graduation from law school, most applicants also invest substantial amounts of time and money into post-graduation Bar preparation training
- Despite the incredible effort of the average test-taker, approximately one out of every five still fails to pass the exam on their initial attempt
- The UBE features three components: (i) a multiple choice test, (ii) an essay test, and (iii) scenario-based performance test
- The multiple choice component, referred to as the Multistate Bar Examination or MBE, is typically worth 50% of an overall bar exam score
- As the MBE is a single component of an exam, most jurisdictions do not require a minimum MBE score
- The MBE is also scaled by jurisdictions and the NCBE after each exam window; for example, a raw score of roughly âˆ¼ 60% may yield an approximate scaled score of 133, which would be enough to pass in a significant number of jurisdictions, including New York, Illinois, and the District of Columbia
- Questions on the MBE are designed to test both legal knowledge and reading comprehension skills, requiring above-average semantic and syntactic command of the English language
- In order to be eligible, the typical applicant must also pass a criminal background check
- The NCBE also maintains statistical information regarding exam performance
- For comparison, we show their reported average accuracy of students by question category in Table 1

## Methods
- The text completion API was used to provide prompts to a large language model.
- Prompts were tested that varied in terms of what was asked (single choice, single choice and explanation, top two choices only, top two choices and explanation, top two choices and re-prompt, rank order all choices, rank order top three choices), how the questions were asked (single choice, single choice and explanation, top two choices and explanation, top two choices and re-prompt), and how the model was asked (rank order all choices, rank order top three choices).
- Prompts that asked the model to rank order all four multiple choice answers improved model accuracy substantially.

### (Hyper)parameters for GPT-3
- Machine learning and computational research are highly sensitive to model parameters or hyperparameters
- In this research, we evaluated how hyperparameters like model "temperature" impacted the performance of the model
- We tested values in {0.0, 0.25, 0.5, 0.75, 1.0} for temperature
- We tested values in {0.75, 1.0} for top p
- We tested values in {1, 2, 4} for best of
- We tested values in {16, 32} for max tokens

### Fine-tuning
- LLMs like GPT-3.5 have received so much interest in part because their zero-shot or few-shot performance is so good.
- OpenAI does make some retraining or "fine-tuning" capabilities available through its API, and these API endpoints do allow for some control of the training process like learning rates or batch sizes.
- We did attempt to fine tune text-davinci-003 by providing it with 200 unseen, simulated MBE bar exam questions with correct and incorrect explanations.
- We provided the training samples both with and without explanatory text from the answer guide.
- In total, we trained six fine-tuned models, altering training prompts, training responses, batch size, learning rate, and prompt weighting.
- However, in all cases, the fine-tuned model significantly underperformed text-davinci-003 itself.

## Results
- In total, 107 exams were executed, with the best performing prompt being prompt style #7 -rank-ordering of the top three choices -performed best.
- This performance was compared to baseline student and passing rates, with GPT significantly exceeding the baseline random chance rate.
- In some categories, GPT's second best answer was highly correlated with correctness.
- GPT's top three answer performance was also strong overall, with the exception of Civil Procedure.

## Conclusion and Future Work
- GPT-3.5 significantly outperforms the baseline rate of random guessing on the MBE portion of NCBE's model Bar Exam
- Without any fine-tuning, it currently achieves a passing rate on two categories of the Bar and achieves parity with human test-takers on one
- Its rank-ordering of possible choices is strongly correlated with correctness in excess of random chance, confirming its general understanding of the legal domain
- Overall, we find that GPT-3.5 significantly exceeds our expectations for performance on this task
