---
title: "Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation"
date: 2022-12-19T17:06:58.000Z
author: "Nuno M. Guerreiro, Pierre Colombo, Pablo Piantanida, André F. T. Martins"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09631v1.webp" # image path/url
    alt: "Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09631)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09631).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/optimal-transport-for-unsupervised-1).

# Abstract
- Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications
- However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust
- It becomes thus crucial to implement effective preventive strategies to guarantee their proper functioning
- In this paper, we address the problem of hallucination detection in NMT by following a simple intuition: as hallucinations are detached from the source content, they exhibit encoder-decoder attention patterns that are statistically different from those of good quality translations
- We frame this problem with an optimal transport formulation and propose a fully unsupervised, plug-in detector that can be used with any attention-based NMT model
- Experimental results show that our detector not only outperforms all previous model-based detectors, but is also competitive with detectors that employ large models trained on millions of samples.

# Paper Content

## Introduction
- Neural machine translation (NMT) has achieved tremendous success (Vaswani et al., 2017;Akhbardeh et al., 2021), becoming the mainstream method in real-world applications and production systems for automatic translation.
- Although these models are becoming evermore accurate, especially in high-resource settings, they may unpredictably produce hallucinations.
- These are severely pathological translations that are detached from the source sequence content (Lee et al., 2018;Müller et al., 2020;Raunak et al., 2021;Guerreiro et al., 2022).
- Crucially, these errors have the potential to seriously harm user trust in hard-to-predict ways (Perez et al., 2022), hence the evergrowing need to develop security mechanisms.
- One appealing strategy to address this issue is to develop effective on-the-fly detection systems.
- In this work, we focus on leveraging the encoderdecoder attention mechanism to develop an on-the-fly hallucination detector.
- This mechanism is responsible for selecting and combining the information contained in the source sequence that is relevant to retain during translation.
- Thus, as hallucinations are translations which content is detached from the source sequence, it is no surprise that connections between anomalous attention patterns and hallucinations have been drawn before in the literature.
- Those patterns usually exhibit very scattered source attention mass across the different tokens in the translation (e.g. most source attention mass is concentrated on a few irrelevant tokens such as punctuation and the end-of-sequence token; Berard et al. 2019).
- Inspired by such observations, previous work has designed ad-hoc heuristics to detect hallucinations that specifically target the anomalous maps.
- While such heuristics can be used to detect hallucinations to a satisfactory extent (Guerreiro et al., 2022), we argue that a more theoretically-founded way of using anomalous attention information for hallucination detection is lacking in the literature.
- Rather than aiming at finding particular patterns, we go back to the main definition of hallucinations and draw the following hypothesis: as hallucinations-contrary to good translations-are not supported by the source content, they may exhibit encoder-decoder attention patterns that are different from those found in good quality translations.
- Based on this hypothesis, we approach the problem of hallucination detection as a problem of anomaly detection with an optimal transport (OT) formulation (Kantorovich, 2006;Peyré et al., 2019).
- Namely, we aim to find translations with source attention mass distributions that are highly distant from those of good translations. Intuitively, the more distant a translation's attention patterns are from those of good translations, the more anomalous it is in light of that distribution.
- Our key contributions can be summarized as follows:
- We propose an OT-inspired fully unsupervised hallucination detector that can be plugged into any attention-based NMT model;
- We find that it is possible to leverage the idea that attention maps for hallucinations are anomalous in light of a reference data distribution to effectively detect hallucinations;
- We show that our detector not only outperforms all previous unsupervised model-based detectors, but is also competitive with detectors that employ large models that have been trained on millions of samples.

## Background

### Neural Machine Translation with Transformer Models
- The autoregressive NMT model defines a probability distribution over an output space of hypotheses conditioned on a source sequence contained in an input space.
- In this work, we focus on models parameterized by an encoder-decoder transformer model with a set of learned weights.
- The multi-head encoder-decoder attention mechanism is responsible for computing, for each generation step, a distribution over all source sentence words that informs the decoder on the relevance of each word in the source sentence for the word to be translated in that step.
- Concretely, for a source sequence of arbitrary length and a target sequence of arbitrary length, given as input a matrix and matrices, the scaled dot-product attention at a single head is computed as:
- Following previous work in the literature, we will focus on the last layer encoder-decoder attention.

### Optimal Transport Problem and Wasserstein Distance
- The first-order Wasserstein distance between two probability distributions is defined
- The Wasserstein distance arises from the method of optimal transport
- The Wasserstein-1 distance is obtained with the choice c(u, v) = u − v 1

### Hallucinations in NMT
- Hallucinations are translations that lie at the extreme end of NMT pathologies
- By definition, these translations contain content that is detached from the source sentence
- To disentangle the different types of hallucinations, they can be categorized as: largely fluent detached hallucinations or oscillatory hallucinations
- The former are translations that bear little or no relation at all to the source content and may be further split according to the severity of the detachment (e.g. strong or full detachment) while the latter are inadequate translations that contain erroneous repetitions of words and phrases
- We illustrate in Appendix A the categories described above through examples of hallucinated outputs found in the hallucinations dataset introduced in Guerreiro et al.

## Detection of Hallucinations in NMT

### Categorization of On-the-Fly Detectors
- On-the-fly hallucinations detectors are systems that can detect hallucinations without access to reference translations
- These detectors are particularly relevant for real-world applications where references are not readily available
- In this work, we propose a new model-based detector that achieves even greater improvements over all previously proposed detectors

### Problem Statement
- In order to build a hallucination detector, one must find a scoring function and a threshold
- Anomaly scoring functions are used to determine if a translation is normal or hallucinatory
- A hallucination is defined as a translation that generates an anomaly score

## Unsupervised Hallucination Detection with Optimal Transport
- Anomalous attention maps have been connected to the hallucinatory mode in several works (Lee et al., 2018;Raunak et al., 2021;Guerreiro et al., 2022).
- Our method builds on this idea and uses the Wasserstein distance (see Section 2.2) to estimate the cost of transforming a translation source mass distribution into a reference distribution.
- Intuitively, the higher the cost of such transformation, the more distant-and hence the more anomalousthe attention of the translation is with respect to that of the reference translation.
- In this scenario, we only rely on the generated translation and its source mass distribution to decide whether the translation is an hallucination or not.
- For a given test sample x ∈ X : 2. We then compute an anomaly score, s wtu (x), by measuring the Wasserstein distance between π M (x) and a reference distribution u:
- A natural choice for u is the uniform distribution, u = 1 n • 1, where 1 is a vector of ones of size n.
- Choice of cost function. We consider the 0/1 cost function, c(i, j) = 1[i = j], as it guarantees that the cost of transporting a unit mass from any token i to any token j = i is constant.
- For this distance function, the problem in Equation 2 has the following closed-form solution (Villani, 2009):
- This is a well-known result in optimal transport: the Wasserstein distance under the 0/1 cost function is equivalent to the total variation distance between the two distributions.
- On this metric space, the Wasserstein distance depends solely on the probability mass that is transported to transform π M (x) to u.
- Importantly, this formulation ignores the starting locations and destinations of that probability mass as the cost of transporting a unit mass from any token i to any token j = i is constant.
- Interpretation of Wass-to-Unif.
- Our method is inspired by observations made in Raunak et al.  2022): attention maps for which the source attention mass is highly concentrated on a very sparse set of tokens (regardless of their location in the source sentence) can be predictive of hallucinations.
- Thus, the bigger the distance between the attention distribution of a test sample and the uniform distribution, the more peaked the former is, and the closer it is to maps that have been shown to be predictive of hallucinations.
- In this scenario, instead of using a single reference distribution, we use a set of reference source mass distributions, R x , obtained from the same model. This allows for a more accurate evaluation of how anomalous a given translation is compared to the data distribution, rather than relying on an arbitrary choice of reference distribution.
- First, we use a held-out dataset D held that contains samples for which the model M generates good quality translations according to an automatic evaluation metric (in this work, we use COMET (Rei et al., 2020a)).
- We use this dataset to construct (offline) a set of held-out source attention distributions R held = {π M (x) ∈ |x| : x ∈ D held }.
- Then, for a given test sample x ∈ X : 1. We generate a translation ŷ = (y 1 , . . . , y m ) and obtain the source mass attention distribution π M (x) ∈ |x| ;
- 2. We apply a length filter to construct the sample reference set R x , by restricting R x to contain source mass distributions of R held correspondent to translations of size 3.
- We compute pairwise Wasserstein-1 distances between π M...

### Model and Data
- The authors use a dataset of translations of news stories from the WMT'18 DE-EN language pair
- The authors use a machine learning model to produce translations of news stories
- The authors use the model to produce translations of news stories with hallucinations
- The authors provide full details about the dataset and the model in Appendix A

### Baseline detectors
- We compare our methods to the two best performing model-based methods in Guerreiro et al. (2022)
- This method consists in computing the proportion of source words with a total incoming attention mass lower than a threshold λ: This method was initially proposed in Berard et al. (2019) and is effective as a detector of fully detached hallucinations (Guerreiro et al., 2022).
- We follow their work and use λ = 0.2.
- Seq-Logprob. This method consists in computing the length-normalised sequence log-probability of the generated translation: This method has been shown to be surprisingly effective for detection of fully and strongly detached hallucinations (Guerreiro et al., 2022).
- Although the main focus of our work is plug-in methods that only rely on model-based features, we provide a comparison to external detectors. This comparison provides an estimate of the upperbound in performance that is helpful to monitor the development of new model-based detectors.
- CometKiwi. We compute the sentence-level quality assessments provided by the reference-free model CometKiwi (Rei et al., 2022), the winner of all tracks on the WMT'22 shared-task on Quality Estimation (Zerva et al., 2022).
- This model has been trained on nearly one million direct assessment annotations. Importantly, contrary to previous reference-free COMET models, the training data of CometKiwi includes human annotations from the MLQE-PE dataset (Fomicheva et al., 2022) that has been shown to include several low-quality translations and hallucinations (Specia et al., 2021;Tang et al., 2022).
- In Appendix C, we show that CometKiwi is significantly more predictive of hallucinations than the previous reference-free COMET versions (Rei et al., 2020b) studied in (Raunak et al., 2022;Guerreiro et al., 2022).

### Evaluation metrics
- We will use the Area Under the Receiver Operating Characteristic curve (AUROC) and the False Positive Rate at 90% True Positive Rate (FPR@90TPR) to evaluate the performance of different detectors.
- These metrics provide a more comprehensive view of the performance of an anomaly detector without being influenced by the choice of threshold.

### Implementation Details
- We use the library POT: Python Optimal Transport (Flamary et al., 2021)
- 6 Results

### Performance on on-the-fly detection
- Wass-Combo is the best model-based detector
- Table 1 shows that Wass-Combo outperforms most other methods both in terms of AUROC and FPR
- When compared to the previous best-performing unsupervised method (Seq-Logprob), Wass-Combo obtains boosts of approximately 4 and 10 points in AUROC and FPR, respectively
- These performance boosts are further evidence that model-based features can be leveraged, in an unsupervised manner, to build effective plug-in detectors
- Nevertheless, the high values of FPR suggest that (i) the task is difficult for model-based detectors, and (ii) there is a significant performance margin to further reduce in future research
- The notion of data proximity is helpful to detect hallucinations
- Table 1 shows that Wassto-Data outperforms the previous best-performing model-based method (Seq-Logprob) both in AU-ROC and FPR (by more than 10%). This supports the idea that encoder-decoder attention patterns for hallucinations are anomalous with respect to those of good model-generated translations, and that our method can effectively measure this level of anomalousness
- On the other hand, compared to Wass-to-Uni, Wass-to-Data shows a significant improvement of 30 FPR points. This highlights the effectiveness of leveraging the data-driven distribution of good translations instead of the ad-hoc uniform distribution.
- Nevertheless, Table 1 and Figure 2 show that combining both methods brings further performance improvements. This suggests that these methods may specialize in different types of hallucinations, and that combining them allows for detecting a broader range of anomalies.

### Do detectors specialize in different types of hallucinations?
- The lower the better.
- Wass-to-Data and Wass-Combo present the mean and standard deviation across five random seeds.
- For Wass-to-Data and Wass-Combo, we present the mean and standard deviation across five random seeds.
- Analysis on a fixed-threshold scenario shows that Wass-Combo outperforms Wass-to-Unif.
- Wass-Combo performs similarly to Wass-to-Unif, and can very easily separate most fully detached hallucinations from other translations on a fixed-threshold scenario.
- Note that the performance of Wass-to-Unif for fully detached hallucinations mirrors closely that of Attn-ign-SRC.
- In contrast, we find that CometKiwi appropriately penalize oscillatory hallucinations, which aligns with observations made in (Guerreiro et al., 2022).
- We offer further evidence that properties of hallucinations-in this case, the source attention mass distributions-are not only different to those of good-quality translations but also to most other model-generated translations.
- Length-filtering the distributions in R held boosts performance significantly.

## Conclusions
- We propose a novel plug-in model-based detector for hallucinations in NMT.
- Unlike previous at- tempts at building an attention-based detector, we do not design ad-hoc heuristics to detect hallucinations, and instead pose the problem of hallucination detection as an optimal transport problem: our detector aims at finding translations with source attention mass distribution that are highly distant from those of good quality translations.
- Through a rigorous comparison between different detectors, we show that our detector not only outperforms all previous model-based detectors, but is also competitive with detectors that use large, pre-trained models that have been trained on millions of samples.
- Importantly, our detector does not require any training data and, thanks to its flexibility, it can be easily deployed in real-world scenarios.
- We show the performance of both these versions in Table 6.
- CometKiwi significantly outperforms the previous iteration of COMET-QE. This hints that training quality estimation models with more negative examples can improve their ability to adequately penalize hallucinations.
- We perform ablations on Wass-to-Data and Wass-Combo for all relevant hyperparameters: the lengthfiltering parameter δ, the maximum cardinality of R, |R| max , the value of k to compute the Wass-to-Data scores (see step 4 in Figure 1), and the threshold for Wass-to-Unif scores to compute Wass-Combo scores.
- The results are shown in Table 7 to Table 10, respectively.
- We also report in Table 11 the performance of Wass-to-Data with a 0/1 cost function instead of the 1 −distance function.
- We show a qualitative analysis on the same fixedthreshold scenario described in Section 6.2 in Fig- ure 1.
- We perform ablations on Wass-to-Data and Wass-Combo for all relevant hyperparameters.
- The results are shown in Table 12.
