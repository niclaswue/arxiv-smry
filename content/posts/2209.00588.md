---
title: "Transformers are Sample Efficient World Models"
date: 2022-09-01T17:03:07.000Z
author: "Vincent Micheli, Eloi Alonso, François Fleuret"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2209-00588v1_Xpo73hOxv.jpg" # image path/url
    alt: "Transformers are Sample Efficient World Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.00588)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.00588).


# Abstract
- Deep reinforcement learning agents are notoriously sample inefficient
- Recently, many model-based methods have been designed to address this issue
- However, while virtually unlimited interaction with a simulated environment
- Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer
- With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games
- Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero

# Paper Content

## Introduction
- Deep reinforcement learning has become the dominant paradigm for developing competent agents in challenging environments
- Most notably, human experts were surpassed by deep RL algorithms in a multitude of arcade, real-time strategy, board, and imperfect information games
- However, a common drawback of these methods is their extremely low sample efficiency
- Experience requirements range from months of gameplay for DreamerV2 in Atari 2600 games to thousands of years for OpenAI Five in Dota2
- While some environments can be sped up for training agents, real-world applications often cannot
- Besides, additional cost or safety considerations related to the number of environmental interactions may arise
- Hence, sample efficiency is a necessary condition to bridge the gap between research and the deployment of deep RL agents in the wild
- Model-based methods constitute a promising direction to achieve data efficiency
- Recently, world models were leveraged in several ways: pure representation learning, lookahead search, and learning in imagination
- The latter approach is particularly appealing because training an agent inside a world model frees it from sample efficiency constraints
- However, this framework relies heavily on accurate world models since the policy is purely trained in imagination
- In a pioneering work, Ha and Schmidhuber [15] successfully built imagination-based agents in toy environments
- SimPLe recently showed promise in the more challenging Atari 100k benchmark
- The Transformer architecture [18] is now ubiquitous in Natural Language Processing, Computer Vision, and Offline Reinforcement Learning
- Transformers particularly shine when they operate over sequences of discrete tokens
- For textual data, one can easily build a vocabulary in multiple ways, but this conversion is not straightforward with images
- A naive approach would consist in treating pixels as image tokens, but standard Transformer architectures scale quadratically with sequence length, making this idea computationally intractable
- To address this issue, VQGAN [30] and DALL-E [31] employ a discrete autoencoder [32] as a mapping from raw pixels to a much smaller amount of image tokens. Combined with an autoregressive Transformer, these methods demonstrate strong unconditional and conditional image generation capabilities. Such results suggest a new approach to design world models.

## Method
- Formulates the problem as a Partially Observable Markov Decision Process (POMDP)
- Image observations x t ∈ R h×w×3 , discrete actions a t ∈ {1, . . . , A}, scalar rewards r t ∈ R, episode termination d t ∈ {0, 1}, discount factor γ ∈ (0, 1), initial observation distribution ρ 0 , and environment dynamics x t+1 , r t , d t ∼ p(x t+1 , r t , d t | x ≤t , a ≤t )
- Reinforcement learning objective is to train a policy π that yields actions maximizing the expected sum of rewards E π [ t≥0 γ t r t ]
- Uses the three standard components to learn in imagination: experience collection, world model learning, and behavior learning
- In the vein of Ha and Schmidhuber [15], Kaiser et al. [16], Hafner et al. [17,3], our agent learns to act exclusively within its model of the world, and we only make use of real experience to learn the environment dynamics
- Procedure to learn the policy and value functions in imagination is described in Section 2.3 of the paper

### From image observations to tokens
- The discrete autoencoder learns a symbolic language of its own to represent high-dimensional images as a small number of tokens
- The encoder E : R h×w×3 → {1, . . . , N } K converts an input image x t into K tokens from a vocabulary of size N
- The input image x t is first passed through a Convolutional Neural Network
- The encoder E produces output y t ∈ R K×d
- The decoder D : {1, . . . , N } K → R h×w×3 turns K tokens back into an image
- The discrete autoencoder is trained on previously collected frames, with an equally weighted combination of a L 2 reconstruction loss, a commitment loss, and a perceptual loss

### Modeling dynamics
- The Transformer G captures the environment dynamics by modeling the language of the discrete autoencoder over time.
- The central role of unfolding imagination is highlighted with the blue arrows in Figure 1.
- Specifically, G operates over sequences of interleaved frame and action tokens.
- An input sequence (z 1 0 , . . . , z K 0 , a 0 , z 1 1 , . . . , z K 1 , a 1 , . . . , z 1 t , . . . , z K t , a t ) is obtained from the raw sequence (x 0 , a 0 , x 1 , a 1 , . . . , x t , a t ) by encoding the frames with E, as described in Section 2.1.
- At each time step t, the Transformer models the three following distributions: Reward: Note that the conditioning for the k-th token also includes z <k t+1 := (z 1 t+1 , . . . , z k−1 t+1 ), the tokens that were already predicted, i.e. the autoregressive process happens at the token level.
- We train G in a self-supervised manner on segments of L time steps, sampled from past experience.
- We use a cross-entropy loss for the transition and termination predictors, and a mean-squared error loss for the reward predictor.

### Learning in imagination
- Together, the discrete autoencoder (E, D) and the Transformer G form a world model, capable of imagination
- The policy π, depicted with purple arrows in Figure 1, exclusively learns in this imagination MDP
- At time step t, the policy observes a reconstructed image observation xt and samples action a t ∼ π(a t |x t ).
- The world model then predicts the reward rt , the episode end dt , and the next observation xt+1 = D(ẑ t+1 ), with ẑt+1 ∼ p G (ẑ t+1 | z 0 , a 0 , ẑ1 , a 1 , . . . , ẑt , a t ).
- This imagination procedure is initialized with a real observation x 0 sampled from past experience, and is rolled out for H steps, the imagination horizon hyperparameter.
- We stop if an episode end is predicted before reaching the horizon.

## Experiments
- The Atari 100k benchmark is a well-established test for reinforcement learning algorithms
- In this work, we focus on the well-established Atari 100k benchmark
- We present the benchmark and its baselines in Section 3.1
- We describe the evaluation protocol and discuss the results in Section 3.2
- Qualitative examples of the world model's capabilities are given in Section 3.3

### Benchmark and Baselines
- Atari 100k consists of 26 Atari games
- By way of comparison, unconstrained Atari agents are usually trained for 50 million steps, a 500 fold increase in experience.
- Multiple baselines were benchmarked on Atari 100k.
- SimPLe [16] trains a policy with PPO [43] in a video generation model.
- CURL [34] develops off-policy agents from high-level image features obtained with contrastive learning.
- DrQ [35] augments input images and averages Q-value estimates over several transformations.
- SPR [13] enforces consistent representations of input images across augmented views and neighbouring time steps.
- The aforementioned baselines carry additional techniques to improve performance.
- Instead, IRIS does not rely on prioritized experience replay [44], epsilon-greedy scheduling, or data augmentations.
- We make a distinction between methods with and without lookahead search.
- Indeed, algorithms such as Monte Carlo Tree Search (MCTS) [6,7,2] can vastly improve agent performance, but they come at a premium in computational resources and code complexity.
- On the contrary, IRIS and the baselines above do not require distributed infrastructures and multiple GPUs.
- Moreover, IRIS could be combined with MCTS, both in imagination and in the real environment. Therefore, methods involving lookahead search should not be seen as direct competitors but rather as potential extensions to learning-only methods.
- MuZero [2] and EfficientZero [14] are the current standard for search-based methods.
- MuZero leverages MCTS as a policy improvement operator, by unrolling multiple hypothetical trajectories in the latent space of a world model.
- EfficientZero improves upon MuZero by introducing a selfsupervised consistency loss, predicting returns over short horizons in one shot, and correcting off-policy trajectories with its world model.

### Results
- The human normalized score is the established measure of performance in Atari 100k
- Agarwal et al. [46] showed that substantial discrepancies may arise between standard point estimates and interval estimates in RL benchmarks
- IRIS outperforms human players on 10 out of 26 games
- When we increase that amount, the world model is able to situate enemies, albeit at the cost of increased computation
- Breakout, Gopher, and Alien are examples of games difficult to simulate where the world model successfully internalizes the game mechanics

### World Model Analysis
- IRIS learns behaviors entirely in its imagination
- The quality of the world model is the cornerstone of our approach
- For instance, it is key that the discrete autoencoder correctly reconstructs elements like a ball, a player, or an enemy
- Similarly, the potential inability of the Transformer to capture important game mechanics, like reward attribution or episode termination, can severely hamper the agent's performance
- Hence, no matter the amount of imagined trajectories, the agent will learn suboptimal policies if the world model is flawed
- While Section 3.2 provides a quantitative evaluation, we aim to complement the analysis with qualitative examples of the abilities of the world model
- Figure 2 shows the generation of many plausible futures in the face of uncertainty
- Figure 4 depicts pixel-perfect predictions in Pong
- Finally, we illustrate in Figure 7 predictions for rewards and episode terminations, which are crucial to the reinforcement learning objective

## Related Work
- Learning in the imagination of world models has been investigated in tabular environments [12], [15], [16], [17], [18], [19], [49].
- DreamerV2 was the first agent learning in imagination to achieve human-level performance in the Atari 50M benchmark.
- Its world model combines a convolutional autoencoder with a recurrent state-space model (RSSM) [48] for latent dynamics learning.

### Reinforcement Learning with Transformers
- The standard Transformer architecture is difficult to optimize
- The authors propose to replace residual connections by gating layers to stabilize the learning procedure
- Our world model did not require such modifications, which is most likely due to its self-supervised learning objective

### Video generation with discrete autoencoders and Transformers
- VQGAN and DALL-E use discrete autoencoders to compress a frame into a small sequence of tokens
- Other works extend the approach to video generation
- GODIVA models sequences of frames instead of a single frame for text conditional video generation
- VideoGPT introduces video-level discrete autoencoders, and Transformers with spatial and temporal attention patterns, for unconditional and action conditional video generation

## Ethics Statement
- The development of autonomous agents for real-world environments raises many safety and environmental concerns.
- During its training period, an agent may cause serious harm to individuals and damage its surroundings.
- It is our belief that learning in the imagination of world models greatly reduces the risks associated with training new autonomous agents.
- Indeed, in this work, we propose a world model architecture capable of accurately modeling environments with very few samples.
- However, in a future line of research, one could go one step further and leverage existing data to eliminate the necessity of interacting with the real world.

## Conclusion
- IRIS is an agent that learns purely in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer
- IRIS sets a new state of the art in the Atari 100k benchmark for methods without lookahead search
- IRIS learns to play video games very well, outperforming MuZero
- IRIS's world model has the ability to generate rich gameplay experiences
- IRIS could be scaled up to computationally demanding and challenging tasks that would benefit from the speed of its world model

## B Actor-critic learning objectives
- Dreamer uses the generic λ-return, which balances bias and variance, as the regression target for the value network
- The value network V is trained to minimize L V , the expected squared difference with λ-returns over imagined trajectories
- To reduce the variance of REINFORCE gradients, we use the value V (x t ) as a baseline

## E Autoenconding frames with varying amounts of tokens
- The length of the input sequence of G is determined by the number of tokens K used to encode a single frame and the number of timesteps L in memory
- Increasing the number of tokens per frame results in better reconstructions, although it requires more compute and memory
- This tradeoff is particularly important in Atari games where enemies and players are moving in mazes with rewards to collect
- Due to the high number of possible configurations, the discrete autoencoder struggles to properly encode frames with only K = 16 tokens
- Indeed, sometimes the player, its enemies, or rewards are not correctly reconstructed, which severely hinders agent performance
- In Figure 10, we show that when increasing the number of tokens per frame to 64, the discrete autoencoder is perfectly capable of dealing with detailed environments such as Alien
- However, this increases the sequence length of G from 340 to 1300
- Therefore, with more computational resources, IRIS would most likely improve in these settings
