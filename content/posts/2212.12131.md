---
title: "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?"
date: 2022-12-23T03:57:54.000Z
author: "Byung-Doh Oh, William Schuler"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12131v1.webp" # image path/url
    alt: "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12131)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12131).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/why-does-surprisal-from-larger-transformer).

# Abstract
- Regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022).
- Analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.

# Paper Content

## Introduction
- Expectation-based theories of sentence processing posit that processing difficulty is largely driven by how predictable upcoming linguistic material is given its context.
- In cognitive modeling, predictability operationalized by information-theoretic surprisal (Shannon, 1948) has been shown to be a strong predictor of behavioral and neural measures of processing difficulty (Demberg and Keller, 2008;Smith and Levy, 2013;Hale et al., 2018;Shain et al., 2020), providing empirical support for this position.
- Recently, it was observed that surprisal from larger variants of the pre-trained GPT-2 LM (Radford et al., 2019) that have more parameters and achieve lower perplexity is less predictive of selfpaced reading times and eye-gaze durations collected during naturalistic reading (Oh et al., 2022).
- As the different variants of the pre-trained GPT-2 model share the primary architecture and training data, this offers an especially strong counterexample to previous work that showed a negative relationship between LM perplexity and predictive power of surprisal estimates (Goodkind and Bicknell, 2018;Hao et al., 2020;Wilcox et al., 2020).
- More broadly, this observation also contradicts the recent 'larger is better' trend of the NLP community, leaving open the question of why larger LMs perform worse.
- However, the Oh et al. (2022) results were part of a follow-up analysis in support of a separate claim about parser surprisal that only examined four model variants, so the results were not tested for statistical significance or extensively explored.
- The current work fills that gap by conducting a detailed linguistic analysis of the positive relationship between LM perplexity and predictive power of surprisal estimates.
- First, the robustness of the trend observed in Oh et al. (2022) was examined by reproducing their results and additionally evaluating surprisal estimates from different families of Transformer-based LMs (GPT-Neo, OPT; Black et al. 2021Black et al., , 2022;;Wang and Komatsuzaki, 2021;Zhang et al., 2022) on their ability to predict human reading times.
- Results from regression analyses show a strictly monotonic, positive log-linear relationship between LM perplexity and fit to reading times for the five GPT-Neo variants and eight OPT variants on two separate datasets, which provides firm empirical support for this trend.
- Subsequently, to provide an explanation for this positive relationship, residual errors from the regression models were analyzed with a focus on identifying linguistic phenomena that surprisal from larger variants accounts for less accurately compared to their smaller counterparts.
- The results show that regression models with surprisal predictors from GPT-2, GPT-Neo, and OPT models generally underpredict reading times at nouns and adjectives, and that the degree of underprediction increases along with model size.
- This indicates that the poorer fit to human reading times achieved by surprisal estimates from larger Transformer-based LMs is primarily driven by their characteristic of assigning lower surprisal values to open-class words, which may be accurately predicted by extensive domain knowledge gleaned from large sets of training examples that are not available to humans.
- This suggests that as Transformer-based LMs get larger, they may be problematic for cognitive modeling because they are trained with non-human learning objectives and different inductive biases on vast quantities of Internet text.

## Related Work
- There is a negative correlation between LM perplexity and fit to human reading times
- Goodkind and Bicknell (2018) compared surprisal estimates from a set of n-gram and LSTM LMs and observed a negative linear relationship between perplexity and regression model fit
- Wilcox et al. (2020) evaluated n-gram, LSTM, Transformer, and RNNG (Dyer et al. 2016) models and replicated the negative relationship, although they note a more exponential relationship at certain intervals
- Merkx and Frank (2021) provided further support for this trend using GRU and Transformer models with different numbers of layers
- Oh et al. (2022) observed a directly contradictory relationship to this using surprisal estimates from pre-trained GPT-2 models (Radford et al. 2019)
- Using self-paced reading times from the Natural Stories Corpus (Futrell et al., 2021) and go-past durations from the Dundee corpus (Kennedy et al., 2003), the authors calculated the increase in log-likelihood (∆LL) to a baseline linear-mixed effects (LME) model due to including a surprisal predictor
- Their results showed that surprisal from the largest XL variant made the smallest contribution to regression model fit, followed by the smaller Large, Medium, and Small variants in that order, revealing a robust positive correlation between LM perplexity and predictive power of surprisal estimates
- The same trend was replicated when unigram surprisal was included in the baseline, as well as when spillover effects were controlled for through the use of continuoustime deconvolutional regression (CDR; Shain and Schuler, 2021)

## Main Experiment: Predictive Power of Language Model Surprisal Estimates
- The positive correlation observed by Oh et al. (2022) and others generalizes to larger Transformer-based models
- Surprise predictors from different variants of the GPT-2, GPT-Neo, and OPT LMs were evaluated on self-paced reading times from the Natural Stories Corpus (Futrell et al., 2021) and go-past eyegaze durations from the Dundee Corpus (Kennedy et al., 2003).

### Response Data

### Predictors
- Surprise estimates were calculated from four different variants of GPT-2 models
- In addition to GPT-2 surprisal, this experiment also evaluates surprisal estimates from five variants of GPT-Neo models
- All of these LMs are decoder-only autoregressive Transformer-based models
- The model capacities of the three LM families are summarized in Table 1
- When a single word was tokenized into multiple subword tokens, negative log probabilities of subword tokens corresponding to w t were added together to calculate S(w t )

### Regression Modeling
- Baseline model: word length, word position, saccade length, fixation on previous word
- 17 full models: word length, word position, saccade length, fixation on previous word, LM surprisal predictor for each word type
- ∆LL: for each regression model

### Results
- On each corpus, similar subsets were identified as driving the trend of MSEs across different LM families.
- On the Natural Stories Corpus, these subsets were primarily determined by the word's syntactic category, such as named entity nouns, nouns before relativizers, attributive and predicative adjectives, and modals.
- The top subsets of the Dundee Corpus were similarly determined by syntactic cat- egory, such as named entity nouns, predicative and passivized adjectives, and single-word noun phrases (e.g. pronouns).
- Subsets defined by the syntactic structure of sentences were less commonly identified from the Natural Stories Corpus, with ends of center-embedded constituents spanning four or more words and words with high DLT costs emerging.
- From the Dundee Corpus, ends of center-embedded constituents, ends of first conjunct constituents (both overall and noun phrases specifically), and beginnings of adjectival noun phrases (e.g. family in a family size pack) were identified.
- Additionally, words preceding a sen-tential clause were identified, which corresponded to conjunctions and ends of adjuncts.
- On most of these subsets, the MSEs of each regression model were higher than those on the entire corpus, which indicates that the misprediction of reading times that pre-trained LM surprisal already has difficulty modeling is exacerbated as the models get larger.
- Subsets such as modals of Natural Stories and first conjunct NP endings of Dundee are counterexamples to this general trend.
- The average surprisal values 10 and SSEs from underpredicted and overpredicted data points in Figure 4 shed more light on the direction and magnitude of mispredictions from each regression model.
- For example, on the subset of named entities, which emerged as the top two subsets across all corpus-by-LM combinations, the larger LM variants show systematically higher SSEs due to underprediction.
- This strong discrepancy highlights a mismatch between human sentence processing and language modeling; named entity terms (e.g. Elvis Presley) have been shown to incur increased processing times compared to their common noun counterparts (e.g. a singer) due to various semantic associations that are retrieved (Proverbio et al., 2001;Wang et al., 2013).
- In contrast, for language models, named entity terms that typically consist of multiple tokens have high mutual information, making it easy for them to accurately predict subsequent tokens given the first (e.g. Presley given Elvis), resulting in especially lower surprisal estimates for larger LM variants.
- Similarly, across the two corpora and three LM families, the trend of MSEs for other nouns as well as adjectives appears to be consistently driven by more severe underpredictions from regression models containing surprisal estimates from larger LM variants.
- On these subsets, the difference in average surprisal values between the smallest and largest LM variants was typically above 2 bits, which is larger than the difference in log perplexity (i.e. corpus-level average surprisal, Figure 2) between these variants.
- This indicates that these subsets represent words that the larger LM variants predict especially accurately, which results in low surprisal estimates that deviate from human reading times.
- In contrast, the subset of modals on the Natural Stories Corpus identified for the GPT-2 and GPT-Neo models shows a completely opposite trend in which more severe overpredictions drive the overall trend of MSEs.
- This seems to be more due to the difference in the estimated regression coefficients rather than the difference in the LM surprisal estimates themselves.
- The average surprisal values on this subset show that their difference between the smallest and largest...

### Calculation of Residual Errors
- Seventeen LME models were used to generate predictions for all data points in the exploratory set of both self-paced reading times and go-past durations.
- A discrepancy between model likelihood and mean squared error (MSE) was found when the LME models were fitted to the Dundee Corpus.
- The by-word intercept was mostly responsible for the discrepancy.
- The residual errors from the newly fitted regression models were analyzed and the by-word intercept was removed.

### Annotation of Data Points
- Each data point in both corpora was associated with various word-and sentence-level properties that are thought to influence real-time processing
- Word-level properties reflect characteristics of the word that generally hold regardless of the surrounding context
- Named entities were identified as words that are part of a proper name
- Sentence-level properties capture the syntactic structure of sentences, either in terms of dependencies or hierarchical phrases
- Left-corner parsing was used to incrementally derive phrasal structures from a series of lexical match and grammatical match decisions at every word

### Iterative Slope-Based Analysis of Residual Errors
- Subsequently, based on the properties annotated in Section 4.2, subsets of data points that strongly drive the trend in Figure 2 were identified.
- To this end, the linear relationship between log perplexity and MSEs was used; subsets of data points that drive the general trend should show larger differences in MSE between regression models, or in other words, have negative slopes that are steeper than the corpus-level slope.
- Based on this idea, for every corpus-LM combination (i.e. {Natural Stories, Dundee} × {GPT-2, GPT-Neo, OPT}), a least-squares regression line was fitted between corpus-level log perplexity and MSEs of each subset defined by the properties outlined in Section 4.2.
- Subsequently, the subset with the steepest negative slope was identified.
- After excluding the identified subset, the above procedure was repeated to identify a new subset that showed the next strongest effect.
- For this analysis, only subsets that contained more than 1% of the data points in each corpus were considered at each iteration.
- Additionally, once the subsets of interest were identified, the data points in each subset were further separated according to whether the regression model underpredicted or overpredicted the target reading times.
- In order to identify whether the trend of MSEs is driven primarily by systematic underprediction or overprediction of reading times, the average surprisal from each LM variant and the sum of squared errors (SSE) were calculated for each subset.
- SSEs instead of MSEs were analyzed because different regression models had different numbers of underpredicted vs. overpredicted data points, and because points close to 0 can distort the MSEs and obscure the overall trend mispredictions.

## Discussion and Conclusion
- Multiple large pre-trained LMs were used to study the relationship between model perplexity and fit to human reading times.
- The trend of MSEs on these words was driven mainly by underpredictions of reading time delays, which were exacerbated as the larger LM variants predicted the words more accurately and assigned lower surprisal values.
- A post-hoc analysis of the residual errors from each regression model was conducted and it was found that the difference in MSEs between regression models containing surprisal predictors from different LM variants was especially large on nouns and adjectives.
- The 'more and more superhuman' predictions of larger LM variants observed in this work are consistent with findings from recent analyses of Transformer-based LMs.
