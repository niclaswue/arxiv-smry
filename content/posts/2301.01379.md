---
title: "A Succinct Summary of Reinforcement Learning"
date: 2023-01-03T22:17:55.000Z
author: "Sanjeevan Ahilan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-01379v1.webp" # image path/url
    alt: "A Succinct Summary of Reinforcement Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01379)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01379).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-succinct-summary-of-reinforcement-learning).

# Abstract
- Single-agent reinforcement learning is a subfield of machine learning that deals with
- The most common reinforcement learning problem is the minimization of a
- There are a number of different algorithms that can be used to solve
- One of the most well-known algorithms is the Q-learning algorithm
- There are also a number of other algorithms that are used in single-agent

# Paper Content

## Fundamentals 2.1 The RL paradigm
- Goal-directed learning through interaction
- Used to model classical (Pavlovian) and operant (instrumental) conditioning
- In neuroscience it has been used to model the dopamine system of the brain
- In economics, it relates to fields such as bounded rationality
- In engineering it has extensive overlap with the field of optimal control

## Agent and environment

## Observability
- The assumption underlying the formalism of Markov decision processes is that the environment is partially observable.
- This assumption underlies the formalism of a partially observable Markov decision process which we describe in Section 5.2.

## Markov processes and Markov reward processes
- A Markov process is a sequence of random states with the Markov property.
- A Markov Reward Process (MRP) is a sequence of random states with the Markov property and a reward function.
- The immediate expected reward in a given state is defined as: r(s) = s ′ P(s, s ′ )r(s, s ′ ).
- The discount factor γ ∈ [0, 1] is used to determine the present value of future rewards.
- Conventionally, a reward received k steps into the future is of worth γ k times what it would be worth if received immediately.

## Markov decision processes
- Single-agent RL can be formalized in terms of Markov decision processes (MDPs).
- The idea of an MDP is to capture the key components available to the learning agent; the agent's sensation of the state of its environment, the actions it takes which can affect the state, and the rewards associated with states and actions.
- An MDP extends the formalism of an MRP to include a finite set of actions on which both P and r depend.
- Discrete-time, infinite-horizon MDPs are described in terms of the 5-tuple S, A, P, r, γ where S is the set of states, A is the set of actions, P : S × A × S → [0, 1] is the state transition probability kernel, r : S ×A×S → R is the immediate reward function and γ ∈ [0, 1) is the discount factor.
- The expected immediate reward for a given state and action is defined as r(s, a) = s ′ P(s, a, s ′ )r(s, a, s ′ ), which we use for convenience subsequently.

## Policies, values and models
- A policy is the agent's behaviour function which denotes the probability of taking action a in state s.
- An agent may also act according to a deterministic policy µ : S → A.
- Given an MDP and a policy π, the observed state sequence is a Markov process S, P π . Similarly, the state and reward sequence is a MRP S, P π , r π , γ in which:
- Starting from any particular state s at time step t = 0, the value function v π (s) is a prediction of the expected discounted future reward given that the agent starts in state s and follows policy π: where r t+1 = r(s t , a t , s t+1 ) which is the solution of an associated Bellman expectation equation:
- In matrix form the Bellman expectation equation can be expressed in terms of the induced MRP: where v π ∈ R |S| and r π ∈ R |S| are the vector of values and expected immediate rewards respectively for each state under policy π.
- We can also define a Bellman expectation backup operator: which has a fixed point of v π .
- An action-value for a policy π can also be defined, which is the expected discounted future reward for executing action a and subsequently following policy π.
- Policies can be evaluated without directly knowing or estimating a model, using instead the directly sampled experience of the environment, an approach which is known as 'model-free'.
- A 'model-based' approach is also possible in which a model is used to predict what the environment will do next.
- The optimal value and state-value functions satisfy Bellman optimality equations:
- The Bellman optimality equation is non-linear with no closed form solution (in general). Solving it therefore requires iterative solution methods.

## Dynamic programming

## Prediction
- dynamic programming can be used to solve known MDPs
- in many cases the MDP is not directly known -instead an agent taking actions in the MDP must learn directly from its experiences
- one approach, known as 'model-free', seeks to solve MDPs without learning transitions or rewards
- for prediction, a key quantity to estimate in this setting is the expected discounted future reward
- a sampled estimate of this, starting from state s t , is known as the return
- Monte-Carlo (MC) methods seek to estimate this directly using complete episodes of experience
- introducing a learning rate α t , the agent's value function can therefore be updated according to 2
- Temporal-difference (TD) learning methods learn from incomplete episodes by bootstrapping
- for example, if learning occurs after a single step, this is known as TD(0), which has the following update
- where r t+1 + γv(s t+1 ) is known as the target
- TD(0) will converge to the solution of the maximum likelihood Markov model which best fits the data (again assuming a suitable sequential decrease in the learning rate)
- unlike MC methods, TD methods introduce bias into the estimated return as the currently estimated value function may be different from the true value function
- however, they generally have reduced variance relative to MC methods

## Control with action-value functions

## Value function approximation
- Deep neural networks are a common approach for approximating the value function or action-value function
- The policy gradient theorem enables model-free learning
- There are a variety of approaches for determining q π

## Baselines
- REINFORCE reduces variance
- introducing baselines reduces bias
- baselines can be approximated by the value function

## Compatible function approximation
- In the general case, our choice to approximate q π with q w introduces bias
- However, in the special case of a compatible function approximator we can introduce no bias and take steps in the direction of the true policy gradient
- This becomes possible when the critic's function approximator reaches a minimum in the mean-squared error

## Deterministic policy gradients
- Model-based RL agents learn to take actions directly from experiences, without ever modelling transitions in the environment or reward functions
- The key benefit is that if the agent can perfectly predict the environment 'in its head', then it no longer needs to interact directly with the environment in order to learn an optimal policy

## Model Learning
- Recall that MDPs are defined in terms of the 5-tuple S, A, P, r, γ .
- A natural starting point is to approximate the state transition function P η ≈ P and immediate reward function r η ≈ r.
- We can then use dynamic programming to learn the optimal policy for an approximate MDP S, A, P η , r η , γ , the performance of which may be worse than for the true MDP.
- Given a fixed set of experiences, a model can be learned using supervised methods.
- For predicting immediate expected scalar rewards, this is a regression problem whereas for predicting the distribution over next states this a density estimation problem.
- Given the simplicity of this framing, a range of function approximators may be employed, including neural networks and Gaussian processes.

## Combining model-free and model-based approaches
- Model-based RL is a highly active area of research
- Recent advances include MuZero and Dreamer
- Latent variables and partial observability allow for more flexible and accurate RL
- Model-free RL is also possible, using e.g. TD learning

## Latent variable models
- Hidden or 'latent' variables correspond to variables which are not directly observed but nevertheless influence observed variables and thus may be inferred from observation.
- In reinforcement learning, it can be beneficial for agents to infer latent variables as these often provide a simpler and more parsimonious description of the world, enabling better predictions of future states and thus more effective control.
- Latent variable models are common in the field of unsupervised learning.
- Given data p(x) we may describe a probability distribution over x according to: where θ x|z parameterises the conditional distribution x|z and θ z parameterises the distribution over z.
- Key aims in unsupervised learning include capturing high-dimensional correlations with fewer parameters (as in probabilistic principal components analysis), generating samples from a data distribution, describing an underlying generative process z which describes causes of x, and flexibly modelling complex distributions even when the underlying components are simple (e.g. belonging to an exponential family).

## Partially observable Markov decision processes
- A partially observable Markov decision process is a generalization of an MDP in which the agent cannot directly observe the true state of the system
- Formally, a POMDP is a 7-tuple S, A, P, r, O, Ω, γ where S is the set of states, A is the set of actions, P : S × A × S → [0, 1] is the state transition probability kernel, r : S × A × S → R is the reward function, O is the set of observations, Ω : S × A × O → [0, 1] is the observation probability kernel and γ ∈ [0, 1) is the discount factor.
- As with MDPs, agents in POMDPs seek to learn a policy π(s α t ) which maximises some notion of cumulative reward, commonly E π [ ∞ t=0 γ t r t+1 ].
- One approach to solving POMDPs is by maintaining a belief state over the latent environment state -transitions for which satisfy the Markov property. Maintaining a belief over states only requires knowledge of the previous belief state, the action taken and the current observation. Beliefs may then be updated according to:
- where η = 1/ s ′ Ω(o ′ |s ′ , a) s∈S P(s ′ |s, a)b(s) is a normalising constant.
- A Markovian belief state allows a POMDP to be formulated as an MDP where every belief is a state. However, in practice, maintaining belief states in POMDPs will be computationally intractable for any reasonably sized problem.
- In order to address this, approximate solutions may be used. Alternatively, agents learning using function approximators which condition on the past can construct their own state representations, which may in turn enable relevant aspects of the state to be approximately Markov.

## Deep reinforcement learning
- Artificial neural networks can be used to learn policies and value functions
- When deep networks are used, they are typically denoted as 'deep' and are typically trained on large amounts of data
- Convolutional neural networks are particularly effective at extracting useful features from images

### Experience replay
- An agent interacts with its environment and receives experiences that can be used for learning.
- The Mnih et al. (2013) deep Q-learning algorithm stores experiences in a replay buffer and samples them at a later point in time for learning.
- At each timestep, the method stores experiences e t = (s t , a t , r t+1 , s t+1 ) in a replay buffer over many episodes.
- After sufficient experience has been collected, Q-learning updates are then applied to randomly sampled experiences from the buffer.
- This breaks the correlation between samples, reducing the variance of updates and the potential to overfit to recent experience.
- Further improvements to the method can be made by prioritised (as opposed to random) sampling of experiences according to their importance.

### Target networks
- When using temporal difference learning with deep function approximators, stability can be an issue.
- After temporal difference updates, the approximated values of both current and target state can change, which can lead to instability.
- To address this, deep RL algorithms often make use of a separate target network that remains stable even whilst the standard network is updated.
- Alternatively, this transition is made more slowly using Polyak averaging.

## Experience replay
- An agent interacts with its environment and receives experiences that can be used for learning.
- The Mnih et al. (2013) deep Q-learning algorithm stores experiences in a replay buffer and samples them at a later point in time for learning.
- At each timestep, the method stores experiences e t = (s t , a t , r t+1 , s t+1 ) in a replay buffer over many episodes.
- After sufficient experience has been collected, Q-learning updates are then applied to randomly sampled experiences from the buffer.
- This breaks the correlation between samples, reducing the variance of updates and the potential to overfit to recent experience.
- Further improvements to the method can be made by prioritised (as opposed to random) sampling of experiences according to their importance.

## Target networks
- When using temporal difference learning with deep function approximators, stability can be an issue.
- After temporal difference updates, the approximated values of both current and target state can change, which can lead to instability.
- To address this, deep RL algorithms often make use of a separate target network that remains stable even whilst the standard network is updated.
- Alternatively, this transition is made more slowly using Polyak averaging.
