---
title: "Diffusion Models Beat GANs on Image Synthesis"
date: 2021-05-11T17:50:24.000Z
author: "Prafulla Dhariwal, Alex Nichol"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2105-05233v4.webp" # image path/url
    alt: "Diffusion Models Beat GANs on Image Synthesis" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2105.05233)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2105.05233).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/diffusion-models-beat-gans-on-image-synthesis).

# Abstract

# Paper Content

## Introduction
- Generative models can generate human-like natural language
- Generative models can generate infinite high-quality synthetic images
- Generative models can be used in a variety of ways, such as generating images from text prompts
- While these models are already powerful, there are still improvements that can be made
- In Section 3, we introduce simple architecture improvements that give a substantial boost to FID
- In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling
- We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples

## Background
- Diffusion models sample from a distribution by reversing a gradual noising process.
- In particular, sampling starts with noise x T and produces gradually less-noisy samples x T −1 , x T −2 , ... until reaching a final sample x 0 .
- Each timestep t corresponds to a certain noise level, and x t can be thought of as a mixture of a signal x 0 with some noise where the signal to noise ratio is determined by the timestep t.
- For the remainder of this paper, we assume that the noise is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations.
- A diffusion model learns to produce a slightly more "denoised" x t−1 from x t .
- Ho et al. [25] parameterize this model as a function θ (x t , t) which predicts the noise component of a noisy sample x t .
- To train these models, each sample in a minibatch is produced by randomly drawing a data sample x 0 , a timestep t, and noise , which together give rise to a noised sample x t (Equation 17).
- The training objective is then || θ (x t , t) − || 2 , i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation 26).
- It is not immediately obvious how to sample from a noise predictor θ (x t , t). Recall that diffusion sampling proceeds by repeatedly predicting x t−1 from x t , starting from x T . Ho et al. [25] show that, under reasonable assumptions, we can model the distribution p θ (x t−1 |x t ) of x t−1 given x t as a diagonal Gaussian N (x t−1 ; µ θ (x t , t), Σ θ (x t , t)), where the mean µ θ (x t , t) can be calculated as a function of θ (x t , t) (Equation 27).
- The variance Σ θ (x t , t) of this Gaussian distribution can be fixed to a known constant [25] or learned with a separate neural network head [43], and both approaches yield high-quality samples when the total number of diffusion steps T is large enough.
- Ho et al. [25] observe that the simple mean-sqaured error objective, L simple , works better in practice than the actual variational lower bound L vlb that can be derived from interpreting the denoising diffusion model as a VAE. They also note that training with this objective and using their corresponding sampling procedure is equivalent to the denoising score matching model from Song and Ermon [58], who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to produce high quality image samples.

### Improvements
- Following the breakthrough work of Song and Ermon [58] and Ho et al. [25], several recent papers propose improvements to diffusion models.
- Here we describe a few of these improvements, which we employ for our models.
- Nichol and Dhariwal [43] find that fixing the variance Σ θ (x t , t) to a constant as done in Ho et al. [25] is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize Σ θ (x t , t) as a neural network whose output v is interpolated as: Here, β t and βt (Equation 19) are the variances in Ho et al. [25] corresponding to upper and lower bounds for the reverse process variances. Additionally, Nichol and Dhariwal [43] propose a hybrid objective for training both θ (x t , t) and Σ θ (x t , t) using the weighted sum L simple + λL vlb . Learning the reverse process variances with their hybrid objective allows sampling with fewer steps without much drop in sample quality.
- We adopt this objective and parameterization, and use it throughout our experiments.
- Song et al. [57] propose DDIM, which formulates an alternative non-Markovian noising process that has the same forward marginals as DDPM, but allows producing different reverse samplers by changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any model θ (x t , t) into a deterministic mapping from latents to images, and find that this provides an alternative way to sample with fewer steps.
- We adopt this sampling approach when using fewer than 50 sampling steps, since Nichol and Dhariwal [43] found it to be beneficial in this regime.

### Sample Quality Metrics
- For comparing sample quality across models, we perform quantitative evaluations using the following metrics.
- Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class.
- One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS [3].
- To better capture diversity than IS, Fréchet Inception Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human judgement [42] as a version of FID that uses spatial features rather than the standard pooled features.
- They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure.
- Finally, Kynkäänniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).
- We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work [27,28,5,25].
- We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage.
- When comparing against other methods, we re-compute these metrics using public samples or models whenever possible.

## Architecture Improvements
- In this section, the authors conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.
- The UNet architecture, introduced by Ho et al. [25], is found to substantially improve sample quality over the previous architectures [58,33] used for denoising score matching.
- Song et al. [60] found that further changes to the UNet architecture improved performance on the CIFAR-10 [31] and CelebA-64 [34] datasets.
- We show the same result on ImageNet 128×128, finding that architecture can indeed give a substantial boost to sample quality on much larger and more diverse datasets at a higher resolution.
- We explore the following architectural changes: increasing depth versus width, holding model size relatively constant, increasing the number of attention heads, using attention at 32×32, 16×16, and 8×8 resolutions rather than only at 16×16, and using the BigGAN [5] residual block for upsampling and downsampling the activations, following [60].
- For all comparisons in this section, we train models on ImageNet 128×128 with batch size 256, and sample using 250 sampling steps.
- We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect.
- We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.
- We also study other attention configurations that better match the Transformer architecture [66]. To this end, we experimented with either fixing attention heads to a constant, or fixing the number of channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations.
- Table 2 shows our results, indicating that more heads or fewer channels per head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, so we opt to use 64 channels per head as our default.

### Adaptive Group Normalization
- We experiment with a layer that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation
- We define this layer as AdaGN(h, y) = y s GroupNorm(h) + y b , where h is the intermediate activations of the residual block following the first convolution
- We had already seen AdaGN improve our earliest diffusion models, and so had it included by default in all our runs
- In Table 3, we explicitly ablate this choice, and find that the adaptive group normalization layer indeed improved FID
- Both models use 128 base channels and 2 residual blocks per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling, and were trained for 700K iterations

## Classifier Guidance
- GANs make heavy use of class labels
- Class information is crucial to the success of these models
- In addition to employing well designed architectures, GANs for conditional image synthesis make use of class labels conditioned on gradients of a classifier
- We use classifiers to condition a diffusion model on input images

### Conditional Reverse Noising Process
- We start with a diffusion model with an unconditional reverse noising process.
- To condition this on a label y, it suffices to sample each transition according to where Z is a normalizing constant.
- It is typically intractable to sample from this distribution exactly, but Sohl-Dickstein et al. [56] show that it can be approximated as a perturbed Gaussian distribution.
- Here, we review this derivation.
- Recall that our diffusion model predicts the previous timestep x t from timestep x t+1 using a Gaussian distribution: Algorithm 1.
- Classifier guided diffusion sampling, given a diffusion model (µ θ (x t ), Σ θ (x t )), classifier p φ (y|x t ), and gradient scale s.
- We can assume that log φ p(y|x t ) has low curvature compared to Σ −1 . This assumption is reasonable in the limit of infinite diffusion steps, where ||Σ|| → 0.
- In this case, we can approximate log p φ (y|x t ) using a Taylor expansion around x t = µ as Here, g = ∇ xt log p φ (y|x t )| xt=µ , and C 1 is a constant.
- This gives We can safely ignore the constant term C 4 , since it corresponds to the normalizing coefficient Z in Equation 2.
- We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by Σg.

### Conditional Sampling for DDIM
- The derivation for conditional sampling is only valid for the stochastic diffusion sampling process
- To this end, we use a score-based conditioning trick adapted from Song et al.
- In particular, if we have a model θ (x t ) that predicts the noise added to a sample, then this can be used to derive a score function:
- We can now substitute this into the score function for p(x t )p(y|x t ):
- Finally, we can define a new epsilon prediction ˆ (x t ) which corresponds to the score of the joint distribution:
- We can then use the exact same sampling procedure as used for regular DDIM, but with the modified noise predictions ˆ θ (x t ) instead of θ (x t ).

### Scaling Classifier Gradients
- To apply classifier guidance to a large scale generative task, we train classification models on ImageNet.
- Our classifier architecture is simply the downsampling trunk of the UNet model with an attention pool [49] at the 8x8 layer to produce the final output.
- We train these classifiers on the same noising distribution as the corresponding diffusion model, and also add random crops to reduce overfitting.
- After training, we incorporate the classifier into the sampling process of the diffusion model using Equation 10, as outlined by Algorithm 1.
- In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1.
- When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.
- Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%.
- Figure 3 shows an example of this effect.
- To understand the effect of scaling classifier gradients, note that s • ∇ x log p(y|x) = ∇ x log 1 Z p(y|x) s , where Z is an arbitrary constant.
- As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x) s .
- When s > 1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent.
- In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.
- In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling p(x).
- It is also possible to train conditional diffusion models, p(x|y), and use classifier guidance in the exact same way.
- Table 4 shows that the sample quality of both unconditional and conditional models can be greatly improved by classifier guidance.
- We see that, with a high enough scale, the guided unconditional model can get quite close to the FID of an unguided conditional model, although training directly with the class labels still helps.
- Guiding a conditional model further improves FID.

## Results
- To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN classes: bedroom, horse, and cat.
- To evaluate classifier guidance, we train conditional diffusion models on the ImageNet model was available at this resolution, so we trained our own.
- Values are taken from a previous paper, due to lack of public models or samples.

### State-of-the-art Image Synthesis
- Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task.
- With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64×64.
- For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs.
- These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.

### Comparison to Upsampling
- We compare guidance to using a two-stage upsampling stack.
- Nichol and Dhariwal [43] and Saharia et al. [53] train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model.
- In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear).
- During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample.
- This greatly improves FID on ImageNet 256×256, but does not reach the same performance as state-of-the-art models like BigGAN-deep [43,53], as seen in Table 5.
- In Table 6, we show that guidance and upsampling improve sample quality along different axes.
- While upsampling improves precision while keeping a high recall, guidance provides a knob to trade   [43] combined with our architecture improvements, which we refer to as ADM-U.
- The base resolution for the two-stage upsampling models is 64 and 128 for the 256 and 512 models, respectively.

## Related Work
- Score-based generative models were introduced by Song and Ermon
- Ho et al. [25] found a connection between this method and diffusion models
- After this breakthrough work, many works followed up with more promising results
- Kong et al. [30] and Chen et al. [8] demonstrated that diffusion models work well for audio
- Jolicoeur-Martineau et al. [26] found that a GAN-like setup could improve samples from these models
- Song et al. [60] explored ways to leverage techniques from stochastic differential equations to improve the sample quality obtained by score-based models
- Song et al. [57] and Nichol and Dhariwal [43] proposed methods to improve sampling speed
- Nichol and Dhariwal [43] and Saharia et al. [53] demonstrated promising results on the difficult ImageNet generation task using upsampling diffusion models
- Goyal et al. [21] described a technique for learning a model with learned iterative generation steps
- Song et al. [60] uses a classifier to generate class-conditional CIFAR-10 images with a diffusion model
- Santurkar et al. [55] demonstrate that a robust image classifier can be used as a stand-alone generative model
- Grathwohl et al. [22] train a model which is jointly a classifier and an energy-based model

## Limitations and Future Work
- While diffusion models are an extremely promising direction for generative modeling, they are still slower than GANs at sampling time due to the use of multiple denoising steps
- One promising work in this direction is from Luhman and Luhman [37], who explore a way to distill the DDIM sampling process into a single step model. The samples from the single step model are not yet competitive with GANs, but are much better than previous single-step likelihood-based models.
- Future work in this direction might be able to completely close the sampling speed gap between diffusion models and GANs without sacrificing image quality.

## Conclusion
- We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs.
- Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks.
- In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity.
- These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling.
- Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis.
