---
title: "Scaling Language-Image Pre-training via Masking"
date: 2022-12-01T18:59:57.000Z
author: "Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-00794v1.webp" # image path/url
    alt: "Scaling Language-Image Pre-training via Masking" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.00794)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.00794).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/scaling-language-image-pre-training-via).

# Abstract
- We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP.
- Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint.
- It leads to a favorable trade-off between accuracy and training time.
- In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline.
- On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data.
- Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons.

# Paper Content

## Introduction
- Language-supervised visual pre-training, e.g., CLIP, has been established as a simple yet powerful methodology for learning representations.
- Pre-trained CLIP models stand out for their remarkable versatility: they have strong zeroshot transferability; they demonstrate unprecedented quality in text-to-image generation (e.g., [53,55]); the pretrained encoder can improve multimodal and even unimodal visual tasks.
- Like the role played by supervised pre-training a decade ago [40], language-supervised visual pre-training is new fuel empowering various tasks today.
- Unlike classical supervised learning with a pre-defined label set, natural language provides richer forms of supervision, e.g., on objects, scenes, actions, context, and their relations, at multiple levels of granularity.
- Due to the complex nature of vision plus language, large-scale training is essential for the capability of language-supervised models.
- For example, the original CLIP models [52] were trained on 400 million data for 32 epochs-which amount to 10,000 Ima-geNet [16] epochs, taking thousands of GPU-days [52,36].
- Even using high-end infrastructures, the wall-clock training With a high masking ratio of 50% or 75%, our FLIP method trains faster and is more accurate than its CLIP counterpart.
- All entries are benchmarked in 256 TPU-v3 cores. Training is done on LAION-400M for 6.4, 12.8, or 32 epochs, for each masking ratio. Accuracy is evaluated by zero-shot transfer on the ImageNet-1K validation set.
- The model is ViT-L/16 [20].
- As the CLIP baseline takes ∼2,500 TPU-days training, a speedup of 3.7× can save ∼1,800 TPU-days.

## Related Work
- Learning with masking
- Denoising Autoencoders with masking noise
- Masked Autoencoder (MAE) method
- Contrastive learning
- Language-supervised learning

## Method
- Image masking reduces the time complexity of image encoding to 1/2 (or 1/4)
- Text masking optionally reduces the time complexity of text encoding to 1/2 (or 1/4)
- The objective of the image/text encoders is to minimize a contrastive loss
- Unmasking is performed without reconstruction or decoder tuning
- The masking ratio can be set to 0% to continue pre-training

### Implementation
- CLIP is used
- Hyperparameters are in the appendix
- Image encoder is a ViT paper [20] with global average pooling
- Text encoder is a non-autoregressive Transformer [62]
- Pad or cut sequences to a fixed length of 32
- JAX is used
- TPU v3 infrastructures are used

## Experiments
- We first ablate the FLIP design.
- The image encoder is ViT-L/16 [20], and the text encoder has a smaller size as per [52].
- We train on LAION-400M [36] and evaluate zeroshot accuracy on ImageNet-1K [16] validation.
- Table 1 shows the ablations trained for 6.4 epochs.
- Fig. 1 plots the trade-off for up to 32 epochs [52].
- The results are benchmarked in 256 TPU-v3 cores, unless noted.
- Masking ratio.
- Table 1a studies the image masking ratios. Here we scale the batch size accordingly (ablated next), so as to roughly maintain the memory footprint.
- The 0% masking entry indicates our CLIP counterpart.
- Masking 50% gives 1.2% higher accuracy than the CLIP baseline, and masking 75% is on par with the baseline.
- Speed-wise, masking 50% or 75% takes only 0.50× or 0.33× wall-clock training time, thanks to the large reduction on FLOPs.
- Batch size.
- We ablate the effect of batch size in Table 1b.
- Increasing the batch size consistently improves accuracy.
- Notably, even using the same batch size of 16k, our 50% masking entry has a comparable accuracy (68.5%, Table 1b) with the 0% masking baseline (68.6%, Table 1a).
- It is possible that the regularization introduced by masking can reduce overfitting, partially counteracting the negative effect of losing information in this setting.
- With a higher masking ratio of 75%, the negative effect is still observed when keeping the batch size unchanged.
- Our masking-based method naturally encourages using large batches.
- There is little extra memory cost if we scale the batch size according to the masking ratio, as we report in Table 1a.
- In practice, the available memory is always a limitation for larger batches.
- For example, the setting in Table 1a has reached the memory limit in our high-end infrastructure (256 TPU-v3 cores with 16GB memory each).
- Text masking.
- Table 1c studies text masking.
- Randomly masking 50% text decreases accuracy by 2.2%.
- This is in line with the observation [29] that language data has higher information-density than images and thus the text masking ratio should be lower.
- As variable-length text sequences are padded to generate a fixed-length batch, we can prioritize masking out the padding tokens.
- Prioritized sampling preserves more valid tokens than randomly masking the padded sequences uniformly.
- It reduces the degradation to 0.4%.
- Unless specified, the baseline setting is: image masking is 50% (batch 32k) or 75% (batch 64k), text masking is 0%, and no unmasked tuning is used.
- While our text masking is more aggressive than typical masked language models (e.g., 15% in [18]), the overall speed gain is marginal. This is because the text encoder is smaller and the text sequence is short.
- The text encoder costs merely 4.4% computation vs. the image encoder (without masking).
- Under this setting, text masking is not a worthwhile trade-off and we do not mask text in our other experiments.
- Inference unmasking.
- By default, we apply our models on intact images at inference-time, similar to [29].
- While masking creates a distribution shift between training and inference, simply ignoring this shift works surprisingly well (Table 1d, 'w/o mask'),...
