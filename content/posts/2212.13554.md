---
title: "NeRN -- Learning Neural Representations for Neural Networks"
date: 2022-12-27T17:14:44.000Z
author: "Maor Ashkenazi, Zohar Rimon, Ron Vainshtein, Shir Levi, Elad Richardson, Pinchas Mintz, Eran Treister"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13554v1.webp" # image path/url
    alt: "NeRN -- Learning Neural Representations for Neural Networks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13554)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13554).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/nern-learning-neural-representations-for).

# Abstract
- Neural representations have been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos
- We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network
- Inspired by coordinate input of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights
- Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction
- In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process
- We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet
- Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations.

# Paper Content

## INTRODUCTION
- Neural networks have been found to be very effective at learning representations over a wide variety of domains
- Recently, NeRF (Mildenhall et al., 2020) demonstrated that a relatively simple neural network can directly learn to represent a 3D scene
- This is done using the general method for neural representations, where the task is modeled as a prediction problem from some coordinate system to an output that represents the scene
- Once trained, the scene is encoded in the weights of the neural network and thus novel views can be rendered for previously unobserved coordinates
- NeRFs outperformed previous view synthesis methods, but more importantly, offered a new view on scene representation
- Following the success of NeRF, there have been various attempts to learn neural representations on other domains as well
- In SIREN (Sitzmann et al., 2020) it is shown that neural representations can successfully model images when adapted to handle high frequencies
- For example, NeRV (Chen et al., 2021) utilizes neural representations for video encoding, where the video is represented as a mapping from a timestamp to the pixel values of that specific frame
- In this paper, we explore the idea of learning neural representations for pre-trained neural networks
- We consider representing a Convolutional Neural Network (CNN) using a separate predictor neural network, resulting in a neural representation for neural networks, or NeRN
- We model this task as a problem of mapping each weight's coordinates to its corresponding values in the original network
- NeRN is trained to map each input's coordinate back to the original kernel weights
- One can then reconstruct the original network by querying NeRN over all possible coordinates
- While a larger predictor network can trivially learn to overfit a smaller original network, we show that successfully creating a compact implicit representation is not trivial
- To achieve this, we propose methods for introducing smoothness over the learned signal, i.e. the original network weights, either by applying a regularization term in the original network training or by applying post-training permutations over the original network weights
- In addition, we design a training scheme inspired by knowledge distillation methods that allows for a more stable optimization process
- Similarly to other neural representations, a trained NeRN represents the weights of the specific neural network it was trained on, which to the best of our knowledge differs from previous weight prediction papers such as Ha et al. (2016); Schürholt et al. (2021); Knyazev et al. (2021); Schürholt et al. (2022)

## RELATED WORK
- Neural representations have recently proven to be a powerful tool in representing various signals using coordinate inputs fed into an MLP
- The superiority of implicit 3D shape neural representations (Sitzmann et al., 2019;Jiang et al., 2020;Peng et al., 2020;Chabra et al., 2020;Mildenhall et al., 2020) over previous representations such as grids or meshes has been demonstrated in Park et al. (2019); Chen & Zhang (2019); Genova et al. (2020)
- Following NeRF's success, additional applications rose for neural representations such as image compression (Dupont et al., 2021), video encoding (Chen et al., 2021), camera pose estimation (Yen- Chen et al., 2021) and more.
- Some of these redesigned the predictor network to complement the learned signal. For example, Chen et al. (2021) adopted a CNN for frame prediction.
- In our work, we adopt a simple MLP while incorporating additional methods to fit the characteristics of convolutional weights.
- Weight prediction refers to generating a neural network's weights using an additional predictor network.
- In Ha et al. (2016) the weights of a larger network are predicted using a smaller internal network, denoted as a HyperNetwork. The HyperNetwork is trained to directly solve the task, while also learning the input vectors for parameter prediction.
- Deutsch (2018) followed this idea by exploring the trade-off between accuracy and diversity in parameter prediction.
- In contrast, we aim to directly represent a pre-trained neural network, using fixed inputs.
- Several works have explored the idea of using a model dataset for weight prediction. For instance, Schürholt et al. (2021) proposes a representation learning approach for predicting hyperparameters and downstream performance. Schürholt et al. (2022) explored a similar idea for weight initialization while promoting diversity.
- Zhang et al. (2018a); Knyazev et al. (2021) leverage a GNN (graph neural network) to predict the parameters of a previously unseen architecture by modeling it as a graph input.
- Knowledge distillation is mostly used for improving the performance of a compressed network, given a pre-trained larger teacher network.
- There are two main types of knowledge used in studentteacher learning. First, response-based methods (Ba & Caruana, 2014;Hinton et al., 2014;Chen et al., 2017;2019) focus on the output classification logits. Second, feature-based methods (Romero et al., 2015;Zagoruyko & Komodakis, 2017) focus on feature maps (activations) throughout the network.
- The distillation scheme can be generally categorized as offline (Zagoruyko & Komodakis, 2017;Huang & Wang, 2017;Passalis & Tefas, 2018;Heo et al., 2019;Mirzadeh et al., 2020;Li et al., 2020) or online (Zhang et al., 2018b;Chen et al., 2020;Xie et al., 2019).

## METHOD
- Representing a convolutional classification network in a computer system
- Design choices made in the creation of NeRN
- Training of NeRN

## DESIGNING NERNS
- NeRN is a neural network that is used to predict the classification of a new input.
- The input to NeRN is a weight coordinate in an original neural network, and the output is the weight value at that coordinate.
- The predicted weights on all possible coordinates compose the predicted neural network, denoted as the reconstructed network.

## I/O modeling
- We propose to learn a mapping between a 3-tuple (l, f, c) to the k×k kernel at channel c of filter f in layer l.
- Since the output size of NeRN is fixed, we set it to the largest kernel size in the original network, and sample from the middle when predicting smaller kernels.
- We model convolutional layers only, and not others such as fully-connected or normalization layers, as their parameters are of negligible size compared to the convolution weights (see Appendix C).
- Positional embeddings Similarly to preceding neural representation works (Nguyen-Phuoc et al., 2022;Chen et al., 2021;Tancik et al., 2020), the input coordinates are first mapped to a high dimensional vector space.
- By using a high dimensional space, NeRN is able to represent high-frequency variations in the learned signal.
- We adopt the positional embeddings used in Vaswani et al. (2017), where b, and N represent the base frequency and the number of sampled frequencies, respectively.
- The NeRN predictor is a 5-layer MLP, which is a simplified version of the architecture used in Park et al. (2019).
- We omit the internal concatenation of the positional embedding, as it did not change our empirical results.

## TRAINING NERNS
- The reconstruction loss between the original and reconstructed network's weights is defined
- The knowledge distillation loss and feature map distillation loss are introduced
- The objective function for training NeRNs is comprised of the following: where L recon , L KD , and L F M D denote the reconstruction, knowledge distillation and feature map distillation losses, respectively
- The α and β coefficients can be used to balance the different losses

## PROMOTING SMOOTHNESS
- While videos, images, and 3D objects all have some inherent smoothness, this is not the case with the weights of a neural network
- We hypothesize that by introducing some form of smoothness between our kernels we can simplify the task for NeRN
- We now present and discuss two different methods to incorporate such smoothness
- Regularization-based smoothness A naive approach to promoting smoothness in the weights of a neural network is to explicitly add a loss term in the training process of the original network that encourages smoothness.
- Interestingly, we show that one can successfully learn a smooth network with slightly inferior performance on the original task simply by adding the smoothness term, where ∆ c stands for the cosine distance, L stands for the number of layers in the network, and F l , C l stand for the number of filters and channels in a specific layer respectively.
- In 1 × 1 layers, we use a l 2 distance instead of the cosine distance, since the weight kernels are scalars.
- While conceptually interesting, this approach requires modifying the training scheme of the original network and access to its training data, and may result in a degradation in accuracy due to the additional loss term.
- Permutation-based smoothness To overcome the downside of regularization, we introduce a novel approach for achieving kernel smoothness, by applying permutations over the pre-trained model's weights.
- That is, we search for a permutation of the kernels that minimizes equation 6 without changing the actual weights.
- Recall that NeRN maps each coordinate to the corresponding kernel, so in practice we keep the order of weights in the original network, and only change the order in which NeRN predicts the kernels.
- To solve the permutation problem we formalize it using graph theory.
- We denote the complete graph where each vertex in G l is a kernel in w (l) , and the edge between vertex i and j is the cosine distance between the i-th and j-th kernels in w (l) (for 1 × 1 kernels we replace the cosine distance with 2 distance).
- Now, the optimal reordering of the weights in layer l is equivalent to the minimal-weight Hamiltonian path (a path that goes through all vertices exactly once) in G l , which is precisely the traveling salesman problem (TSP) (Reinelt, 1994).
- While TSP is known to be NP-Hard, we propose to use an approximation to the optimal solution using a greedy solution. That is, for G l , start from an arbitrary vertex in V l and recursively find the closest vertex V l that has not yet been visited, until visiting all the vertices.
- We evaluated this method in our experiments presented in the following section.
- We consider two variants of this approach: Cross-filter permutations. In each layer, consider w (l) as a list of all kernels in layer l. We calculate the permutation across the entire list. The disadvantage of this approach is the overhead of saving the calculated ordering to disk.
- For a layer with F l filters and C l kernels each, the overhead is
- For standard ResNet variants, saving these permutations entails a 4%-6% size overhead.
- In-filter permutations. To reduce the overhead of the cross-filter permutations, we introduce in-filter permutations. For layer l, we first calculate the permutation of kernels inside each filter independently and then compute the permutation of the permuted filters.
- The overhead for a layer with F l filters and C l kernels each is
- For standard ResNet variants, saving these permutations entails a 2%-3% size overhead.

## Smoothness in positional embeddings
- NeRN learns to represent kernels by mapping from the positional embedding of a given coordinate to its corresponding kernel
- As adjacent convolution kernels are relatively similar to one another after we promote smoothness, we want to create similarly behaving positional embeddings
- This means that we want positional embeddings which are, on the one hand, slowly changing with respect to adjacent kernel coordinates, and on the other hand, highly separable with respect to distant coordinates
- Doing so allows us to introduce an inductive bias towards networks that have slowly changing kernels within each layer

## EXPERIMENTS
- Uses NeRN to predict weights of ResNet architectures
- Evaluates proposed method on three standard vision classification benchmarks - CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009)
- Shows the effectiveness of promoting smoothness in the original network's weights via weight permutations
- Shows the ability of NeRN to learn using noise images instead of the task data
- Presents some relevant ablation experiments.

## CIFAR-10
- For these experiments, we start by training ResNet20/56 to be used as the original networks.
- These ResNet variants are specifically designed to fit the low input size of CIFAR by limiting the input downsampling factor to ×4.
- We train NeRN for ∼70k iterations, using a task input batch size of 256.
- In addition, a batch of 2 12 reconstructed weights for the gradient computation is sampled in each iteration.
- Results are presented in Table 1.
- As expected, increasing the predictor size results in significant performance gains.
- In addition, while promoting smoothness improves results across all experiments, the improvement is more significant for smaller predictors since in such cases, the predictor typically has a lower capacity to capture the desired signal.
- Regularization-Based Smoothness Here we show the results of applying smoothness via a regularization term on the original network training.
- Since incorporating an additional loss term results in a degradation in accuracy, there is an inherent tradeoff between the original network's accuracy and NeRN's ability to reconstruct the network.
- An optimal regularization factor balances the two, achieving a high absolute reconstructed accuracy.
- Figure 3 demonstrates this using several regularization factors, where the optimal is 5 • 10 −6 .
- The complete results appear in Appendix E.1.

## CIFAR-100
- The paper trains a ResNet56 model to be used as the original model for CIFAR-10
- The setup is similar to that of CIFAR-10, only they train NeRN for ∼90k iterations
- These experiments show similar trends, where promoting smoothness and using larger predictors results in better reconstruction
- The complete results appear in Appendix E.2

## IMAGENET
- Learning to represent the ImageNet-pretrained ResNet18 from torchvision Paszke et al. (2019)
- Permutation-based smoothness is applied as post-training, allowing NeRN to learn to represent a network that was trained on a large-scale dataset even without access to the training scheme of the original model
- For ResNet-18, we predict only the 3 × 3 convolutions in the network which constitute more than 98% of the entire convolutional parameters, while skipping the first 7 × 7 convolution and the downsampling layers
- We train NeRN for 160k iterations, using a task input batch size of 32 (4 epochs)
- In addition, a batch of 2 16 reconstructed weights for the gradient computation is sampled in each iteration
- Results are presented in Table 3, where for evaluation we use the script provided by Wightman (2019)

## DATA-FREE TRAINING
- In the previous experiments, NeRN was trained with images from the training data of the original network.
- Ideally, we would like to be able to reconstruct a network without using the original task data.
- However, distilling knowledge using out-of-domain data is a non-trivial task.
- This is evident in the recent experiments by Beyer et al. (2022), where distilling knowledge using out-of-domain data achieved significantly worse results than in-domain data.
- Consequently, one would assume distilling knowl-edge from noise inputs should prove to be an even more difficult task, as the extracted features might not carry a meaningful signal.
- Interestingly, we show that given our combined losses and method, NeRN achieves good reconstruction results without the need for any meaningful input data, i.e by using noise as input.
- In fact, using this method with noise sampled from a Gaussian distribution only performs slightly worse than training with the task data.

## RECONSTRUCTING NON-SMOOTH NETWORKS
- The presented method for permutation smoothness offers a very small overhead
- One might consider the possibility of reconstructing a model without promoting any kind of smoothness
- To do so, we must examine our proposed positional embeddings once more
- Recall the two versions we presented in Figure 2
- We denote the blue plot as smooth embeddings and the red plot as nonsmooth embeddings
- In this case, the predictor not only does not utilize the smooth embeddings, but is poorly affected by them
- Table 5 demonstrates how using the non-smooth embeddings allows for a better reconstruction in non-smooth networks
- However, note that the results are inferior to those gained by smooth networks

## ABLATION EXPERIMENTS
- The ablation experiments show that the distillation losses are important
- The weight sampling methods for gradient computation need to take into account the reconstruction loss

## ADDITIONAL APPLICATIONS
- NeRNs encode the network weights themselves
- This observation means that NeRN implicitly learns weight importance
- We can visualize important filters, which will be those with the lowest relative reconstruction error

## Meta-Compression
- NeRN offers a compact representation of the original network, which is a neural network by itself.
- We propose that one can compress the NeRN predictor to achieve a more disk-size economical representation.
- To demonstrate this, we apply naive magnitude-based pruning on the predictor.
- The results are presented in Figure 5.

## CONCLUSION
- We propose a technique to learn a neural representation for neural networks (NeRN), where a predictor MLP, reconstructs the weights of a pretrained CNN.
- Using multiple losses and a unique learning scheme, we present satisfying reconstruction results on popular architectures and benchmarks.
- We further demonstrate the importance of weight smoothness in the original network, as well as ways to promote it.
- We finish by presenting two possible applications for NeRN, (1) weight importance analysis, where the importance is measured by NeRN's accuracy, and (2) meta-compression, where the predictor is pruned to achieve a disk-size compact representation, possibly without data.
