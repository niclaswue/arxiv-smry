---
title: ""No, to the Right" -- Online Language Corrections for Robotic Manipulation via Shared Autonomy"
date: 2023-01-06T15:03:27.000Z
author: "Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, Dorsa Sadigh"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02555v1.webp" # image path/url
    alt: "No, to the Right -- Online Language Corrections for Robotic Manipulation via Shared Autonomy" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02555)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02555).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/no-to-the-right-online-language-corrections).

# Abstract
- Systems for language-guided human-robot interaction must satisfy two key
- Existing instruction-following agents cannot adapt, lacking the ability to incorporate online natural language supervision, and even if they could, require hundreds of demonstrations to learn even simple policies.
- In this work, we address these problems by presenting Language-Informed Latent Actions with Corrections (LILAC), a framework for incorporating and adapting to natural language corrections - "to the right," or "no, towards the book" - online, during execution.
- We explore rich manipulation domains within a shared autonomy paradigm. Instead of discrete turn-taking between a human and robot, LILAC splits agency between the human and robot: language is an input to a learned model that produces a meaningful, low-dimensional control space that the human can use to guide the robot.
- Each real-time correction refines the human's control space, enabling precise, extended behaviors - with the added benefit of requiring only a handful of demonstrations to learn. We evaluate our approach via a user study where users work with a Franka Emika Panda manipulator to complete complex manipulation tasks. Compared to existing learned baselines covering both open-loop instruction following and single-turn shared autonomy, we show that our corrections-aware approach obtains higher task completion rates, and is subjectively preferred by users because of its reliability, precision, and ease of use.

# Paper Content

## Shared Autonomy Regime
- User reaches an "irrecoverable" state (gets stuck!)
- Figure 1: LILAC: Whereas prior work only allows for issuing a single language utterance for the entire task ("Pick up the book and insert it into the bookshelf" -solid line), our approach allows users to provide language corrections at any point during execution, allowing the robot to adapt online ("Tilt down a little bit!" -right window).

## INTRODUCTION
- Research in natural language for robotics has focused on dyadic, turn-based interactions between humans and robots, often in the instruction following regime [2,3,46,49].
- In this paradigm a human "Pick up the book and insert it into the bookshelf!" I am stuck! "Tilt down a little bit" Figure 2: A user interacts with our system. [Left] The user utters "pick up the book and insert it into the bookshelf," inducing a low-dimensional controller (depicted with the joystick and shaded inputs). [Middle] This control space is state and languageconditioned: pressing down brings the end-effector close to the book, while holding up/left after grasping the book moves the end-effector towards the shelf. However, this static controller is not enough; the user gets stuck! [Right] Our approach allows users to provide real-time corrections ("tilt down a little bit") refining the control space so the user can complete the task.
- This explicit division of agency between humans and robots places a tremendous burden on learning; existing systems either require large amounts of language-aligned demonstration data to learn policies [10,34,44,45], or make other restrictive assumptions about known environment dynamics, in addition to the ability to perform perfect object localization and affordance prediction to plug into task and motion planners [27,37].
- Coupled with the sample inefficiency of these approaches is their lack of adaptivity.
- One way to enable such adaptation is through natural language corrections -from the simple "left!" or "tilt down a little bit" (as in Figure 1), to the more complex "no, towards the blue marker."
- While recent work tries to get at the spirit of this idea by learning from dialogue [47,48], post-hoc corrections [7,8,12,43], or implicit feedback [22], none of these approaches work in real-time.
- Instead, we argue that scalable systems for language-driven human-robot interaction must be able to handle online corrections in a manner that is both adaptive and sample efficient.
- We introduce a novel approach -LILAC: Language-Informed Latent Actions with Corrections -that presents a generalizable framework for adapting to online natural language corrections built within a shared autonomy [1,14,20,30] paradigm for human-robot collaboration.
- With LILAC, a user provides a stream of language utterances, starting with a high-level goal ("Pick up the book and insert it into the bookshelf"), with each utterance shaping the control the user is afforded over the robot.
- At any point during execution, a user can provide a new utterance -a correction like "tilt down a little bit!" -which updates that control space online, reflecting the user's intent in real time.
- Working in a shared autonomy setting like this gives us the adaptivity mentioned above, but also allows us to develop correction-aware systems with extreme sample efficiency.
- With LILAC, we can learn to perform complex manipulation tasks like those in Figure 1 from 10-20 demonstrations instead of the thousands to tens of thousands of demonstrations required by fully autonomous imitation or reinforcement learning approaches [10,19,33,34].

## RELATED WORK
- LILAC builds off of a rich body of work spanning methods for incorporating language corrections, learning language-conditioned policies, and incorporating other forms of corrective feedback.
- Most relevant to our approach are recent methods for incorporating various types of natural language corrections in the context of robotic manipulation.
- These methods can be stratified based on the assumptions they make, and when during execution a user provides a correction.
- For example, Broad et al. [5] enables data-efficient online corrections (similar to LILAC) using distributed correspondence graphs to ground language, via use of a semantic parser that maps language to a predefined space of correction groundings; these groundings are brittle and hand-designed, additionally requiring access to a motion planner (and fully known environment dynamics) to identify a fulfilling set of actions for the robot to execute.
- In contrast, later work in incorporating corrections removes the need for brittle, hand-designed correction primitives, instead using post-hoc corrections provided at the end of task execution to define composable cost functions that are fed to a trajectory optimizer [43]; the post-hoc nature of these corrections is limiting, especially in cases where tasks have irreversible or "hard-to-reset" components, and the trajectory optimizer requires non-trivial knowledge of the environment.
- Later work relaxes these prior knowledge assumptions, but can only incorporate correction information post-hoc, directly "modifying" trajectory waypoints following a language correction in both 2D [7] and 3D [8] environments using massive datasets of paired corrections and demonstrations.
- In contrast to these approaches, LILAC is a shared autonomy approach that operates online, in real-time, without a need for massive amounts of data, prior environment dynamics, or full state knowledge.
- Learning Language-Conditioned Policies.
- More general than incorporating corrections is a tremendous body of work on learning language-conditioned policies in both the full and shared autonomy regimes.
- Early work in this space used semantic parsers to map natural language instructions to predefined motion planning primitives, given modest sized datasets [2,3,27,46].
- While these approaches were able to accomplish a limited range of tasks with high reliability, reliance on predefined primitives and motion planners made it hard to scale these approaches to more complex manipulation domains, where environment knowledge is hard to come by, and hand-defined primitives are brittle and limiting.
- As a result, more recent work in this space learn language-conditioned policies directly via imitation learning from large datasets of paired (language, demonstration) pairs [19,34,38,44,45].
- While expressive, these approaches are still tremendously data hungry, requiring hundreds or thousands of examples to learn even the simplest tasks.
- To address the sample efficiency problem, other work such as LILA [23] have turned to the shared autonomy regime, learning collaborative human-robot policies from orders of magnitude fewer demonstrations.
- LILA is the starting point of our proposed approach.
- Other approaches tackle learning from other forms of corrective feedback, such as physical corrections [28,31], targeted interventions wherein a human fully assumes control over a robot via remote teleoperation [17,25,35], critiques [9,13], as well as trajectory preferences [4,11].
- More recently, Schmittle et al. [42] have proposed a metaalgorithm for online learning from multiple types of corrective feedback (excluding language).
- While this work is promising, we focus our approach on language corrections, a natural communication modality for human users.

## LILAC: FRAMING CORRECTIONS
- LILAC is a machine learning architecture that incorporates natural language corrections during the course of execution.
- The LILAC architecture is depicted in Figure 3; solid lines denote inference, while dashes denote training.
- LILAC incorporates natural language corrections in a data-driven way; this work focuses on directional corrections (e.g., "to the left") and referential corrections (e.g., "towards the blue marker") -all with a generalizable and scalable procedure that can be extended to other more complex types of corrections.

## Problem Statement
- The problem is a sequential decision making problem defined by elements (S, A, T, U, C*, Z)
- The environment is a 6-DoF delta in end-effector pose (Cartesian coordinates for position and Euler angles for orientation)
- The transition function is a (stochastic) unobserved transition function
- The user can provide an arbitrary number of online corrections throughout the episode
- The goal of LILAC is to learn a function F(S, A, T, U, C*, Z) that maps the current robot and environment state, low-dimensional control input, initial high-level utterance provided by the user, and (possibly empty) stack of language corrections to a high-dimensional robot action that is to be executed in the environment

## Modeling: Inference & Learning
- Given the current state  and language  and c, F  maps lowdimensional user control inputs  to the high-dimensional robot actions .
- We define â = F  (, , , c) as: where  is a hyperparameter denoting the hidden dimensionality of the model (we set  = 128 for this work).
- To encode language ( and c), we adopt a last-in-first-out strategy for selectively encoding utterances, only encoding on the most recent utterance - at the beginning of an interaction, then the most recent correction  ′ .
- We embed this utterance with a frozen variant of the Distil-RoBERTa language model [41] as released by the Sentence-BERT project [40].
- This process is backed by an "unnatural language processing" nearest neighbors index [36] where inference-time utterances are mapped onto the closest existing training exemplars, which are then retrieved and fed to the rest of the model.
- EncodeLanguage  is another two-layer MLP that takes in the retrieved embedding and outputs ℎ language .
- Next, we consider how to fuse the state and language embeddings.
- A key component of LILAC and its ability to remain data efficient is the GPT-3 [6] "gating" module (shown in orange in Figure 3), and denoted by the scalar value  ∈ [0, 1] in the equations above.
- The gating value  reflects a simple insight -certain utterances require different amounts of object/state dependence.
- For example, an utterance such as "go left" does not require reasoning over any objects in the environment ( = 0).
- We use this gating value  to module the amount of state information in ℎ gated by taking the convex combination of  with ℎ state and a vector bias  , where  = 0 obviates  state .
- We incorporate language by using FiLM [39], mapping ℎ language to affine transformation parameters  ∈ R  and  ∈ R  via separate learned two-layer MLPs.
- We then apply the resulting transformation elementwise to ℎ gated , producing ℎ fused =  • ℎ gated + .
- Finally, we predict basis vectors  bases ∈ R × (recall that  is the dimensionality of the high-DoF robot action space, while  is the dimensionality of the user's low-DoF control interface).
- bases uniquely defines the user's control manifold for the given timestep; we learn a two-layer MLP Projection  that takes ℎ fused and outputs a matrix with the required dimensions.
- To serve as an appropriately conditioned control manifold, we run modified Gram-Schmidt to orthonormalize the bases.
- The final high-DoF robot action â ∈ R  is then the matrix-vector product between  bases and the user's input  ∈ R  .

## Gating Instructions vs. Corrections
- Key to scaling LILAC is the insight that various forms of correction language are generalizable across states -in other words, different language utterances require different amounts of object/environment state-dependence.
- Formally defining the "state-dependence" of a language utterance is hard; one heuristic might be to categorize different utterances based on the number of referents present; an utterance like "grab the thing on the side table and place it on the table" as in Figure 1 has 3 referents, indicating a large degree of state dependence; the robot must ground the utterance in the objects of the environment to resolve the correct behavior.
- However, an utterance like "no, to the left!" has no explicit referents; one can resolve the utterance by relying solely on the user's static reference frame and induced deltas in end-effector space.
- To operationalize this idea with LILAC, further contributing to the sample efficiency of our approach, we use a gating function (orange, in Figure 3) that given language, predicts a discrete value  ∈ {0, 1}.
- A value of 0 signifies a state-independent utterancefor example, the correction "tilt down a little bit. " Appropriately, in our architecture, this zeroes out any state-dependent information (see the term in Figure 3), and predicts an action solely based on the provided language.
- Critically, the fused representation of language and state provided to the rest of the network defining in F  is modulated by  -more detail in §4.4.
- Using GPT-3 to Identify Corrections. In this work, we construct a prompt harness with GPT-3 [6] to output . We do this because characterizing the state-dependence of an utterance is difficult; while the aforementioned reference counting heuristic may work in some cases, utterances such as "no, the blue!" have implicit referents (in this case, perhaps a marker, or cup) that need to be grounded in the environment state.
- Many other phenomena make it hard to define heuristics for computing  -anaphora, null referents ("move the robot left"), etc. Instead of crafting grammars or heuristics, we opt to tap into the power of large language models with in-context learning abilities, that learn to extrapolate given a prompt and small set of examples.
- We specifically build off of GPT-3 text-davinci-002 (175B parameters) [6]. To define our prompt, we allocated a 10 minute budget to iteratively engineer the input/output examples and task description, using a held-out set of 5 language utterances to provide signal.

## Reproducibility
- LILAC is an open source codebase that spans data collection, model definition, training, and real-robot deployment
- All MLPs detailed in §4.2 use  = 128 and the GELU activation [15]
- For stability, we add a single Batch Normalization layer [18] before feeding the concatenated state representation to the state encoder
- Training LILAC is efficient, and can be run on consumer laptop CPUs, eschewing the need for expensive GPUs
- We train for 50 epochs, with a batch size of 512, using the AdamW optimizer [26] with default learning rate of 0.001 and weight decay of 0.01

## USER STUDY PRELIMINARIES
- The fully autonomous imitation learning approach fails to make it beyond the first stage of the task, while LILA is able to reach the drawer as well as the cup but fails to precisely aim and grasp the object.
- LILAC gets stuck at the same place, but is able to recover as the user issues low-level corrections to precisely maneuver the end-effector and fully complete the tasks.

## USER STUDY RESULTS
- Success rate averaged across users is highest for LILAC
- Subjectively, LILAC is preferred by users
- LILAC is more adaptable than LILA and achieves better success rates across all subtasks

## Training State Distribution LILA (No Corrections) LILAC (Ours)

## DISCUSSION
- The current approach has limitations that need to be addressed in future work
- Future work should focus on different types of language corrections that are more context sensitive
- Corrections such as "rotate" or "tilt" can be ambiguously interpreted
- Making the underlying high-level control spaces more natural and intuitive will be crucial for scaling LILAC to more complex, temporally extended tasks.

## Conclusion.
- LILAC is a language-informed latent action model that can handle online corrections
- LILAC is subjectively preferred by users and objectively performant than both baselines
- LILAC marks a strong step forward in adaptive language-driven approaches for shared autonomy
