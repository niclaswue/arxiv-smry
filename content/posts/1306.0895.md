---
title: "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances"
date: 2013-06-04T14:45:10.000Z
author: "Marco Cuturi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1306-0895v1.webp" # image path/url
    alt: "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1306.0895)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1306.0895).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/sinkhorn-distances-lightspeed-computation-of-1).

# Abstract
- Optimal transportation distances are a family of parameterized distances for histograms
- We propose a new family of optimal transportation distances that look at transportation
- Our new distance is faster to compute than the classical optimal transportation distances

# Paper Content

## Introduction
- Optimal transportation distances (Villani, 2009)
- They are parameterized by the ground metric
- Compared to other classic distances or divergences, such as Hellinger, χ 2 , Kullback-Leibler or Total Variation, they are the only ones to be parameterized
- This parameter -the ground metric -plays an important role to handle high-dimensional histograms: the ground metric provides a natural way to handle redundant features that are bound to appear in high-dimensional histograms (think synonyms for bags-of-words), in the same way that Mahalanobis distances can correct for statistical correlations between vector coordinates
- The central role played by histograms and bags-of-features in most data analysis tasks and the good performance of optimal transportation distances in practice has generated ample interest, both from a theoretical point of view (Levina and Bickel, 2001;Indyk and Thaper, 2003;Naor and Schechtman, 2007;Andoni et al., 2009) and a pracical aspect, mostly to compare images (Grauman and Darrell, 2004;Ling and Okada, 2007;Gudmundsson et al., 2007;Shirdhonkar and Jacobs, 2008)
- However, when these restrictions do not apply, computing a single distance between a pair of histograms of dimension in the few hundreds can take more than a few seconds
- This issue severely hinders the applicability of optimal transportation distances in large-scale data analysis and goes as far as putting into question their relevance within the field of machine learning
- Our aim in this paper is to show that the optimal transportation problem can be regularized by an entropic term, following the maximum-entropy principle
- We argue that this regularization is intuitive given the geometry of the optimal transportation problem and has, in fact, been long known and favored in transportation theory (Erlander and Stewart, 1990)
- From an optimization point of view, this regularization has multiple virtues, among which that of turning this LP into a strictly convex problem that can be solved extremely quickly with the Sinkhorn-Knopp matrix scaling algorithm (Sinkhorn and Knopp, 1967;Knight, 2008)
- This algorithm exhibits linear convergence and can be trivially parallelized -it can be vectorized. It is therefore amenable to large scale executions on parallel platforms such as GPGPUs
- From a practical perspective, we show that, on the benchmark task of classifying MNIST digits, Sinkhorn distances perform better than the EMD and can be computed several orders of magnitude faster over a large sample of dimensions without making any assumption on the ground metric

## Reminders on Optimal Transportation
- U(r, c) contains all nonnegative d × d matrices with row and column sums r and c respectively.
- U(r, c) has a probabilistic interpretation: for X and Y two multinomial random variables taking values in {1, • • • , d}, each with distribution r and c respectively, the set U(r, c) contains all possible joint probabilities of (X, Y ).
- Given a d×d cost matrix M , the cost of mapping r to c using a transportation matrix (or joint probability) P can be quantified as P, M .
- The following problem: is called an optimal transportation problem between r and c given cost M . An optimal table P for this problem can be obtained with the network simplex (Ahuja et al., 1993, §9) as well as other approaches (Orlin, 1993).
- The optimum of this problem, d M (r, c), is a distance (Villani, 2009, §6.1) whenever the matrix M is itself a metric matrix, namely whenever M belongs to the cone of distance matrices (Avis, 1980;Brickell et al., 2008).

## Sinkhorn Distances

## Computing Sinkhorn Distances with the Sinkhorn-Knopp Algorithm
- The Sinkhorn distance is defined through a hard constraint on the entropy of h(P ) relative to h(r) and h(c).
- In what follows, we consider the same program with a Lagrange multiplier for the entropy constraint, , where P λ = argmin
- By duality theory we have that for every pair (r, c), to each α corresponds an
- We call d λ M the dual-Sinkhorn divergence and show that it can be computed at a much cheaper cost than the classical optimal transportation problem for reasonable values of λ.
- M . When λ > 0, the solution P λ is unique by strict convexity of minus the entropy. In fact, P λ is necessarily of the form u i e −λmij v j , where u and v are two non-negative vectors uniquely defined up to a multiplicative factor.
- This well known fact in transportation theory (Erlander and Stewart, 1990) can be indeed checked by forming the Lagrangian L(P, α, β) of the objective of Equation (2) using α, β ≥ 0 d for each of the two equality constraints in U (r, c).
- For these two cost vectors α, β, We obtain then, for any couple (i, j), that if ∂L ∂p λ ij = 0, then and thus recover the form provided above.
- P λ is thus, by Sinkhorn and Knopp's theorem (1967), the only matrix with row-sum r and column-sum c of the form Given e −λM and marginals r and c, it is thus sufficient to run enough iterations of Sinkhorn and Knopp's algorithm to converge to a solution P λ of that problem.
- We provide a one line implementation in Algorithm 1.

## Experimental Results
- We test the performance of Sinkhorn distances on the MNIST digits dataset, on which the ground metric has a natural interpretation in terms of pixel distances.
- Each digit is provided as a vector of intensities on a 20 × 20 pixel grid.
- We convert each image into a histogram by normalizing each pixel intensity by the total sum of all intensities .
- We consider a subset of N points in the training set of the database, where N ranges within {3, 5, 12, 17, 25} × 10 3 datapoints.
- Experimental setting.
- For each subset of size N , we provide mean and standard deviation of classification error using a 4 fold (3 test, 1 train) cross validation scheme repeated 6 times, resulting in 24 different experiments.
- We study the performance of different distances with the following parameter selection scheme: for each distance d, we consider the kernel e −d/t , where t > 0 is chosen by cross validation individually for each training fold within the set {1, q 10 (d), q 20 (d), q 50 (d)}, where q s is the s% quantile of a subset of distances observed in the training fold.
- We regularize non-positive definite kernel matrices resulting from this computation by adding a sufficiently large diagonal term.
- SVM's were run with libsvm (one-vsone) for multiclass classification, the regularization constant C being selected by 2 folds/2 repeats cross-validation on the training fold in the set 10 −2:2:4
- We set the ground metric M to be the Euclidean distance between the 20 × 20 points in the grid, resulting in a 400 × 400 distance matrix.
- We also tried to use Mahalanobis distances on this example with a positive definite matrix equal to exp(-tM.^2), t>0, as well as its inverse, with varying values of t but none of the results proved competitive.
- For the Independence kernel, since any Euclidean distance matrix is valid, we consider [m a ij ] where a ∈ {0.01, 0.1, 1} and choose a by cross-validation on the training set.
- Smaller values of a seem to be preferable.
- We select the entropic penalty λ of Sinkhorn distances so that the matrix e −λM is relatively diagonally dominant and the resulting transportation too far the classic optimal transportation.
- We select λ for each training fold by internal cross-validation within {5, 7, 9, 11} × 1/q 50 (M ) where q 50 (M ) is the median distance between pixels on the grid.
- We set the number of fixed-point iterations to an arbitrary number of 20 iterations.
- In most (though not all) folds, the value λ = 9 comes up as the best setting.
- The Sinkhorn distance beats by a safe margin all other distances, including the EMD.
- Does the Sinkhorn Distance Converge to the EMD?.
- We study in this section the convergence of Sinkhorn distances towards classical optimal transportation distances as λ gets bigger.
- Because of the additional penalty that appears in (2) program, d λ M (r, c) is necessarily larger than d M (r, c), and we expect this gap to decrease as λ increases.
- Figure 3 illustrates this by plotting the boxplot of distributions of (d λ M (r, c) − d M (r, c))/d M (r, c) over 40 2 pairs of distinct points taken in the MNIST database.
- As can be observed, even with large values of λ, Sinkhorn distances hover above the values of EMD distances by about 10%.
- For practical values of λ such as λ = 9 selected above we do not expect the Sinkhorn distance to be numerically close to the EMD, nor believe it to be a desirable property.

## Conclusion
- Regularizing the optimal transportation problem with an intuitive entropic penalty opens the door for new research directions and potential applications at the intersection of optimal transportation theory and machine learning.
- Based on preliminary evidence, it seems that Sinkhorn distances do not perform worse than the EMD, and may in fact perform better in applications.
- Sinkhorn distances are parameterized by a regularization weight λ which should be tuned having both computational and performance objectives in mind, but we have not observed a need to establish a trade-off between both.
- The proof above suggests a faster way to compute the Independence kernel.
- Given a matrix M , one can indeed pre-compute the vector of norms u as well as a Cholesky factor L of K above to preprocess a dataset of histograms by premultiplying each observations r i by L and only store Lr i as well as precomputing its diagonal term r T i u.
- Note that the independence kernel is positive definite on histograms with the same 1-norm, but is no longer positive definite for arbitrary vectors.
