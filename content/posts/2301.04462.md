---
title: "An Analysis of Quantile Temporal-Difference Learning"
date: 2023-01-11T13:41:56.000Z
author: "Mark Rowland, Rémi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G. Bellemare, Will Dabney"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04462v1.webp" # image path/url
    alt: "An Analysis of Quantile Temporal-Difference Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04462)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04462).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/an-analysis-of-quantile-temporal-difference).

# Abstract
- Quantile temporal-difference learning is a distributional reinforcement learning algorithm that has been successful in several large-scale applications
- Despite these empirical successes, a theoretical understanding of QTD has been elusive until now
- The core result of this paper is a proof of convergence to the fixed points of a related family of dynamic programming procedures with probability 1
- This establishes connections between QTD and non-linear differential inclusions through stochastic approximation theory and non-smooth analysis

# Paper Content

## Introduction
- Distributional reinforcement learning is a type of reinforcement learning where the agent aims to predict the full probability distribution over future returns it will encounter
- The family of algorithms for distributional reinforcement learning is based on the notion of learning quantiles of the return distribution
- QTD has been particularly successful in combination with deep reinforcement learning, and has been a central component in several recent real-world applications of reinforcement learning
- In this paper, we prove the convergence of QTD -notably under weaker assumptions than are required in typical proofs of convergence for classical TD learning -establishing it as a sound algorithm with theoretical convergence guarantees
- The proof relies on the stochastic approximation framework set out by Benaïm et al. (2005), arguing that the QTD algorithm approximates a continuous-time differential inclusion, and then constructing a Lyapunov function to demonstrate that the limiting behaviour of trajectories of the differential inclusion matches that of the QDP algorithms introduced earlier

## Background
- We use a binary tree to represent a set of nodes.
- A path from a root node to a leaf node is a sequence of edges in the tree.
- A path from a leaf node to a root node is a path that includes the root node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a leaf node is a path that includes the root node.
- A path from a leaf node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a leaf node is a path that includes the root node.
- A path from a leaf node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a leaf node is a path that includes the root node.
- A path from a leaf node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is a path that does not include the leaf node.
- A path from a root node to a non-leaf node is...

### Markov decision processes
- The Markov decision process is a model for how an agent interacts with an environment
- The transition kernel is a function that specifies how the agent's state changes based on its previous state and the current environment
- The reward distribution function is a function that specifies how the agent's reward changes based on its previous state and the current environment
- The discount factor is a parameter that determines how much the agent's reward should change based on its current state
- The agent's policy is a function that specifies how the agent will act in the current environment

### Predicting expected returns and the return distribution
- The quality of the agent's performance on the trajectory is quantified by the discounted return, or simply the return, given by ∞ t=0 γ t R t .
- The return is a random variable, whose sources of randomness are the random selections of actions made according to π, the randomness in state transitions, and the randomness in rewards observed.
- Typically in reinforcement learning, a single scalar summary of performance is given by the expectation of this return over all these sources of randomness.
- For a given policy, this is summarised across each possible starting state via the value function V π : X → R, defined by
- Learning the value function of a policy π from sampled trajectories generated through interaction with the environment is a central problem in reinforcement learning, referred to as the policy evaluation task.
- Each expected return is a scalar summary of a much more rich, complex object: the probability distributions of the random return in Equation (1) itself.
- Distributional reinforcement learning (Bellemare et al., 2023) is concerned with the problem of learning to predict the probability distribution over returns, in contrast to just their expected value.
- Mathematically, the goal is to learn the return-distribution function η π : X → P(R); for each state x ∈ X , η π (x) is the probability distribution of the random return in Expression (1) when the trajectory begins at state x, and the agent acts using policy π.
- Mathematically, we have where D π x extract the probability distribution of a random variable under P π x .
- There are several distinct motivations for aiming to learn these more complex objects. First, the richness of the distribution provides an abundance of signal for an agent to learn from, in contrast to a single scalar expectation.
- The strong performance of deep reinforcement learning agents that incorporate distributional predictions is hypothesised to be related to this fact (Dabney et al., 2018b;Barth-Maron et al., 2018;Dabney et al., 2018a;Yang et al., 2019).
- Second, learning about the full probability distribution of returns makes possible the use of risk-sensitive performance criteria; one may be interested in not only the expected return under a policy, but also the variance of the return, or the probability of the return being under a certain threshold.
- Unlike the value function V π , which is an element of R X , and can therefore be straightforwardly represented on a computer (up to floating-point precision), the return-distribution function η π is not representable.
- Each object η π (x) is a probability distribution over the real numbers, and, informally speaking, probability distributions have infinitely many degrees of freedom.
- Distributional reinforcement learning algorithms therefore typically work with a subset of distributions that are amenable to parametrisation on a computer (Bellemare et al., 2023).
- Common choices of subsets include categorical distributions (Bellemare et al., 2017), exponential families (Morimura et al., 2010b), and mixtures of Gaussian distributions (Barth-Maron et al., 2018).

### Monte Carlo and temporal-difference learning
- The classical TD learning update rule is an approximation to Monte Carlo learning
- First, we may observe that, under the condition that all reward distributions have finite variance, V π (x) is the unique minimiser of the following loss function over v:
- This well-known characterisation of the expectation of a random variable is readily verified by, for example, observing that the loss is convex and differentiable in v, and solving the equation ∂ v L π x (v) = 0
- This motivates an approach to learning V π (x) based on stochastic gradient descent on the loss function L π x
- We maintain an estimate V ∈ R X of the value function, and each time a trajectory (X t , A t , R t ) t≥0 beginning at state x is observed, we can obtain an unbiased estimator of the negative gradient of L π x (V (x)) as and update V (x) by taking a step in the direction of this negative gradient, with some step size α
- This is a Monte Carlo algorithm, so called because it uses Monte Carlo samples of the random return to update the estimate V
- A popular alternative to this Monte Carlo algorithm is temporal-difference learning, which replaces samples from the random return with a bootstrapped approximation to the return, obtained from a transition (x, A, R, X ) by combining the immediate reward R with the current estimate of the expected return obtained at X , resulting in the return estimate and the corresponding update rule
- While the mean-return estimator in Expression (4) is generally biased, since V (X ) is not generally equal to the true expected return V π (X ), it is often a lower-variance estimate, since we are replacing the random return from X with an estimate of its expectation
- This motivates the TD learning rule in Expression (5) based on the Monte Carlo update rule in Expression (3), with the understanding that this algorithm can be applied more generally, with access only to sampled transitions (rather than full trajectories), and may result in more accurate estimates of the value function, due to lower-variance updates, and the propensity of TD algorithms to "share information" across states

## Quantile temporal-difference learning and quantile dynamic programming
- Algorithms of study
- Algorithms of study
- Algorithms of study

### Quantile regression
- To motivate QTD, we begin by considering how we might adapt a Monte Carlo algorithm such as that in Expression (3) to learn about the distribution of returns, rather than just their expected value.
- We cannot learn the return distribution in its entirety with a finite collection of parameters; the space of return distributions is infinite-dimensional, so we must instead be satisfied with learning an approximation of the return distribution by selecting a probability distribution representation (Bellemare et al., 2023, Chapter 5) -a subset of probability distributions parametrised by a finite-dimensional set of parameters.
- The approach of quantile temporal-difference learning is to learn an approximation of the form an equally-weighted mixture of Dirac deltas, for each state x ∈ X .
- The quantile-based approach to distributional reinforcement learning aims to have the particle locations (θ(x, i)) m i=1 approximate certain quantiles of η π (x).
- Definition 3.1. for a probability distribution ν ∈ P(R) and parameter τ ∈ (0, 1), the set of τ -quantiles of ν is given by the set where Expanding on this definition, if the set {z ∈ R : F ν (z) = τ } is non-empty, then the τquantiles are precisely the values z such that P Z∼ν (Z ≤ z) = τ .
- If however this set is empty (which may arise when F ν has points of discontinuity), then the quantile is the smallest value y such that P Z∼ν (Z ≤ y) > τ .
- Note also that if F ν is strictly increasing, this guarantees uniqueness of each τ -quantile for τ ∈ (0, 1); this is often a useful property in the analysis we consider later.
- These different cases are illustrated in Figure 1.
- The generalised inverse CDF of ν, F −1 ν : (0, 1) → R, is defined by and provides a way of uniquely specifying a quantile for each level τ .
- We also introduce the notation which corresponds to the right-most or greatest τ -quantile; notice the strict inequality that appears in the definition, in contrast to that of , as is the case for τ = τ 1 and τ = τ 3 in Figure 1.
- However, if F ν has a flat region for the value τ (as is the case for τ = τ 2 in Figure 1), then F −1 ν (τ ) and F −1 ν (τ ) are distinct, and correspond to the boundary points of this flat region.
- Algorithmically, we aim for θ(x, i) to approximate a τ i -quantile of η π (x), where τ i = 2i−1 /2m.
- To build learning algorithms that achieve this, we require an incremental algorithm that updates θ(x, i) in response to samples from the target distribution η π (x), which converges to a 2i−1 /2m-quantile of η π (x).
- Such an approach is available by using the quantile regression loss.
- We define the quantile regression loss associated with distribution ν ∈ P(R) and quantile level τ ∈ (0, 1) as a function of v by This loss is the expectation of an asymmetric absolute value loss, in which positive and negative errors are weighted according to the parameters τ and 1 − τ respectively.
- Just as the expected squared loss encountered above encodes the mean as its unique minimiser, the quantile regression loss encodes the τ -quantiles of ν as the unique minimisers; see e.g. Koenker (2005) for further background.
- Thus, applying the quantile regression loss to the problem of estimating τ -quantiles of the return distribution, we arrive at the loss Given an observed return t≥0 γ t R t from the state x, we therefore have that an unbiased estimator of the negative gradient 1 of this loss is Technically speaking, we are assuming that differentiation and expectation can be...

### Quantile temporal-difference learning
- The quantile temporal-difference learning algorithm is motivated by modifying the Monte Carlo algorithm
- The quantile temporal-difference learning algorithm updates its parameters on the basis of sample transitions (x, r, x ) generated through interaction with the environment
- The QTD update makes use of temporal-difference errors r + γθ(x , j) − θ(x, i), and has two key differences to the use of analogous quantities in classical TD learning.

### Motivating examples
- QTD is able to learn a median quantile for a given environment
- This median quantile is able to converge to a point
- The expected update direction changes smoothly

### Quantile dynamic programming
- Recall the QTD update given in Equation ( 10).
- As described in Section 3.2, this update serves, on average, to move θ(x, i) in the direction of the τ i -quantiles of the distribution of the random variable R + θ(X , J), where (x, R, X ) is a random transition generated by interacting with the environment using π.
- Suppose we were able to update θ(x, i) not just with a single gradient step in this direction, but instead were able to update it to take on exactly this quantile value. This motivates a dynamic programming alternative to QTD, quantile dynamic programming (QDP), which directly calculates these quantiles iteratively, in a similar manner to iterative policy evaluation in classical reinforcement learning (Bertsekas and Tsitsiklis, 1996).
- The mathematical structure of such an algorithm is given in Algorithm 2.
- This stops short of being an implementable algorithm, since we do not describe in what format the transition probabilities and reward distributions are available, which are required to evaluate the inverse CDFs that arise in the algorithm. However, for MDPs in which transition probabilities and reward distributions are available, QDP is an algorithmic framework of interest in its own right, and to this end we provide several concrete implementations in Appendix B.
- The QDP template in Algorithm 2 is parametrised by the interpolation parameters λ ∈ [m] . These parameters control exactly which quantile is chosen when the desired quantile level τ i corresponds to a flat region of the CDF for the distribution ν (the second case in Figure 1).
- QDP was originally presented by Bellemare et al. (2023) in the case λ(x, i) ≡ 0; the presentation here generalises QDP to a family of algorithms, parametrised by λ.
- Our interest in QDP stems from the fact that QTD can be viewed as approximating the behaviour of the QDP algorithms, without requiring access to the transition structure and reward distributions of the environment. In particular, we will show that under appropriate conditions, the asymptotic behaviour of QTD and QDP are equivalent: they both converge to the same limiting points.
- Figure 4 illustrates the behaviour of the QDP algorithm in the environment described in Example 3.3; since the reward distributions in this example have strictly increasing CDFs, QDP behaves identically for all choices of interpolation parameters λ.
- TD and QDP appear to have the same asymptotic behaviour, converging to the same limiting point. In cases where QTD appears to converge to a set, such as in the bottom-right plot of Figure 3.3, the relationship is slightly more complicated, and there is a correspondence between the asymptotic behaviour of QTD and the family of dynamic programming algorithms parametrised by λ, as illustrated at the bottom of Figure 4. Thus, to understand the asymptotic behaviour of QTD, we begin by analysing the asymptotic behaviour of QDP.

## Convergence of quantile dynamic programming
- The update QDP performs is decomposed into the composition of several operators.
- The first transformation performed by Algorithm 2 is to assign the distribution of R + γG(X ), where R, X are the random reward and next-state encountered from the initial state x with policy π, and (G(y) : y ∈ X ) is an independent collection of random variables, with each G(y) distributed according to η(y).
- T π : P(R) X → P(R) X is the distributional Bellman operator (Bellemare et al., 2017;Rowland et al., 2018;Bellemare et al., 2023).
- In terms of the above definition via distributions of random variables, T π can be written where (x, R, X ) is a random environment transition beginning at x, independent of (G(y) : y ∈ X ), and D π extracts the distribution of its argument when (x, R, X ) is generated by sampling an action from π.
- See Bellemare et al. ( 2023) for further background on the distributional Bellman operator.
- In general, T π η may comprise much more complicated distributions than η itself, with many more atoms, or possibly infinite support, if reward distributions are infinitely-supported.
- Algorithm 2 is mathematically equivalent to repeated application of the operator Π λ T π to an initial collection of quantile estimates.

### Convergence analysis
- Π λ T π is a contraction mapping with respect to an appropriate metric over return-distribution functions
- The projection operator Π λ cannot expand distances as measured by w∞ , generalising the proof given by Bellemare et al. (2023) in the case λ ≡ 0
- Proposition 4.4: The projected operator is a γ-contraction with respect to w∞ follows directly from Propositions 4.2 and 4.3
- Next, observe that w∞ assigns finite distance to all pairs of return-distribution functions in F X Q,m , and further, this set is complete with respect to w∞
- Banach's fixed point theorem allows us to find the unique fixed point ηπ λ in F X Q,m

## Convergence of quantile temporal-difference learning
- The QTD update is a noisy Euler discretization of a differential equation or differential inclusion.
- The trajectories of the differential equation/inclusion converge to some set of fixed points in a suitable way, and the asymptotic behaviour of QTD, forming a noisy Euler discretization, matches the asymptotic behaviour of the true trajectories.
- The QTD iterates converge to the same set of fixed points as the true trajectories.

### The QTD differential equation
- Taking the expectation over the random variables R k (x) and X k (x) in Equation ( 13) conditional on the algorithm history up to time k yields an expected increment of
- Assumption 5.2 guarantees the global existence and uniqueness of solutions to the QTD differential equation
- The trajectories of the QTD ODE are obtained precisely by integrating the vector fields that appear in these plots.

### The QTD differential inclusion
- The right-hand side of the QTD ODE in Equation ( 16) is modified to the difference is the strict inequality.
- In general, solutions may not even exist for this differential equation.
- The situation is illustrated in the bottom-left panel of Figure 3; the lines in this plot illustrate points of discontinuity of the vector field to be integrated, and there are instances where the vector field either side of such a line of discontinuity "pushes" back into the discontinuity.
- In such cases, the differential equation has no solution in the usual sense.
- This phenomenon is known as sliding, or sticking, from cases when it arises in the modelling of physical systems with potentially discontinuous forces (such as static friction models in mechanics).
- Filippov (1960) proposed a method to deal with such non-existence issues, by relaxing the definition of the dynamics at points of discontinuity.
- Technically, Filippov's proposal is to allow the derivative to take on any value in the convex hull of possible limiting values as we approach the point of discontinuity.
- In our case, we consider redefining the dynamics at points of discontinuity as follows: where .
- This refines the dynamics so that for each coordinate (x, i), the derivative may take on either the left or right limit around ϑ t (x, i), or any value in between; this is a looser relaxation than Filippov's proposal, and is easier to work with in our analysis.
- Equation ( 17) is a differential inclusion, as opposed to a differential equation; the derivative is constrained to a set at each instant, rather than constrained to a single value.
- We refer to Equation ( 17) specifically as the QTD differential inclusion (or QTD DI).

### Solutions of differential inclusions
- Definition 5.4: A solution to a differential inclusion is a path that satisfies the integration condition (18)
- Proposition 5.5: For a set-valued map H : R n ⇒ R n , there exists a constant C > 0 such that for all z ∈ R n , the differential inclusion ∂ t z t ∈ H(z t ) has a global solution.

### Asymptotic behaviour of differential inclusion trajectories
- The QTD differential inclusion is a differential inclusion that includes a QDP.
- A Lyapunov function is a continuous function that decreases along trajectories of the differential inclusion.
- The QTD differential inclusion has a set of fixed points.

### QTD as a stochastic approximation to the QTD differential inclusion
- The abstract stochastic approximation result at the heart of the convergence proof of QTD is presented below.
- It is a special case of the general framework described by Benaïm et al. (2005), the proof of which is given in Appendix A.2.
- Theorem 5.7 states that if there exists a Lyapunov function L for this differential inclusion and a subset Λ ⊆ R n , then there exists a sequence (θ k ) k≥0 satisfying where: k=0 is a bounded martingale difference sequence with respect to the natural filtration generated by (θ k ) ∞ k=0 ; that is, there is an absolute constant C such that w k ∞ < C almost surely, and E[w k |θ 0 , . . . , θ k ] = 0.
- The intuition behind the conditions of the theorem are as follows. The Marchaud map condition ensures the differential inclusion of interest has global solutions. The existence of the Lyapunov function guarantees that trajectories of the differential inclusion converge in a suitably stable sense to Λ. The step size conditions, martingale difference condition, and boundedness conditions mean that the iterates (θ k ) ∞ k=0 will closely track the differential inclusion trajectories, and hence exhibit the same asymptotic behaviour.
- What makes the relaxation to the differential inclusion work in this analysis? We have already seen that some kind of relaxation of the dynamics is required in order to define a valid continuous-time dynamical system; the original ODE may not have solutions in general.
- If we relax the dynamics too much (an extreme example would be the differential inclusion ϑ t (x, i) ∈ R), what goes wrong? The answer is that there are too many resulting solutions, which do not exhibit the desired asymptotic behaviour. Thus, the differential inclusion in Equation ( 17) is in some sense just the right level of relaxation of the differential equation we started with, since trajectories of the QTD DI are still guaranteed to converge to the QDP fixed points.

### A Lyapunov function for the QDP fixed points
- The function is a Lyapunov function for the equilibrium point θπ m .
- The function takes on the value 0 only at θπ m .
- The function is a decreasing function.
- The function is contractive.
- The function is a Lyapunov function for the differential inclusion in Equation ( 17).

### Extension to asynchronous QTD
- An asynchronous version of QTD computes the sequence (θ k ) k≥0 defined by an initial estimate θ ∈ R X × [m] , a sequence of transitions (X k , R k , X k ) k≥0 , and the update rule for x = X k , and θ k+1 (x, i) = θ k (x, i) otherwise.
- The step size β x,k depends on both x and k, and is typically selected so that each state individually makes use of a fixed step size sequence (α k ) ∞ k=0 , by taking β x,k = α k l=0 1{X l =x} .
- This models the online situation where a stream of experience (X k , R k ) k≥0 is generated by interacting with the environment using the policy π, and updates are performed setting X k = X k+1 , and also the setting in which the tuples (X k , R k , X k ) k≥0 are sampled i.i.d. from a replay buffer, among others.

## Analysis of the QTD limit points
- Each return-distribution function ηπ λ is in the image of the projection Π λ ;
- The magnitude of this approximation error is not immediately clear;
- Each application of the projection Π λ in the dynamic programming process causes some loss of information;
- Measuring approximation error in w∞ typically turns out to be uninformative;
- In particular, fixed points ηπ λ that intuitively provide a good approximation to η π may have high w∞ -distance;
- The bound is proportional to max(τ 1 , max((τ i+1 − τ i )/2 : i = 2, . . . , m − 1), 1 − τ m ), which is minimised precisely by the choice of (τ i ) m i=1 used by QTD.

### Instance-dependent bounds
- The result above implicitly assumes the worst-case projection error is incurred at all states with each application of the Bellman operator.
- In environments where this is not the case, the fixed point can be shown to be of considerably better quality.
- We describe an example of an instance-dependent quality bound here.
- Proposition 6.3 states that if all reward distributions in the MDP are supported on [R min , R max ], then for any λ ∈ [0, 1] X ×[m] , we have Remark 6.4.
- One particular upshot of this bound for practitioners is that for agents in near-deterministic environments using near-deterministic policies, it may be possible to use m = o((1−γ) −1 ) quantiles and still obtain accurate approximations to the return-distribution function via QTD and/or QDP.
- It is interesting to contrast this result for quantile-based distributional reinforcement learning against the case when using categorical distribution representations (Bellemare et al., 2017;Rowland et al., 2018;Bellemare et al., 2023).
- In this latter case, fixed point error continues to be accumulated even when the environment has solely deterministic transitions and rewards, due to the well-documented phenomenon of the approximate distribution 'spreading its mass out' under the Cramér projection (Bellemare et al., 2017;Rowland et al., 2018;Bellemare et al., 2023).
- Our observation here leads to immediate practical advice for practitioners (in environments with mostly deterministic transitions, a quantile representation may be preferred to a categorical representation, leading to less approximation error), and raises a general question that warrants further study: how can we use prior knowledge about the structure of the environment to select a good distribution representation?

### Qualitative analysis of QDP fixed points
- Quantitative upper bounds on the quality of the fixed point learnt by QTD and guarantees that with enough atoms an arbitrarily accurate approximation of the return-distribution function (as measured by w 1 ) can be learnt.
- In the case of m = 2, half of the probability mass is placed on the greatest possible return in this MDP -namely 20 -even though with probability 1 the true return is less than this value.
- What is the cause of this behaviour from QDP? This question is answered by investigating the dynamic programming update itself in more detail.
- As m increases, we observe from the CDF plot that there is always one particle that learns this maximal return of 20, but that this has less effect on the other quantiles; indeed in the orange curve, we obtain a very good approximation (in w 1 ) to the true return distribution despite this particle with a maximal value of 20 remaining present.

## Related work
- Differential inclusions have been used to model stochastic approximation algorithms
- The ODE method was introduced by Ljung (1977) as a means of analysing stochastic approximation algorithms
- Standard references on the subject include Kushner and Yin (2003); Borkar (2008); Benveniste et al. (2012); see also Meyn (2022) for an overview in the context of reinforcement learning
- The framework we follow in this paper is set out by Benaïm (1999), and was extended by Benaïm et al. (2005) to allow for differential inclusions
- Perkins and Leslie (2013) later extended this analysis further to allow for asynchronous algorithms, building on the approach introduced by Borkar (1998), and extended, with particular application to reinforcement learning, by Borkar and Meyn (2000)
- Differential inclusions have found application across a wide variety of fields, including control theory (Wazewski, 1961), economics (Aubin, 1991) differential game theory (Krasovskii andSubbotin, 1988), andmechanics (Monteiro Marques, 2013)
- The approach to modelling differential equations with discontinuous right-hand sides via differential inclusions was introduced by Filippov (1960)
- Standard references on the theory of differential inclusions include Aubin and Cellina (1984); Clarke et al. (1998); Smirnov (2002); see also Bernardo et al. (2008) on the related field of piecewise-smooth dynamical systems
- Joseph and Bhatnagar (2019) also use tools combining stochastic approximation and differential inclusions from Benaïm et al. (2005) to analyse (sub-)gradient descent as a means of estimating quantiles of fixed distributions
- Within reinforcement learning and related fields more specifically, differential inclusions have played a key role in the analysis of game-theoretic algorithms based on fictitious play (Brown, 1951;Robinson, 1951); see Benaïm et al. (2006); Leslie and Collins (2006); Benaïm and Faure (2013) for examples
- More recently, Gopalan and Thoppe (2022) used differential inclusions to analyse TD algorithms for control with linear function approximation
- Quantile regression was introduced by Koenker and Bassett (1978)
- Quantile temporal-difference learning may be viewed as fusing quantile regression with the bootstrapping approach (learning a guess from a guess, as Sutton and Barto (2018) express it) that is core to much of the reinforcement learning methodology
- Quantiles in reinforcement learning were introduced by Dabney et al. (2018b)
- A variety of modifications and extensions were then considered in the deep reinforcement learning setting (Dabney et al., 2018a;Yang et al., 2019;Zhou et al., 2020;Luo et al., 2021), as well as further developments on the theoretical side (Lhéritier and Bondoux, 2022)
- A summary of the appraoch is presented by Bellemare et al. (2023)

## Conclusion
- We have provided the first convergence analysis for QTD, a popular and effective distributional reinforcement learning algorithm.
- In contrast to the analysis of many classical temporal-difference learning algorithms, this has required the use of tools from the field of differential inclusions and branches of stochastic approximation theory that deal with the associated dynamical systems.
- Due to the structure of the QTD algorithm, such as its bounded-magnitude updates, these convergence guarantees hold under weaker conditions than are generally used in the analysis of TD algorithms.
- These results establish the soundness of QTD, representing an important step towards understanding its efficacy and practical successes, and the theoretical tools used here are likely to be useful in further analyses of (distributional) reinforcement learning algorithms.
- We believe further research into the theory, practice and applications of QTD, at a variety of scales, are important directions for foundational reinforcement learning research.
