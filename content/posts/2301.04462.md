---
title: "An Analysis of Quantile Temporal-Difference Learning"
date: 2023-01-11T13:41:56.000Z
author: "Mark Rowland, Rémi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G. Bellemare, Will Dabney"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04462v1.webp" # image path/url
    alt: "An Analysis of Quantile Temporal-Difference Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04462)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04462).


# Abstract
- Quantile temporal-difference learning is a distributional reinforcement learning algorithm that has been successful in several large-scale applications
- Despite these empirical successes, a theoretical understanding of QTD has been elusive until now
- The core result of this paper is a proof of convergence to the fixed points of a related family of dynamic programming procedures with probability 1
- This establishes connections between QTD and non-linear differential inclusions through stochastic approximation theory and non-smooth analysis

# Paper Content

## Introduction
- Distributional reinforcement learning is a method for learning how to predict future returns
- The family of algorithms used in distributional reinforcement learning is based on the notion of learning quantiles of the return distribution
- QTD has been particularly successful in combination with deep reinforcement learning
- In this paper, we prove the convergence of QTD- notably under weaker assumptions than are required in typical proofs of convergence for classical TD learning -establishing it as a sound algorithm with theoretical convergence guarantees
- The proof relies on the stochastic approximation framework set out by Benaïm et al. (2005), arguing that the QTD algorithm approximates a continuous-time differential inclusion
- We then analyse the limit points of QTD, bounding their approximation error to the true return distributions of interest, and investigating the kinds of approximation artefacts that arise empirically.

## Background
- We use curly braces to indicate a list of items.
- We use square brackets to indicate a list of items, but only if they are not preceded by a comma.
- We use a colon to indicate a list of items, but only if they are not preceded by a comma.
- We use a semicolon to indicate a list of items, but only if they are not preceded by a comma.
- We use a plus sign to indicate a list of items that are followed by a comma.
- We use a minus sign to indicate a list of items that are followed by a comma.
- We use a slash to indicate a list of items that are not followed by a comma.
- We use a colon to indicate a list of items, but only if they are not preceded by a comma.
- We use a semicolon to indicate a list of items, but only if they are not preceded by a comma.
- We use a plus sign to indicate a list of items that are followed by a comma.
- We use a minus sign to indicate a list of items that are followed by a comma.
- We use a slash to indicate a list of items that are not followed by a comma.
- We use a comma to separate two items in a list.
- We use a semicolon to separate two items in a list.
- We use a comma to separate two items in a list, but only if they are not preceded by a plus sign or a minus sign.
- We use a comma to separate two items in a list, but only if they are not preceded by a plus sign or a minus sign.
- We use a semicolon to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a plus sign.
- We use a semicolon to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a semicolon to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a comma.
- We use a comma to separate two items in a list, but only if they are not preceded by a...

### Markov decision processes
- The Markov decision process is a model for how an agent interacts with an environment
- The transition kernel is a function that specifies how the agent's state changes based on its previous state and the environment
- The reward distribution function is a function that specifies how likely the agent is to receive a reward for each possible action it takes
- The discount factor is a parameter that determines how much the agent's reward is discounted when it is compared to the reward received for an action taken by an environment with the same state and action
- The agent's policy is a function that specifies how the agent will act in each state

### Predicting expected returns and the return distribution
- The quality of the agent's performance on the trajectory is quantified by the discounted return, or simply the return, given by ∞ t=0 γ t R t .
- The return is a random variable, whose sources of randomness are the random selections of actions made according to π, the randomness in state transitions, and the randomness in rewards observed.
- Typically in reinforcement learning, a single scalar summary of performance is given by the expectation of this return over all these sources of randomness.
- For a given policy, this is summarised across each possible starting state via the value function V π : X → R, defined by
- Learning the value function of a policy π from sampled trajectories generated through interaction with the environment is a central problem in reinforcement learning, referred to as the policy evaluation task.
- Each expected return is a scalar summary of a much more rich, complex object: the probability distributions of the random return in Equation (1) itself.
- Distributional reinforcement learning (Bellemare et al., 2023) is concerned with the problem of learning to predict the probability distribution over returns, in contrast to just their expected value.
- Mathematically, the goal is to learn the return-distribution function η π : X → P(R); for each state x ∈ X , η π (x) is the probability distribution of the random return in Expression (1) when the trajectory begins at state x, and the agent acts using policy π.
- Mathematically, we have where D π x extract the probability distribution of a random variable under P π x .
- There are several distinct motivations for aiming to learn these more complex objects. First, the richness of the distribution provides an abundance of signal for an agent to learn from, in contrast to a single scalar expectation.
- The strong performance of deep reinforcement learning agents that incorporate distributional predictions is hypothesised to be related to this fact (Dabney et al., 2018b;Barth-Maron et al., 2018;Dabney et al., 2018a;Yang et al., 2019).
- Second, learning about the full probability distribution of returns makes possible the use of risk-sensitive performance criteria; one may be interested in not only the expected return under a policy, but also the variance of the return, or the probability of the return being under a certain threshold.
- Unlike the value function V π , which is an element of R X , and can therefore be straightforwardly represented on a computer (up to floating-point precision), the return-distribution function η π is not representable.
- Each object η π (x) is a probability distribution over the real numbers, and, informally speaking, probability distributions have infinitely many degrees of freedom.
- Distributional reinforcement learning algorithms therefore typically work with a subset of distributions that are amenable to parametrisation on a computer (Bellemare et al., 2023).
- Common choices of subsets include categorical distributions (Bellemare et al., 2017), exponential families (Morimura et al., 2010b), and mixtures of Gaussian distributions (Barth-Maron et al., 2018).

### Monte Carlo and temporal-difference learning
- TD learning updates are an approximation to Monte Carlo learning
- First, we may observe that, under the condition that all reward distributions have finite variance, V π (x) is the unique minimiser of the following loss function over v:
- This well-known characterisation of the expectation of a random variable is readily verified by, for example, observing that the loss is convex and differentiable in v, and solving the equation ∂ v L π x (v) = 0
- This motivates an approach to learning V π (x) based on stochastic gradient descent on the loss function L π x
- We maintain an estimate V ∈ R X of the value function, and each time a trajectory (X t , A t , R t ) t≥0 beginning at state x is observed, we can obtain an unbiased estimator of the negative gradient of L π x (V (x)) as and update V (x) by taking a step in the direction of this negative gradient, with some step size α
- This is a Monte Carlo algorithm, so called because it uses Monte Carlo samples of the random return to update the estimate V
- A popular alternative to this Monte Carlo algorithm is temporal-difference learning, which replaces samples from the random return with a bootstrapped approximation to the return, obtained from a transition (x, A, R, X ) by combining the immediate reward R with the current estimate of the expected return obtained at X , resulting in the return estimate and the corresponding update rule
- While the mean-return estimator in Expression (4) is generally biased, since V (X ) is not generally equal to the true expected return V π (X ), it is often a lower-variance estimate, since we are replacing the random return from X with an estimate of its expectation
- This motivates the TD learning rule in Expression (5) based on the Monte Carlo update rule in Expression (3), with the understanding that this algorithm can be applied more generally, with access only to sampled transitions (rather than full trajectories), and may result in more accurate estimates of the value function, due to lower-variance updates, and the propensity of TD algorithms to "share information" across states

## Quantile temporal-difference learning and quantile dynamic programming
- Algorithms of study
- Algorithms of study
- Algorithms of study
- Algorithms of study
- Algorithms of study
- Algorithms of study
- Algorithms of study
- Algorithms of study

### Quantile regression
- To motivate QTD, we begin by considering how we might adapt a Monte Carlo algorithm such as that in Expression (3) to learn about the distribution of returns, rather than just their expected value.
- We cannot learn the return distribution in its entirety with a finite collection of parameters; the space of return distributions is infinite-dimensional, so we must instead be satisfied with learning an approximation of the return distribution by selecting a probability distribution representation (Bellemare et al., 2023, Chapter 5) -a subset of probability distributions parametrised by a finite-dimensional set of parameters.
- The approach of quantile temporal-difference learning is to learn an approximation of the form an equally-weighted mixture of Dirac deltas, for each state x ∈ X .
- The quantile-based approach to distributional reinforcement learning aims to have the particle locations (θ(x, i)) m i=1 approximate certain quantiles of η π (x).
- Definition 3.1. For a probability distribution ν ∈ P(R) and parameter τ ∈ (0, 1), the set of τ -quantiles of ν is given by the set where Expanding on this definition, if the set {z ∈ R : F ν (z) = τ } is non-empty, then the τquantiles are precisely the values z such that P Z∼ν (Z ≤ z) = τ .
- If however this set is empty (which may arise when F ν has points of discontinuity), then the quantile is the smallest value y such that P Z∼ν (Z ≤ y) > τ .
- These different cases are illustrated in Figure 1.
- The generalised inverse CDF of ν, F −1 ν : (0, 1) → R, is defined by and provides a way of uniquely specifying a quantile for each level τ .
- In cases where there is not a unique τ -quantile (see Figure 1), F −1 ν (τ ) corresponds to the left-most or least valid τ -quantile.
- We also introduce the notation which corresponds to the right-most or greatest τ -quantile; notice the strict inequality that appears in the definition, in contrast to that of , as is the case for τ = τ 1 and τ = τ 3 in Figure 1.
- However, if F ν has a flat region for the value τ (as is the case for τ = τ 2 in Figure 1), then F −1 ν (τ ) and F −1 ν (τ ) are distinct, and correspond to the boundary points of this flat region.
- Algorithmically, we aim for θ(x, i) to approximate a τ i -quantile of η π (x), where τ i = 2i−1 /2m.
- To build learning algorithms that achieve this, we require an incremental algorithm that updates θ(x, i) in response to samples from the target distribution η π (x), which converges to a 2i−1 /2m-quantile of η π (x).
- Such an approach is available by using the quantile regression loss.

### Quantile temporal-difference learning
- The quantile temporal-difference learning algorithm is motivated by the modification of the Monte Carlo algorithm that led to the TD algorithm.
- The QTD update makes use of temporal-difference errors r + γθ(x, j) − θ(x, i), and the presence of these terms causes the learnt parameters to make distinct predictions.
- The discussion above provides motivation for the form of the QTD update given in Algorithm 1, and intuition as to why this algorithm might perform reasonably, and learn a sensible approximation to the return distribution.

### Motivating examples
- QTD is a learning algorithm that can be used to approximate a distribution
- The behaviour of QTD depends on the characteristics of the environment
- QTD can exhibit a wide range of behaviours depending on the characteristics of the environment

### Quantile dynamic programming
- QTD updates the θ(x, i) to move in the direction of the τ i -quantiles of the distribution of the random variable R + θ(X , J).
- QDP directly calculates the τ i -quantiles.
- The asymptotic behaviour of QTD and QDP are equivalent.

## Convergence of quantile dynamic programming
- Algorithm 2 decomposes the update QDP performs into the composition of several operators.
- The first transformation performed by Algorithm 2 is to assign the distribution of R + γG(X ), where R, X are the random reward and next-state encountered from the initial state x with policy π, and (G(y) : y ∈ X ) is an independent collection of random variables, with each G(y) distributed according to η(y).
- T π : P(R) X → P(R) X is the distributional Bellman operator (Bellemare et al., 2017;Rowland et al., 2018;Bellemare et al., 2023).
- In terms of the above definition via distributions of random variables, T π can be written where (x, R, X ) is a random environment transition beginning at x, independent of (G(y) : y ∈ X ), and D π extracts the distribution of its argument when (x, R, X ) is generated by sampling an action from π.
- See Bellemare et al. ( 2023) for further background on the distributional Bellman operator.
- In general, T π η may comprise much more complicated distributions than η itself, with many more atoms, or possibly infinite support, if reward distributions are infinitely-supported.
- Algorithm 2 is mathematically equivalent to repeated application of the operator Π λ T π to an initial collection of quantile estimates.

### Convergence analysis
- The distributional Bellman operator is a contraction with respect to the Wasserstein-∞ metric
- The projection operator is a non-expansion with respect to the Wasserstein-∞ metric
- The projected operator is a γ-contraction with respect to the Wasserstein-∞ metric
- The fixed point ηπ λ depends on λ and m

## Convergence of quantile temporal-difference learning
- The QTD update is a noisy Euler discretisation of a differential equation
- The trajectories of the differential equation/inclusion converge to some set of fixed points in a suitable way
- The asymptotic behaviour of QTD, forming a noisy Euler discretisation, matches the asymptotic behaviour of the true trajectories

### The QTD differential equation
- Taking the expectation over the random variables R k (x) and X k (x) in Equation ( 13) conditional on the algorithm history up to time k yields an expected increment
- Assumption 5.2 guarantees that the two "difficult" cases of flat and vertical regions of CDFs (see Figure 1) do not arise; we will lift this assumption later.
- Calling back to Figure 3, the trajectories of the QTD ODE are obtained precisely by integrating the vector fields that appear in these plots.
- In contrast to the ODE that emerges when analysing classical TD learning (both in tabular and linear function approximation settings) (Tsitsiklis and Van Roy, 1997), the right-hand side of Equation ( 16) is non-linear in the parameters ϑ t , meaning that we are outside the domain of linear stochastic approximation methods.

### The QTD differential inclusion
- If F (T π θ)(x) is not continuous at θ(x, i), then the right-hand side of the QTD ODE in Equation ( 16) is modified to the difference is the strict inequality.
- Now the right-hand side of the differential equation itself is not continuous; in general, solutions may not even exist for this differential equation.
- The situation is illustrated in the bottom-left panel of Figure 3; the lines in this plot illustrate points of discontinuity of the vector field to be integrated, and there are instances where the vector field either side of such a line of discontinuity "pushes" back into the discontinuity.
- In such cases, the differential equation has no solution in the usual sense. This phenomenon is known as sliding, or sticking, from cases when it arises in the modelling of physical systems with potentially discontinuous forces (such as static friction models in mechanics).
- Filippov (1960) proposed a method to deal with such non-existence issues, by relaxing the definition of the dynamics at points of discontinuity. Technically, Filippov's proposal is to allow the derivative to take on any value in the convex hull of possible limiting values as we approach the point of discontinuity.
- In our case, we consider redefining the dynamics at points of discontinuity as follows: where . This refines the dynamics so that for each coordinate (x, i), the derivative may take on either the left or right limit around ϑ t (x, i), or any value in between; this is a looser relaxation than Filippov's proposal, and is easier to work with in our analysis.
- Equation ( 17) is a differential inclusion, as opposed to a differential equation; the derivative is constrained to a set at each instant, rather than constrained to a single value.
- We refer to Equation ( 17) specifically as the QTD differential inclusion (or QTD DI).

### Solutions of differential inclusions
- Definition 5.4: A solution to a differential inclusion is a path that starts and ends at the same point and is continuous.
- Proposition 5.5: If H is a Marchaud map, then there exists a constant C such that for all z ∈ R n , the differential inclusion ∂ t z t ∈ H(z t ) has a global solution.

### Asymptotic behaviour of differential inclusion trajectories
- The QTD differential inclusion is a differential inclusion that includes a QDP.
- A Lyapunov function is a continuous function that decreases along trajectories of the differential inclusion.
- Lyapunov functions are a central tool in dynamical systems for demonstrating convergence.

### QTD as a stochastic approximation to the QTD differential inclusion
- The abstract stochastic approximation result at the heart of the convergence proof of QTD is presented below
- It is a special case of the general framework described by Benaïm et al. (2005), the proof of which is given in Appendix A.2
- Theorem 5.7 states that if there exists a Lyapunov function L for this differential inclusion and a subset Λ ⊆ R n , then there exists a sequence (θ k ) k≥0 satisfying where: k=0 is a bounded martingale difference sequence with respect to the natural filtration generated by (θ k ) ∞ k=0 ; that is, there is an absolute constant C such that w k ∞ < C almost surely, and E[w k |θ 0 , . . . , θ k ] = 0
- The intuition behind the conditions of the theorem are as follows: the Marchaud map condition ensures the differential inclusion of interest has global solutions, the existence of the Lyapunov function guarantees that trajectories of the differential inclusion converge in a suitably stable sense to Λ, and the step size conditions, martingale difference condition, and boundedness conditions mean that the iterates (θ k ) ∞ k=0 will closely track the differential inclusion trajectories, and hence exhibit the same asymptotic behaviour

### A Lyapunov function for the QDP fixed points
- The function is a Lyapunov function for the equilibrium point θπ m .
- The function takes on the value 0 only at θπ m .
- The function is continuous, non-negative, and takes on the value 0 only at θπ m .
- The function is a decreasing function.
- The function is a contractive function.
- The function is a Lyapunov function for the differential inclusion in Equation ( 17).

### Extension to asynchronous QTD
- QTD is a synchronous algorithm that converges in the synchronous case
- An asynchronous version of QTD computes the sequence (θ k ) k≥0 defined by an initial estimate θ ∈ R X × [m] , a sequence of transitions (X k , R k , X k ) k≥0 , and the update rule for x = X k , and θ k+1 (x, i) = θ k (x, i) otherwise.
- β x,k depends on both x and k, and is typically selected so that each state individually makes use of a fixed step size sequence (α k ) ∞ k=0 , by taking β x,k = α k l=0 1{X l =x} .
- This models the online situation where a stream of experience (X k , R k ) k≥0 is generated by interacting with the environment using the policy π, and updates are performed setting X k = X k+1 , and also the setting in which the tuples (X k , R k , X k ) k≥0 are sampled i.i.d. from a replay buffer, among others.

## Analysis of the QTD limit points
- Each return-distribution function ηπ λ is in the image of the projection Π λ
- The magnitude of this approximation error is not immediately clear
- Each application of the projection Π λ in the dynamic programming process causes some loss of information
- Measuring approximation error in w∞ typically turns out to be uninformative
- For any λ ∈ [0, 1] X × [m] , if all reward distributions are supported on [R min , R max ], then we have where
- The bound also provides motivation for the specific values of (τ i ) m i=1 that QTD uses.

### Instance-dependent bounds
- The result above implicitly assumes the worst-case projection error is incurred at all states with each application of the Bellman operator.
- In environments where this is not the case, the fixed point can be shown to be of considerably better quality.
- We describe an example of an instance-dependent quality bound here.
- Proposition 6.3 states that if all reward distributions in the MDP are supported on [R min , R max ], then for any λ ∈ [0, 1] X ×[m] , we have Remark 6.4.
- One particular upshot of this bound for practitioners is that for agents in near-deterministic environments using near-deterministic policies, it may be possible to use m = o((1−γ) −1 ) quantiles and still obtain accurate approximations to the return-distribution function via QTD and/or QDP.
- It is interesting to contrast this result for quantile-based distributional reinforcement learning against the case when using categorical distribution representations (Bellemare et al., 2017;Rowland et al., 2018;Bellemare et al., 2023).
- In this latter case, fixed point error continues to be accumulated even when the environment has solely deterministic transitions and rewards, due to the well-documented phenomenon of the approximate distribution 'spreading its mass out' under the Cramér projection (Bellemare et al., 2017;Rowland et al., 2018;Bellemare et al., 2023).
- Our observation here leads to immediate practical advice for practitioners (in environments with mostly deterministic transitions, a quantile representation may be preferred to a categorical representation, leading to less approximation error), and raises a general question that warrants further study: how can we use prior knowledge about the structure of the environment to select a good distribution representation?

### Qualitative analysis of QDP fixed points
- QTD can approximate a return-distribution function accurately
- The approximation errors in QTD are due to the way in which QTD updates particles with backed-up particles
- The increase in m prevents pathological self-loops/small cycles from "leaking out" and degrading the quality of other quantile estimates.

## Related work
- Differential inclusions have been used to model stochastic approximation algorithms
- The ODE method was introduced by Ljung (1977) as a means of analysing stochastic approximation algorithms
- Standard references on the subject include Kushner and Yin (2003); Borkar (2008); Benveniste et al. (2012); see also Meyn (2022) for an overview in the context of reinforcement learning
- The framework we follow in this paper is set out by Benaïm (1999), and was extended by Benaïm et al. (2005) to allow for differential inclusions
- Perkins and Leslie (2013) later extended this analysis further to allow for asynchronous algorithms, building on the approach introduced by Borkar (1998), and extended, with particular application to reinforcement learning, by Borkar and Meyn (2000)
- Differential inclusions have found application across a wide variety of fields, including control theory (Wazewski, 1961), economics (Aubin, 1991) differential game theory (Krasovskii andSubbotin, 1988), andmechanics (Monteiro Marques, 2013)
- The approach to modelling differential equations with discontinuous right-hand sides via differential inclusions was introduced by Filippov (1960)
- Standard references on the theory of differential inclusions include Aubin and Cellina (1984); Clarke et al. (1998); Smirnov (2002); see also Bernardo et al. (2008) on the related field of piecewise-smooth dynamical systems
- Joseph and Bhatnagar (2019) also use tools combining stochastic approximation and differential inclusions from Benaïm et al. (2005) to analyse (sub-)gradient descent as a means of estimating quantiles of fixed distributions
- Within reinforcement learning and related fields more specifically, differential inclusions have played a key role in the analysis of game-theoretic algorithms based on fictitious play (Brown, 1951;Robinson, 1951); see Benaïm et al. (2006); Leslie and Collins (2006); Benaïm and Faure (2013) for examples
- More recently, Gopalan and Thoppe (2022) used differential inclusions to analyse TD algorithms for control with linear function approximation
- Quantile regression was introduced by Koenker and Bassett (1978)
- Quantile temporal-difference learning may be viewed as fusing quantile regression with the bootstrapping approach (learning a guess from a guess, as Sutton and Barto (2018) express it) that is core to much of the reinforcement learning methodology
- Quantiles in reinforcement learning were introduced by Dabney et al. (2018b)
- A variety of modifications and extensions were then considered in the deep reinforcement learning setting (Dabney et al., 2018a;Yang et al., 2019;Zhou et al., 2020;Luo et al., 2021), as well as further developments on the theoretical side (Lhéritier and Bondoux, 2022)
- A summary of the appraoch is presented by Bellemare et al. (2023)

## Conclusion
- The differential inclusion updates in a direction that is consistent with the direction of the perturbed solution.
- The differential inclusion updates in a direction that is consistent with the direction of the perturbed solution, as long as the noise is bounded.
- The differential inclusion updates in a direction that is consistent with the direction of the perturbed solution, as long as the noise is bounded.
- The differential inclusion updates in a direction that is consistent with the direction of the perturbed solution, as long as the noise is bounded.
- The differential inclusion updates in a direction that is consistent with the direction of the perturbed solution, as long as the noise is bounded.

## B. Implementations of quantile dynamic programming
- The algorithm can be used when the reward CDFs are available as input
- The algorithm can be used when the CDFs of the reward distributions are continuous

## C. Convergence of asynchronous QTD updates
- Step size restrictions
- Conditions on the sequence of states (X k ) k≥0 to be updated
- Modified differential inclusion
- Conditions on the reward distribution
- Targets ascending according to outcomes
