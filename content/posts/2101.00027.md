---
title: "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
date: 2020-12-31T19:00:10.000Z
author: "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2101-00027v1.webp" # image path/url
    alt: "The Pile: An 800GB Dataset of Diverse Text for Language Modeling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2101.00027)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2101.00027).


# Abstract
- increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability
- the pile is an 825 GiB english text corpus targeted at training large-scale language models
- evaluation of the untuned performance of GPT-2 and GPT-3 on the pile shows that these models struggle on many of its components, such as academic writing
- models trained on the pile improve significantly over both raw cc and cc-100 on all components of the pile, while improving performance on downstream evaluations

# Paper Content

## Introduction
- Recent breakthroughs in general-purpose language modeling have demonstrated the effectiveness of training massive models on large text corpora for downstream applications
- As the field continues to scale up language model training, the demand for high-quality massive text data will continue to grow
- The growing need for data in language modeling has caused most existing large-scale language models to turn to the Common Crawl for most or all of their data
- While training on the Common Crawl has been effective, recent work has shown that dataset di-1 versity leads to better downstream generalization capability
- Additionally, large-scale language models have been shown to effectively acquire knowledge in a novel domain with only relatively small amounts of training data from that domain
- These results suggest that by mixing together a large number of smaller, high quality, diverse datasets, we can improve the general cross-domain knowledge and downstream generalization capabilities of the model compared to models trained on only a handful of data sources
- To address this need, we introduce the Pile: a 825.18 GiB English text dataset designed for training large scale language models
- The Pile is composed of 22 diverse and high-quality datasets, including both established natural language processing datasets and several newly introduced ones
- In addition to its utility in training large language models, the Pile can also serve as a broad-coverage benchmark for cross-domain knowledge and generalization ability of language models
- We introduce new datasets derived from the following sources: PubMed Central, ArXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Office, PubMed, Ubuntu IRC, HackerNews, YouTube, PhilPapers, and NIH ExPorter. We also introduce OpenWebText2 and BookCorpus2, which are extensions of the original OpenWebText (Gokaslan and Cohen, 2019) and BookCorpus (Zhu et al., 2015;Kobayashi, 2018) datasets, respectively
- In addition, we incorporate several existing highquality datasets: Books3 (Presser, 2020), Project Gutenberg (PG-19) (Rae et al., 2019), Open-Subtitles (Tiedemann, 2016), English Wikipedia, DM Mathematics (Saxton et al., 2019), EuroParl (Koehn, 2005), and the Enron Emails corpus (Klimt and Yang, 2004)
- To supplement these, we also in-1 arXiv:2101.00027v1 [cs.CL] 31 Dec 2020 perform an exploratory analysis of the text within the Pile to provide a detailed picture of the data
- We hope that our extensive documentation of the construction and characteristics of the Pile will help researchers make informed decisions about potential downstream applications
- Finally, we make publicly available the preprocessing code for the constituent datasets of the Pile and the code for constructing alternative versions2

### Contributions
- The introduction of a 825.18 GiB englishlanguage dataset for language modeling
- The introduction of 14 new language modeling datasets
- Evaluations demonstrating significant improvements across many domains by GPT-2sized models trained on this new dataset, compared to training on CC-100 and raw Common Crawl

## The Pile Datasets
- The Pile is composed of 22 datasets
- Each dataset has a weight that is based on its quality
- The Pile is updated every day
- The Pile is designed to be more accurate than other datasets

### Pile-CC
- Common Crawl is a collection of website crawls from 2008 onwards
- Due to the raw nature of the dataset, Common Crawl has the advantage of including text from diverse domains
- However, the quality of the data varies
- Pile-CC uses jus-Text on Web Archive files to extract higher quality text

### PubMed Central
- PubMed Central is a subset of the PubMed online repository
- It provides open, full-text access to nearly five million publications
- Most publications indexed by PMC are recent
- Indexing is mandated for all NIH funded research starting from 2008

### Books3
- Books3 is a dataset of books derived from a copy of the contents of the Bibliotik private tracker made available by Shawn Presser.
- Bibliotik consists of a mix of fiction and nonfiction books and is almost an order of magnitude larger than our next largest book dataset.
- We included Bibliotik because books are invaluable for long-range context modeling research and coherent storytelling.

### OpenWebText2
- OWT2 is a generalized web scrape dataset inspired by WebText and OpenWebTextCorpus
- It includes more recent content from Reddit submissions up until 2020
- It includes content from multiple languages, document metadata, and open source replication code
- It is considered a high quality general purpose dataset

### ArXiv
- ArXiv is a preprint server for research papers
- ArXiv papers are predominantly in the fields of Math, Computer Science, and Physics
- ArXiv is included in the hopes that it will be a source of high quality text and math knowledge
- ArXiv papers are written in LaTeX, a common typesetting language for mathematics, computer science, physics, and some adjacent fields

### GitHub
- GitHub is a large corpus of open-source code repositories
- Motivated by the ability of GPT-3 to generate plausible code completions despite its training data not containing any explicitly gathered code datasets, we included GitHub in the hopes that it would enable better downstream performance on code-related tasks.

### FreeLaw
- The Free Law Project provides access to and analytical tools for academic studies in the legal realm
- CourtListener provides bulk downloads for millions of legal opinions from federal and state courts
- While the full dataset provides multiple modalities of legal proceedings, including dockets, bibliographic information on judges, 3 https://www.courtlistener.com/ and other metadata, we focused specifically on court opinions due to an abundance of full-text entries

### Stack Exchange
- The Stack Exchange Data Dump4 contains an anonymized set of all user-contributed content on the Stack Exchange network
- The Stack Exchange Data Dump4 is one of the largest publicly available repositories of question-answer pairs
- The Stack Exchange Data Dump4 covers a wide range of subjects

### USPTO Backgrounds
- USPTO Backgrounds is a dataset of background sections from patents granted by the United States Patent and Trademark Office
- The dataset includes a large volume of technical writing on applied subjects, aimed at a non-technical audience
- The dataset is derived from the USPTO's published bulk archives

### Wikipedia (English)
- Wikipedia is a standard source of high-quality text for language modeling.
- It is also valuable as it is written in expository prose, and spans many domains.

### PubMed Abstracts
- PubMed abstracts consists of the abstracts from 30 million publications
- PMC provides full-text access, but the subset of coverage is significantly limited and biased towards recent publications
- MED-LINE expands the coverage of biomedical abstracts from 1946 to present day

### Project Gutenberg
- PG-19 is a dataset of classic Western literature
- The dataset is used for long-distance context modeling

### OpenSubtitles
- The OpenSubtitles dataset is an English language dataset of subtitles from movies and television shows.
- Subtitles provide an important source of natural dialog, as well as an understanding of fictional formats other than prose.
- This data could be useful for creative writing tasks such as screenwriting, speechwriting, and interactive storytelling.

### DeepMind Mathematics
- The DeepMind Mathematics dataset consists of a collection of mathematical problems from topics such as algebra, arithmetic, calculus, number theory, and probability.
- One major weakness of large language models has been performance on mathematical tasks (Brown et al., 2020), which may be due in part to a lack of math problems in the training set.
- By explicitly including a dataset of mathematical problems, we hope to improve the mathematical ability of language models trained on the Pile.

### BookCorpus2
- BookCorpus2 is an expanded version of the original BookCorpus
- BookCorpus is unlikely to have significant overlap with Project Gutenberg and Books3, which consist of published books
- BookCorpus is commonly used as dataset for training language models

### Ubuntu IRC
- The Ubuntu IRC dataset is derived from the publicly available chatlogs
- Chatlog data provides an opportunity to model real-time human interactions
- This is a unique opportunity because it features a level of spontaneity not typically found in other modes of social media

### EuroParl
- EuroParl is a multilingual parallel corpus
- It has been used for machine translation, but also in other fields of NLP
- The most current version is from the European Parliament
- EuroParl is a large parallel corpus of texts from the European Parliament

### YouTube Subtitles
- The YouTube Subtitles dataset is a parallel corpus of text gathered from human generated closedcaptions on YouTube.
- In addition to providing multilingual data, Youtube Subtitles is also a source of educational content, popular culture, and natural dialog.

### PhilPapers
- The PhilPapers7 dataset consists of open-access philosophy publications from an international database.
- We included PhilPapers because it spans a wide body of abstract, conceptual discourse.

### NIH Grant Abstracts: ExPORTER
- The NIH Grant abstracts provides a bulk-data repository for awarded applications.
- The dataset contains examples of high-quality scientific writing.

### Hacker News
- Hacker News 9 is a link aggregator operated by Y Combinator
- Users submit articles that are "anything that gratifies one's intellectual curiosity"
- Submitted articles tend to focus on computer science and entrepreneurship topics
- Users can comment on submitted stories, resulting in comment trees discussing and critiquing submitted stories
- We scrape, parse, and include these comment trees in our analysis

### Enron Emails
- The Enron Emails dataset is a valuable corpus commonly used for research about the usage patterns of email
- We included Enron Emails to aid in understanding the modality of email communications, which is typically not found in any of our other datasets
- The Pile can be used as a broad-coverage dataset for benchmarking language models

### Benchmarking Guidelines
- The Pile is provided as train, validation, and testing splits.
- The validation and testing components each contain 0.1% of the data, sampled uniformly at random.
- While this is a far smaller percentage than most datasets, the sheer size of the dataset results in over 1 GiB of validation and testing data each.
- We highlight that while we have made efforts to deduplicate documents within the Pile (See: Section D.2), it is still possible that some documents are duplicated across the train/validation/test splits.
- Our preferred metric is bits per UTF-8 encoded byte (BPB).
- Bits per byte is preferred over bits per character or perplexity when using Pile as a metric due to its invariance to different tokenization schemes and the ambiguity of measuring characters in Unicode.
- To compute bits per byte from a given negative log likelihood loss, we compute, where L T is the length of the dataset in tokens and L B is the length of the dataset in UTF-8 encoded bytes.
- We find that L T /L B is 0.29335 GPT-2tokens/byte across the Pile; dataset-specific values of L T /L B can be found in Table 7.

### Test Perplexity with GPT-2 and GPT-3
- Computes the test perplexity of the constituent datasets of the Pile
- Uses GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020)
- Evaluates on one-tenth of the respective test sets
- Reports the perplexity converted to bits per UTF-8 encoded byte (BPB)
- Interestingly, while GPT-2 and GPT-3 were not trained on the Pile, there still appears to be a clear scaling law without diminishing returns

### Relative Componentwise GPT-3 Pile Performance
- Determining which components GPT-3 underperforms on provides information about which Pile components are most dissimilar to the distribution of text (web pages and books) that GPT-3 was trained on.
- These components would thus make especially good candidates for supplementing GPT-3 training data.
- These results are also valuable for determining which types of datasets to emphasize for future iterations of the Pile.
- Due to the difference in entropy of different datasets, directly comparing perplexity of GPT-3 on different Pile components is not an accurate indication of relative performance.
- Ideally we would train a GPT-3 model from scratch on the Pile and compare the difference in loss per dataset with that of the original GPT-3.
- Because of resource constraints, we instead use a GPT-2 model trained from scratch on the Pile (see Section 4) to construct a proxy measure.
- To construct our proxy, we first measure the improvement from the GPT-2-Pile model to GPT-3 on each component.
- Then, we normalize our results by setting the change on OpenWebText2 to be zero.
- This computation is shown in the equation below:
- As GPT-3 was trained on data very similar to OWT2, this gives us a proxy for how much better GPT-3 would do if it were trained on the Pile.
- The results are shown in Figure 3.
- As a sanity check, we observe that datasets that are contained in, or are extremely similar to, GPT-3's training set (Books3, Wikipedia (en), Pile-CC and Project Gutenberg) score close to zero on our metric.
- GPT-3 appears to perform poorly on datasets pertaining to research or academic writing like PubMed Central, PubMed Abstracts, and ArXiv; domain-specific datasets like FreeLaw, Hack-erNews, and USPTO Backgrounds; and on datasets containing predominantly text distinct from natural language, like GitHub and DM Mathematics.
- In addition, the majority of datasets see less of an improvement than OpenWebText2.
- As such, we expect a GPT-3 sized model trained on Pile to perform significantly better on research related tasks, software tasks, and symbol manipulation tasks than the base model.
- Additionally, this experiment provides evidence that the majority of Pile components are not redundant with the predominantly web-based GPT-3 training data.

## Evaluation
- To confirm the effectiveness of the Pile for improving language modeling quality, we train architecturally-identical 1.3 billion parameter models based on those in Brown et al. (2020) on different datasets and evaluate on the WikiText and LAMBADA tasks.
- We also report results on the Pile as a measure of more cross-domain generalization.

### Methodology
- We decontaminate any instances of the evaluation sets using the same 13-gram overlap filtering as in Brown et al. (2020) and downsample to 40GB to control for dataset size.
- As we control for dataset size, we emphasize that our evaluation is generous to CC-100 (en), which is about 1/3 the size of the Pile in reality.
- We compare the following datasets: the Pile, the En-  et al., 2019;Conneau et al., 2020), and a sample of raw CC WET files filtered for English-only.

### Results
- The Pile outperforms traditional language modeling benchmarks
- The Pile improves significantly on WikiText and LAMBADA
- However, models trained on Pile improve significantly over both Raw CC and CC-100 on all components of the Pile

## Structural Statistics
- The Structural Statistics provide more coarse-grained and statistical information about the Pile.
- In Section 6, we provide a closer investigation and documentation of the textual content within the Pile datasets.

### Document Lengths and Tokenization
- The dataset consists of a large number of documents.
- We analyze the distribution of document lengths, as well as the number of bytes-per-token.
- While the majority of documents in the Pile are short, there is a long tail of very long documents.

### Language and Dialects
- The vast majority of NLP research is done on English
- For the Pile, we took a similar approach to the dataset used by Brown et al. (2020) and focused predominantly on English, while also not explicitly filtering out other languages when collecting our own data
- When evaluating a multilingual dataset, our main criteria for inclusion was whether the English component of the dataset merited inclusion alone
- We plan to create a fully multi-lingual expansion of the Pile as future work
- Using fasttext (Suárez et al., 2019a), we determine that the Pile is 97.4% English

## Investigating and Documenting the Datasets
- As the scale of machine learning research has grown, scrutiny has been placed on the ever larger datasets that models are trained on
- While this issue has been raised within AI ethics and bias research, it has not been a focal point of concern within the language modeling community
- Despite the proliferation of work exploring and documenting issues with datasets, no dataset intended to train massive language models has been seriously documented by its creators
- Therefore, our analyses serve two goals: to address ethical concerns about the Pile, and to promote and normalize the practice of engaging with the AI ethics literature.

### Documenting Methods
- The Pile is a research project that uses two different frameworks for documentation
- The first, the datasheets methodology, is recommended by several methodologists and appears to be used more frequently by practitioners
- The second, the data statements methodology, was proposed specifically for natural language processing and has been well received
- The datasheet and data statement will be featured in the GitHub repository where the code for the Pile is stored and will also be available as separate documents
- In addition to the datasheet and data statement, there is additional information that may be helpful to people training language models.

### Topical Distribution
- Performed a topic modeling analysis on the components of the Pile
- Used Gensim to train 16-topic Latent Dirichlet Allocation models on each component of the validation set of the Pile concurrently
- Filtered the Pile for English only for this analysis
- Evaluated the Pile-CC topic model on the documents of OpenWebText2 as a baseline of comparison
- Documented the topical clusters inferred from the LDA models for each component

### Pejorative Content
- The Pile can contain pejorative, sexually explicit, or otherwise objectionable content
- We used the profanity-checker Python package to determine whether a given string is profane
- The Pile appears less profane than Pile-CC
- We also broke each dataset down on a sentence level, to allow profanity-checker to check entire sentences.

### Bias and Sentiment Co-occurrence
- The different language models that were trained on the Pile picked up unexpected biases
- We looked at co-occurrence tests to see what words bias towards specific categories
- We found that words like "military", "criminal", and "offensive" strongly bias towards men, while "little", "married", "sexual", and "happy" bias towards women
- We found that "Buddhist" has the highest sentiment, followed by "Hindu", "Christian", "Atheist", and "Muslim"
- We found that "black" had the lowest sentiment, at -0.15

### Author Consent and Public Data
- There is disagreement surrounding the ethics of repurposing data protected by terms of service in research contexts
- Many consider doing so a moral obligation or a good measure to guard against misuse
- On the other hand, there is significant disagreement surrounding the ethics of repurposing data protected by terms of service in research contexts
- People typically do not read Terms of Service
- Each of these datasets were published by researchers and are passed around freely on the internet
- Our use of the YouTube Subtitles dataset does not constitute significantly increased harm beyond that which has already been done by the widespread publication of these datasets

## Implications and Broader Impacts
- The Pile is a new algorithm that can scale datasets to much larger sizes than previous algorithms.
- There are many concerns about how the emergence of stronger AI systems will influence the wider world, and we believe that they merit serious thought.
- In this section we discuss the legal ramifications of the Pile, and then consider the impact of the Pile to AI alignment from two angles: accelerating AI timelines and the dangers posed by unaligned language models.

### Legality of Content
- Under pre 1984 (and affirmed in subsequent rulings such as aff 2013; Google 2015), noncommercial, not-for-profit use of copyright media is preemptively fair use.
- Our use is transformative, in the sense that the original form of the data is ineffective for our purposes and our form of the data is ineffective for the purposes of the original documents.
- Although we use the full text of copyright works, this is not necessarily disqualifying when the full work is necessary.
- In our case, the long-term dependencies in natural language require that the full text be used in order to produce the best results.
- Copyright law varies by country, and there may be additional restrictions on some of these works in particular jurisdictions.

### Acceleration of AI Timelines
- There is serious concern that AI systems may soon be meaningfully more capable than humans in all relevant economic tasks
- Relatedly, there are serious unresolved questions surrounding how to properly align such powerful AI systems with human interests
- There are several pragmatic responses to this view: 1. Due to human competition, curiosity, and cultural diversity, halting technological development is incredibly difficult, if not impossible.
- High powered language models, along with their more general successors, must be capable of viewing morally problematic content without adopting it in their output.
- We elaborate on this in the following section.
- With this in mind, we accept the reality that the Pile could potentially accelerate AI timelines. However, we hope our efforts to establish best practices, such as thoroughly documenting the contents of our data, will help encourage diligence for downstream researchers on alignment problems.

### Negative LM Output
- There are potential negative effects of powerful language models, such as the ability to mass produce low quality content for the purpose of Search Engine Optimization.
- Directly solving these problems would require sweeping changes to the architecture of the Internet, such as vastly expanded Public Key Infrastructure and distributed authentication of identity.
- However, attacking this problem from the training set side is unproductive and ultimately leads us away from optimal solutions.

## Related Work

## B Excluded Datasets
- The Pile includes data from 1800-today from the US Congressional Record, Fanfiction, and Literotica.
- The Pile excludes data from the US Congressional Record because it contains a lot of racist content.
- The Pile excludes data from Fanfiction because it is a short-form writing style that is not represented in most language modeling datasets.
- The Pile excludes data from Literotica because it would require a lot of investigation, assessment, and care to include it in the Pile.

## C Dataset Details
- Common Crawl was used to obtain the CC dataset
- The CC dataset was processed using jusText, which is a text extraction tool that is designed for creating text corpora
- The jusText tool was compared to other text extraction tools, and it was found to be the best option for the CC dataset
- The CC dataset was filtered for quality using a ratio of 3, which was determined by a study that used a different ratio
- The T E X sources of all papers on arXiv up to the July 2020 dump were downloaded

### C.21 NIH Grant abstracts: ExPORTER
- The NIH provides a bulk-data repository for awarded applications through the ExPORTER service
- These data come from the NIH, but also other other Health and Human Services agencies (ACF, AHRQ, CDC, HRSA, FDA), and the VA
- Additionally, the NIH provides a legacy data format named CRISP for awarded applications during the fiscal years 1970-2009
- We merged both the ExPORTER and CRISP data to form a consolidated dataset of awarded applications
- Entries were deduplicated based off their application ID, and excluded if their abstract text was missing or too short
- Small grants, especially administrative ones, consisted solely of short boilerplate
- For this reason, we further deduplicated on abstract text
- All grants types were considered, including new applications (Application Type Code 1) and renewals (Application Type Code 2)
- The text was then minimally parsed to remove administrative boilerplate

## D General Data Processing
- Iterates over multiple datasets to combine them into a single, larger dataset
- Uses a weighted random sampling process to draw documents from each dataset
- Uses a quadratic Minhash similarity comparison to determine which documents to keep
- Uses a in-memory LSH persistence method to avoid corruption when de-duplicating datasets

### D.3 Downstream Validation Leakage
- To avoid leakage of data from downstream evaluations, recent work (Radford et al., 2019;Brown et al., 2020;Shoeybi et al., 2019) has removed any data in the training set that may overlap with the evaluation metrics.
- We decided not to perform any such removal, because it is impossible to anticipate all potential downstream evaluation metrics, and so any particular selection of metrics would inevitably either become obsolete as the choice of benchmarks in the field changes, or potentially hinder the development of new benchmarks for models trained on Pile.
- For models trained on Pile and evaluated on metrics other than Pile's own validation and test sets, we encourage authors to remove overlaps between Pile and the validation data of these additional downstream evaluations.
- We do not anticipate that such leakage removal will hurt model performance, as the validation sets of most benchmarks are very small in relation to the size of the Pile, and so choosing to evaluate on more metrics will not be a disadvantage for any model.
- As part of our exploratory analysis, we calculated the counts of all 13-grams across Common Crawl.
- We chose n = 13 due to its use in prior work (Brown et al., 2020).
- There were a total of 40,216,231,078 different 13-grams in this dataset.
- The 1000 most common range from 11 million occurrences down to 20k.
- The most frequently occurring 13-grams were character repetitions used for styling such as "----", " * * * * ", "! ! ! !", at 11 million, 5.8 million and 1.1 million respectively.
- Other characters used in this manner include the following: "# . > ?".
- In the 264k count range, we see repetitions of badly formatted HTML escape characters "; &nbsp", "; amp".
- Boilerplate from standard forum software appears around the 180k occurrences range, such as the following: "select the forum that you want to visit from the selection below".
- Overall, a large amount of common HTML and CSS is included in the top 1000, along with boilerplate text from Amazon Affiliate Advertising, Tokens TripAdvisor, SimplyHired, Associated Press, Post-Media, The FCC etc.
- PHP error messages and password login prompts also made an appearance.

### E.2 Benchmark Perplexity Computation
- Tokenize the document
- Divide the document into segments of up to the maximum sequence length of the model
- Predict the logits of the each segment
- Compute the perplexity for the whole Pile

## F Data Samples
- The trier of fact is the judge or jury in a criminal trial.
- The trier of fact is the person who decides whether the defendant is guilty or not.
- The trier of fact is the person who decides whether the victim deserves compensation.
- The trier of fact is the person who decides whether the defendant should be punished.
- The trier of fact is the person who decides whether the victim should be punished.
