---
title: "Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning"
date: 2022-01-28T10:13:35.000Z
author: "Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2201-12023v3.webp" # image path/url
    alt: "Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2201.12023)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2201.12023).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/alpa-automating-inter-and-intra-operator).

# Abstract
- Alpa automates the model-parallel training of large deep learning models
- Alpa distributes the training of large deep learning models by viewing parallelisms as two hierarchical levels: inter-operator and intra-operator parallelisms
- Based on it, Alpa constructs a new hierarchical space for massive model-parallel execution plans
- Alpa implements an efficient runtime to orchestrate the two-level parallel execution on distributed compute devices
- Our evaluation shows Alpa generates parallelization plans that match or outperform hand-tuned model-parallel training systems even on models they are designed for.

# Paper Content

## Introduction
- Several advances in deep learning have been a direct result of significant increases in model size.
- For example, scaling language models, such as GPT-3, to hundreds of billions of parameters and training on much larger datasets enabled fundamentally new capabilities.
- However, training these extremely large models on distributed clusters currently requires a significant amount of engineering effort that is specific to both the model definition and the cluster environment.
- For example, training a large transformer-based language model requires heavy tuning and careful selection of multiple parallelism dimensions.
- Training the large Mixture-of-Expert (MoE) transformers model on TPU clusters requires manually tuning the partitioning axis for each layer, whereas training the same model on an AWS GPU cluster calls for new pipeline schemes that can depend on the choices of partitioning.
- More generally, efficient large-scale model training requires tuning a complex combination of data, operator, and pipeline parallelization approaches at the granularity of the individual tensor operators.
- Correctly tuning the parallelization strategy has been shown to deliver an order of magnitude improvements in training performance, but depends on strong machine learning (ML) and system expertise.
- Automating the parallelization of large-scale models would significantly accelerate ML research and production by enabling model developers to quickly explore new model designs without regard for the underlying system challenges.
- Unfortunately, it requires navigating a complex space of plans that grows exponentially with the dimensions of parallelism and the size of the model and cluster.
- For example, when all parallelism techniques are enabled, figuring out the execution plan involves answering a web of interdependent questions, such as how many data-parallel replicas to create, which axis to partition each operator along, how to split the model into pipeline stages, and how to map devices to the resulting parallel executables.
- The interplay of different parallelization methods and their strong dependence on model and cluster setups form a combinatorial space of plans to optimize.
- Recent efforts to automatically parallelize model training are constrained to the space of a single model-parallelism approach, or rely on strong assumptions on the model and cluster specifications.
- Our key observation is that we can organize different parallelization techniques into a hierarchical space and map these parallelization techniques to the hierarchical structure of the compute cluster.
- Different parallelization techniques have different bandwidth requirements for communication, while a typical compute cluster has a corresponding structure: closely located devices can communicate with high bandwidth while distant devices have limited communication bandwidth.
- With this observation in mind, in this paper, we take a different view from conventional data and model parallelisms, and re-categorize ML parallelization approaches as intra-operator and inter-operator parallelisms.
- Intra-operator parallelism partitions ML operators along one or more tensor axes (batch or non-batch) and dispatches the partitions to distributed devices (Fig. 1c); inter-operator parallelism, on the other hand, slices the model into disjoint stages and pipelines the execution of stages on different sets of devices (Fig. 1d).
- They take place at two different granularities of the model computation, differentiated by whether to partition operators.
- Given that, a parallel execution plan can be expressed hierarchically by specifying the plan in each parallelism category, leading to a number of advantages.
- First, intra-and interoperator parallelisms feature distinct characteristics: intraoperator parallelism has better device utilization, but results in communicating at every split and merge of partitioned operators, per training iteration; whereas inter-operator parallelism only communicates between adjacent stages, which can be light if sliced properly, but incurs device idle time due to scheduling constraints.

## Background: Distributed Deep Learning
- DL computation is commonly represented by popular ML frameworks
- In practice, model developers define the dataflow graph.
- An execution engine then optimizes and executes it on a compute device.

### Conventional View of ML Parallelism
- Data parallelism: The training data is partitioned across distributed workers, but the model is replicated.
- Operator parallelism: When the model is too large to fit in one device, operator parallelism is an effective model parallelism option.
- Pipeline parallelism: Instead of partitioning ops, pipeline parallelism places different groups of ops from the model graph, referred as stages, on different workers; meanwhile, it splits the training batch as a number of microbatches, and pipelines the forward and backward passes across microbatches on distributed workers.
- Manual combination of parallelisms: Recent development shows the approaches mentioned above need to be combined to scale out today's large DL models.
- Automatic combination of parallelisms: The configurations of each individual parallelism, their interdependence, and their dependence on model and cluster setups form an intractable space, which prevents the trivial realization of automatically combining these parallelisms.

### Intra-and Inter-Operator Parallelisms
- Different from the conventional view, in this paper, we recatalog existing parallelization approaches into two orthogonal categories: intra-operator and inter-operator parallelisms.
- Intra-operator parallelism involves partitioning operators along some tensor axis, while inter-operator parallelism does not.
- Intra-operator parallelism results in substantial communication among distributed devices, while inter-operator parallelism does not.
- Concurrent work has proposed similar categorization, but Alpa is the first end-to-end system that uses this categorization to automatically generate parallel plans from the full space.

## Overview
- Alpa is a compiler that generates model-parallel execution plans by hierarchically optimizing the plan at two different levels: intra-op and inter-op parallelism.
- At the intra-op level, Alpa minimizes the cost of executing a stage (i.e., subgraph) of the computational graph, with respect to its intra-operator parallelism plan, on a given device mesh, which is a set of devices that may have high bandwidth between each other (e.g., GPUs within a single server).
- Different meshes might have different numbers of computing devices according to the workload assigned.
- At the inter-op level, Alpa minimizes the inter-op parallelization latency, with respect to how to slice the model and device cluster into stages and device meshes and how to map them as stage-mesh pairs.
- The inter-op optimization depends on knowing the execution cost of each stage-mesh pair reported by the intra-op optimizer.
- Through this hierarchical optimization process, Alpa generates the execution plan consisting of intra-op and inter-op plans which are 2 Device placement [36] is another case of inter-op parallelism, which partitions the model graph and executes them on different devices but does not saturate pipelines using multiple microbatches. Hence pipeline parallelism is often seen as a better alternative to it because of less device idle time.
- The developers uses a Python decorator @parallelize to annotate functions that need to be parallelized.
- The rest of the program is kept intact.
- locally near-optimal at their respective level of the hierarchy.
- To achieve this, Alpa implements three novel compilation passes as Fig. 3 shows.
- Given a model description, in the form of a Jax [9] intermediate representation (IR), and a cluster configuration, the inter-op compilation pass slices the IR into a number of stages, and slices the device cluster into a number of device meshes.
- The inter-op pass uses a Dynamic Programming (DP) algorithm to assign stages to meshes and invokes the intra-op compilation pass on each stage-mesh pair, to query the execution cost of this assignment. Once invoked, the intra-op pass optimizes the intra-op parallel execution plan of the stage running on its assigned mesh, by minimizing its execution cost using an Integer Linear Programming (ILP) formulation, and reports the cost back to the inter-op pass.
- By repeatedly querying the intra-op pass for each allocation of a stage-mesh pair, the inter-op pass uses the DP to minimize the inter-op parallel execution latency and obtains the best slicing scheme of stages and meshes.
- Given the output hierarchical plan and a designated pipeline-parallel schedule, each stage is first compiled as a parallel executable on its located mesh.
- A runtime orchestration pass is invoked to fulfill the communication requirement between two adjacent stages that require communication between the two meshes they locate on.
- The runtime orchestration pass then generates static instructions specific to each mesh according to the pipeline-parallel schedule and invokes the execution on all meshes.
- API.
- Alpa requires developers to annotate functions to be parallelized, such as the train_step(), using a Python decorator @parallelize.
- Upon the first call to train_step(), Alpa traces the whole function to get the model IR, invokes the compilation, and converts the function to a parallel version.
- Since the inter-op pass depends on the intra-op pass, in the following text, we first describe the intra-op pass, followed by the inter-op pass, and finally the runtime orchestration pass.

## Intra-Operator Parallelism
- Alpa optimizes the intra-operator parallelism plan within a device mesh.
- Alpa adopts the SPMD-style intra-op parallelism [31,57] which partitions operators evenly across devices and executes the same instructions on all devices, as per the fact that devices within a single mesh have equivalent compute capability.
- This SPMD style significantly reduces the space of intra-op parallelism plans; meanwhile, it conveniently expresses and unifies many important approaches such as data parallelism, ZeRO, Megatron-LM's operator parallelism, and their combinations, which are not fully covered by existing automatic operators parallelism systems, such as Tofu [55] and FlexFlow [25].
- Unlike systems that perform randomized search [25] or assume linear graphs [55], Alpa formalizes the problem as an integer linear programming (ILP) and shows it can be solved efficiently for computational graphs with tens of thousands of operators.

### The Space of Intra-Operator Parallelism
- Given an operator in the computational graph, there are multiple possible parallel algorithms to run it on a device mesh.
- For a set of physical devices, there can be multiple logical views.
- The mapping between physical devices and the logical device mesh view is optimized by the inter-op pass.
- In the rest of this section, we consider one fixed device mesh view.
- Sharding Spec.
- We use sharding spec to define the layout of a tensor.
- For an N-dimensional tensor, its sharding spec is defined as means the i-th axis of the tensor is partitioned.
- Otherwise, the i-th axis is replicated.
- After we define which tensor axes are partitioned, we then have to map the partitioned tensor axes to mesh axes.
- We only consider 2-dimensional device meshes, so a partitioned tensor axis can be mapped to either the first or the second axis of the device mesh, or both.
- We added a superscript to S to denote the device assignment.
- For instance, S 0 means the partitions are along the 0-th axis of the mesh, S 01 means the partitions take place along both mesh axes.
- S 0 R means the tensor is row-partitioned into two parts -The first part is replicated on device 0 and device 1, and the second part is replicated on device 2 and device 3.
- Table 1 shows all possible sharding specs of a 2-dimensional tensor on a 2 × 2 mesh with 4 devices.
- When an input tensor of an operator does not satisfy the sharding spec of the chosen parallel algorithm for the operator, a layout conversion, namely resharding, is required, which might require cross-device communication.

### ILP Formulation
- The total execution cost of a computational graph is the sum of the compute and communication costs on all nodes and the resharding costs on all edges.
- We formulate the cost minimization as an ILP and solve it optimally with an off-the-shelf solver.
- For node v, the number of possible parallel algorithms is k v .
- It then has a communication cost vector c v of length k v , or c v ∈ R k v , where c vi is the communication cost of the i-th algorithm. Similarly, node v has a compute cost vector d v ∈ R k v .
- For each node v, we define an one-hot decision vector s v ∈ {0, 1} k v to represent the algorithm it uses. s vi = 1 means we pick the i-th algorithm for node v.
- For the resharding cost between node v and node u, we define a resharding cost matrix R vu ∈ R k v ×k u , where R vui j is the resharding cost from the output of i-th strategy of node v to the input of j-th strategy of node u.
- The objective of the problem is where the first term is the compute and communication cost of node v, and the second is the resharding cost of the edge (v, u).
- In this formulation, s is the variable, and the rest are constant values.
- The term s v R vu s u in Eq. 1 is quadratic, and cannot be fed into an ILP solver.
- We linearize [19] the quadratic term by introducing a new decision vector e vu ∈ {0, 1} k v •k u which represents the resharding decision between node v and u.
- Although we can use profiling to get the accurate costs for c v , d v , and R vu , we use the following methods to estimate them for simplicity.
- For communication costs c v and R vu , we compute the numbers of communicated bytes and divide them by the mesh dimension bandwidth to get the costs.
- For compute costs d v , we set all of them as zero following the same motivation in [55]. This is reasonable because: (1) For heavy operators such as matmul, we do not allow replicated computation. All parallel algorithms always evenly divide the work to all devices, so all parallel algorithms of one operator have the same arithmetic complexity; (2) For lightweight operators such as element-wise operators, we allow replicated computation of them, but their computation costs are negligible.
- To simplify the graph, we merge computationally-trivial operators, such as element-wise operators, transpose, and reduction, into one of their operands and propagate the sharding spec from the operand. This greatly reduces the number of nodes in the graph, thus the ILP problem size.
- We do a breathfirst-search and compute the depth of each node. The node is merged to the deepest operand. Once the parallel plan is decided by ILP, we also apply a set of post-ILP communication optimizations, such as replacing all-reduce with reduce-scatter and all-gather, whenever applicable, because the latter reduces the number of replicated tensors and corresponding computations, while keeping the communication volume the same. This achieves the effect of weight update sharding [56] or ZeRO optimizer [44].

## Inter-Operator Parallelism
- Develop methods to slice the model and device cluster into stage-mesh pairs
- Our optimization goal is to minimize the end-to-end pipeline execution latency for the entire computational graph

### The Space for Inter-Operator Parallelism
- Assume the computational graph contains a sequence of operators following the topology order of the graph, notated as o, . . . , o, where the inputs of an operator oare from operators o, . . . , o.
- We slice the operators into S stages s, where each stage sconsists of operators (o, . . . , o), and we assign each stage sto a submesh of size n× m, sliced from a computer cluster that contains devices, notated as the cluster mesh with shape N × M.
- Let t= t(s, Mesh(n, m)) be the latency of executing stage son a submesh of n× m, minimized by the ILP and reported back by the intra-op pass ( §4).
- As visualized in Fig. 5, assuming we have B different input microbatches for the pipeline, the total minimum latency for the entire computation graph is written as:

### DP Formulation
- To ensure all submeshes (n 1 , m 1 ), . . . , (n S , m S ) fully cover the N × M cluster mesh, we reduce the available submesh shapes into two options: (1) one-dimensional submeshes of sizes (1, 1), (1, 2), (1, 4) . . . (1, 2 m ) and ( 2) two-dimensional submeshes of size (2, M), (3, M), . . . , (N, M) that fully use the second dimension of the cluster mesh (i.e., on a GPU cluster, this means using all compute devices in each physical machine).
- To assign physical devices in the cluster to the resulting submeshes find by the DP algorithm, we enumerate by assigning devices to larger submeshes first and then to smaller ones.
- When there are multiple pipeline stages with the same submesh shape, we tend to put neighboring pipeline stages closer on the device mesh to reduce communication latency.
- The simplification on submesh shapes works well for most available cloud deep learning setups: On AWS [3], the GPU instances have 1, 2, 4, or 8 GPUs; on GCP [20], the TPU instances have 8, 32, 128, 256 or 512 TPUs.
- The set of submesh shapes (n, m) excluded by the assumption is with n > 1 and m < M, which we observe lead to inferior results, since an alternative submesh with shape (n , M) where n • M = n • m has more devices that can communicate with high bandwidth.
- With this reduction, we only need to ensure that ∑ S i=1 n i • m i = N • M.
- To find T * in Eq. 2, we develop a DP algorithm.
- The DP first enumerates the second term t max = max 1≤ j≤S t j and minimizes the first term t total (t max ) = ∑ 1≤i≤S t i for each different t max .
- Specifically, we use the function F(s, k, d;t max ) to represent the minimal total latency when slicing operators o k to o K into s stages and putting them onto d devices so that the latency of each stage is less than t max .
- We start with F(0, K + 1, 0;t max ) = 0, and derive the optimal substructure of F as = min , and derive the optimal total latency as The value of t intra ((o k , . . . , o i ), Mesh(n s , m s ), s) is determined by the intra-op pass. It is the lowest latency of executing the subgraph (o k , . . . , o i ) on mesh Mesh(n s , m s ) with s subsequent stages.
- Note that Mesh(n s , m s ) is a set of physical devices -hence, we enumerate all the potential choices of logical device mesh shapes (n l , m l ) satisfying n l • m l = n s • m s .
- For each choice, we query the intra-op pass with subgraph (o k , . . . , o i ), logical mesh (n l , m l ), and other intra-op options as inputs and get an intra-op plan.
- We then compile the subgraph with this plan and all other low-level compiler optimizations (e.g., fusion, memory planning) to get an executable for precise profiling.
- The executable is profiled in order to get the stage latency (t l ) and the memory required on each device to run the stage (mem stage ) and to store the intermediate activations (mem act...

## Parallelism Orchestration
- After stages, device meshes, and their assignments are decided, Alpa compiles each stage against its assigned device mesh, respecting the intra-op parallelism plan output by the ILP solver.
- The compilation depends on XLA and GSPMD, and generates parallel executables for each stage-mesh pair.
- When needed, the compilation automatically inserts collective communication primitives (see §4) to address the within-mesh communication caused by intra-op parallelism.
- At the inter-op level, Alpa implements an additional parallelism orchestration pass to address the cross-mesh communication between stages, and generate static instructions for inter-op parallel execution.
- Cross-mesh resharding. Existing manual systems, such as Megatron-LM, constrain all pipeline stages to have the same degrees of data and tensor model parallelism, so the communication between pipeline stages is trivially realized by P2P send/recv between corresponded devices of two equivalent device meshes (Fig. 6a).
- In Alpa, the device meshes holding two adjacent stages might have different mesh shapes, and the tensor to communicate between two stages might have different sharding specs (Fig. 6b and Fig. 6c).
- We call this communication pattern as cross-mesh resharding, which is a many-to-many multicast problem.
- Given the sharding specs of the tensor on the sender and receiver mesh, Alpa generates a communication plan to address cross-mesh sharding in two iterations.
- In the first iteration, Alpa calculates the correspondences between tensor partitions (a.k.a. tiles) on the source and destination mesh. Based on that, it generates P2P send/recv primitives between the source devices and destination devices to fulfill the communication.
- It then takes a second iteration to identify opportunities where the destination tensor has a replication in its sharding spec. In this case, the tensor only needs to be transferred once between two meshes, then exchanged via all-gather across the devices on the destination mesh using its higher bandwidth (Fig. 6) -it rewrites send/recv generated at the first iteration into all-gather to avoid repeated communication.
- We call this approach as local all-gather cross-mesh resharding.
- Since the communication between stages is normally small by our design, our experiments show that it performs satisfactorily well ( §8.5).
- We defer the development of the optimal cross-mesh resharding plan to future work.

## Limitations and Discussion
- Our view of parallelisms is more flexible than existing work
- Compared to existing work, Alpa's hierarchical view of inter-and intraop parallelisms significantly advances them
- With three major flexibility, Alpa can unify all existing model parallelism approaches and generalize to model architectures and cluster setups with more heterogeneity
- Despite these advantages, Alpa's optimization algorithms currently have a few limitations

## Evaluation
- Alpa is implemented in Python and C++ and uses Jax as the frontend and XLA as the backend.
- The compiler passes are implemented on Jax's and XLA's intermediate representation (i.e., Jaxpr and HLO).
- For the distributed runtime, we use Ray [37] actor to implement the device mesh worker, XLA runtime for executing computation, and NCCL [41] for communication.
- We evaluate Alpa on training large-scale models with billions of parameters, including GPT-3 [10], GShard Mixtureof-Experts (MoE) [31], and Wide-ResNet [59].
- The testbed is a typical cluster consisting of 8 nodes and 64 GPUs.

### End-to-End Performance
- First, the grid-searched parameters of the manual plan are better than those of the plan generated by Alpa on most settings.
- Second, the grid-searched parameters of the plan generated by Alpa are better on some settings, but worse on others.
- The plan generated by Alpa is better on GPT-3 if the number of GPUs is larger than 8.
- The plan generated by Alpa is worse on GPT-3 if the number of GPUs is smaller than 8.
- The plan generated by Alpa is better on Wide-ResNet if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on Wide-ResNet if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on MoE if the number of GPUs is larger than 8.
- The plan generated by Alpa is worse on MoE if the number of GPUs is smaller than 8.
- The plan generated by Alpa is better on DeepSpeed if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on DeepSpeed if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Flexflow if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on Flexflow if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Tofu if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on Tofu if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Alpa if the number of GPUs is larger than 8.
- The plan generated by Alpa is worse on Alpa if the number of GPUs is smaller than 8.
- The plan generated by Alpa is better on GPT-3 if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on GPT-3 if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Wide-ResNet if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on Wide-ResNet if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on MoE if the number of GPUs is larger than 8.
- The plan generated by Alpa is worse on MoE if the number of GPUs is smaller than 8.
- The plan generated by Alpa is better on DeepSpeed if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on DeepSpeed if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Flexflow if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on Flexflow if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Tofu if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on Tofu if the number of GPUs is smaller than 16.
- The plan generated by Alpa is better on Alpa if the number of GPUs is larger than 8.
- The plan generated by Alpa is worse on Alpa if the number of GPUs is smaller than 8.
- The plan generated by Alpa is better on GPT-3 if the number of GPUs is larger than 16.
- The plan generated by Alpa is worse on GPT-3 if the number of GPUs is smaller than 16.
- The plan generated...

### Intra-Op Parallelism Ablation Study
- We study the effectiveness of our intra-operator parallelism optimization algorithm.
- We compare our ILP-based solution against alternatives such as ZeRO optimizer and rule-based partitioning strategies.
- Experimental setup. We run a weak scaling benchmark in terms of model size similar to §8.1, but disable pipeline parallelism and gradient accumulation to control variables.
- The benchmark is done on one AWS p3.16xlarge instance with 8 GPUs.
- In order to simulate an execution environment of large-scale training in one node, we use larger hidden sizes, smaller batch sizes, and smaller numbers of layers, compared to the model configurations in §8.1.
- Baselines. We compare automatic solutions for intra-operator parallelism.
- "Data" is vanilla data parallelism.
- "ZeRO-2" [44] is a memory-efficient version of data parallelism which partitions gradients and optimizer states.
- "ZeRO-3" [44] additionally partitions parameters on top of "ZeRO-2".
- "Heuristic" uses a rule combined with the sharding propagation in GSPMD. It marks the largest dimension of every input tensor as partitioned and runs sharding propagation to get the sharding specs for all nodes in the graph.
- "ILP" is our solution based on the ILP solver.
- Results. As shown in Fig. 8, "Data" runs out of memory quickly and cannot train large models.
- "ZeRO-2" and "ZeRO-3" resolve the memory problem of data parallelism, but they do not optimize for communication as they always communicate the gradients.
- When the gradients are much larger than activations, their performance degenerates.
- "Heuristic" solves the memory issue by partitioning all tensors, but can be slowed down by larger communication.
- "Auto-sharding" performs best in all cases and maintains a near-linear scaling, because it figures out the correct partition plan that always minimizes the communication overhead.

### Inter-Op Parallelism Ablation Study
- We study the effectiveness of our inter-operator parallelism optimization algorithm.
- We use "DP" to denote our algorithm.
- Experimental setup.
- We report the performance of three variants of our DP algorithm on GPT and Wide-ResNet.
- The number of GPUs benchmark settings are the same as the settings in §8.1.
- Baselines.
- We compare our DP algorithm with two rule-based algorithms.
- "Equal operator" disables our DP-based operator clustering but assigns the same number of operators to each cluster.
- "Equal layer" restricts our DP algorithm to use the same number of layers for all stages.
- Results.
- Fig. 9 shows the result.
- "DP" always outperforms "Equal operator". This is because "Equal operator" merges operator that should be put onto different device meshes.
- Alpa's algorithm can cluster operators based on the communication cost and computation balance.
- Whether "DP" can outperform "Equal layer" depends on the model architecture.

### Cross-Mesh Resharding
- We evaluate our generalized local all-gather optimization for cross-mesh resharding between meshes with different shapes on Wide-ResNet
- The signal send/recv is a synthetic case where we only send 1 signal byte between stages, which can be seen as an upper bound of the perfor-
- We show that our optimization is better than the state-of-the-art for cross-mesh resharding between meshes with different shapes

### Case Study: Wide-ResNet
- We visualize the parallelization strategies Alpa finds for Wide-ResNet on 16 GPUs
- On 4 GPUs, Alpa uses only intra-operator parallelism.
- The intra-operator solution partitions along the batch axis for the first dozens of layers and then switches to partitioning the channel axis for the last few layers.
- On 16 GPUs, Alpa slices the model into 3 stages and assigns 4, 4, 8 GPUs to stage 1, 2, 3, respectively.
- Data parallelism is preferred in the first two stages because the activation tensors are larger than weight tensors.
- In the third stage, the ILP solver finds a non-trivial way of partitioning the convolution operators.

## Related Work
- Systems for data-parallel training: Horovod [47], BytePS [26], Au-toDist [60], ZeRO [44,56], MiCS [61], Alpa
- Systems for model-parallel training: Mesh-TensorFlow [48], GSPMD [31,57], OneFlow [58], ColocRL [36], Gpipe [22], PipeDream [38,39], TensorOpt [11], Varuna [2], Piper [50]
- Techniques for training large-scale models: Memory optimization [12,14,21,23,28,46], communication compression [6,53], low-precision training [35]
- Compilers for deep learning: Compiler techniques have been introduced to optimize the execution of DL models [13,24,34,51,52,54,62]

## Conclusion
- Alpa is a new architecture for automated modelparallel distributed training
- Alpa constructs a hierarchical space and uses a set of compilation passes to derive efficient parallel execution plans at each parallelism level
- Alpa orchestrates the parallel execution on distributed compute devices on two different granularities
- We prove the following theorem which shows we can always find a solution that fully covers the cluster mesh (N, M) with our selected submesh shapes in §5.2: (1) onedimensional submeshes of shape (1, 1), (1, 2), (1, 4) . . . (1, 2 m ) where 2 m = M and (2) two-dimensional submeshes of shape (2, M), (3, M), . . . , (N, M)
