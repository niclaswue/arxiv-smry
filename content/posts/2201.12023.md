---
title: "Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning"
date: 2022-01-28T10:13:35.000Z
author: "Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2201-12023v3.webp" # image path/url
    alt: "Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2201.12023)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2201.12023).


# Abstract
- Alpa automates model-parallel training of large deep learning (DL) models by generating execution plans that unify data, operator, and pipeline parallelism.
- Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations. They do not suffice to scale out complex DL models on distributed compute devices.
- Alpa distributes the training of large DL models by viewing parallelisms as two hierarchical levels: inter-operator and intra-operator parallelisms. Based on it, Alpa constructs a new hierarchical space for massive model-parallel execution plans.
- Alpa designs a number of compilation passes to automatically derive efficient parallel execution plans at each parallelism level.
- Alpa implements an efficient runtime to orchestrate the two-level parallel execution on distributed compute devices.
- Our evaluation shows Alpa generates parallelization plans that match or outperform hand-tuned model-parallel training systems even on models they are designed for. Unlike specialized systems, Alpa also generalizes to models with heterogeneous architectures and models without manually-designed plans. Alpa's source code is publicly available at https://github.com/alpa-projects/alpa

# Paper Content

## Introduction
- Several advances in deep learning have been a direct result of significant increases in model size.
- For example, scaling language models, such as GPT-3, to hundreds of billions of parameters and training on much larger datasets enabled fundamentally new capabilities.
- However, training these extremely large models on distributed clusters currently requires a significant amount of engineering effort that is specific to both the model definition and the cluster environment.
- For example, training a large transformer-based language model requires heavy tuning and careful selection of multiple parallelism dimensions.
- Training the large Mixture-of-Expert (MoE) transformers model on TPU clusters requires manually tuning the partitioning axis for each layer, whereas training the same model on an AWS GPU cluster calls for new pipeline schemes that can depend on the choices of partitioning.
- More generally, efficient large-scale model training requires tuning a complex combination of data, operator, and pipeline parallelization approaches at the granularity of the individual tensor operators.
- Correctly tuning the parallelization strategy has been shown to deliver an order of magnitude improvements in training performance, but depends on strong machine learning (ML) and system expertise.
- Automating the parallelization of large-scale models would significantly accelerate ML research and production by enabling model developers to quickly explore new model designs without regard for the underlying system challenges.
- Unfortunately, it requires navigating a complex space of plans that grows exponentially with the dimensions of parallelism and the size of the model and cluster.
- For example, when all parallelism techniques are enabled, figuring out the execution plan involves answering a web of interdependent questions, such as how many data-parallel replicas to create, which axis to partition each operator along, how to split the model into pipeline stages, and how to map devices to the resulting parallel executables.
- The interplay of different parallelization methods and their strong dependence on model and cluster setups form a combinatorial space of plans to optimize.
- Recent efforts to automatically parallelize model training are constrained to the space of a single model-parallelism approach, or rely on strong assumptions on the model and cluster specifications.
- Our key observation is that we can organize different parallelization techniques into a hierarchical space and map these parallelization techniques to the hierarchical structure of the compute cluster.
- Different parallelization techniques have different bandwidth requirements for communication, while a typical compute cluster has a corresponding structure: closely located devices can communicate with high bandwidth while distant devices have limited communication bandwidth.
- With this observation in mind, in this paper, we take a different view from conventional data and model parallelisms, and re-categorize ML parallelization approaches as intra-operator and inter-operator parallelisms.
- Intra-operator parallelism partitions ML operators along one or more tensor axes (batch or non-batch) and dispatches the partitions to distributed devices (Fig. 1c); inter-operator parallelism, on the other hand, slices the model into disjoint stages and pipelines the execution of stages on different sets of devices (Fig. 1d).
- They take place at two different granularities of the model computation, differentiated by whether to partition operators.
- Given that, a parallel execution plan can be expressed hierarchically by specifying the plan in each parallelism category, leading to a number of advantages.
- First, intra-and interoperator parallelisms feature distinct characteristics: intraoperator parallelism has better device utilization, but results in communicating at every split and merge of partitioned operators, per training iteration; whereas inter-operator parallelism only communicates between adjacent stages, which can be light if sliced properly, but incurs device idle time due to scheduling constraints.
- We can harness the asymmetric nature of communication bandwidth in a compute cluster, and map intra-operator parallelism to devices connected with high communication bandwidth, while orchestrating the interoperator parallelism between distant devices with relatively lower bandwidth in between.
- Second, this hierarchical design allows us to solve each level near-optimally as an individual tractable sub-problem. While the joint execution plan is not guaranteed globally optimal, they demonstrate strong performance empirically for training various large models.
- Guided by this new problem formulation, we design and implement Alpa, the first compiler that automatically generates parallel execution plans covering all data, operator, and pipeline parallelisms.
- Given the model description and a cluster configuration, Alpa achieves this by partitioning the cluster into a number of device meshes, each of which contains devices with preferably high-bandwidth connections, and partitioning the computation graph of the model into stages.

## Background: Distributed Deep Learning
- DL computation is commonly represented by popular ML frameworks
- In practice, model developers define the dataflow graph
- An execution engine then optimizes and executes it on a compute device.
- When either the model or data is large that a single device cannot complete the training in a reasonable amount of time, we resort to ML parallelization approaches that parallelize the computation on distributed devices.

### Conventional View of ML Parallelism
- Data parallelism: The training data is partitioned across distributed workers, but the model is replicated.
- Operator parallelism: When the model is too large to fit in one device, operator parallelism is an effective model parallelism option.
- Pipeline parallelism: Instead of partitioning ops, pipeline parallelism places different groups of ops from the model graph, referred as stages, on different workers; meanwhile, it splits the training batch as a number of microbatches, and pipelines the forward and backward passes across microbatches on distributed workers.
- Manual combination of parallelisms: Recent development shows the approaches mentioned above need to be combined to scale out today's large DL models.
- Automatic combination of parallelisms: The configurations of each individual parallelism, their interdependence, and their dependence on model and cluster setups form an intractable space, which prevents the trivial realization of automatically combining these parallelisms.

### Intra-and Inter-Operator Parallelisms
- Different from the conventional view, in this paper, intra-operator and inter-operator parallelisms are distinguished by if they involve partitioning operators along any tensor axis.
- Intra-operator parallelism is when an operator works on multidimensional tensors and we can partition the tensor along some dimensions, assign the resulting partitioned computations to multiple devices, and let them execute different portions of the operator at the same time.
- Inter-operator parallelism is when operators are assigned to devices without any partitioning.
- Concurrent work has proposed similar categorization, but Alpa is the first end-to-end system that uses this categorization to automatically generate parallel plans from the full space.

## Overview
- Alpa is a compiler that generates model-parallel execution plans by hierarchically optimizing the plan at two different levels: intra-op and inter-op parallelism.
- At the intra-op level, Alpa minimizes the cost of executing a stage (i.e., subgraph) of the computational graph, with respect to its intra-operator parallelism plan, on a given device mesh, which is a set of devices that may have high bandwidth between each other (e.g., GPUs within a single server).
- Different meshes might have different numbers of computing devices according to the workload assigned.
- At the inter-op level, Alpa minimizes the inter-op parallelization latency, with respect to how to slice the model and device cluster into stages and device meshes and how to map them as stage-mesh pairs.
- The inter-op optimization depends on knowing the execution cost of each stage-mesh pair reported by the intra-op optimizer.
- Through this hierarchical optimization process, Alpa generates the execution plan consisting of intra-op and inter-op plans which are 2 Device placement [36] is another case of inter-op parallelism, which partitions the model graph and executes them on different devices but does not saturate pipelines using multiple microbatches. Hence pipeline parallelism is often seen as a better alternative to it because of less device idle time.
- The developers uses a Python decorator @parallelize to annotate functions that need to be parallelized.
- The rest of the program is kept intact.
- locally near-optimal at their respective level of the hierarchy.
- To achieve this, Alpa implements three novel compilation passes as Fig. 3 shows.
- Given a model description, in the form of a Jax [9] intermediate representation (IR), and a cluster configuration, the inter-op compilation pass slices the IR into a number of stages, and slices the device cluster into a number of device meshes.
- The inter-op pass uses a Dynamic Programming (DP) algorithm to assign stages to meshes and invokes the intra-op compilation pass on each stage-mesh pair, to query the execution cost of this assignment. Once invoked, the intra-op pass optimizes the intra-op parallel execution plan of the stage running on its assigned mesh, by minimizing its execution cost using an Integer Linear Programming (ILP) formulation, and reports the cost back to the inter-op pass.
- By repeatedly querying the intra-op pass for each allocation of a stage-mesh pair, the inter-op pass uses the DP to minimize the inter-op parallel execution latency and obtains the best slicing scheme of stages and meshes.
- Given the output hierarchical plan and a designated pipeline-parallel schedule, each stage is first compiled as a parallel executable on its located mesh.
- A runtime orchestration pass is invoked to fulfill the communication requirement between two adjacent stages that require communication between the two meshes they locate on.
- The runtime orchestration pass then generates static instructions specific to each mesh according to the pipeline-parallel schedule and invokes the execution on all meshes.
- API. Alpa has a simple API shown in Fig. 4.

## Intra-Operator Parallelism
- Alpa optimizes the intra-operator parallelism plan within a device mesh
- Alpa adopts the SPMD-style intra-op parallelism [31,57] which partitions operators evenly across devices and executes the same instructions on all devices, as per the fact that devices within a single mesh have equivalent compute capability
- This SPMD style significantly reduces the space of intra-op parallelism plans; meanwhile, it conveniently expresses and unifies many important approaches such as data parallelism, ZeRO, Megatron-LM's operator parallelism, and their combinations, which are not fully covered by existing automatic operators parallelism systems, such as Tofu [55] and FlexFlow [25]
- Unlike systems that perform randomized search [25] or assume linear graphs [55], Alpa formalizes the problem as an integer linear programming (ILP) and shows it can be solved efficiently for computational graphs with tens of thousands of operators

### The Space of Intra-Operator Parallelism

### ILP Formulation
- The total execution cost of a computational graph is the sum of the compute and communication costs on all nodes and the resharding costs on all edges.
- We formulate the cost minimization as an ILP and solve it optimally with an off-the-shelf solver.
- For node v, the number of possible parallel algorithms is k v .
- It then has a communication cost vector c v of length k v , or c v ∈ R k v , where c vi is the communication cost of the i-th algorithm. Similarly, node v has a compute cost vector d v ∈ R k v .
- For each node v, we define an one-hot decision vector s v ∈ {0, 1} k v to represent the algorithm it uses. s vi = 1 means we pick the i-th algorithm for node v.
- For the resharding cost between node v and node u, we define a resharding cost matrix R vu ∈ R k v ×k u , where R vui j is the resharding cost from the output of i-th strategy of node v to the input of j-th strategy of node u.
- The objective of the problem is where the first term is the compute and communication cost of node v, and the second is the resharding cost of the edge (v, u).
- In this formulation, s is the variable, and the rest are constant values.
- The term s v R vu s u in Eq. 1 is quadratic, and cannot be fed into an ILP solver.
- We linearize [19] the quadratic term by introducing a new decision vector e vu ∈ {0, 1} k v •k u which represents the resharding decision between node v and u.
- Although we can use profiling to get the accurate costs for c v , d v , and R vu , we use the following methods to estimate them for simplicity.
- For communication costs c v and R vu , we compute the numbers of communicated bytes and divide them by the mesh dimension bandwidth to get the costs.
- For compute costs d v , we set all of them as zero following the same motivation in [55]. This is reasonable because: (1) For heavy operators such as matmul, we do not allow replicated computation. All parallel algorithms always evenly divide the work to all devices, so all parallel algorithms of one operator have the same arithmetic complexity; (2) For lightweight operators such as element-wise operators, we allow replicated computation of them, but their computation costs are negligible.
- To simplify the graph, we merge computationally-trivial operators, such as element-wise operators, transpose, and reduction, into one of their operands and propagate the sharding spec from the operand. This greatly reduces the number of nodes in the graph, thus the ILP problem size.
- We do a breathfirst-search and compute the depth of each node. The node is merged to the deepest operand. Once the parallel plan is decided by ILP, we also apply a set of post-ILP communication optimizations, such as replacing all-reduce with reduce-scatter and all-gather, whenever applicable, because the latter reduces the number of replicated tensors and corresponding computations, while keeping the communication volume the same. This achieves the effect of weight update sharding [56] or ZeRO optimizer [44].

## Inter-Operator Parallelism
- Develop methods to slice the model and device cluster into stage-mesh pairs
- Optimization goal is to minimize the end-to-end pipeline execution latency for the entire computational graph
- Previous works have considered simplified problems

### The Space for Inter-Operator Parallelism
- The computational graph contains a sequence of operators following the topology order of the graph, notated as o 1 , . . . , o K , where the inputs of an operator o k are from operators o 1 , . . . , o k−1 .
- We slice the operators into S stages s 1 , . . . , s S , where each stage s i consists of operators (o l i , . . . , o r i ), and we assign each stage s i to a submesh of size n i × m i , sliced from a computer cluster that contains devices, notated as the cluster mesh with shape N × M.
- Let t i = t intra (s i , Mesh(n i , m i )) be the latency of executing stage s i on a submesh of n i × m i , minimized by the ILP and reported back by the intra-op pass.
- As visualized in Fig. 5, assuming we have B different input microbatches for the pipeline, the total minimum latency for the entire computation graph is written as:
- The overall latency contains two terms: the first term is the total latency of all stages, interpreted as the latency of the first microbatch going through the pipeline; the second term is the pipelined execution time for the rest of B − 1 microbatches, which is bounded by the slowest stage (stage 3 in Fig. 5).

### DP Formulation
- To ensure all submeshes (n 1 , m 1 ), . . . , (n S , m S ) fully cover the N × M cluster mesh, we reduce the available submesh shapes into two options: (1) one-dimensional submeshes of sizes (1, 1), (1, 2), (1, 4) . . . (1, 2 m ) and ( 2) two-dimensional submeshes of size (2, M), (3, M), . . . , (N, M) that fully use the second dimension of the cluster mesh (i.e., on a GPU cluster, this means using all compute devices in each physical machine).
- To assign physical devices in the cluster to the resulting submeshes find by the DP algorithm, we enumerate by assigning devices to larger submeshes first and then to smaller ones.
- When there are multiple pipeline stages with the same submesh shape, we tend to put neighboring pipeline stages closer on the device mesh to reduce communication latency.
- The simplification on submesh shapes works well for most available cloud deep learning setups: On AWS [3], the GPU instances have 1, 2, 4, or 8 GPUs; on GCP [20], the TPU instances have 8, 32, 128, 256 or 512 TPUs.
- The set of submesh shapes (n, m) excluded by the assumption is with n > 1 and m < M, which we observe lead to inferior results, since an alternative submesh with shape (n , M) where n • M = n • m has more devices that can communicate with high bandwidth.
- With this reduction, we only need to ensure that ∑ S i=1 n i • m i = N • M.
- To find T * in Eq. 2, we develop a DP algorithm.
- The DP first enumerates the second term t max = max 1≤ j≤S t j and minimizes the first term t total (t max ) = ∑ 1≤i≤S t i for each different t max .
- Specifically, we use the function F(s, k, d;t max ) to represent the minimal total latency when slicing operators o k to o K into s stages and putting them onto d devices so that the latency of each stage is less than t max .
- We start with F(0, K + 1, 0;t max ) = 0, and derive the optimal substructure of F as = min , and derive the optimal total latency as The value of t intra ((o k , . . . , o i ), Mesh(n s , m s ), s) is determined by the intra-op pass. It is the lowest latency of executing the subgraph (o k , . . . , o i ) on mesh Mesh(n s , m s ) with s subsequent stages.
- Note that Mesh(n s , m s ) is a set of physical devices -hence, we enumerate all the potential choices of logical device mesh shapes (n l , m l ) satisfying n l • m l = n s • m s .
- For each choice, we query the intra-op pass with subgraph (o k , . . . , o i ), logical mesh (n l , m l ), and other intra-op options as inputs and get an intra-op plan.
- We then compile the subgraph with this plan and all other low-level compiler optimizations (e.g., fusion, memory planning) to get an executable for precise profiling.
- The executable is profiled in order to get the stage latency (t l ) and the memory required on each device to run the stage (mem stage ) and to store the intermediate activations (mem act ).
- We check whether the required memory fits the device memory (mem device ) according to the chosen pipeline execution schedule.
- For example, for 1F1B schedule [17,39], we check We pick the logical mesh shape that minimizes t l and fits into the device memory. If none of them fits, we set t intra = ∞.

## Parallelism Orchestration
- After stages, device meshes, and their assignments are decided, at the intra-op level, Alpa compiles each stage against its assigned device mesh, respecting the intra-op parallelism plan output by the ILP solver.
- The compilation depends on XLA [51] and GSPMD [57], and generates parallel executables for each stage-mesh pair.
- When needed, the compilation automatically inserts collective communication primitives (see §4) to address the within-mesh communication caused by intra-op parallelism.
- At the inter-op level, Alpa implements an additional parallelism orchestration pass to address the cross-mesh communication between stages, and generate static instructions for inter-op parallel execution.
- Cross-mesh resharding. Existing manual systems, such as Megatron-LM [45,49], constrain all pipeline stages to have the same degrees of data and tensor model parallelism, so the communication between pipeline stages is trivially realized by P2P send/recv between corresponded devices of two equivalent device meshes (Fig. 6a).
- In Alpa, the device meshes holding two adjacent stages might have different mesh shapes, and the tensor to communicate between two stages might have different sharding specs (Fig. 6b and Fig. 6c).
- We call this communication pattern as cross-mesh resharding, which is a many-to-many multicast problem.
- Given the sharding specs of the tensor on the sender and receiver mesh, Alpa generates a communication plan to address cross-mesh sharding in two iterations.
- In the first iteration, Alpa calculates the correspondences between tensor partitions (a.k.a. tiles) on the source and destination mesh. Based on that, it generates P2P send/recv primitives between the source devices and destination devices to fulfill the communication. It then takes a second iteration to identify opportunities where the destination tensor has a replication in its sharding spec. In this case, the tensor only needs to be transferred once between two meshes, then exchanged via all-gather across the devices on the destination mesh using its higher bandwidth (Fig. 6) -it rewrites send/recv generated at the first iteration into all-gather to avoid repeated communication.
- We call this approach as local all-gather cross-mesh resharding.
- Since the communication between stages is normally small by our design, our experiments show that it performs satisfactorily well ( §8.5).
- We defer the development of the optimal cross-mesh resharding plan to future work.

## Limitations and Discussion
- Alpa's hierarchical view of inter-and intraop parallelisms significantly advances them with three major flexibility: (1) pipeline stages can contain an uneven number of operators or layers; (2) pipeline stages in Alpa might be mapped to device meshes with different shapes; (3) within each stage, the data and operator parallelism configuration is customized non-uniformly on an operator-by-operator basis. Together, they allow Alpa to unify all existing model parallelism approaches and generalize to model architectures and cluster setups with more heterogeneity.
- Despite these advantages, Alpa's optimization algorithms currently have a few limitations: (1) Alpa does not model the communication cost between different stages because the cross-stage communication cost is by nature small. In fact, modeling the cost in either the DP or ILP is possible, but would require enumerating exponentially more intra-op passes and DP states.
- The inter-op pass currently has a hyperparameter: the number of micro-batches B, which is not optimized by our current formulation but can be searched by enumeration.
- The inter-op pass models pipeline parallelism with a static linear schedule, without considering more dynamic schedules that, for example, parallelize different branches in a computational graph on different devices.
- Alpa does not optimize for the best scheme of overlapping computation and communication; Alpa can only handle static computational graphs with all tensor shapes known at compilation time. Nevertheless, our results on weak scaling ( §8) suggest that Alpa is able to generate near-optimal execution plans for many notable models.

## Evaluation
- Alpa is implemented in Python and C++
- It uses Jax as the frontend and XLA as the backend
- The compiler passes are implemented on Jax's and XLA's intermediate representation (i.e., Jaxpr and HLO)
- For the distributed runtime, we use Ray [37] actor to implement the device mesh worker, XLA runtime for executing computation, and NCCL [41] for communication
- We evaluate Alpa on training large-scale models with billions of parameters, including GPT-3 [10], GShard Mixtureof-Experts (MoE) [31], and Wide-ResNet [59]
- The testbed is a typical cluster consisting of 8 nodes and 64 GPUs

### End-to-End Performance
- We target three types of models listed in Table 4, covering models with both homogeneous and heterogeneous architectures.
- GPT-3 is a homogeneous transformer-based LM by stacking many transformer layers whose model parallelization plan has been extensively studied [40,49].
- GShard MoE is a mixed dense and sparse LM, where mixture-of-experts layers are used to replace the MLP at the end of a transformer, every two layers.
- Wide-ResNet is a variant of ResNet with larger channel sizes. It is vastly different from the transformer models and there are no existing manually designed strategies.
- To study the ability to train large models, we follow common ML practice to scale the model size along with the number of GPUs, with the parameter range reported in Table 4.
- For GPT-3, we increase the hidden size and the number of layers together with the number of GPUs following [40], whereas for MoE we mainly increase the number of experts suggested by [31,57].
- For Wide-ResNet, we increase the channel size and width factor in convolution layers.
- For each model, we adopt the suggested global batch size per ML practice [10,31,40,59] to keep the same statistical behavior. We then tune the best microbatch size for each model and system configuration that maximizes the system performance.
- The gradients are accumulated across microbatches.
- The detailed model specifications are provided in Appendix B.
- Baselines. For each model, we compare Alpa against a strong baseline. We use Megatron-LM v2 [40] as the baseline system for GPT-3. Megatron-LM is the state-of-the-art system for training homogeneous transformer-based LMs on GPUs.
- It combines data parallelism, pipeline parallelism, and manuallydesigned operator parallelism (denoted as TMP later). The combination of these techniques is controlled by three integer parameters that specify the parallelism degrees assigned to each technique.
- We grid-search the three parameters following the guidance of their paper and report the results of the best configuration. Megatron-LM is specialized for GPT-like models, so it does not support other models in Table 4.
- The number of GPUs
- The number of GPUs
- The number of GPUs
- We use DeepSpeed [45] as the baseline for MoE. Deep-Speed provides a state-of-the-art implementation for training MoE on GPUs. It combines handcrafted operator parallelism for MoE layers and ZeRO-based [44] data parallelism. The combination of these techniques is controlled by several integer parameters that specify the parallelism degree assigned to each technique.
- We also grid-search them and report the best results.
- The performance of DeepSpeed on GPT-3 is similar to or worse than Megatron-LM, so we skip it on GPT-3.
- Note that original GShard-MoE [31] implementation is only available on TPUs, thus we do not include its results, though their strategies [31] are covered by Alpa 's strategy space.
- For large Wide-ResNet, there is no specialized system or manually designed plan for it. We use Alpa to build a baseline "PP-DP" whose space only consists of data parallelism and pipeline parallelism, which mimics the parallelism space of PipeDream [38] and Dapple [17].
- For all models, we also include the results of using Alpa with only one of intra-and inter-operator parallelism, which mimics the performance of some other auto-parallel systems.
- The open-source Flexflow [25] does not support the models we evaluate, as it lacks support for many necessary operators (e.g., layer normalization [5], mixed-precision operators).
- Tofu [55] only supports single node execution and is not opensourced.
- Due to both theoretical and practical limitations, we do not include their results and we do not expect Flexflow or Tofu to outperform the state-of-the-art manual baselines in our evaluation.
- Alpa does not modify the semantics of the synchronous gradient descent algorithm, thus does not affect the model convergence. Therefore, we measure training throughput in our evaluation. We evaluate weak scaling of the system when increasing the model size along with the number of GPUs. Following [40], we use the aggregated peta floatingpoint operations per second (PFLOPS) of the whole cluster as the metric.
- We measure it by running a few batches with dummy data after proper warmup. All our results (including those in later sections) have a standard deviation within 0.5%, so we skip the error bars in our figures.

### Intra-Op Parallelism Ablation Study
- We study the effectiveness of our intra-operator parallelism optimization algorithm.
- We compare our ILP-based solution against alternatives such as ZeRO optimizer and rule-based partitioning strategies.
- Experimental setup.
- We run a weak scaling benchmark in terms of model size similar to §8.1, but disable pipeline parallelism and gradient accumulation to control variables.
- The benchmark is done on one AWS p3.16xlarge instance with 8 GPUs.
- In order to simulate an execution environment of large-scale training in one node, we use larger hidden sizes, smaller batch sizes, and smaller numbers of layers, compared to the model configurations in §8.1.
- Baselines.
- We compare automatic solutions for intra-operator parallelism.
- "Data" is vanilla data parallelism.
- "ZeRO-2" [44] is a memory-efficient version of data parallelism which partitions gradients and optimizer states.
- "ZeRO-3" [44] additionally partitions parameters on top of "ZeRO-2".
- "Heuristic" uses a rule combined with the sharding propagation in GSPMD.
- It marks the largest dimension of every input tensor as partitioned and runs sharding propagation to get the sharding specs for all nodes in the graph.
- "ILP" is our solution based on the ILP solver.
- Results.
- As shown in Fig. 8, "Data" runs out of memory quickly and cannot train large models.
- "ZeRO-2" and "ZeRO-3" resolve the memory problem of data parallelism, but they do not optimize for communication as they always communicate the gradients.
- When the gradients are much larger than activations, their performance degenerates.
- "Heuristic" solves the memory issue by partitioning all tensors, but can be slowed down by larger communication.
- "Auto-sharding" performs best in all cases and maintains a near-linear scaling, because it figures out the correct partition plan that always minimizes the communication overhead.

### Inter-Op Parallelism Ablation Study
- DP algorithm is effective
- DP algorithm outperforms Equal operator
- DP algorithm outperforms Equal layer on GPT

### Cross-Mesh Resharding
- We evaluate our generalized local all-gather optimization for cross-mesh resharding between meshes with different shapes on Wide-ResNet
- "signal send/recv" is a synthetic case where we only send 1 signal byte between stages, which can be seen as upper bound of the perfor-
- We show that our optimization is better than the state-of-the-art for cross-mesh resharding between meshes with different shapes
- Our optimization is faster and more efficient than the state-of-the-art

### Case Study: Wide-ResNet
- We visualize the parallelization strategies Alpa finds for Wide-ResNet on 16 GPUs
- On 4 GPUs, Alpa uses only intra-operator parallelism.
- The intra-operator solution partitions along the batch axis for the first dozens of layers and then switches to partitioning the channel axis for the last few layers.
- On 16 GPUs, Alpa slices the model into 3 stages and assigns 4, 4, 8 GPUs to stage 1, 2, 3, respectively.
- Data parallelism is preferred in the first two stages because the activation tensors are larger than weight tensors.
- In the third stage, the ILP solver finds a non-trivial way of partitioning the convolution operators.

## Related Work
- Systems for data-parallel training: Horovod [47], BytePS [26], Au-toDist [60], ZeRO [44,56], MiCS [61], Alpa
- Systems for model-parallel training: Mesh-TensorFlow [48], GSPMD [31,57], OneFlow [58], ColocRL [36], Gpipe [22], TensorOpt [11], Varuna [2], Piper [50]
- Techniques for training large-scale models: Memory optimization [12,14,21,23,28,46], communication compression [6,53], low-precision training [35]
- Compilers for deep learning: Compiler techniques have been introduced to optimize the execution of DL models [13,24,34,51,52,54,62]

## Conclusion
- Alpa is a new architecture for automated modelparallel distributed training
- It is built on top of a new view of machine learning parallelization approaches: intra-and interoperator parallelisms
- Alpa constructs a hierarchical space and uses a set of compilation passes to derive efficient parallel execution plans at each parallelism level
- Alpa orchestrates the parallel execution on distributed compute devices on two different granularities

## Acknowledgement
- The research was funded by gifts from Alibaba Group, Amazon Web Services, Ant Group, CapitalOne, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk, and VMware.
- The research was also reviewed by Shibo Wang, Yu Emma Wang, Jinliang Wei, Zhen Zhang, Siyuan Zhuang, and anonymous reviewers.

## A Proof of Submesh Shape Covering
- The theorem states that we can always find a solution that fully covers the cluster mesh (N, M) with our selected submesh shapes.
- The theorem is proven in §5.2.
- The theorem holds for all m = 1, 2, . . . , k − 1.

## B Model Specifications
- For GPT-3 models, we use sequence length = 1024 and vocabulary size = 51200 for all models.
- Other parameters of the models are listed in Table 6.
- For GShard MoE models, we use sequence length = 1024 and vocabulary size = 32000 for all models.
- Other parameters of the models are listed in Table

## C Extra Case Study
- Generates parallelization plans for a computational graph
- Common parallelization techniques for training a 2-layer Multi-layer Perceptron (MLP)
- An example to demonstrate Alpa's API for Jax
- Sharding specs of a 2-dimentional tensor on a 2 × 2 device mesh
- Cross-mesh resharding
- End-to-end evaluation results
- Visualization of the parallel strategy of Wide-ResNet on 16 GPUs
- GPT models
