---
title: "Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression"
date: 2022-07-21T16:10:24.000Z
author: "Yeying Jin, Wenhan Yang, Robby T. Tan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2207-10564v2_5VXHrbS_c.jpg" # image path/url
    alt: "Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2207.10564)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2207.10564).


# Abstract
- Night images suffer from low light and uneven distributions of light
- Most existing night visibility enhancement methods focus mainly on
- To address this problem, we need to suppress the light effects in bright regions while
- With this idea in mind, we introduce an unsupervised method that integrates a
- Our decomposition network learns to decompose shading, reflectance and
- Our light-effects suppression network further suppresses the light effects and
- Our method outperforms state-of-the-art methods in suppressing night light

# Paper Content

## Introduction
- Most existing nighttime visibility enhancement methods focus mainly on boosting the intensity of low-light regions,
- Our goal in this paper is to suppress the light effects while, at the same time, boosting the intensity of dark regions.
- To achieve our goal, we introduce a network architecture that integrates layer decomposition and light-effects suppression in one unified framework.
- To distinguish light effects from background regions, we propose utilizing the estimated light-effects layer as guidance for our unsupervised light-effects suppression network.
- To restore the background details, we introduce novel unsupervised losses based on the structure and HF-features consistency. Our perceptual structure information and HF texture information are less affected by light effects. Thus, they can be employed to preserve background details, and, importantly, to suppress unwanted artefacts.
- Our experiments and evaluations show that our method is effective in suppressing light-effects regions and enhancing dark regions, outperforming state-of-the-art methods both quantitatively and qualitatively.

## Related Work
- Sharma and Tan introduce a method to suppress light effects and improve the dynamic range for night images
- The method is the first that can suppress light effects and improve the dynamic range for night images, but suffers from artefacts and missing details
- In the field of night image dehazing, a few methods have been proposed to suppress glow due to haze/fog particles
- Zhang et al. [44] use maximum reflectance prior for haze and glow removal
- Ancuti et al. [2,3] use a fusion process and the Laplace operator to deglow and dehaze
- Yan et al. [38,39] propose a semisupervised method [37] employing a grayscale guided network
- However, all these methods are designed for glow suppression in haze or foggy night, and not for removing light effects in clear night images
- Most of these night image enhancement methods, however, are not designed to suppress night light effects
- Recently, zero-shot learning methods (e.g. [19,13]) have been proposed for low-light enhancement
- Most of these night image enhancement methods, however, are not designed to suppress night light effects and enhance low light regions simultaneously

## Proposed Method
- An unsupervised framework is proposed that integrates a decomposition network and a light-effects suppression network
- The decomposition network is based on an image-layer model and produces three separate layers: shading, reflectance, and light-effects layers
- Input these layers into a light-effects suppression network to obtain the final output, where light effects are suppressed and dark regions are boosted
- This network learns from unpaired data and is guided by estimated light-effects layer

### Model-Based Layer Decomposition Network
- Our decomposition is based on the following image-layer model: where I represents the input night image, G represents the light-effects layer, R and L are the reflectance and shading layers, respectively.
- In order to obtain a background scene that is free from light effects, we use three separate networks and our novel unsupervised losses to obtain the light effects (G), shading (L), and reflectance (R) layers.
- The three networks are trained using unsupervised losses, which will be discussed in the subsequent paragraphs.
- To resolve the decomposition ambiguity problem, it is important to provide proper initial estimates of the layers.
- For the shading layer, we employ a shading map L i obtained by taking the maximum value of the three color channels, for each pixel [14].
- For the light-effects layer, we use a light-effects map G i , computed using the relative smoothness technique [22]. This is extracted using the second-order Laplacian filter from the input image, since light effects are smooth variations.
- We define the loss function for the initialization step as: Gradient Exclusion Loss.
- The gradients of the light effects layer have a short tail distribution, similar to that of 'glow' [23]. In contrast, the gradients of the background image have a long tail distribution [22]. Hence, we employ a gradient exclusion loss to recover the uncorrelated layers {G, J init }, where the goal is to separate the two layers as far as possible in the gradient space.
- The definition of the loss follows [10,46]: where • F is the Frobenius norm, G ↓n and J ↓n init represent G and J init downsampled using the bilinear interpolation, and the parameters λ G ↓n and λ J ↓n init are normalization factors.
- To minimize any color shift in our decomposition output, inspired by the Gray World assumption [5,13,32], we use a color-constancy prior, which encourages the range of the intensity values of the three color channels in the background image J init to be balanced: where (c1, c2) ∈ {(r, g), (r, b), (g, b)} denotes a combination of two color channels.
- Finally, we employ a weighted sum loss to combine the losses: Gradient Exclusion Loss + Color Constancy Loss + Reconstruction Loss.

### Light-Effects Suppression Network
- To better suppress light effects, we integrate our decomposition network with an unpaired light-effects suppression network.
- We design this network to suppress light effects by using the guidance of our estimated light-effects layer, enforcing the network to focus on light-effects regions.
- As shown in Fig. 2, our network comprises a generator φ gen and a classifier Γ gen .
- It refines the initially estimated background scene (J init ), and generates the final light-effects-free output (J refine ).
- The details are as follows.
- Light-Effects Layer Guidance
- We employ the estimated light-effects layer G to guide our training process, as shown in Fig. 4.
- The light-effects layer is taken as part of the input of our encoder-decoder network, and is modulated with the feature maps of the network at different scales. Specifically, we concatenate J init with the light-effects layer G, and then we input them to our network φ gen .
- By resizing the light-effects layer, G, to fit the size of each feature map, and multiplying it with all the intermediate feature maps, our light-effects layer can guide our network to focus more on light-effects regions.
- Fig. 3b and Fig. 12 show some results of our light-effects layers, demonstrating that our method can successfully separate white and multi-color light effects.
- Light-Effects Suppression
- Besides the light-effects layer, our suppression network is also guided by an attention mechanism [15,18,16].
- The basic idea is that, we input the light-effects and light-effects-free unpaired images into our encoderdecoder network.
- We then, use a domain classifier to judge whether the encoded features come from a certain domain, i.e., to judge whether the input is lighteffects or light-effects-free.
- Using this domain classification, the activated feature regions can form an attention map [49] that is useful when guiding our network in suppressing light effects.
- More specifically, as shown in Fig. 4, our network φ gen contains an auxiliary classifier Γ gen .
- One of the inputs of the network is the concatenation of J init and G. Another input is a light-effects-free reference image, J ef , concatenated with a dummy all zero map G 0 , which of course has no light effects.
- Our classifier, Γ gen , then performs domain classification based on the encoded features from f e = (G, J init ) or f ef = (G 0 , J ef ).
- To train the auxiliary classifier Γ gen , we use the following attention loss: Structure and HF-Features Consistency Losses
- To address hallucination/artefacts [31], and also to preserve background details, we employ two constraints: structure consistency, based on features obtained from the VGG network [17]; and HF-features consistency, based on the HF features obtained from the guided filter [35].
- As shown in Fig. 5, to obtain the structure information and HF-features that are more robust to light effects, we adaptively fuse the RGB color channels of the input night image by applying: I gray (x) = c .
- Note that the range of I c (x) is [0,1], thus 0.5 is the median of the intensity range.
- Our weight has a low value if a pixel in a color channel is either low (under-exposed) or high (e.g., a light-effects pixel).
- We define σ = 0.2, which measures how well-exposed a pixel is. This makes the resulting grayscale image I gray less affected by light effects, as can be observed in Fig. 5 and Fig. 6.
- Table 1. User study evaluation on the real night data, our method obtained the highest mean and lowest standard deviation (the max score is 7), showing our method is realistic, light-effects (L.E.) suppressed, and has good visibility.

## Experimental Results
- Collect night images from the internet
- Use these images for our unpaired training
- Randomly selected 210 outputs were presented to the 12 participants
- Ranks the methods from unrealistic (1) to realistic (7); light effects still present (1) to suppressed (7); poor visibility (1) to good visibility (7)
- Table 1 shows the user study results
- Table 2 shows the quantitative results on the night data
- Fig. 7 shows the qualitative results on real night images
- Fig. 8 shows the evaluation on the Dark Zurich [30] dataset
- Low-light enhancement can boost the brightness of low light images with no light effects
- Table 3 shows quantitative comparisons on the LOL-test dataset
- Fig. 9 shows the low-light enhancement results on the LOL-test [7] dataset
- Table 4 shows the results of our method on the LOL-Real [42] dataset
- Fig. 10 shows the results of our method on the LOL-test-split dataset
- Fig. 11 shows the effectiveness of our framework
- Fig. 12 shows the effectiveness of our light-effects layer guidance
- Fig. 13 shows the effectiveness of our unsupervised decomposition

## Conclusion
- The paper proposes a method to suppress light effects, and at the same time, boost the intensity of dark regions, from a single night image
- The paper casts the problem of light-effects suppression as an unsupervised decomposition problem
- The paper proposes an integrated network consisting of layer decomposition and light-effects suppression networks
- The paper's experiments show that its method outperforms the state-of-the-art visibility enhancement and light effects suppression methods.
