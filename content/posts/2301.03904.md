---
title: "RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration"
date: 2023-01-10T11:07:16.000Z
author: "Yvan Tortorella, Luca Bertaccini, Luca Benini, Davide Rossi, Francesco Conti"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03904v1.webp" # image path/url
    alt: "RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03904)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03904).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/redmule-a-mixed-precision-matrix-matrix).

# Abstract
- TinyML is a machine learning approach that is used on power budgets of a few tens of mW
- The current training algorithms for TinyML rely on floating-point matrix operations, which are too expensive for this scenario
- RedMule is a low-power accelerator that supports FP16 and FP8 input/output tensors and can achieve high performance on a few mW power budget
- This paper presents RedMule and shows that it can achieve high performance on a few mW power budget while retaining flexibility to tackle other classes of common linear algebra problems

# Paper Content

## I. INTRODUCTION
- The number of Internet of Things (IoT) devices connected and executing Machine Learning (ML) and, in particular, Deep Learning (DL) based algorithms increased considerably
- To reduce the amount of data sent over the network, improve energy efficiency, and prevent network congestion, the computation has been moved increasingly from data centers to energy-efficient IoT end-nodes with low power budgets (a few mW average, a hundred mW peak)
- There is not yet an equal contribution targeting ultra-low-power embedded systems
- In desktop, mobile, and data center computing, single and double-precision Floating-Point (FP) operations are typically employed for DL and linear algebra applications, providing high accuracy at an acceptable area and energy cost
- However, on embedded devices, power and area constraints are much tighter
- Recently, a significant effort has gone into adapting linear algebra-based algorithms as well as online learning [7] to low-precision formats, such as FP16 [8], [9] and FP8 [10], [11], while incurring in little accuracy loss
- These algorithmic advancements enabled performance and energy efficiency gains [12], [13], opening the way for deploying continual learning and adaptation of DL models on extreme-edge computing systems such as smart wearable devices
- However, the computational capabilities of microcontroller units (MCUs), typically used in these devices, are minimal, especially concerning the execution of FP operations
- In this paper, we present RedMulE (Reduced-precision matrix Multiplication Engine), a TinyML-class fully parametric open-source hardware accelerator designed to support on-chip mixed FP precision (FP8, FP16) linear algebra within RISC-Vbased Parallel Ultra-Low-Power (PULP) [14] clusters
- Since GEMM is commonly known to be the key kernel behind DL and ML training algorithms, RedMulE enables the deployment of on-chip learning and adaptation capabilities while efficiently supporting GEMM-Ops, on ultra-low-power System on-Chips (SoCs) suitable for TinyML applications
- We prototyped our design within an 8-core PULP cluster in 22 nm CMOS technology, instantiating a RedMulE instance with 48 internal Computing Elements (CEs). RedMulE occupies only 0.15 mm 2 , accounting for 24% of the entire cluster area. It achieves up to 15× speedup during regular FP16 GEMM and up to 62× during GEMM-Ops with respect to parallel execution on the RISC-V cores, reaching up to 58.5 GFLOPS (99.4% CEs utilization) at 613 MHZ and 0.8 V. In its best efficiency point, i.e. 470 MHz at 0.65 V, RedMulE achieves up to 772 GFLOPS/W and 1.19 TFLOPS/W energy efficiency for GEMM and GEMM-Ops respectively, while reaching 44.8 GFLOPS.

## II. RELATED WORK
- There is a strong interest in executing linear algebra-based algorithms like inference and training of NNs
- This interest led to the development of various hardware platforms specialized in this task
- One example is the NVIDIA's recent Hopper H100
- Embedding DL-based algorithms on ultra-low-power TinyML SoCs for extreme-edge devices is challenging due to the strict power, energy, and cost constraints imposed
- However, extreme-edge inference is achievable in practical cases and can be performed employing low-precision integer arithmetic
- On the contrary, extreme-edge NNs training faces large memory requirements and the need for FP calculations, which typically leads to power envelopes exceeding the TinyML constraints

## A. Inference Accelerators
- Diana [21] features a digital NN inference accelerator and an analog in-memorycomputing core integrated within a shared memory subsystem working only with narrow integer formats
- DNPU [22] is a fully-digital energy-efficient DL processor for convolutional and recursive NN inference acceleration designed in 65 nm technology and based on a heterogeneous architecture supporting 16-bit fixed-point arithmetic
- Gemmini [23] is a 16 × 16 systolic accelerator designed for inference of deep NNs with 8-bit multiply-accumulate units with runtime-programmable weight stationary and output stationary dataflows

## B. On-Device Learning
- Many works investigated algorithms like direct feedback alignment or equilibrium propagation
- However, such methods have been demonstrated to be less effective than the classical backpropagation method due to severe convergence difficulties
- TinyOL [25] and [26] focus on training NNs using the low-budget Arduino Nano microcontroller based on Cortex-M core
- On the other hand, PULP Trainlib [27], [28], and [29] are all examples of approaches to enable on-device learning and adaptation on RISC-V multi-core PULP clusters like Vega [30], that provide mixed FP precision capabilities, spanning from IEEE 754 Standard FP32 and FP16 to bfloat
- However, the low speed and number of available floating point units typical of ultra-low-power microcontrollers limit the performance of these libraries.

## C. Training Accelerators
- To address the limited training performance achievable by software libraries running on low-power processors,
- Several researchers turned to hardware acceleration [15],
- Cambricon-Q [31] is a training-oriented chip for high accuracy and energy efficiency based on 8-bit fixed-point arithmetic,
- However, many common training algorithms require floating-point operations to ensure convergence [32],
- Most training-oriented chips employing FP arithmetic are all characterized by power envelopes unsuitable for extreme-edge applications,
- IBM proposes [33], [34], an AI computing platform featuring 8 × 8 mixed-precision engines supporting FP16 and hybrid FP8 training, while [35] support only FP16 and FP32,
- Similarly, LNPU [36] supports mixed 8-bit and 16-bit FP precision for on-chip training,
- While these chips consume significantly less power than data-center GPUs during NN training (i.e. a few Watts as opposed to hundreds of Watts), they still do not meet the tens of mW power constraints of TinyML devices,
- Recently, a few training-oriented SoCs that fit the power budget of extreme-edge applications have been presented,
- T-PIM [37] is a processing-in-memory accelerator in 28 nm technology for on-device learning,
- It reaches up to 250 GOPS/W during training with 0% of sparsity and within a power envelope of 51.23 mW at 280 MHz operating frequency,
- However, T-PIM and all the recently proposed PIM approaches do not support FP computations and are not suitable for standard backpropagation,
- To support NNs training at reduced power budgets, many training-oriented chips extensively employ pruning to increase sparsity during training [38], lacking generality,
- For example, TSUNAMI [39] and Trainer [40] are both accelerators designed for extreme-edge NN inference and training, meeting the TinyML power constraints by employing pruning and zero skipping,
- Anders et al. [41] propose a reconfigurable accelerator for dense-sparse matrix multiplications for mixed-precision computations, suitable for training-oriented applications since it features FP16 multiplications and FP32 accumulations with low area occupation and high energy efficiency,
- However, such an accelerator is not parametric, thus not allowing a fast scale-up at design time when higher performance is needed,
- In addition, its integration into a real system has not been evaluated, and it does not support compressed FP8 input/output tensors, which allows for training larger NN models on edge devices where the memory resources are limited.

## D. GEMM-Ops Chips
- All examples of training and inference-oriented chips mentioned so far target only the most common DL operations (such as matrix multiplications and convolutions).
- However, a large set of kernels share the same computational structure as GEMM but do not rely on multiplication and addition as elementary operations, falling into the GEMM-Ops scope.
- Graph analytics, such as breadth-first search [2], [3], short-distance problems [4] that are commonly used for path planning optimization in embedded drones navigation [42], and minimum spanning tree, used for computer vision [43], are examples of applications that make use of GEMM-Ops.
- SIMD 2 [6] addresses this issue by building functional units for GEMM-Op acceleration on top of NVIDIA Streaming Multiprocessor architecture, resembling the TensorCores structure and providing dedicated ISA extensions.
- The design is implemented in 45 nm technology.
- Adding all the SIMD 2 extensions to the baseline matrix multiplication unit results in up to 15.8× speedup with respect to executing the same kernel on CUDA cores at the cost of 69% of area overhead.

## III. BACKGROUND

## A. Generalized Matrix-Matrix Operations
- Generalized Matrix-Matrix Operations (GEMM-Ops) are all the operations of the kind f 2(Y, f 1(X, W)), in particular they can be expressed as: where • corresponds to f 1() and corresponds to f 2().
- Group 1 includes all the GEMM-Ops where the • operator can be of the +/× kind while can be min/max.
- Group 2 contains the GEMM-Ops kernels where the • operator also belongs to the min/max kind.
- X is a matrix of size M × N , W is a matrix of size N × K, while Z and Y have size M × K.
- The similarity of GEMMs and GEMM-Ops makes matrix computing units good candidates to be extended for supporting GEMM-Ops, extending their flexibility to accelerate generalized parallel algebraic operators.
- This class of algorithms is also well-suited for ML applications since matrices are the baseline structure of all DL models.

## B. Asymptotic Optimality of Linear Algebra Acceleration Strategies
- Memory load/store operations enlarge the gap between theoretical and practical performance and efficiency.
- As analyzed by Pedram [45], scalar dot products and vector units do not guarantee the best trade-off between the number of operations performed per memory load/store access.
- As shown in Fig. 1a, a simple scalar dot product that operates on a N -dimensional array performs 2 × N operations (N multiplications + N additions).
- The memory operations performed in this kernel are N loads of X, N loads of W , one load of Y and one store of Z.
- The resultant arithmetic intensity is: (2) 2-Dimensional L×H arrays exploit block-dot products (outer product) microkernels to perform GEMMs.
- Let us consider an L × H 2D array that can operate on L × 1 and 1 × H vectors, each made of N elements, like those shown in Fig. 1b.
- The operations performed on the two vectors are 2×L×H, repeated N times.
- The resulting load/store operations are L × N loads of X, H × N loads of W , L × H loads of Y and L × H stores of Z.
- With these changes, Equation 2 becomes: (3) Equation 3 shows that if L = H, the number of operations is quadratic with the size of the 2-D array, while the number of memory accesses remains linear.

## IV. ARCHITECTURE
- The PULP cluster is a hardware template that is used to create clusters of computers.
- The RedMulE micro-architecture is the hardware architecture that is used in the PULP cluster.

## A. PULP Cluster and RedMulE
- The architecture of a PULP cluster, a multi-core architecture that features a parametric number (2-16) of 32-bit RISC-V general-purpose cores
- The PULP cluster also features an event unit for flexible internal synchronization and a dedicated Direct Memory Access Controller (DMAC) to efficiently move data between the TCDM and external memories
- The cores, the DMAC, and the accelerators access the shared TCDM through a single-cycle latency Heterogeneous Cluster Interconnect (HCI)
- The PULP cluster also features a DataMover that is a tiny accelerator capable of transposing 3-dimensional tensors stored in the TCDM, with 33% less time than eight RISC-V cores and up to 50× increased energy efficiency
- The Streamer is a specialized memory access unit that connects RedMulE to the HCI shallow branch through a single wide port of parametric size (multiple of 32-bit), used for load and store operations
- The Streamer is connected to three internal buffers: an X-Buffer that changes all the L inputs of a column once every H × (P + 1) cycles; a W-Buffer made of H shift registers, each broadcasting a new W-element to all the L CEs of a column every cycle; a Z-Buffer that buffers the computed Z-elements

## C. RedMulE Computational Model
- Fig. 4a shows how RedMulE performs a GEMM-Op visualising it on the computed matrices
- Fig. 4b and Fig. 4d show the detailed sequence of the operations within a row of CEs providing an example of GEMM execution
- For this discussion, let us focus on a RedMulE implementation that features L = 12, H = 4, and P = 3
- The RedMulE operation starts by pre-loading the Z-Buffer with L rows from the Ymatrix, each row made of H × (P + 1) = 16 FP16 elements (256-bit memory width/16-bit internal precision), namely y 0,0 -y 0,15 for Row 0, y 1,0 -y 1,15 for Row 1, and so on.
- Then RedMulE pre-loads the X-Buffer as well, following the same pattern, and then loads a set of H × (P + 1) = 16 W-elements (w 0,0 -w 0,15 ) inside the first shift register of the W-buffer. Each W-element is broadcasted to all the L CEs in the first Datpath column.
- While W-elements are broadcasted, the Z-Buffer pushes Y-elements in the CEs array cycle-bycycle to perform the operation during the execution of the • one.
- After P + 1 cycles, each of the L CEs in the first column forwards its computed partial result to the neighbour CE in the second column.
- The accelerator loads another set of H × (P + 1) W-elements (w 1,0 -w 1,15 ) to broadcast them to all the CEs in the second column. Once all the H CEs of a row have completed their computations, calculating a subset of H × (P + 1) row-column intermediate results, RedMulE activates its feedback (accumulate = 1) to provide the intermediate results to the accumulation input of the first CEs of the given row, then reiterating the computation.
- Immediately after, the Streamer reloads the next Y-submatrix in the Z-Buffer so that it will be ready for the next calculation.
- During the Z-Buffer reload operation, the X-Buffer provides a new X-operand to the first column of CEs, and a new set of H ×(P +1) W-elements is reloaded in the first W shift register. After (P + 1) cycles, all the L CEs of the first column produce a new partial product and provide it to the CEs in the second column.
- The X-Buffer provides a new X-operand at the input of the second column of CEs, and the W-Buffer loads a new set of H × (P + 1) Welements in the second W shift register for broadcasting, and the computation continues.
- Fig. 4d shows the detailed sequence of data within the pipeline of a row of CEs from the beginning of a GEMM operation until the moment of the reuse of the partial results (accumulate = 1).

## V. IMPLEMENTATION AND MEASUREMENTS

## A. Experimental Setup
- We focus our experiments on a RedMulE 12x4 instance with H = 4, L = 12, P = 3, resulting in 48 CEs and a 288-bit wide HCI port, for 256-bit + 32-bit non-word-aligned accesses.
- We also address a RedMulE 12x8 since, as described in Section V-B3, it uses the same memory interface with twice the number of CEs.
- Our experiments target GlobalFoundries 22 nm technology using Synopsys Design Compiler for synthesis (slow corner at f targ = 250 MHz, V DD = 0.59 V, T = 125 °C) and Cadence Innovus for full-cluster Place&Route in the same operating point.
- RedMulE's timing analysis and power extraction were made using Prime Time with 100% annotated switching activity from post-layout simulation in typical corner at 25 °C, targeting two operating points: 470 MHz at 0.65 V for high energy efficiency and 613 MHz at 0.8 V for high performance.

## B. Performance Evaluation 1) GEMM Performance Evaluation:
- Square and rectangular matrices are a synthetic benchmark to evaluate RedMulE's computation latency in cycles against the SW execution on 8 parallel RISC-V cores sharing 4 FPUs.
- RedMulE reaches a peak throughput of more than 95.4 OP/cycle, where we count both and • as one "OP", e.g. for a regular GEMM we count 1 MAC = 2 OPs.
- RedMulE achieves up to 99.4% of CEs utilization on 96 × 96 FP16 matrices (55 kB memory occupation), leading to 58.5 GFLOPS at 613 MHz with 0.80 V supply.
- Fig. 5a shows the number of computing cycles required to compute various matrices during parallel FP16 software executed on 8 RISC-V cores and compares them on RedMulE, showing that it reaches 15× average speedup over the software on large matrices.
- This performance increase with respect to the software counterpart settles around 13× with larger matrices since also the software execution becomes more efficient in those cases.
- We also consider the acceleration of a small 8 × 8 × 8 case, as shown in Fig. 5a in which the accelerator is under-utilized, but it still introduces 3.5× speedup over the software parallel execution.
- FP16 Network Training: To further evaluate Red-MulE performance on a real-case NN training, our target is TinyMLPerf [49], and in particular, we focused on the ResNet [50] example.
- For the software infrastructure, we rely on the pulp-TrainLib [27], and we compared RedMulE with a software baseline executed on 8 RISC-V cores sharing 4 FPUs.
- The library takes into consideration all the training steps for the calculation of the gradients and backpropagation.
- Fig. 5b shows the execution of a single step in the ResNet8 network when using 8 RISC-V cores in parallel and when using RedMulE for the matrix multiplication execution.
- RedMulE accelerates the matrix multiplication execution of 14.6× with respect to the parallel RISC-V execution in SW, speeding up the entire single step of the ResNet8 of 3.1×.
- RedMulE keeps its utilization constant at 99.1% (47.6 MAC/cycle) with the only exceptions in the first and the last layers where it drops to 93.2% (44.7 MAC/cycle) and 32.3% (15.5 MAC/cycle) due to leftovers that do not allow to exploit the full potential of the array.
- From Fig. 5b, it is also evident that the data reorganization during the Im2Col accounts for approximately 3 Millions computing cycles.
- To solve this problem, we augment RedMulE's operation with the support of the DataMover engine, halving the number of computing cycles required to perform the two Im2Col operations and thus speeding up the overall training step execution up to 4.9×.
- As all the devices included in the PULP cluster (RISC-V cores and accelerators) are designed for synergistic cooperation and share the memory, the heterogeneity of the architecture can be efficiently and fully exploited.
- GEMM-Ops Performance Evaluation: To evaluate the GEMM-Ops performance, in Fig. 5c we compare the RedMulE GEMM-Ops execution against parallel SW execution on the RISC-V cores.
- RedMulE always takes the same number of computing cycles to perform any of the supported GEMM-Ops, while the parallel execution on the general-purpose cores changes depending on the executed kernel.
- All the kernels belonging to Group 1...

## D. RedMulE Power
- At a cluster level, the power consumption in the efficiency point amounts to 59.3 mW during GEMM operation.
- The RedMulE contribution dominates the power envelope accounting for 66.8% of the overall consumption, while the TCDM banks and the HCI interconnect contribution is 13.3%.
- In this operating point, we reach a cluster peak energy efficiency of 755 GFLOPS/W during GEMM execution, corresponding to 12.5× higher energy efficiency with respect to the software baseline.
- During the execution of the algorithms belonging to GEMM-Ops' Group 1 on RedMulE, the cluster-level power dissipation reaches 53.2 mW, leading to 842 GFLOPS/W, which is 57.2× higher than SW execution.
- On the other hand, during the execution of the algorithms in GEMM-Ops' Group 2, the power consumption is further reduced to 37.6 mW resulting in 1.19 TFLOPS/W, thus 81.2× more efficient than software execution.

## VI. COMPARISON WITH THE STATE-OF-THE-ART
- RedMulE is a PULP cluster designed for TinyML applications
- It is 1.9× faster than DNPU and 16× more energy efficient
- It is also compared with Diana and Gemmini, two other chips designed in the same technology node
- Diana achieves 44.5% less performance than RedMulE 12x8 and 12% less performance than RedMulE 12x4 in the energy efficient mode
- Gemmini features one order of magnitude less energy efficiency than RedMulE 12x4 despite it features 5× the number of CEs and works with 8-bit integer format
- RedMulE 12x4 is more power efficient than other platforms specifically designed for on-chip training
- TSUNAMI and Trainer are conceived for energyefficient embedded training and extensively use pruning and sparse matrices generation to increase energy efficiency and reduce the number of required MAC operations during training
- SIMD 2 is 36.1× more power consuming than RedMulE

## VII. CONCLUSION
- Reduced-Precision Matrix Multiplication Engine
- Open-source cluster-coupled accelerator
- FP16 GEMM-Ops computation
- Floating-Point Unitsbased Computing Elements (CEs)
- Internal buffers
- Memory interface configurations
- Integrated instance of RedMulE
- 22 nm technology
- Performance speedup introduced by RedMulE over RISC-V cores reaches up to 62×
- Best performance point (at 613 MHz, 0.8 V) achieves 506 GFLOPS/W @ 58.5 GFLOPS
- Best efficiency point (at 470 MHz, 0.65 V) achieves 775 GFLOPS/W @ 44.8 GFLOPS
