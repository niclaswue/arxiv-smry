---
title: "NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields"
date: 2022-10-24T22:49:55.000Z
author: "Antoni Rosinol, John J. Leonard, Luca Carlone"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-13641v1.webp" # image path/url
    alt: "NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.13641)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.13641).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/nerf-slam-real-time-dense-monocular-slam-with).

# Abstract
- Uses dense monocular SLAM to provide accurate pose estimates and depth-maps
- Uses real-time hierarchical volumetric neural radiance fields to fit the scene in
- Uses uncertainty-based depth loss to achieve good photometric accuracy while

# Paper Content

## Introduction
- Monocular 3D reconstruction remains one of the most difficult computer vision problems
- Achieving 3D reconstructions in real-time from images alone is a key enabler for applications in robotics, surveying, and gaming, such as autonomous vehicles, crop monitoring, and augmented reality
- While many 3D reconstruction solutions are based on RGB-D or Lidar sensors, scene reconstruction from monocular imagery provides a more convenient solution
- RGB-D cameras can fail under certain conditions, such as direct sunlight, and Lidar remains heavier and more expensive than a monocular RGB camera
- Alternatively, stereo cameras simplify the depth estimation problem to a 1D disparity search, but rely on accurate calibration of the cameras that is prone to miscalibration during practical operations
- Instead, monocular cameras are inexpensive, lightweight, and represent the simplest sensor configuration to calibrate
- Unfortunately, monocular 3D reconstruction is a challenging problem due to the lack of explicit measurements of the depth of the scene
- Nevertheless, great progress has been recently made towards monocular 3D reconstructions by leveraging deep-learning approaches
- Given that deeplearning currently achieves the best performance for optical flow [30], and depth [36] estimation, a plethora of works have tried to use deep-learning modules for SLAM
- For example, using depth estimation networks from monocular images [29], multiple images, as in multi-view stereo [14], or using end-to-end neural networks [4]
- However, even with the improvements due to deeplearning, building both geometrically and photometrically accurate 3D maps of the scene from a casually taken monocular video in real-time is not currently possible
- Neural Radiance Fields (NeRFs) have recently enabled 3D representations of the world that are photometrically accurate
- Unfortunately, NeRFs are difficult to infer, given the costly volumetric rendering necessary to build a NeRF, leading to slow reconstructions
- Further, NeRFs originally required ground-truth pose estimates to converge. Nevertheless, recent work shows that it is possible to fit a radiance field in real-time given posed-images [17], while others show that the ground-truth poses are not strictly necessary [6,15], as long as a sufficiently good initial estimate is given
- Hence, real-time pose-free NeRF reconstructions could lead to metrically accurate 3D maps
- Despite this, a fundamental problem of NeRF representations, given no depth supervision, is that the parametrization of the surfaces by a density is prone to 'floaters', ghost geometry that appears because of bad initializations or convergence to bad local minima
- It has been shown that adding depth supervision significantly improves the removal of this ghost geometry, and the depth signal leads to faster convergence of radiance fields [10]
- Given these findings, our insight is that having a dense monocular SLAM pipeline, that outputs close-to-perfect pose estimates, together with dense depth maps and uncertainty estimates, provides the right information for building neural radiance fields of the scene on the fly
- Our experiments show that this is indeed possible, and that compared to other approaches we achieve more accurate reconstruc- tions.

## Related Work
- Dense monocular SLAM has been used to map indoor environments
- Neural radiance fields have been used to map outdoor environments
- The literature at the intersection of both fields is growing
- Dense monocular SLAM has been used to map indoor environments
- Neural radiance fields have been used to map outdoor environments
- The literature at the intersection of both fields is growing

### Dense SLAM
- The main challenges to dense SLAM are (1) the computational complexity, due to the shear amount of depth variables to be estimated, and (2) dealing with ambiguous or missing information to estimate the depth of the scene, such as textureless surfaces or aliased images.
- Historically, the first problem has been bypassed by decoupling the pose and depth estimation.
- For example, DTAM [19] achieves dense SLAM by using the same paradigm as the sparse PTAM [13], which tracked the camera pose first and then the depth, in a de-coupled fashion.
- The second problem is also typically avoided by using RGB-D or Lidar sensors, that provide explicit depth measurements, or stereo cameras that simplify depth estimation.
- However, recent research on dense SLAM has achieved impressive results in these two fronts.
- To reduce the number of depth variables, CodeSLAM [4] optimizes instead the latent variables of an auto-encoder that infers depth maps from images.
- By optimizing these latent variables, the dimensionality of the problem is significantly reduced, while the resulting depth maps remain dense.
- Tandem [14] is able to reconstruct 3D scenes with only monocular images by using a pre-trained MVSNet-style neural-network on monocular depth estimation, and then decoupling the pose/depth problem by performing frameto-model photometric tracking.
- Droid-SLAM [31] shows that by adapting a state-of-the-art dense optical flow estimation architecture [30] to the problem of visual odometry, it is possible to achieve competitive results in a variety of challenging datasets (such as the Euroc [5] and TartanAir [34] datasets), Droid-SLAM avoids the dimensionality problem by using downsampled depth maps that are subsequently upsampled using a learned upsampling operator.
- Rosinol et al. [23] further show that dense monocular SLAM can reconstruct faithful 3D meshes of the scene by weighting the depths estimated in dense SLAM by their marginal covariance, and subsequently fusing them in a volumetric representation.
- The resulting mesh is geometrically accurate, but due to the limitations of TSDF representations, their reconstruction lacks photometric detail and is not fully complete.
- Our approach is inspired by the work from Rosinol et al. [23], where we replace the volumetric TSDF for a hierarchical volumetric neural radiance field as our map representation.
- By using radiance fields, our approach achieves photometrically accurate maps and improves the completeness of the reconstruction, while also allowing the optimization of poses and the map simultaneously.
- To enable photometrically accurate maps, NeRFs have proved to be a useful representation that allows capturing view-dependent effects while maintaining multi-view consistency.
- NeRF [16] is the seminal work that gave name to this representation, and sparked a revolution in terms of new papers that tried to improve several aspects, the most prominent ones being: reducing the training time to build a NeRF and the improvement of the underlying 3D reconstruction.
- While the vanilla NeRF approach, using one large MLP, requires hours of training to converge, several authors show that a smaller MLP, combined with 3D spatial data structures to partition the scene, leads to substantial speed-ups.
- In particular, NGLOD [27] proposes to use tiny MLPs in a volumetric grid, leading to faster reconstructions, but not quite real-time.
- Plenoxels [40] further improved the speed by parametrizing the directional encoding using spherical harmonics, while bypassing the use of an MLP.
- Instant-NGP [17] shows that with a hash-based hierarchical volum...

## Methodology

### Tracking: Dense SLAM with Covariances
- We use as our tracking module Droid-SLAM [31], which provides dense depth maps and poses for every keyframe.
- Starting from a sequence of images, Droid-SLAM first computes the dense optical-flow p ij between pairs of frames i and j, using a similar architecture to Raft [30].
- At the core of Raft is a Convolutional GRU (ConvGRU in Fig. 2) that, given the correlation between pairs of frames and a guess of the current optical flow p ij , computes a new flow p ij , as well as a weight Σ pij for each optical flow measurement.
- With these flows and weights as measurements, Droid-SLAM solves a dense bundle adjustment (BA) problem where the 3D geometry is parametrized as a set of inverse depth maps per keyframe.
- This parametrization of the structure leads to an extremely efficient way of solving the dense BA problem, which can be formulated as a linear least-squares problem.
- To solve the linear least-squares problem, we take the Schur complement of the Hessian to compute the reduced camera matrix H T , which does not depend on the depths, and has a much smaller dimensionality of R c×c .
- The resulting smaller problem over the camera poses is solved by taking the Cholesky factorization of H T = LL T , where L is the lower-triangular Cholesky factor, and then solving for the poses T by front and backsubstitution.
- As shown at the bottom of Fig. 2, given these poses T , we can solve for the depths d.
- Furthermore, given poses T and depths D, Droid-SLAM proposes to compute the induced optical-flow and feeds it again as an initial guess to the ConvGRU network, as seen on the left side of Fig. 2.
- The blue arrows in Fig. 2 show the tracking loop, and corresponds to Droid-SLAM.
- Then, inspired by Rosinol et al. [23], we further compute the marginal covariances of both the dense depth maps and the poses from Droid-SLAM (purple arrows in Fig. 2).
- For this, we need to leverage the structure of the Hessian, which we block-partition as follows: where H is the Hessian matrix, b the residuals, C is the block camera matrix, and P is the diagonal matrix corresponding to the inverse depths per pixel per keyframe.
- We represent by ∆ξ the delta updates on the lie algebra of the camera poses in SE(3), while ∆d is the delta update to the per-pixel inverse depths.
- E is the camera/depth off-diagonal Hessian's block matrices, and v and w correspond to the pose and depths residuals.
- From this block-partitioning of the Hessian, we can efficiently calculate the marginal covariances for the dense depths Σ d and poses Σ T , as shown in [23]:
- We refer to [23] for details on how to compute these in realtime.
- Finally, given all the information computed by the tracking module -the poses, the depths, their respective marginal covariances, as well as the input RGB images -we can optimize our radiance field's parameters and refine the camera poses simultaneously.

### Mapping: Probabilistic Volumetric NeRF
- Given the dense depth-maps for each keyframe, it is possible to depth-supervise our neural volume.
- Unfortunately, the depth-maps are extremely noisy due to their density, since even textureless regions are given a depth value.
- Figure 3 shows that the resulting pointcloud from dense monocular SLAM is particularly noisy and contains large outliers.
- Supervising our radiance field given these depth-maps can lead to biased reconstructions, as shown later in Sec. Rosinol et al. [23]
- Inspired by these results, we use the depth uncertainty estimation to weight the depth loss which we use to supervise our neural volume.
- Figure 1 shows the input RGB image, its corresponding depth-map uncertainty, the resulting pointcloud (after thresholding its uncertainty by σ d ≤ 1.0 for visualization), and our results when using our uncertainty-weighted depth loss.
- Given the uncertainty-aware losses, we formulate our mapping loss as:
- which we minimize with respect to both poses T and neural parameters Θ, given hyper-parameter λ D balancing depth and color supervision (we set λ D to 1.0).
- In particular, our depth loss is given by:
- where D is the rendered depth, and D, Σ D are the dense depth and uncertainty as estimated by the tracking module.
- We render the depth D as the expected ray termination distance, similarly to [10,16].
- Each depth per pixel is computed by sampling 3D positions along the pixel's ray, evaluating the density σ i at sample i, and alpha-compositing the resulting densities, similarly to standard volumetric rendering: with d i the depth of a sample i along the ray, and δ i = d i+1 − d i the distance between consecutive samples.
- σ i is the volume density, generated by evaluating an MLP at the 3D world coordinate of sample i.
- We refer to [17] for more details on the inputs given to the MLP.
- Lastly, T i is the accumulated transmittance along the ray up to sample i, defined as:

### Architecture
- The tracking thread continuously minimizes the BA re-projection error for an active window of keyframes
- The mapping thread always optimizes all of the keyframes received from the tracking thread, and does not have a sliding window of active frames
- The only communication between these threads happens when the tracking pipeline generates a new keyframe
- On every new keyframe, the tracking thread sends the current keyframes' poses with their respective images and estimated depth-maps, as well as the depths' marginal covariances, to the mapping thread
- Only the information currently available in the sliding optimization window of the tracking thread is sent to the mapping thread

### Implementation Details
- We perform all computations in Pytorch and CUDA
- We use an RTX 2080 Ti GPU for all our experiments
- We use Instant-NGP as our hierarchical volumetric neural radiance field map
- We modify Instant-NGP to add our proposed mapping loss L M in Eq. (3)
- We compute depth and pose uncertainties using Rosinol's approach
- We use the same GPU for both tracking and mapping

## Results
- We evaluate our approach against competing techniques
- We provide ablation experiments to evaluate the improvements deriving from our proposed architecture and suggested uncertainty-aware loss

### Datasets
- We use two datasets for evaluation: the Cube-Diorama dataset and the Replica dataset.
- The Cube-Diorama dataset is a synthetic dataset generated with Blender, which provides ground-truth poses, depths, and images.
- Using this dataset, we are able to do accurate ablation experiments to evaluate the benefits of our proposed approach.
- The Replica dataset provides real-world scenes which we use to evaluate and compare our approach with related work.
- The dataset consists of high quality 3D reconstructions of 5 offices and 3 apartments.
- For evaluation, we use the data generated by rendering a random trajectory of 2000 RGB and depth frames per scene (as generated by the authors of iMAP).

### Methods for Evaluation
- Classical TSDF-Fusion [9]
- Probabilistic TSDF-Fusion from Rosinol et al. [23] (σ-Fusion)
- iMAP [26]
- Nice-SLAM [42]
- Replica dataset
- Ground-truth depth
- Unsupervised learning
- Hash-based volumetric representation
- Uniform weight
- Depth-maps uncertainties
- Large MLP
- Hierarchical dense volumetric grids
- Depth-maps rendered from ground-truth meshes

### Geometric and Photometric Accuracy
- The Replica dataset allows us to evaluate the geometric and photometric quality of the different approaches.
- In particular, we use the L1 depth error between the estimated and the ground-truth depth-maps as a proxy for geometric accuracy (Depth L1), as well as the peak signal-to-noise ratio (PSNR) between the input RGB images and the rendered images for photometric accuracy.
- Table 1 shows the results achieved by the different approaches we evaluate.
- The first two rows correspond to iMAP and Nice-SLAM under their default setup, with ground-truth depth used as supervisory signal.
- Nice-SLAM is superior to iMAP in terms of geometric and photometric accuracy.
- The following rows correspond to approaches that do not use the ground-truth depth-maps.
- We run TSDF-Fusion and σ-Fusion with the poses and depth-maps estimated by our tracking module, with σ-Fusion further using the depths' uncertainties.
- While σ-Fusion achieves better geometric reconstructions than TSDF-Fusion, both achieve poor photometric accuracy (we render the resulting 3D mesh extracted using marching cubes).
- We also run Nice-SLAM without ground-truth depth.
- As expected, Nice-SLAM's results deteriorate when not using groundtruth depth, but it still achieves competitive results when compared to TSDF fusion, particularly in terms of photometric accuracy (PSNR).
- Finally, our approach achieves the best results in terms of photometric accuracy, and geometric accuracy by a substantial margin in most scenes.
- Compared to Nice-SLAM, we achieve up to 179% PSNR improvements in office-1, and up to 86% better L1 accuracy in room-2, with the best combined improvements seen in office-1 (179% better PSNR, 80% better L1).

### Depth Loss Ablation
- Depth supervision of neural radiance fields from raw depth-maps, either estimated from dense SLAM or coming from RGB-D, is prone to errors
- For dense monocular SLAM, this is particularly problematic, since depth values are estimated even for textureless or aliased regions
- Fig. 4 shows that the ideal scenario is to use ground-truth pose and ground-truth depth for fast and accurate neural radiance field reconstruction
- If groundtruth depths are not provided, but ground-truth poses are available, the radiance field also converges, although at a slower pace
- Instead, if we provide noisy poses and no depth-maps, the radiance field does not converge in less than 60s
- Our approach aims to reach this last result
- The bottom-right image in Fig. 4 shows that our approach can achieve great results despite using noisy poses and depths, as long as these are weighted by their uncertainty
- Weighting the tracking's depth estimates achieves the best PSNR and L1 depth metrics: it achieves similar PSNR than when not using depths, while having the same L1 error than when using depths

### Real-Time Performance
- The pipeline is capable of running at 12 frames per second
- The tracking thread runs at 15 frames per second on average, creating around 10 keyframes per second, depending on the amount of motion
- We add a keyframe every time the mean optical flow is larger than 2.5 pixels
- The mapping thread runs at 10 frames per second on average

## Limitations
- Our approach requires ∼11Gb of GPU memory to operate
- The resulting memory requirements can be alleviated in two ways: on the one hand, correlation volumes can be computed on the fly, reducing the need to store dense correlation volumes; on the other hand, the volumetric information can be streamed to the CPU for inactive regions, only loading to GPU a sliding window around the region of interest, similarly to [35]

## Conclusion
- We show that dense monocular SLAM provides the ideal information for building a NeRF representation of a scene from a casually taken monocular video.
- The estimated poses and depth-maps from dense SLAM, weighted by their marginal covariance estimates, provide the ideal source of information to optimize a hierarchical hash-based volumetric neural radiance field.
- With our approach, users can generate a photometrically and geometrically accurate reconstruction of the scene in real-time.
- Future work can leverage our approach to extend the definition of metric-semantic SLAM [24], which typically only considers geometric and semantic properties, by building representations that are also photometrically accurate.
