---
title: "NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields"
date: 2022-10-24T22:49:55.000Z
author: "Antoni Rosinol, John J. Leonard, Luca Carlone"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-13641v1.webp" # image path/url
    alt: "NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.13641)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.13641).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/nerf-slam-real-time-dense-monocular-slam-with).

# Abstract
- Proposed a novel 3D mapping pipeline for scene reconstruction from monocular images
- Leveraged advances in dense monocular SLAM and real-time hierarchical volumetric neural radiance fields
- Proposed uncertainty-based depth loss for good photometric and geometric accuracy
- Better accuracy than competing approaches (up to 179% better PSNR and 86% better L1 depth)
- Real-time performance using only monocular images

# Paper Content

## Introduction
- 3D reconstruction from monocular images is a difficult computer vision problem
- Applications in robotics, surveying, and gaming can be enabled by achieving 3D reconstructions in real-time from images alone
- RGB-D and Lidar sensors are used for 3D reconstruction, but monocular cameras are simpler and cheaper
- Monocular 3D reconstruction is challenging due to lack of explicit measurements of depth
- Deep-learning approaches have been used to try to solve the problem
- Neural Radiance Fields (NeRFs) can provide photometrically accurate 3D representations, but are difficult to infer
- Recent work has shown that NeRFs can be fit in real-time without ground-truth poses
- Adding depth supervision can improve removal of ghost geometry and lead to faster convergence
- Combining dense monocular SLAM with hierarchical volumetric neural radiance fields can build accurate radiance fields from a stream of images in real-time

## Related work
- Dense monocular SLAM
- Neural radiance fields

### Dense slam
- Computational complexity of dense SLAM
- Historically, decoupling pose and depth estimation
- RGB-D or Lidar sensors provide explicit depth measurements
- Recent research on dense SLAM has achieved impressive results
- CodeSLAM optimizes latent variables of auto-encoder to reduce depth variables
- Tandem reconstructs 3D scenes with monocular images
- Droid-SLAM adapts state-of-the-art dense optical flow estimation
- Rosinol et al. weight depths estimated in dense SLAM by marginal covariance
- Our approach uses hierarchical volumetric neural radiance field as map representation
- NeRFs capture view-dependent effects while maintaining multi-view consistency
- NGLOD, Plenoxels, and Instant-NGP improve speed of NeRFs
- Mono-SDF, NGLOD, Plenoxels, and Instant-NGP improve 3D reconstruction
- Our approach uses probabilistic dense SLAM to weight supervisory signals
- iNeRF, Barf, iMap, Nice-SLAM, VolBA, Orbeez-SLAM, and Abou-Chakra et al. use hierarchical volumetric map for real-time SLAM

## Methodology

### Tracking: dense slam with covariances
- Droid-SLAM provides dense depth maps and poses for every keyframe
- Computes dense optical-flow between pairs of frames
- ConvGRU computes new flow and weight for each optical flow measurement
- 3D geometry is parametrized as a set of inverse depth maps per keyframe
- Solves dense bundle adjustment problem as a linear least-squares problem
- Computes marginal covariances for dense depths and poses
- Optimizes radiance field's parameters and refines camera poses

### Mapping: probabilistic volumetric nerf
- Given dense depth-maps, it is possible to depth-supervise a neural volume.
- Depth-maps are noisy due to their density.
- Rosinol et al. [23] shows that the uncertainty of the depth estimates can be used to weight the depth values.
- Our mapping loss is formulated as a combination of depth and color supervision.
- Our approach achieves accurate results in real-time.

### Architecture
- Pipeline consists of tracking and mapping threads running in real-time and in parallel
- Tracking thread minimizes BA re-projection error for active window of keyframes
- Mapping thread optimizes all keyframes received from tracking thread
- Communication between threads happens when tracking thread generates new keyframe
- Tracking thread sends poses, images, depth-maps, and marginal covariances to mapping thread
- Tracking thread sliding window of active frames consists of at most 8 keyframes
- Tracking thread generates new keyframe when mean optical-flow between previous and current frame is higher than threshold
- Mapping thread is in charge of rendering for interactive visualization of reconstruction

### Implementation details
- Perform computations in Pytorch and CUDA
- Use RTX 2080 Ti GPU for experiments
- Use Instant-NGP as hierarchical volumetric neural radiance field map
- Modify to add proposed mapping loss
- Use Droid-SLAM as tracking frontend
- Re-use pretrained weights
- Compute depth and pose uncertainties using Rosinol's approach
- Compute uncertainties in real-time
- Use same GPU for tracking and mapping
- Allows use of two separate GPUs for tracking and mapping

## Results
- Evaluated approach against competing techniques
- Provided ablation experiments to evaluate improvements from proposed architecture and suggested uncertainty-aware loss

### Datasets
- Used two datasets for evaluation: Cube-Diorama and Replica
- Cube-Diorama is a synthetic dataset with ground-truth poses, depths, and images
- Replica dataset provides real-world scenes to evaluate and compare approach with related work
- Replica dataset consists of high quality 3D reconstructions of 5 offices and 3 apartments
- Evaluation uses data generated by rendering a random trajectory of 2000 RGB and depth frames per scene

### Methods for evaluation
- 4 approaches are compared: classical TSDFfusion, probabilistic TSDF-Fusion, iMAP, and Nice-SLAM
- TSDF-Fusion and σ-Fusion use a hash-based volumetric representation to fuse posed depth-maps
- iMAP uses one large MLP to represent the 3D scene
- Nice-SLAM uses hierarchical dense volumetric grids for mapping

### Geometric and photometric accuracy
- Replica dataset allows evaluation of geometric and photometric quality of different approaches
- L1 depth error and PSNR used as proxies for accuracy
- Nice-SLAM superior to iMAP in terms of accuracy
- TSDF-Fusion and σ-Fusion use estimated poses and depths
- σ-Fusion better than TSDF-Fusion in terms of geometric accuracy
- Nice-SLAM without ground-truth depth still competitive
- Our approach best in terms of photometric and geometric accuracy
- Up to 179% PSNR and 86% L1 accuracy improvements compared to Nice-SLAM

### Depth loss ablation
- Depth maps are often noisy and have outliers.
- Using ground-truth pose and ground-truth depth leads to fast and accurate neural radiance field reconstruction.
- Using noisy poses and no depth-maps does not lead to convergence in less than 60s.
- Our approach can achieve great results despite using noisy poses and depths, if they are weighted by their uncertainty.

### Real-time performance
- Pipeline runs at 12 frames per second with 640x480 resolution
- Tracking thread runs at 15 frames per second, creating 10 keyframes per second
- Keyframe added when mean optical flow is larger than 2.5 pixels
- Mapping thread runs at 10 frames per second
- Pipeline reconstructs scene in real-time at 10 frames per second

## Limitations
- 11GB of GPU memory is needed for the approach
- Memory requirements can be too large for robotics applications with low compute power
- Memory requirements can be reduced by computing correlation volumes on the fly
- Volumetric information can be streamed to the CPU for inactive regions

## Conclusion
- Dense monocular SLAM provides information to build a NeRF representation of a scene from a casually taken monocular video.
- Estimated poses and depth-maps from dense SLAM are used to optimize a hierarchical hash-based volumetric neural radiance field.
- Real-time photometrically and geometrically accurate reconstructions of the scene can be generated.
- Our approach can be used for high-level scene understanding, such as for building 3D Dynamic Scene Graphs.
