---
title: "High Fidelity Neural Audio Compression"
date: 2022-10-24T17:52:02.000Z
author: "Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2210-13438v1.webp" # image path/url
    alt: "High Fidelity Neural Audio Compression" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.13438)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.13438).


# Abstract
- State-of-the-art real-time, high-fidelity audio codec
- Leveraging neural networks
- Simplifying and speeding up training by using a single multiscale spectrogram adversary
- Weight of a loss now defines the fraction of the overall gradient it should represent, decoupling the choice of this hyper-parameter from the typical scale of the loss
- lightweight Transformer models can be used to further compress the obtained representation by up to 40%

# Paper Content

## Introduction
- Streaming audio and video have accounted for the majority of the internet traffic in 2021
- Lossy signal compression is an increasingly important problem
- Audio codecs typically employ a carefully engineered pipeline combining an encoder and a decoder to remove redundancies in the audio content and yield a compact bitstream
- Leveraging neural networks as trained transforms via an encoder-decoder mechanism has been explored
- The problems arising in lossy neural compression models are twofold: first, the model has to represent a wide range of signals, such as not to overfit the training set or produce artifact laden audio outside its comfort zone
- We solve this by having a large and diverse training set (described in Section 4.1), as well as discriminator networks (see Section 3.4) that serve as perceptual losses, which we study extensively in Section 4.5.1, Table 2
- But the evaluation of lossy audio codecs necessarily relies on human perception, so we ran extensive human evaluation for multiple points in this design space, both for speech and music
- Those evaluations (MUSHRA) consist in having humans listen to, compare, and rate excerpts of speech or music compressed with competitive codecs and variants of our method, and the uncompressed ground truth
- Our best model, EnCodec , reaches state-of-the-art scores for speech and for music at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels

## Related Work
- Recent advancements in neural audio generation enabled computers to efficiently generate natural sounding audio
- The first convincing results were achieved by autoregressive models such as WaveNet (Oord et al., 2016), at the cost of slow inference
- While many other approaches were explored (Yamamoto et al., 2020a;Kalchbrenner et al., 2018;Goel et al., 2022), the most relevant ones here are those based on Generative Adversarial Networks (GAN) (Kumar et al., 2019;Yamamoto et al., 2020a;Kong et al., 2020;Andreev et al., 2022)
- Our work uses and extends similar adversarial losses to limit artifacts during audio generation
- Audio Codec
- Low bitrate parametric speech and audio codecs have long been studied (Atal & Hanauer, 1971;Juang & Gray, 1982), but their quality has been severely limited
- Despite some advances (Griffin & Lim, 1985;McCree et al., 1996), modeling the excitation signal has remained a challenging task
- The current state-of-the-art traditional audio codecs are Opus (Valin et al., 2012) and Enhanced Voice Service (EVS) (Dietz et al., 2015)
- Neural based audio codecs have been recently proposed and demonstrated promising results (Kleijn et al., 2018;Valin & Skoglund, 2019b;Lim et al., 2020;Kleijn et al., 2021;Zeghidour et al., 2021;Omran et al., 2022;Lin et al., 2022;Jayashankar et al., 2022;Li et al.;Jiang et al., 2022)
- In Valin & Skoglund (2019b), an LPCNet (Valin & Skoglund, 2019a) vocoder was conditioned on hand-crafted features and a uniform quantizer
- Gârbacea et al. (2019) conditioned a WaveNet based model on discrete units obtained from a VQ-VAE (Van Den Oord et al., 2017;Razavi et al., 2019) model, while Skoglund & Valin (2019) tried feeding the Opus codec (Valin et al., 2012) to a WaveNet to further improve its perceptual quality
- Jayashankar et al. (2022); Jiang et al. (2022) propose an auto-encoder with a vector quantization layer applied over the latent representation and minimizing the reconstruction loss, while Li et al. suggested using Gumbel-Softmax (GS) (Jang et al., 2017) for representation quantization
- The most relevant related work to ours is the SoundStream model (Zeghidour et al., 2021), in which the authors propose a fully convolutional encoder decoder architecture with a Residual Vector Quantization (RVQ) (Gray, 1984;Vasuki & Vanathi, 2006) layers
- Audio Discretization
- Representing audio and speech using discrete values was proposed to various tasks recently

## Model
- An audio signal can be represented by a sequence of samples
- The encoder network E produces a compressed representation z q
- The decoder network G reconstructs the time-domain signal x from z q
- The whole system is trained end-to-end to minimize a reconstruction loss

### Encoder & Decoder Architecture
- The encoder-decoder architecture has a sequential modeling component applied over the latent representation
- The encoder model E has a 1D convolution with C channels and a kernel size of 7 followed by B convolution blocks
- Each convolution block is composed of a single residual unit followed by a down-sampling layer consisting of a strided convolution, with a kernel size of K of twice the stride S
- The residual unit contains two convolutions with kernel size 3 and a skip-connection
- The encoder model E is followed by a two-layer LSTM for sequence modeling and a final 1D convolution layer with a kernel size of 7 and D output channels
- Following Zeghidour et al. (2021); Li et al. (2021), we use C = 32, B = 4 and (2, 4, 5, 8) as strides
- We use ELU as a non-linear activation function (Clevert et al., 2015) either layer normalization (Ba et al., 2016) or weight normalization (Salimans & Kingma, 2016)
- We use two variants of the model, depending on whether we target the low-latency streamable setup, or a high fidelity non-streamable usage
- With this setup, the encoder outputs 75 latent steps per second of audio at 24 kHz, and 150 at 48 kHz
- The decoder mirrors the encoder, using transposed convolutions instead of strided convolutions, and with the strides in reverse order as in the encoder, outputting the final mono or stereo audio
- In the non-streamable setup, we use for each convolution a total padding of K − S, split equally before the first time step and after the last one (with one more before if K − S is odd)
- We further split the input into chunks of 1 seconds, with an overlap of 10 ms to avoid clicks, and normalize each chunk before feeding it to the model, applying the inverse operation on the output of the decoder, adding a negligible bandwidth overhead to transmit the scale
- We use layer normalization (Ba et al., 2016), computing the statistics including also the time dimension in order to keep the relative scale information
- For the streamable setup, all padding is put before the first time step
- For a transposed convolution with stride s, we output the s first time steps, and keep the remaining s steps in memory for completion when the next frame is available, or discarding it at the end of a stream
- We notice a small gain over the objective metrics by keeping a form of normalization, as demonstrated in Table A.3.

### Residual Vector Quantization
- We use Residual Vector Quantization (RVQ) to quantize the output of the encoder as introduced by Zeghidour et al. (2021).
- Vector quantization consists of projecting an input vector onto the closest entry in a codebook of a given size. RVQ refines this process by computing the residual after quantization, and further quantizing it using a second codebook, and so forth.
- We follow the same training procedure as described by Dhariwal et al. (2020) and Zeghidour et al. (2021). The codebook entry selected for each input is updated using an exponential moving average with a decay of 0.99, and entries that are not used are replaced with a candidate sampled from the current batch.
- We use a straight-through-estimator (Bengio et al., 2013) to compute the gradient of the encoder, e.g. as if the quantization step was the identity function during the backward phase.
- Finally, a commitment loss, consisting of the MSE between the input of the quantizer and its output, with gradient only computed with respect to its input, is added to the overall training loss.
- By selecting a variable number of residual steps at train time, a single model can be used to support multiple bandwidth target (Zeghidour et al., 2021).

### Language Modeling and Entropy Coding
- We use a Transformer to keep compression/decompression on a single CPU core fast
- The model consists of 5 layers, 8 heads, 200 channels, a dimension of 800 for the feed-forward blocks, and no dropout
- At train time, we select a bandwidth and the corresponding number of codebooks N q
- For a time step t, the discrete representation obtained at time t − 1 is transformed into a continuous representation using learnt embedding tables, one for each codebook, and which are summed
- For t = 0, a special token is used instead
- The output of the Transformer is fed into N q linear layers with as many output channels as the cardinality of each codebook (e.g. 1024), giving us the logits of the estimated distribution over each codebook for time t
- We thus neglect potential mutual information between the codebooks at a single time step
- This allows to speedup inference (as opposed to having one time step per codebook, or a multi-stage prediction) with a limited impact over the final cross entropy
- Each attention layer has a causal receptive field of 3.5 seconds,
- The range based arithmetic coder is used to leverage the estimated probabilities given by the language model
- Evaluation of the same model might lead to different results on different architectures, or with different evaluation procedures due to floating point approximations
- This can lead to decoding errors as the encoder and decoder will not use the exact same code

### Training objective
- The reconstruction loss term is comprised of a time and a frequency domain loss term.
- We minimize the L1 distance between the target and compressed audio over the time domain, i.e. t (x, x) = x − x 1 .
- For the frequency domain, we use a linear combination between the L1 and L2 losses over the mel-spectrogram using several time scales (Yamamoto et al., 2020b;Gritsenko et al., 2020).
- Formally, where S i is a 64-bins mel-spectrogram using a normalized STFT with window size of 2 i and hop length of 2 i /4, e = 5, . . . , 11 is the set of scales, and α represents the set of scalar coefficients balancing between the L1 and L2 terms.
- Unlike Gritsenko et al. (2020), we take α i = 1.
- We introduce a perceptual loss term based on a multi-scale STFT-based (MS-STFT) discriminator, illustrated in Figure 2.
- The MS-STFT discriminator consists in identically structured networks operating on multi-scaled complex-valued STFT with the real and imaginary parts concatenated.
- Each sub-network is composed of a 2D convolutional layer (using kernel size 3 x 8 with 32 channels), followed by 2D convolutions with increasing dilation rates in the time dimension of 1, 2 and 4, and a stride of 2 over the frequency axis.
- A final 2D convolution with kernel size 3 x 3 and stride (1, 1) provide the final prediction.
- We use 5 different scales with STFT window lengths of [2048,1024,512,256,128].
- At 24 kHz, we train the model to support the bandwidths 1.5, 3, 6, 12, and 24 kbps by selecting the appropriate number of codebooks to keep in the RVQ step, as explained in Section 3.2.
- At 48 kHz, we train to support 3, 6, 12 and 24 kbps.
- We also noticed that using a dedicated discriminator per-bandwidth is beneficial to the audio quality. Thus, we select a given bandwidth for the entire batch, and evaluate and update only the corresponding discriminator.
- We add a commitment loss l w between the output of the encoder, and its quantized value, with no gradient being computed for the quantized value.
- For each residual step c ∈ {1, . . . C} (with C depeding on the bandwidth target for the current batch), noting z c the current residual and q c (z c ) the nearest entry in the corresponding codebook, we define l w as
- The generator is trained to optimize the following loss, summed over the batch, where λ t , λ f , λ g , λ f eat , and λ w the scalar coefficients to balance between the terms.

## Experiments and Results

### Dataset
- We use the clean and with entropy coding (hollow) for speech
- We use Lyra-v2 for neural audio
- We use EVS and Opus for competitive standard codecs
- The audio samples are from speech and music
- The ground truth is 16bits 24kHz wave.
- For general audio, we use on AudioSet (Gemmeke et al., 2017) together with FSD50K (Fonseca et al., 2021)
- For music, we rely on the Jamendo dataset (Bogdanov et al., 2019) for training and evaluation
- Data splits are detailed in Appendix A.1
- For training and validation, we define a mixing strategy which consists in either sampling a single source from a dataset or performing on the fly mixing of two or three sources
- Specifically, we have four strategies: (s1) we sample a single source from Jamendo with probability 0.32; (s2) we sample a single source from the other datasets with the same probability; (s3) we mix two sources from all datasets with a probability of 0.24; (s4) we mix three sources from all datasets except music with a probability of 0.12

### Baselines
- Opus is a versatile speech and audio codec standardized by the IETF in 2012
- EVS is a codec standardized in 2014 by 3GPP and developed for Voice over LTE (VoLTE)
- Opus and EVS are used as traditional digital signal processing baselines
- MP3 compression at 64 kbps is used as an additional baseline for the stereophonic signal compression case
- Opus and EVS are compared to the SoundStream model from the official implementation available in Lyra 21 at 3.2 kbps and 6 kbps on audio upsampled to 32 kHz
- Results are reported in Table A.2 in the Appendix A.3.

### Evaluation Methods
- We use the MUSHRA protocol to evaluate perceptual quality.
- We use ViSQOL to evaluate the quality of objective metrics.
- We use the SI-SNR to evaluate the quality of objective metrics.

### Training
- Trained all models for 300 epochs
- Used 8 A100 GPUs
- Used the balancer introduced in Section 3.4 with weights λ t = 0.1, λ f = 1, λ g = 3, λ feat = 3 for the 24 kHz models
- Used λ g = 4, λ feat = 4 for the 48 kHz model

### Results
- EnCodec outperforms all evaluated baselines in terms of MUSHRA score
- When considering the same bandwidth, EnCodec is superior to Lyra-v2 using 6kbps and Opus at 12kbps
- Streamable vs. non-streamable setups have a small degradation in performance, but the EnCodec model remains strong
- The balancer significantly stabilizes the training process

### Latency and computation time
- The initial latency is 13.3 ms for the 24 kHz streaming EnCodec model and 1 second for the 48 kHz non-streaming version.
- The real time factor is 10 times faster than real time.
- The gain from the entropy coding comes at a cost, although the processing is still faster than real time and could be used for applications where latency is not essential (e.g. streaming).

## Conclusion
- We presented EnCodec, a state-of-the-art real-time neural audio compression model, producing high-fidelity audio samples across a range of sample rates and bandwidth.
- We showed subjective and objective results from 24kHz monophonic at 1.5 kbps (Figure 3) to 48kHz stereophonic (Table 4).
- We improved sample quality by developing a simple but potent spectrogram-only adversarial loss which efficiently reduces artifacts and produce high-quality samples.
- Besides, we stabilized training and improved the interpretability of the weights for losses through a novel gradient balancer.
- Finally, we also demonstrated that a small Transformer model can be used to further reduce the bandwidth by up to 40% without further degradation of quality, in particular for applications where low latency is not essential (e.g. music streaming).
