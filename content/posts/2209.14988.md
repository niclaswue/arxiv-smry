---
title: "DreamFusion: Text-to-3D using 2D Diffusion"
date: 2022-09-29T17:50:40.000Z
author: "Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-14988v1.webp" # image path/url
    alt: "DreamFusion: Text-to-3D using 2D Diffusion" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.14988)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.14988).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/dreamfusion-text-to-3d-using-2d-diffusion).

# Abstract
- Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs.
- Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist.
- In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis.
- We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator.
- Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss.
- The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.

# Paper Content

## INTRODUCTION
- Generative image models conditioned on text now support high-fidelity, diverse and controllable image synthesis
- These quality improvements have come from large aligned image-text datasets
- Diffusion models are particularly effective at learning high-quality image generators with a stable and scalable denoising objective
- Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data
- In this work, we develop techniques to transfer pretrained 2D image-text diffusion models to 3D object synthesis, without any 3D data
- Though 2D image generation is widely applicable, simulators and digital media like video games and movies demand thousands of detailed 3D assets to populate rich interactive environments
- 3D assets are currently designed by hand in modeling software like Blender and Maya3D, a process requiring a great deal of time and expertise
- Text-to-3D generative models could lower the barrier to entry for novices and improve the workflow of experienced artists
- 3D generative models can be trained on explicit representations of structure like voxels (Wu et al., 2016;Chen et al., 2018) and point clouds (Yang et al., 2019;Cai et al., 2020;Zhou et al., 2021), but the 3D data needed is relatively scarce compared to plentiful 2D images
- Our approach learns 3D structure using only a 2D diffusion model trained on images, and sidesteps this issue
- GANs can learn controllable 3D generators from photographs of a single object category, by placing an adversarial loss on 2D image renderings of the output 3D object or scene
- Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text
- Neural Radiance Fields, or NeRF (Mildenhall et al., 2020) are an approach towards inverse rendering in which a volumetric raytracer is combined with a neural mapping from spatial coordinates to color and volumetric density
- NeRF has become a critical tool for neural inverse rendering (Tewari et al., 2022)
- Originally, NeRF was found to work well for "classic" 3D reconstruction tasks: many images of a scene are provided as input to a model, and a NeRF is optimized to recover the geometry of that specific scene, which allows for novel views of that scene from unobserved angles to be synthesized
- Many 3D generative approaches have found success in incorporating NeRF-like models as a building block within a larger generative system
- Dream Fields (Jain et al., 2022) is one such approach that uses frozen image-text joint embedding models from CLIP and an optimization-based approach to train NeRFs
- This work showed that pretrained 2D image-text models may be used for 3D synthesis

## DIFFUSION MODELS AND SCORE DISTILLATION SAMPLING
- The generative model p is trained to slowly add structure starting from random noise p(z T ) = N (0, I) with transitions p φ (z t−1 |z t ).
- The optimal reverse process step is also Gaussian and related to an optimal MSE denoiser.
- Transitions are typically parameterized as p φ (z t−1 |z t ) = q(z t−1 |z t , x = xφ (z t ; t)) where q(z t−1 |z t , x) is a posterior distribution derived from the forward process and xφ (z t ; t) is a learned approximation of the optimal denoiser.
- Instead of directly predicting xφ , Ho et al. (2020) trains an image-to-image U-Net φ (z t ; t) that predicts the noise content of the latent
- The predicted noise can be related to a predicted score function for the smoothed density ∇ zt log p(z t ) through Tweedie's formula (Robbins, 1992):
- Training the generative model with a (weighted) evidence lower bound (ELBO) simplifies to a weighted denoising score matching objective for parameters φ (Ho et al., 2020;Kingma et al., 2021):
- where w(t) is a weighting function that depends on the timestep t.

## Score Distillation Sampling Ancestral Sampling
- Figure 2 compares 2D sampling methods from a text-to-image diffusion model
- For score distillation sampling, as an example we use an image generator that restricts images to be symmetric by having x = (flip(θ), θ).

## HOW CAN WE SAMPLE IN PARAMETER SPACE, NOT PIXEL SPACE?
- Existing approaches for sampling from diffusion models generate a sample that is the same type and dimensionality as the observed data the model was trained on.
- We are not interested in sampling pixels; we instead want to create 3D models that look like good images when rendered from random angles.
- Such models can be specified as a differentiable image parameterization (DIP, Mordvintsev et al., 2018), where a differentiable generator g transforms parameters θ to create an image x = g(θ).
- DIPs allow us to express constraints, optimize in more compact spaces (e.g. arbitrary resolution coordinate-based MLPs), or leverage more powerful optimization algorithms for traversing pixel space.
- For 3D, we let θ be parameters of a 3D volume and g a volumetric renderer.
- To learn these parameters, we require a loss function that can be applied to diffusion models.
- Our approach leverages the structure of diffusion models to enable tractable sampling via optimization -a loss function that, when minimized, yields a sample.
- We optimize over parameters θ such that x = g(θ) looks like a sample from the frozen diffusion model.
- To perform this optimization, we need a differentiable loss function where plausible images have low loss, and implausible images have high loss, in a similar style to DeepDream (Mordvintsev et al., 2015).
- We first investigated reusing the diffusion training loss (Eqn. 1) to find modes of the learned conditional density p(x|y).
- While modes of generative models in high dimensions are often far from typical samples (Nalisnick et al., 2018), the multiscale nature of diffusion model training may help to avoid these pathologies.
- Minimizing the diffusion training loss with respect to a generated datapoint x = g(θ) gives θ * = arg min θ L Diff (φ, x = g(θ)).
- In practice, we found that this loss function did not produce realistic samples even when using an identity DIP where x = θ.
- Concurrent work from Graikos et al. (2022) shows that this method can be made to work with carefully chosen timestep schedules, but we found this objective brittle and its timestep schedules challenging to tune.
- To understand the difficulties of this approach, consider the gradient of L Diff : Generator Jacobian where we absorb the constant α t I = ∂z t /∂x into w(t).
- In practice, the U-Net Jacobian term is expensive to compute (requires backpropagating through the diffusion model U-Net), and poorly conditioned for small noise levels as it is trained to approximate the scaled Hessian of the marginal density.
- We found that omitting the U-Net Jacobian term leads to an effective gradient for optimizing DIPs with diffusion models: Intuitively, this loss perturbs x with a random amount of noise corresponding to the timestep t, and estimates an update direction that follows the score function of the diffusion model to move to a higher density region.
- While this gradient for learning DIPs with diffusion models may appear ad hoc, in Appendix A.4 we show that it is the gradient of a weighted probability density distillation loss (van den Oord et al., 2018) using the learned score functions from the diffusion model:
- < l a t e x i t s h a 1 _ b a s e 6 4 = " k m u W H I d x i K x W j 0 W 9 8 i g g i y X 3 a 4 4 = " 0 q y U v a t y p X 5 Z q t 5 m...

## THE DREAMFUSION ALGORITHM
- Now that we have demonstrated how a diffusion model can be used as a loss within a generic continuous optimization problem to generate samples, we will construct our specific algorithm that allows us to generate 3D assets from text.
- For the diffusion model, we use the Imagen model from Saharia et al. (2022), which has been trained to synthesize images from text.
- We only use the 64 × 64 base model (not the super-resolution cascade for generating higher-resolution images), and use this pretrained model as-is with no modifications.
- To synthesize a scene from text, we initialize a NeRF-like model with random weights, then repeatedly render views of that NeRF from random camera positions and angles, using these renderings as the input to our score distillation loss function that wraps around Imagen.
- As we will demonstrate, simple gradient descent with this approach eventually results in a 3D model (parameterized as a NeRF) that resembles the text.

## NEURAL RENDERING OF A 3D MODEL
- NeRF is a technique for generating realistic images from unseen views
- NeRF is a volumetric raytracer and an MLP
- Rendering an image from a NeRF is done by casting a ray for each pixel from a camera's center of projection through the pixel's location in the image plane and out into the world
- Sampled 3D points µ along each ray are then passed through an MLP, which produces 4 scalar values as output: a volumetric density τ (how opaque the scene geometry at that 3D coordinate is) and an RGB color c.
- These densities and colors are then alpha-composited from the back of the ray towards the camera, producing the final rendered RGB value for the pixel
- In the traditional NeRF use-case we are given a dataset of input images and associated camera positions and the NeRF MLP is trained from random initialization using a mean squared error loss function between each pixel's rendered color and the corresponding ground-truth color from the input image
- Our model is built upon mip-NeRF 360 (Barron et al., 2022), which is an improved version of NeRF that reduces aliasing
- While mip-NeRF 360 was originally designed for 3D reconstruction from images, its improvements are also helpful for our generative text-to-3D task
- Shading. Traditional NeRF models emit radiance, which is RGB color conditioned on the ray direction from which the 3D point is being observed.
- In contrast, our MLP parameterizes the color of the surface itself, which is then lit by an illumination that we control (a process commonly referred to as "shading").
- Previous work on generative or multiview 3D reconstruction using NeRF-like models have proposed a variety of reflectance models
- We use an RGB albedo ρ (the color of the material) for each point: where τ is volumetric density.
- Calculating the final shaded output color for the 3D point requires a normal vector indicating the local orientation of the object's geometry.
- This surface normal vector can be computed by normalizing the negative gradient of density τ with respect to the 3D coordinate µ: n = −∇ µ τ / ∇ µ τ (Yariv et al., 2020;Srinivasan et al., 2021).
- With each normal n and material albedo ρ, assuming some point light source with 3D coordinate and color ρ , and an ambient light color a , we render each point along the ray using diffuse reflectance (Lambert, 1760;Ramamoorthi & Hanrahan, 2001) to produce a color c for each point
- With these colors and our previously-generated densities, we approximate the volume rendering integral with the same rendering weights w i used in standard NeRF (Equation 5).
- As in previous work on text-to-3D generation (Hong et al., 2022;Michel et al., 2021), we find it beneficial to randomly replace the albedo color ρ with white (1, 1, 1) to produce a "textureless" shaded output.
- This prevents the model from producing a degenerate solution in which scene content is drawn onto flat geometry to satisfy the text conditioning.
- For generating single objects instead of scenes, a reduced bounding sphere can be useful.
- Geometry regularizers. The mip-NeRF 360 model we build upon contains many other details that we omit for brevity.
- We include a regularization penalty on the opacity along each ray similar to Jain et al. (2022) to prevent unneccesarily filling in of empty space.
- To prevent pathologies in the density field where normal vectors face backwards away from the camera we use a modified version of the orientation loss proposed in Ref-NeRF (Verbin et al., 2022).

## TEXT-TO-3D SYNTHESIS
- Given a pretrained text-to-image diffusion model, a differentiable image parameterization in the form of a NeRF, and a loss function whose minima are good samples, we have all the components needed for text-to-3D synthesis.
- For each text prompt, we train a randomly initialized NeRF from scratch.
- Each iteration of DreamFusion optimization performs the following: (1) randomly sample a camera and light, (2) render an image of the NeRF from that camera and shade with the light, (3) compute gradients of the SDS loss with respect to the NeRF parameters, (4) update the NeRF parameters using an optimizer.
- We detail each of these steps below, and present pseudocode in Appendix 8.

## EXPERIMENTS
- DreamFusion is a zero-shot text-to-3D generative model that is able to generate coherent 3D scenes from a variety of text prompts
- We compare DreamFusion to existing zero-shot text-to-3D generative models, identify the key components of our model that enable accurate 3D geometry, and explore the qualitative capabilities of DreamFusion such as the compositional generation shown in Figure 4
- We present a large gallery of 3D assets, extended videos, and meshes at dreamfusion3d.github.io
- 3D reconstruction tasks are typically evaluated using reference-based metrics like Chamfer Distance, which compares recovered geometry to some ground truth. The view-synthesis literature often uses PSNR to compare rendered views with a held-out photograph. These reference-based metrics are difficult to apply to zero-shot text-to-3D generation, as there is no "true" 3D scene corresponding to our text prompts.
- Following Jain et al. (2022), we evaluate the CLIP R-Precision, an automated metric for the consistency of rendered images with respect to the input caption. The R-Precision is the accuracy with which CLIP (Radford et al., 2021) retrieves the correct caption among a set of distractors given a rendering of the scene.
- We use the 153 prompts from the object-centric COCO validation subset of Dream Fields. We also measure R-Precision on textureless renders to evaluate geometry since we found existing metrics do not capture the quality of the geometry, often yielding high values when texture is painted on flat geometry.
- Table 1 reports CLIP R-Precision for DreamFusion and several baselines. These include Dream Fields, CLIP-Mesh (which optimizes a mesh with CLIP), and an oracle that evaluates the original captioned image pairs in MS-COCO. We also compare against an enhanced reimplementation of Dream Fields where we use our own 3D representation (Sec. A.3)
- Despite this, DreamFusion outperforms both baselines on color images, and approaches the performance of ground truth images.
- While our implementation of Dream Fields performs nearly at chance when evaluating geometry (Geo) with textureless renders, DreamFusion is consistent with captions 58.5% of the time.

## DISCUSSION
- DreamFusion is an effective technique for text-to-3D synthesis
- DreamFusion works by transferring scalable, high-quality 2D image diffusion models to the 3D domain
- DreamFusion does not require 3D or multi-view training data
- 2D image samples produced using DreamFusion tend to lack diversity compared to ancestral sampling
- 3D results exhibit few differences across random seeds
- The problem of 3D reconstruction from 2D observations is widely understood to be highly ill-posed

## ETHICS STATEMENT
- Generative models carry with them several ethical concerns
- DreamFusion uses the Imagen diffusion model as a prior, which inherits any problematic biases and limitations
- The LAION-400M subset of Imagen's dataset was found to contain undesirable images
- Imagen is conditioned on features from a pretrained large language model, which itself may have unwanted biases

## REPRODUCIBILITY STATEMENT
- The mip-NeRF 360 model that we build upon is publicly available through the "MultiNeRF" code repository (Mildenhall et al., 2022).
- While the Imagen diffusion model is not publicly available, other conditional diffusion models may produce similar results with the DreamFusion algorithm.
- To aid reproducibility, we have included a schematic overview of the algorithm in Figure 3, pseudocode for Score Distillation Sampling in Figure 8, hyperparameters in Appendix A.2, and additional evaluation setup details in Appendix A.3.
- Derivations for our loss are also included in Appendix A.4.
- annealing of a scale parameter has a similar effect as the annealing used by Park et al. (2021) but uses integrated positional encoding instead of traditional positional encoding.
- The underlying sinusoidal positional encoding function uses frequencies 2 0 , 2 1 , . . . , 2 L−1 , where we set L = 8.
- MLP architecture changes. Our NeRF MLP consists of 5 ResNet blocks (He et al. , 2016) with 128 hidden units, Swish/SiLU activation (Hendrycks & Gimpel, 2016), and layer normalization (Ba et al., 2016) between blocks.
- We use an exp activation to produce density τ and a sigmoid activation to produce RGB albedo ρ.
- Shading hyperparameters. For the first 1k steps of optimization we set the ambient light color a to 1 and the diffuse light color ρ to 0, which effectively disables diffuse shading. For the remaining steps we set a = [0.1, 0.1, 0.1] and ρ = [0.9, 0.9, 0.9] with probability 0.75, otherwise a = 1, ρ = 0, i.e. we use diffuse shading 75% of the time.
- When shading is on ( ρ > 0), we choose textureless shading (ρ = 1) with probability 0.5.
- Spatial density bias. To aid in the early stages of optimization, we add a small "blob" of density around the origin to the output of the MLP. This helps focus scene content at the center of the 3D coordinate space, rather than directly next to the sampled cameras.
- We use a Gaussian PDF to parameterize the added density: Representative settings are λ τ = 5 for the scale parameter and σ τ = 0.2 for the width parameter. This density is added to the τ output of the NeRF MLP.
- Additional camera and light sampling details.
- Uniformly sampling camera elevation φ cam in angular space does not produce uniform samples over the surface of the sphere -the area around the pole is oversampled. We found this bias to be helpful in practice, so we sample φ cam from this biased distribution with probability 0.5, otherwise we sample from a true uniform-in-area distribution on a half-sphere.
- The sampled camera position is perturbed by a small uniform offset U(−0.1, 0.1) 3 .
- For orientation loss L orient , we find reasonable weights to lie in [10 −1 , 10 −3 ].
- If orientation loss is too high, surfaces become oversmoothed.
- In most experiments, we set the weight to 10 −2 .
