---
title: "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language"
date: 2022-12-20T18:06:03.000Z
author: "Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13894v1.webp" # image path/url
    alt: "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13894)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13894).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/lambada-backward-chaining-for-automated).

# Abstract
- Remarkable progress has been made on automated reasoning with knowledge specified as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference.
- These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning.
- The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is significantly more efficient at proof-finding problems.
- We import this intuition into the LM setting and develop a Backward Chaining algorithm, which we call LAMBADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference.
- We show that LAMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.

# Paper Content

## Introduction

## Related Work
- Pretraining an LM on a corpora relevant to the target reasoning task can lead to improvements.
- Pretraining is, however, costly especially for larger LMs.
- Implicit Reasoning: These approaches typically finetune LMs to produce the desired output directly given the input (Clark et al., 2020;Betz et al., 2020;Saeed et al., 2021;Han et al., 2022); reasoning hap-pens implicitly in the parameters of the LM.
- It has been shown that finetuning LMs on logical reasoning tasks makes them learn spurious correlations (Zhang et al., 2022b;Schlegel et al., 2022), and struggle with multi-hop reasoning (Kassner et al., 2020).
- Besides, finetuning large LMs is costly and requires a large amount of training data.
- Explicit Reasoning: Recently, approaches that generate the intermediate reasoning steps such as the chain (or tree) of reasoning (Wei et al., 2022;Nye et al., 2022;Dalvi et al., 2021;Zelikman et al., 2022;Zhang et al., 2022a) have shown massive improvement for many reasoning tasks (Suzgun et al., 2022).
- We compare against a widely successful prompting strategy, named Chain-of-Thought (CoT) (Wei et al., 2022), from this category.
- Verifiers: To improve CoT, some works train a verifier using chain-level labels: the verifier takes a reasoning chain produced by the model as input and judges the quality of the chain (Cobbe et al., 2021;Shen et al., 2021;Jhamtani and Clark, 2020;Zelikman et al., 2022).
- Using this verifier, one can then generate multiple reasoning chains (e.g., by running the algorithm multiple times with different decoding temperatures) and use the best chain according to the verifier.
- Since LAMBADA also generates proofs, verifiers are also applicable to our algorithm.
- Length generalization: A number of approaches specifically look into whether LMs can generalize from examples requiring shorter reasoning chains (shown to them either as demonstration or as finetuning data) to examples requiring longer chains (Anil et al., 2022;Tafjord et al., 2020).
- In this paper, we compare against the second approach.
- Natural Language Inference (NLI): Logical reasoning can also be understood as identifying whether a logical entailment relation holds between two propositions (premise and hypothesis; the premise is a set of facts and rules, and the hypothesis is the statement to be proved).
- In this sense, models trained to perform NLI are also relevant, although inferences under NLI adopt a more relaxed notion of entailment rather than purely logical (Dagan et al., 2013;Bowman et al., 2015;Williams et al., 2018).
- In this work, we focus on performing automated reasoning over facts, i.e. natural language assertions such as "Nice people are red", that are coherent but not necessarily grounded in reality.
- A rule is a natural language statement that is either of the form, or can be rewritten in the form, "If P then Q" e.g. "Rough, nice people are red" can be rewritten as "If a person is rough and nice, then they are red".
- P is called the antecedent and Q is called the consequent of the rule.
- A theory C consists of facts F = {f 1 , f 2 , . . . , f n } and rules R = {r 1 , r 2 , . . . , r m }.
- We let G represent a goal that we would like to prove or disprove based on the facts and rules.

### Backward Chaining
- Backward chaining is a strategy for reasoning that starts from the goal and recursively breaks the goal into sub-goals based on the rules that can be applied to it, until the sub-goals can be proved or disproved based on the facts or no more rules can be applied to break the sub-goal down further.
- The outcome of BC for a goal is either PROVED (e.g., for the goal in Example 2), DISPROVED (e.g., for the goal in Example 2 if the second rule was "Rough, nice people are not red."), or UN-KNOWN (e.g., for a goal "Fiona is smart?" in Example 2).
- To use BC for text-based reasoning, we introduce four LM-based modules: Fact Check, Rule Selection, Goal Decomposition, and Sign Agreement.
- The four modules (and their sub-modules) are implemented by inference to a pretrained LM, where for each one we provide relevant in-context demonstrations to the LM in the prompt (cf. Appendix D.1 for the details of the prompts).
- We start by describing these four modules and then proceed to describing the full algorithm.

## Experimental Setup
- We compare against two main baselines for logical reasoning: Chain of Thought (CoT) (Wei et al., 2022), a state-of-the-art neural reasoning approach based on explicit reasoning, and Selection-Inference (SI) (Creswell et al., 2022), a state-of-the-art modular reasoning approach.
- We experiment with two challenging logical reasoning datasets: ProofWriter (Tafjord et al., 2020) is a commonly used synthetic dataset for measuring a model's logical reasoning ability when facts and rules are expressed in naturalistic language form. It contains two subsets: one with open-world assumption (OWA) and another with closed-world assumption (CWA). In this paper, we use the OWA subset.
- Each example in ProofWriter is a (theory, goal) pair and the label is one of {PROVED, DISPROVED, UN-KNOWN} where UNKNOWN indicates that the goal can neither be proved nor disproved.
- The dataset is divided into five parts, each part requiring 0, ≤ 1, ≤ 2, ≤ 3 and ≤ 5 hops of reasoning, respectively.
- We report two sets of results on this dataset: 1with examples labeled UNKNOWN removed (for fair comparison with previous work), and 2-with all three labels.
- Note that in this paper, we do not use the proof chains provided in the ProofWriter dataset.
- For both cases, to reduce overall experimentation cost, we restricted the experiments to the first 1000 examples in the test set.
- Hereafter, we refer to the first subset as ProofWriter-PD and the second as ProofWriter-PUD.

## Experimental Results
- LAMBADA outperforms the other two baselines on the ProofWriter datasets
- LAMBADA performs better on the higher depths of PrOntoQA
- CoT performs relatively well on the ProofWriter-PD dataset

### Proof Accuracy
- To understand the reason behind the high accuracy of CoT on higher depths of ProofWriter-PD, we randomly selected 50 examples from depth-5 of the dataset where CoT predicted the result correctly and manually verified if the proof chain is correct or not.
- For comparison, we also manually verified the proofs generated by LAMBADA following a similar procedure. The results are reported in Figure 3. While LAM-BADA mostly produced correct chains when the predicted label was correct, CoT only generated the correct chain for 28% of the examples.
- We observe three dominant reasons for the chains being wrong (cf. Appendix B.4 for examples): 1-hallucinating rules or facts, 2-not understanding conjunction, and 3-making invalid derivations, with hallucination being the main source of error (48% of the examples).
- The hallucinated facts and rules mostly resulted in short-cuts to the correct answer. This hints at the possibility of spurious correlations in ProofWriter-PD that can be exploited by CoT.
- For example, we found that in 9.2% of the examples which require 1+ reasoning hops, the consequent of one of the rules in the theory is the same as the goal to be proved, and for 98.9% of these examples the label is PROVED. In several of these examples, CoT simply concluded that the goal can be proved in 0 hops based on a hallucinated fact.
- The spurious correlations also explain the fluctuations in the CoT performance across different depths, as the performance depends on how much those correlations appear in the few-shot demonstrations. This result is consistent with previous work that shows when LMs are asked to solve logical reasoning endto-end, they rely on spurious correlations (Zhang et al., 2022b).

### The role of scale
- The performance of the Goal Decomposition and Sign Agreement modules remains comparable, but the performance for the Fact Check and Rule Selection modules drops significantly when using PaLM 62B
- When using PaLM 8B, the results for all components drops significantly

### Number of Inference Calls
- LAMBADA requires significantly fewer inference calls, especially at higher depths.
- For example, for Depth-1, LAMBADA requires 3.8x fewer calls whereas for Depth-5 it requires 11.8x fewer calls.

### Qualitative Analysis
- LAMBADA first calls the Fact Check module; the module specifies that the goal can neither be proved nor disproved.
- LAMBADA then calls the Rule Selection module which selects the third rule.
- The rule is sent to the Goal Decomposition module which produces the subgoal "Fiona is rough".
- The Fact Check module cannot prove or disprove the sub-goal, so the Rule Selection module is called which selects the first and second rules.
- Starting with the first rule, the Goal Decomposition module breaks the sub-goal into "Fiona is young", the Fact Check module fails to prove or disprove it, and then since the maximum depth reaches LAMBADA stops exploring this branch.
- Then LAMBADA considers the second selected rule for which the Goal Decomposition module produces the sub-goal "Fiona is furry".
- The Fact Check module specifies that this sub-goal can be proved.
- Then the Sign Agreement module is called (not shown on the Figure) to find that for both the rules used on the proof path, the signs agree, and so the goal is proved.

## Conclusion
- We developed LAMBADA, an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the backward chaining (BC) algorithm for high-level reasoning.
- We showed that LAMBADA achieves significant improvements over competitive existing approaches such as Chain-of-Thought and Selection-Inference both in terms of prediction accuracy (predicting if a statement can be proved or disproved based on a theory) and proof accuracy. Furthermore, we demonstrated how LAMBADA efficiently searches the entire proof space to accurately conclude that a statement can neither be proved nor disproved based on the theory.
- Although we only do experiments on formal reasoning problems and datasets, we believe our key insight on the efficacy of goal-directed reasoning with LMs is widely applicable and can be adapted to other NLP tasks where multi-step inference may be required.
- Going beyond the specific design of LAMBADA and its specialized modules, it would be useful to find other BC-inspired methods that might even incorporate BC into the LM directly e.g. a BC version of Chain-of-Thought.
- Since LAMBADA is a recursive algorithm, during the proof of an example Algorithm 1 may be called with the same goal multiple times. For instance, consider the goal "Fiona is round?" for the theory in Example 1. The goal unifies with the third rule and breaks down to two sub-goals "Fiona is nice?" and "Fiona is red?". The first sub-goal is then proved with a fact checking. The second sub-goal unifies with the second rule and breaks down to two sub-goals "Fiona is rough?" and "Fiona is nice?". The first one can be proved with a fact check; for the second one, since we have already proved this sub-goal, we can save a Fact Check call if we cache previous results.
- Note that the result of a call to LAMBADA can be different depending on the input max depth. For example, the algorithm may return UNKNOWN when called for the theory and goal in Example 1 with max depth 0, and return PROVED when called with max depth 1.
- Specifically, if we can prove/disprove a goal at depth d, we can conclude that it can be proved/disproved at depths ≥ d as well and we can get the value from the cache. Moreover, if the algorithm returns UNKNOWN for a goal at depth d, we can conclude that it will also return UNKNOWN at depths < d. Therefore, if the algorithm is called for a theory and goal at depth d, we also check other depths to see if we have the results for other depths that apply to this case.
- Besides having a cache for the entire algorithm that avoids redundant computations when the truth of a goal has been previously computed for a theory, each individual module can also have its own cache as it is possible that the module is called for the same theory and goal.
- We show one such example in Figure 7 (to be discussed in Section B). LAMBADA may sometimes run into loops. For example, to prove a (sub-)goal "Fiona is round?", after recursively identifying rules that reached, the algorithm stops expanding this branch. Moreover, the branch for "Dave is cold." is no longer pursued because there was a conjunction between the sub-goals and one of them failed. Moving on to the right branch in Figure 7, the algorithm calls the Goal Decomposition module for the goal and Rule3. Since we have previously computed it, the sub-goals "Dave is blue." and "Dave is cold." are returned from the cache. Fact Check is called on "Dave is blue." and since it has been computed before, the result (fail...
