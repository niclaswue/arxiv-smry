---
title: "Quantifying Local Extrinsic Curvature in Neural Manifolds"
date: 2022-12-20T16:46:44.000Z
author: "Francisco E. Acosta, Sophia Sanborn, Khanh Dao Duc, Manu Madhav, Nina Miolane"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10414v2.webp" # image path/url
    alt: "Quantifying Local Extrinsic Curvature in Neural Manifolds" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10414)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10414).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/quantifying-local-extrinsic-curvature-in).

# Abstract
- Neural manifold hypothesis suggests activity of neural population forms a low-dimensional manifold
- Dimensionality reduction techniques don't provide explicit parameterization of manifold or capture global structure
- Topological data analysis methods reveal shared topological structure between neural manifolds and task variables
- Leverage tools from Riemannian geometry and topologically-aware deep generative models to study geometry of neural manifolds
- Computes explicit parameterization and estimates local extrinsic curvature

# Paper Content

## Introduction
- Machine learning uses the manifold hypothesis to explain real-world data
- Neural population activity is hypothesized to form low-dimensional manifolds
- Dimensionality reduction techniques can reveal lower-dimensional structure
- Topological data analysis can reveal shared topological structure
- Few methods exist to explicitly quantify and parameterize the geometric structure of neural manifolds
- This paper introduces a novel approach to study the geometry of neural manifolds

## A riemannian approach to neural population geometry
- Topological methods are robust to continuous deformations of the neural manifold.
- Interested in quantifying and modeling nonlinear deformations of a population of neurons.
- Hypothesize that curvature imposed by deformations may correlate with perceptual or behavioral variables.

### Preliminaries
- VAEs are a type of generative deep latent variable model used for unsupervised learning
- VAEs encode input data in a compact latent representation and then decode the latent representation to reconstruct the original input data
- The latent variables take value in Euclidean space, z ∈ R L where L < N
- The neural manifold of interest is represented by an immersion f going from the template manifold M * to the neural manifold M ⊂ R N
- The map f parameterizes the manifold f (S 1 ) immersed in R N with angular coordinates in S 1
- The immersion f induces a Riemannian metric structure on M through the first fundamental form
- The Riemannian metric of the neural manifold allows us to calculate geometric quantities like angles, distances, areas and various types of curvatures
- Riemannian geometry of deep learning has focused on intrinsic notions of curvatures
- We suggest instead to use an extrinsic notion of curvature, such as the mean curvature vector
- The norms of the mean curvatures of the circle, torus and sphere are given in Example 1

### Methods
- Propose a method to compute pullback metric, second fundamental form, and mean curvature vector of neural manifolds
- Method uses a variational autoencoder
- Topology of neural manifold determined with existing topological data analysis methods
- Template manifold chosen to be simplest smooth manifold homeomorphic to neural manifold
- Mapping f from template manifold to neural manifold learned with tailored variational autoencoder
- VAE is "topologically-aware" with latent space constrained to be template manifold
- VAE trained with tailored loss function
- Mean curvature H of M computed with map f
- VAE pipeline for hyperspheres and 2-torus implemented
- Latent space constrained to be template manifold
- Kullback-Leibler divergence with posterior and prior distributions defined on template manifold
- Geodesic loss between latent variables and task variables
- Automatic differentiation used to compute pullback metric and curvatures from f
- VAE implementation uses Geomstats software

## Experiments
- Synthetic datasets of circles, spheres, and tori are distorted by small Gaussian "bumps" and embedded in a high-dimensional space
- Random N-dimensional rotation and i.d.d. Gaussian noise are added
- Parameter α modulates the amplitude around the ring, creating extrinsic curvature
- Parameter α introduces curvature in the vicinity of the north and south poles of S 2
- Parameter α introduces extrinsic curvature by stretching the torus on opposite sides
- Mean curvature vector is calculated from true synthetic immersion and the learned immersion
- Normalized error is calculated and varies between 0.5% and 5% of the actual curvature for noise levels that match real neural data

## Conclusion & future work
- Our approach estimates the geometry of synthetic neural manifolds generated from smooth deformations of circles, spheres, and tori.
- These manifolds have topology that is highly relevant across multiple sensory modalities.
- Lemma 5 states that two immersions of a smooth manifold can be related by a rotation and translation.
- The pullback metric of a circle is a 1x1 matrix.
- The pullback metric of a sphere is a 2x2 matrix.
- The pullback metric of a torus is a 2x2 matrix.
- Intrinsic curvature makes no reference to a higher dimensional embedding space.
- The Riemann curvature tensor, Ricci curvature tensor, and scalar curvature of a one-dimensional manifold are all 0.
- The scalar curvatures of the neural manifolds from Fig. 1 are computed in (Cox, 2019).
- The second fundamental form of a surface in R3 is a matrix of second partial derivatives of a twice differentiable function.
- The mean curvature vector of a surface in R3 is defined by its second fundamental form.
- The mean curvature vector of a surface in R3 can also be defined by the curvature of a plane curve.
- The mean curvature vector of a submanifold of N is defined by the trace of the inverse of the Riemannian metric matrix.
- The mean curvature vector of an immersed manifold is defined by the trace of the inverse of the Riemannian metric matrix.
- The mean curvature vector of a circle immersed in R N is given.
