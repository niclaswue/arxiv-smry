---
title: "Quantifying Local Extrinsic Curvature in Neural Manifolds"
date: 2022-12-20T16:46:44.000Z
author: "Francisco E. Acosta, Sophia Sanborn, Khanh Dao Duc, Manu Madhav, Nina Miolane"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10414v2.webp" # image path/url
    alt: "Quantifying Local Extrinsic Curvature in Neural Manifolds" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10414)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10414).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/quantifying-local-extrinsic-curvature-in).

# Abstract
- The neural manifold hypothesis postulates that the activity of a neural population forms a low-dimensional manifold within the larger neural state space, whose structure reflects the structure of the encoded task variables.
- Many dimensionality reduction techniques have been used to study the structure of neural manifolds, but these methods do not provide an explicit parameterization of the manifold, and fail to capture the global structure of topologically nontrivial manifolds. Topological data analysis methods can reveal the shared topological structure between neural manifolds and the task variables they represent, but fail to capture much of the geometric information including distance, angles, and curvature.
- In this work, we leverage tools from Riemannian geometry and topologically-aware deep generative models to introduce a novel approach for studying the geometry of neural manifolds. This approach computes an explicit parameterization of the manifolds and estimates their local extrinsic curvature. Our approach correctly estimates the geometry of synthetic neural manifolds generated from smooth deformations of circles, spheres, and tori. We expect this approach to open new avenues of inquiry exploring geometric neural correlates of perception and behavior, and provide a new means to compare representations in biological and artificial neural systems.

# Paper Content

## Introduction
- The manifold hypothesis postulates that many kinds of real-world data typically occupy a lower-dimensional manifold embedded in the high-dimensional data space.
- Several approaches for revealing manifold structure in neural population activity rely on dimensionality reduction techniques like PCA, Isomap, LLE, and tSNE (Pearson, 1901;Tenenbaum et al., 2000;Roweis and Saul, 2000;van der Maaten and Hinton, 2008).
- While these techniques can reveal the existence of lower-dimensional structure in neural population activity, they do not provide an explicit parameterization of the neural manifold, and often misrepresent manifolds with non-trivial topology.
- Methods based on topological data analysis (TDA) like persistent homology have begun to reveal shared topological structure in neural representations and the task variables they encode-i.e. structure that is invariant under continuous deformations of the manifold (Gardner et al., 2022;Chaudhuri et al., 2019;Curto, 2017).
- Topological methods can identify when two manifolds have the same number of holes-differentiating, for example, a ring from a sphere or a torus (see topologies in Fig. 1). However, these methods fail to reveal many geometric properties of the manifolds-i.e., structure that changes under continuous deformations, such as distances, angles, or curvature (see vertical axis in Fig. 1).
- To date, few methods exist to explicitly quantify and parameterize the geometric structure of neural manifolds.
- A geometric parameterization of neural manifolds would permit a more precise understanding of the behavioral or perceptual relationships between points in the neural state space.
- In this work, we introduce a new approach for studying the geometry of neural manifolds that computes an explicit parameterization of the manifolds and estimates their local extrinsic curvature, hence providing a concrete quantification of their "neural shape".
- We combine methods from Riemannian geometry (Shao et al., 2018;Hauser and Ray, 2017;Kuhnel et al., 2018) with topologically-aware deep generative models (Falorsi et al., 2018;Davidson et al., 2018).
- Our method uses a topologically-aware variational autoencoder (VAE), whose latent space has the (known) topology of the neural manifold, shown here as the ring S 1 .
- The decoder provides an estimate of the differentiable function f whose derivatives yield the Riemannian metric, as well as the intrinsic and extrinsic curvatures of the neural manifold.

## A Riemannian Approach to Neural Population Geometry
- Topological methods have received sustained attention in neuroscience because they are robust to continuous deformations of the neural manifold.
- Indeed, manifold structure that is latent in a neural population may be nonlinearly deformed in the neural data space.
- Thus robustness to deformation is critical to the successful identification of neural manifold structure.
- Here, we are interested in explicitly quantifying and modeling the nonlinear deformations that a population of neurons applies to a latent topological manifold.
- We hypothesize that the curvature imposed by these deformations may correlate with perceptual or behavioral variables.

### Preliminaries
- Variational autoencoders are a type of generative deep latent variable model widely used for unsupervised learning
- A variational autoencoder uses an autoencoder architecture to implement variational inference: it is trained to encode input data in a compact latent representation, and then decode the latent representation to reconstruct the original input data
- Consider an N-dimensional dataset of k vectors x 1 , . . . , x k ∈ R N
- The variational autoencoder models each data vector x i as being sampled from a likelihood distribution p(x i |z i ) with lower-dimensional unobserved latent variable z i
- The likelihood distribution is usually taken to be Gaussian, so we may write the reconstructed input as
- The function f is here represented by a neural network called the decoder
- The variational autoencoder achieves its objective by minimizing an upper-bound of the negative log-likelihood, which writes as the sum of a reconstruction loss and a Kullback-Leibler (KL) divergence
- In the standard VAE setting, the latent variables take value in Euclidean space, z ∈ R L where L < N, and their prior distribution p(z) is assumed to be Gaussian with unit variance, p(z) = N (0, I L).
- While these assumptions are mathematically convenient, they are not suitable for modeling data whose latent variables lie on manifolds with nontrivial topology
- In subsection 2.2.2 we detail the necessary adaptations made to the...

### Methods
- We propose a method that uses a variational autoencoder to compute the pullback metric, second fundamental form, and mean curvature vector of neural manifolds generated from data.
- We assume that we have established the topology of the neural manifold using existing methods of topological data analysis (Ghrist, 2008) such as the ones successfully employed in neuroscience and reviewed in the introduction.
- Define the template manifold M * .
- We denote M the neural manifold of interest.
- First, the topology of M can be determined with existing topological data analysis methods, or postulated based on the topological structure of the task variables.
- We define the template manifold M * as the "simplest" smooth manifold homeomorphic to M.
- For a neural manifold with the topology of a one-dimensional ring, we choose M * to be the circle, S 1 .
- We aim to find the mapping f : M * → M that characterizes the smooth deformation from M * to M, i.e., the geometry of M.
- In practice, we will consider template manifolds that are either n-spheres S n , or direct products of n-spheres, as these templates already cover the topologies of relevant neural manifolds: S 1 , S 2 and T 2 = S 1 × S 1 .
- We learn the deformation f : M * → M.
- We propose to learn the mapping f with a tailored variational autoencoder (VAE) trained to reconstruct the dataset of neural activity in R N for N neurons.
- After training, its decoder provides an estimate f of f -see Fig. 2.
- The map f gives us a continuous parameterization of the neural manifold: the points p of the template manifold M * parameterize the points f (p) on the neural manifold M; we can further use f to extract geometric signatures of the immersed manifold, such as its curvatures.
- Our VAE is "topologically-aware" in the sense that its latent space is constrained to be the template manifold M * , i.e., to have the topology of the neural manifold M.
- We train the VAE with a tailored loss function L = L rec + L KL + L latent .
- Here L rec is the mean squared error, L KL is a Kullback-Leibler divergence with posterior and prior distributions defined on the template manifold, and L latent is a geodesic loss between the latent variables in M * and the corresponding task variables, see 2.2.3.
- We implement the VAE pipeline for hyperspheres following the work in Davidson et al. (2018), and devise an implementation for the 2-torus relying on the fact that T 2 = S 1 × S 1 .
- We compute the mean curvature H of M
- The map f is a diffeomorphism from M * to the neural manifold M, and an immersion of M * into the neural state space R N .
- We use a decoder f that is twice differentiable, via neural network activation functions such as tanh(•) and softplus(•). As such, f allows us to endow a pullback metric on the latent space template manifold M * .
- It follows from 4 and the fact that the Christoffel symbols associated with the Euclidean metric on the neural state space vanish, see appendix 11, that we can compute an estimate for the mean curvature vector Ĥα (p) as:
- We use automatic differentiation to compute the pullback metric and curvatures from f .
- Our implementation uses the software Geomstats (Miolane et al., 2020), and extends it by implementing Definition 4.
- We give details on how to constrain the latent space to be the...

## Experiments

## Conclusion & Future Work
- Our approach correctly estimates the geometry of synthetic neural manifolds generated from smooth deformations of circles, spheres, and tori.
- These manifolds have topology that is highly relevant across multiple sensory modalities, including the ring structure in the head direction circuit (Chaudhuri et al., 2019)
- This appendix provides the derivations of the pullback metrics for the classical neural manifolds.
- Firstly, we provide a lemma that will be used in the subsequent derivations.
- Consider p ∈ M and u, v ∈ T p M. The pullback metric given by f writes, by definition: where g is the pullback metric given by f .
- Example 2 (Pullback metric of the circle)
- Consider a circle of radius R defined by the immersion: where P ∈ SO(N ) represents a rotation in R N , and t ∈ R N a translation in R N illustrating that the circle can be placed and oriented in any direction in R N . The pullback metric of this circle is the 1 × 1 matrix defined for all θ ∈ S 1 : which is the desired formula.
- Proof We compute the pullback metric g on S 1 given by the immersion f , by considering P = I N and t = 0 without loss of generality thanks to Lemma 5. Since the manifold S 1 is one-dimensional, g(θ) is a matrix of size 1 × 1, i.e. by a single scalar g 11 (θ).
- By definition for e 1 = (1, ) the basis vector of the one-dimensional tangent space T θ S 1 :
- Example 3 (Pullback metric of the sphere)
- Consider a sphere of radius R defined by the immersion: where P ∈ SO(N ) represents a rotation in R N , and t ∈ R N a translation in R N illustrating that the sphere can be placed and oriented in any direction in R N . The pullback metric of this sphere is the 2 × 2 matrix defined for all θ, φ ∈ S 2 :
- We compute the pullback metric g given by the immersion f , by considering P = I N and t = 0 without loss of generality thanks to Lemma 5. The pullback metric of this sphere is the 2 × 2 matrix defined for all θ, φ ∈ S 2 by where g(θ, φ) ij = 3 k=1 ∂f k ∂x i (θ, φ). ∂f k ∂x j (θ, φ) and we use the conventions θ = x 1 , φ = x 2 .
- In what follows, for conciseness of the derivations, we do not write...
