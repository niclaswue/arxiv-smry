---
title: "A Length-Extrapolatable Transformer"
date: 2022-12-20T18:56:20.000Z
author: "Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10554v1.webp" # image path/url
    alt: "A Length-Extrapolatable Transformer" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10554)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10554).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-length-extrapolatable-transformer).

# Abstract
- Position modeling is critical in Transformers
- In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences
- We define attention resolution as an indicator of extrapolation
- Then we propose two designs to improve the above metric of Transformers
- Specifically, we introduce a relative position embedding to explicitly maximize attention resolution
- Moreover, we use blockwise causal attention during inference for better resolution
- We evaluate different Transformers variants with language modeling
- Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings

# Paper Content

## Introduction
- Transformer should be sensitive to order
- Position translation can't hurt the representation a lot
- After that, a good sequence model needs to deal with any input length
- When a Transformer satisfies the principles above, we will evaluate the performance

### Order Variance
- Transformer aims to capture long-term dependency efficiently
- With effective position information, Transformer models should be variant with permuting the order

### Translation Invariance
- A sentence's meaning is variant with padding before or after the whole sentence.
- For a Transformer model f (input, mask), any input sequence X = [x 0 , x 1 , ..., x n ] with mask M = [m 0 , m 1 , ..., m n ], the output should be same with the padding one: Obviously, relative positions (Shaw et al., 2018;Raffel et al., 2020;Wang et al., 2020a;Su et al., 2021) have this property instead of absolute ones (Vaswani et al., 2017;Devlin et al., 2019).
- Even though absolute sinusoidal embedding has a similar property (Vaswani et al., 2017): P E pos+k can be represented as a linear function of P E pos , the addition operation in the initial word embedding messes the attention weight, where the spread form of QK T has 4 components whose geometric connection with position is unclear.

### Length Extrapolation
- Transformer models with a suitable design should be capable of dealing with any input length
- First, learnable absolute position embedding (Devlin et al., 2019) is not able to extrapolate at all because it does not have any pre-defined position knowledge
- With the evaluation of perplexity on different length (Press et al., 2021), almost every position embedding's performance drops significantly
- Alibi (Press et al., 2021) solves this problem by adding an exponential decay on the attention matrix, which lower the influence of outof-distribution position like a soft sliding window
- However, the absence of long-term dependency contributes to a performance drop compared with other relative strategies

## A Length-Extrapolatable Transformer
- We define attention resolution as the indicator of length extrapolation.
- We propose two ways to improve the resolution metric, i.e., improve the length extrapolation of Transformers.
- The first way is to introduce a relative position encoding method.
- The second way is to propose to use blockwise causal masking during inference.

### Attention Resolution

### Improve Resolution by Position Encoding
- The scaling factor A q , A k is unnecessary because q, k is obtained by a linear transformation.
- If ξ = 0, the form is the same as ROPE (Su et al., 2021). Geometrically, the transformation provides a rotation on vectors.
- If the relative angle between q and k is larger, the inner product is smaller. However, the cosine value is not monotony if the rotating angle is large than π, which causes an unstable phenomenon that the expectation of the inner product oscillates dramatically with the growth of relative distance.
- Following the parameters (Vaswani et al., 2017;Su et al., 2021) }, we will calculate the expectation as follows. For generate models, we assume E(∠q) ≤ E(∠k) to ensure the monotony:
- The inference here is different from (Su et al., 2021) because of two reasons: 1) there is an additional assumption brought by language models; 2) the inequality scaling of (Su et al., 2021) is too strong to lose generality.
- We calculate expectation instead of the upper bound. Now we define a function to represent the property of relative position:
- Stabilizing the curve of g[n] is an intuitive way. Even though attention bias can achieve this goal, we do not hope additional position calculation. Instead, we can achieve this goal by selecting a good ζ to maximize R(g ζ ). Obviously, the oscillation mainly comes from large θ i . Manually setting ζ can achieve this goal:
- where ζ i becomes smaller when θ i is larger. In this way, we punish the oscillation of unstable dimensions and keep the distribution of stable ones.
- Numerical optimization methods are tried to find optimal values for ζ. However, the results rely on the initial value and lack control when the hidden dimension changes. Besides, the numerical precision should be considered because of fp16's range.
- Finally, we find a sub-optimal solution by manually setting γ to both satisfy the resolution is recognizable (R(g ζ ) is partially optimized) and ζ n i can be represented by fp16 when n is big (8192 in our setting). The optimized value ζ will be used as the final value in LEX Transformer.
- The curves of ζ = 1, ζ are shown in Figure 1. The default rotary embedding contributes to a dramatic oscillation, especially in the large relative distance, which causes bad extrapolation performance and restricts the model's convergence speed. After adding a decay, the curve is almost stable, especially on long-term dependency. What's more, it does not hurt pure rotation's fitting ability because ζ n i ≈ 1 when i is large or n is small. In that way, short-term and long-term dependencies are divided continuously.
- Finally, we have Extrapolatable Position Embed- Algorithm 1: Attention with XPOS def rot(x):

### Blockwise Causal Attention
- Blockwise causal attention works well with ROPE
- Alibi performs well without windowed attention
- XPOS's perplexity without BCA increases by about 1 in 2048, and 8 in 4096

## Experiments

### Pre-training
- Uses 1024 hidden dimension, 16 heads, and 24 layers, comparable to medium-size GPT-3 (Brown et al., 2020)
- Uses Adam optimizer with β 1 = 0.9, β 2 = 0.98, = 10 −6
- Uses code based on TorchScale

### Language Modeling
- We measure perplexity on arXiv
- For every document, we select its first 4k tokens and divide them into the target length to fairly compare the perplexity of different lengths
- XPOS has a stable advantage on others with 1∼3 perplexity drop
- For lengths 2048 and 4096, we use BCA in all position embeddings, and the following ablation study will discuss the performance without that
- Press et al. ( 2021)'s experiment shows that most of the position strategies can't deal with input length longer than pre-training directly
- In our experiment, with the improvement brought by BCA, ROPE gets a better performance while Absolute still can't extrapolate
- XPOS shows a stable decrease when the sequence length increases, which satisfies the assumption that a longer context makes the prediction better
- While others' perplexity increases when the input length is 4096
- Here, XPOS's advantage towards ROPE is worth analyzing.

### Measuring Resolution
- The resolution is crucial for building an effective Transformer
- To verify the claim, we evaluate the resolution of different Transformer variants empirically
- Equation 8 estimates the expectation of attention score for LEX
- Denote attention score of query i and key j (before softmax) as e ij , the expectation of s[n] is as follows:
- The resolution can be calculated by combining Equation 3 and 11
- The final expectation is the average of different input text.
- Resolution is calculated in every layer, and the average resolution is shown in Table 3
- For Alibi (Press et al., 2021) stable resolution comes from explicit decay, but it prevents the model from learning position dependency itself.
- Besides, we run an ablation on BCA. In length 2048, we measure the resolution with/without block. The result supports that BCA helps model distinguish positions better.
- In this part, we discuss the necessity of the combination of vector rotation and exponential decay.
- XPOS without rotation means Equation 10 degenerates to θ i = 0:
- After pre-training, we test the perplexity on the valid split of training corpus with 1k length. The result in Table 4 shows that simple scaling operation can't perform as well as LEX. Therefore, the combination of rotation and decay means the combination of in-distribution and out-of-distribution ability.
- Long-sequence Transformers aim to solve two key problems. First, the computation or memory consumption is not efficient enough for long sequences. Second, there is a trade-off between performance and efficiency.
- One popular solution (Wang et al., 2020b;Katharopoulos et al., 2020;Choromanski et al., 2020) is linear attention, i.e., using a kernel-based or low-rank approximation to replace vanilla attention.
- The methods typically target efficiency while underperforming vanilla Transformers for regular length.
- Another strand is sparse attention (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Xiong et al., 2021), which usually leverages structured sparsity to reduce computation.
- For causal sequence modeling, the recurrent-style designs (Dai et al., 2019;Hutchins et al., 2022;Ma et al., 2022b) are also competitive.
- In comparison, we focus on the extrapolation issue (Press et al., 2021) for language modeling, i.e., training on short texts while evaluating long texts.
- The training process is kept the same as vanilla Transformers, i.e., training on short sequences, and using dense attention computation.
- The capability of long-sequence modeling is given for free during inference. So the training efficiency (which is typically expensive for large-scale language models) is not affected compared with previous work.
- Moreover, the performance on regular length is perfectly retained, without trade-offs for long-sequence modeling.
- Absolute sinusoidal position embedding is proposed by Vaswani et al. (2017).
- where PE pos+k is represented as a linear function of PE pos to restore a relative-position property.
- et al. (2018) propose relative position embedding as an alternative approach.
- Denote e ij as attention weight, α ij = softmax(e ij ), o i as output, we have:
- where a K ij = ω K clip(i−j,k) , a V ij = ω V clip(i−j,k) , and ω K and ω V are learnable parameters.
- The clipping strategy helps length generalization but cannot distinguish the positions that are larger than k.
- Yang et al. (2019) and He et al. (2020) further reparameterize the relative position vectors for better performance.
- T5 (Raffel et al., 2020) uses a simpler strategy to encode relative position: where log-bucket scalars...
