---
title: "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks"
date: 2022-12-21T01:52:17.000Z
author: "Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10717v1.webp" # image path/url
    alt: "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10717)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10717).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/hidden-poison-machine-unlearning-enables).

# Abstract
- Introducing a new attack vector in the context of machine unlearning and other settings
- Adversary adds carefully crafted points to the training dataset with minimal impact on model's predictions
- Adversary triggers request to remove subset of points, unleashing attack and negatively affecting model's predictions
- Attack is a clean-label targeted attack, with goal of causing model to misclassify a specific test point
- Attack is tested on CIFAR-10, Imagenette, and Imagewoof datasets

# Paper Content

## Introduction
- ML research traditionally assumes a static pipeline
- Practical deployments are more dynamic
- Dynamic settings increase vulnerability to malicious actors
- Camouflaged data poisoning attacks introduced
- Attack takes place in two phases
- Attack goal is to misclassify one point in training set
- Algorithm based on gradient-matching approach
- Camouflage set constructed to undo impact of poison set
- Attack efficacy demonstrated on various models and datasets

### Preliminaries
- Governments around the world have introduced legislation requiring organizations to delete user data upon request
- Machine unlearning is the process of removing user data from machine learning models
- Retraining from scratch is the ideal way to perform data deletion, but it can take a long time
- Machine unlearning research has focused on fast methods for data deletion and other implications
- Data poisoning is when an adversary modifies the training data to negatively impact the model's behaviour

### Related work
- Marchant et al. (2022) proposed a novel poisoning attack on unlearning systems
- Primary goal of unlearning systems is to "unlearn" datapoints quickly
- Marchant et al. (2022) craft poisoning schemes to trigger unlearning algorithm to retrain from scratch
- Adversary in our work is trying to misclassify a target test point
- Types of attacks include label flipping, watermarking, and clean label attacks
- Works on indiscriminate and backdoor poisoning attacks are beyond the scope of our work
- Machine unlearning studied through exact unlearning and probabilistic notion of unlearning
- Works studied algorithms for empirical risk minimization and generalization loss
- Implementations of machine unlearning in several contexts

## Camouflaged poisoning attacks via unlearning
- Describes components of a camouflaged poisoning attack
- Explains how attack can be realized using machine unlearning

### Threat model and approach
- Attacker has access to victim's model architecture, ability to query gradients, and a target sample to attack
- Attacker introduces poison points and camouflage points to training dataset
- Attacker triggers attack by submitting unlearning request to remove camouflage points
- Victim trains ML model on modified dataset and executes unlearning request by retraining from scratch
- Attacker's goal is to change prediction of model on target sample from y target to y adversarial
- Attacker only allowed to introduce a set of points much smaller than clean training dataset
- Attacker only allowed to generate points by altering base images by less than ε distance in ∞ norm
- Attacker implements attack by generating poison samples and then generating camouflage samples to cancel their effects
- Attacker minimizes adversarial loss by finding ∆ such that for any model parameter θ, gradients match
- Attacker minimizes cosine-similarity loss between two gradients to make poison generation efficient

### Camouflaging poisoned points
- Camouflage images are designed to neutralize the effect of poison images
- Label flipping is a simple and effective procedure to generate camouflages
- Label flipping only works for binary classification problems trained with linear loss
- Attack is not clean label as camouflage images are generated with opposite label to ground truth
- Attack vulnerable to simple data purification techniques by victim
- Main procedure to generate camouflages is based on gradient matching idea
- Objective is to find ∆ such that model minimizes original-target loss
- Minimizing camouflage samples will also minimize original-target loss
- Relax condition to be satisfied only for fixed model θ cp
- Minimize cosine-similarity loss using Adam optimizer
- Choose ∆ * with smallest value of ψ(∆ * , θ cp )

## Experimental evaluation
- Generate poison points with Algorithm 2
- Generate camouflage points with Algorithm 1
- Repeat experiments K times with different seed
- Report mean and standard deviation
- Poisoning successful if model predicts adversarial label
- Camouflaging successful if model predicts correct label
- Camouflaged poisoning successful if both poisoning and camouflaging successful

### Evaluations on cifar-10
- Evaluated camouflaged poisoning attack on CIFAR-10 dataset
- CIFAR-10 is a multiclass classification problem with 10 classes and 6,000 color images in each class
- Converted CIFAR-10 dataset into a binary classification dataset (Binary-CIFAR-10) by merging 10 classes into two groups
- Trained linear SVM with hinge loss
- Pre-processing stage: each image in training dataset was normalized to have 2-norm 1
- Generated poison points by computing cosine-similarity loss and optimizing with Adam optimizer
- Evaluated label flipping and gradient matching to generate camouflages
- Evaluated with various popular large scale neural networks architectures
- Trained with cross-entropy loss using mini-batch SGD
- Poisoning successful at least 80% of the time in most experiments
- Camouflaging successful at least 70% of time for most architectures, but not as successful for MobileNetV2 and Resnet-50

### Evaluations on imagenette and imagewoof
- Evaluated attack vector on multiclass classification problem on Imagenette and Imagewoof datasets
- Imagenette dataset consists of 10 classes and 13394 images, divided into training and test datasets
- Imagewoof dataset consists of 10 classes and 12954 images, divided into training and test datasets
- Evaluated attack on two different neural network architectures-VGG-16 and ResNet-18
- Camouflaging successful for at least 50% of the time when poisoning was successful
- Fluctuation in model's validation accuracy can be up to 7%
- Additional experiments in Appendix B.5

## Conclusion
- Demonstrated a new attack vector, camouflaged poisoning attacks, against machine learning pipelines
- Attack framework can accommodate any method for targeted poisoning
- Attack extends to multiple targets
- Attack requires grey-box knowledge
- Attack should also be effective against approximate unlearning
- Defense could potentially thwart watermarking poisoning attacks
- Attack success rate changes with relative sizes of poison and camouflage samples
- Poison and camouflage samples transfer across models
- Attack successful even when model trained with data augmentation
- Defense of data sanitization not effective against attack
- Attack success rate reported for different neural network architectures and threat models
- Visualizations of poisons and camouflages provided
