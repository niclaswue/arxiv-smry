---
title: "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks"
date: 2022-12-21T01:52:17.000Z
author: "Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10717v1.webp" # image path/url
    alt: "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10717)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10717).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/hidden-poison-machine-unlearning-enables).

# Abstract
- introduces camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced
- an adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal
- the adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected
- in particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof

# Paper Content

## Introduction
- Machine learning research traditionally assumes a static pipeline: data is gathered, a model is trained once and subsequently deployed.
- This paradigm has been challenged by practical deployments, which are more dynamic in nature.
- After initial deployment more data may be collected, necessitating additional training.
- Or, as in the machine unlearning setting (Cao and Yang, 2015), we may need to produce a model as if certain points were never in the training set to begin with.
- 2
- While such dynamic settings clearly increase the applicability of ML models, they also make them more vulnerable. Specifically, they open models up to new methods of attack by malicious actors aiming to sabotage the model.
- In this work, we introduce a new type of data poisoning attack on models that unlearn training datapoints. We call these camouflaged data poisoning attacks.
- The attack takes place in two phases. In the first stage, before the model is trained, the attacker adds a set of carefully designed points to the training data, consisting of a poison set and a camouflage set.
- In the second phase, the attacker triggers an unlearning request to delete the camouflage set after the model is trained. That is, the model must be updated to behave as though it were only trained on the training set plus the poison set.
- At this point, the attack is fully realized, and the model's performance suffers in some way.
- While such an attack could harm the model by several metrics, in this paper, we focus on targeted poisoning attacks -that is, poisoning attacks where the goal is to misclassify one particular point in the training set.
- Our contributions are the following:
- 1. We introduce camouflaged data poisoning attacks, demonstrating a new attack vector in dynamic settings including machine unlearning.
- 2. We realize these attacks in the targeted poisoning setting, giving an algorithm based on the gradient-matching approach of Geiping et al. (2021).
- In order to make the model behavior comparable to as if the poison set were absent, we construct the camouflage set by generating a new set of points that undoes the impact of the poison set, an idea which may be of broader interest to the data poisoning community.
- 3. We demonstrate the efficacy of these attacks on a variety of models (SVMs and neural networks) and datasets (CIFAR-10 (Krizhevsky, 2009), Imagenette (Howard, 2019), and Imagewoof (Howard, 2019)).

### Preliminaries
- Machine learning legislation is being introduced around the world
- This legislation requires organizations to delete information they have collected about a user upon request
- A natural question is whether that further obligates the organizations to remove that information from downstream machine learning models trained on the data -current guidances (Information Commissioner's Office, 2020) and precedents (Federal Trade Commission, 2021) indicate that this may be the case
- This goal has sparked a recent line of work on machine unlearning (Cao and Yang, 2015)
- The simplest way to remove a user's data from a trained model is to remove the data from the training set, and then retrain the model on the remainder (also called "retraining from scratch")
- This is the ideal way to perform data deletion, as it ensures that the model was never trained on the datapoint of concern
- The downside is that retraining may take a significant amount of time in modern machine learning settings
- Most work within machine unlearning has studied fast methods for data deletion
- A related line of work has focused more on other implications of machine unlearning, particularly the consequences of an adaptive and dynamic data pipeline (Gupta et al., 2021;Marchant et al., 2022)
- Our work fits into the latter line: we show that the potential to remove points from a trained model can expose a new attack vector
- Since retraining from scratch is the ideal result that other methods try to emulate, we focus on unlearning by retraining from scratch

### Related work
- Marchant et al propose a novel poisoning attack on unlearning systems
- The primary goal of many machine unlearning systems is to "unlearn" datapoints quickly, i.e., faster than retraining from scratch
- Marchant et al craft poisoning schemes via careful noise addition, in order to trigger the unlearning algorithm to retrain from scratch on far more deletion requests than typically required
- While both our work and theirs are focused on data poisoning attacks against machine unlearning systems, the adversaries have very different objectives
- In our work, the adversary is trying to misclassify a target test point, whereas in theirs, they try to increase the time required to unlearn a point
- In targeted data poisoning, there are a few different types of attacks
- Our focus is on both clean-label poisoning and camouflage sets
- While there are also works on indiscriminate (Biggio et al., 2012;Xiao et al., 2015;Muñoz-González et al., 2017;Steinhardt et al., 2017;Diakonikolas et al., 2019;Koh et al., 2022) and backdoor (Gu et al., 2017;Tran et al., 2018;Sun et al., 2019) poisoning attacks, these are beyond the scope of our work
- Cao and Yang (2015) initiated the study of machine unlearning through exact unlearning, wherein the new model obtained after deleting an example is statistically identical to the model obtained by training on a dataset without the example
- A probabilistic notion of unlearning was defined by Ginart et al. (2019), which in turn is inspired from notions in differential privacy (Dwork et al., 2006)

## Camouflaged poisoning attacks via unlearning
- The camouflaged poisoning attack is a type of cyberattack that uses deception to harm a victim.
- The attack can be realized using machine unlearning.

### Threat model and approach

### Camouflaging poisoned points
- label flipping can be used to generate clean-label camouflages for general losses and multi-class classification problems
- gradient matching can be used to generate clean-label camouflages for general losses and multi-class classification problems

## Experimental Evaluation
- The paper describes experiments in which they try to poison and camouflage points in a dataset
- Each experiment is repeated K times, with a different seed each time
- Poisoning is successful if the model trained on the set of points predicted the label y adversarial on the target image
- Camouflaging is successful if the model trained on the set of points predicted the correct label y target on the target image, provided that poisoning was successful

### Evaluations on Cifar-10
- We extensively evaluate our camouflaged poisoning attack on models trained on the CIFAR-10 dataset (Krizhevsky, 2009).
- CIFAR-10 is a multiclass classification problem with 10 classes, with 6,000 color images in each class (5,000 training + 1,000 test) of size 32 × 32.
- We follow the standard split into 50,000 training images and 10,000) validation / test images.
- In order to perform evaluations on SVM, we first convert the CIFAR-10 dataset into a binary classification dataset (which we term as Binary-CIFAR-10) by merging the 10 classes into two groups: animal (y = +1) and machine (y = −1).
- Images (in the training and the test dataset) that were originally labeled (bird, cat, deer, dog, frog, horse) are instead labeled animal, and the remaining images, with original labels (airplane, cars, ship, truck ), are labeled machine.
- We train a linear SVM (no kernel was used) with the hinge loss: (f (x, θ), y) = max{0, 1 − yf (x, θ)}.
- The training was done using the svm.LinearSVC class from Scikit-learn (Pedregosa et al., 2011) on a single CPU.
- In the pre-processing stage, each image in the training dataset was normalized to have 2 -norm 1.
- Each training on Binary-CIFAR-10 dataset took 25 -30 seconds.
- In order to generate the poison points, we first use torch.autograd to compute the cosine-similarity loss (4), and then optimize it using Adam optimizer with learning rate 0.001.
- Each poison and camouflage generation took about 40 -50 seconds (for b p = b c = 0.2%).
- We evaluate both label flipping and gradient matching to generate camouflages, and different threat models (ε, b p , b c ); the results are reported in Table 1.
- For each of our experiments we chose K = 10 seeds.
- Each trained model had validation accuracy of around 81.63% on the clean dataset S cl , which did not change significantly when we retrained after adding poison samples and / or camouflage samples.
- Note that the efficacy of the camouflaged poisoning attack was more than 70% in most of the experiments.
- We provide a sample of the generated poisons and camouflages in Figure 3 in the appendix.
- We perform extensive evaluations on the multiclass CIFAR-10 classification task with various popular large scale neural networks architectures including VGG-11, VGG-16 (Simonyan and Zisserman, 2015), ResNet-18, ResNet-34, ResNet-50 (He et al., 2016), andMobileNetV2 (Sandler et al., 2018).
- Each model is trained with cross-entropy loss (f (x, θ), y) = − log(Pr(y = f (x, θ))) on a single GPU using PyTorch (Paszke et al., 2019), and using mini-batch SGD with weight decay 5e-4, momentum 0.9, learning rate 0.01, batch size 100, and 40 epochs over the training dataset.
- Each training run took about 45 minutes.
- The poison and camouflage sets were generated using gradient matching by first defining the cosine-similarity loss using torch.autograd and then minimizing it using Adam with a learning rate of 0.1.
- Each poison/camouflage generation took about 1.5 hours.

### Evaluations on Imagenette and Imagewoof
- The attack vector is successful on the challenging multiclass classification problem on the Imagenette and Imagewoof datasets
- The success rate of the attack varies depending on the threat model and neural network architecture used
- We provide additional experiments on the CIFAR-10 dataset to show that the attack is robust to random deletions of generated poison and camouflage samples, data augmentation, and successfully transfers when the victim model is different from the model on which poison and camouflage samples were generated

## Conclusion
- We demonstrated a new attack vector, camouflaged poisoning attacks, against machine learning pipelines where training points can be unlearned.
- This shows that as we introduce new functionality to machine learning systems, we must be aware of novel threats that emerge.
- We outline a few extensions of our approach and interesting directions for further research:
- Our method for generating poison and camouflage points was based on the gradient-matching attack of Geiping et al. (2021). However, the attack framework could accommodate effectively any method for targeted poisoning, e.g., the methods of Aghakhani et al. (2021) or Huang et al. (2020), or any method introduced in the future. Additionally, similar to Geiping et al. (2021) (and several other works in the targeted data poisoning literature), the proposed attack extends straightforwardly to a collection of targets (rather than a single target).
- In order to generate our poisons and camouflages, our proposed approach needs the ability to query gradients of a trained model at new samples, thus the attack is not black box.
- While the main contribution of the paper was to expose that machine unlearning (which is a new and emerging but still underdeveloped technology) can lead to a new kind of vulnerability called a camouflaged poisoning attack, performing such an attack in a black-box setting is an interesting research question.
- In our attack scenario, the victim retrains a new model from scratch (on the leftover training data) to acknowledge the unlearning request, i.e., the victim performs exact unlearning.
- In the past few years there has been a lot of research effort for developing approximate unlearning algorithms under various structural assumptions (e.g., convexity (Sekhari et al., 2021;Guo et al., 2020) , availability of large memory / compute (Bourtoule et al., 2021;Brophy and Lowd, 2021), etc.), and one may wonder what happens under approximate unlearning. We believe that our attack should also be effective against approximate unlearning. This is because the objective of approximate unlearning is to output a model (computationally fast and using small intermediate memory) that is statistically-indistinguishable from the model retrained from scratch on the remaining data.
- Since, in our experiments we directly evaluate on the retrained model the latter (which is the underlying objective that all approximate unlearning methods aim to emulate), we are hopeful that our attack will also succeed in approximate unlearning. However, unfortunately, we do not have experimental results to back this up, and are unaware of any provably effective approximate unlearning methods that work for large-scale deep learning settings considered in this paper.
- Investing this further would be an interesting direction for future research.
- It is also important to understand how to defend against camouflaged attacks. As observed by Geiping et al. (2021), it is unlikely that differential privacy (Dwork et al., 2006) would be an effective defense, as preventing attacks in the non-camouflaged setting incurs too significant a loss in accuracy.
- Another direction is to reduce the knowledge needed by the adversary, thereby creating stronger attacks. For example, while our setting requires grey-box knowledge, one could instead consider a black-box model to attack ML APIs.
- Finally, it is interesting to determine what other types of threats can be camouflaged, e.g., indiscriminate (Lu et al., 2022) or backdoor poisoning attacks (Chen et al., 2017;Saha et al., 2020).
