---
title: "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding"
date: 2023-01-02T21:08:27.000Z
author: "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-00876v2.webp" # image path/url
    alt: "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00876)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00876).


# Abstract
- Reading comprehension of legal text can be a particularly challenging task
- To address this challenge, we introduce the Merger Agreement Understanding Dataset
- Our fine-tuned Transformer baselines show promising results, with models performing
- As the only expert-annotated merger agreement dataset, MAUD is valuable as a

# Paper Content

## Introduction
- The Merger Agreement Understanding Dataset (MAUD) is a legal reading comprehension dataset curated under the supervision of highly specialized mergers-and-acquisitions (M&A) lawyers and used in the American Bar Association's 2021 Public Target Deal Points Study.
- The dataset and code for MAUD can be found at github.com/TheAtticusProject/maud.
- Public target company acquisitions are the most prominent business transactions, valued at hundreds of billions of dollars each year. Merger agreements are the legal documents that enable these acquisitions, and key clauses in these merger agreements are called "deal points."
- Lawyers working on the ABA Study perform contract review on merger agreements. In general, contract review is a two-step process. First, lawyers extract key legal clauses from the contract (an entity extraction task). Second, they interpret the meaning of these legal clauses (a reading comprehension task).
- In the ABA Study, the lawyers extract deal points from merger agreements, and for each deal point they answer a set of standardized multiplechoice questions. Models trained on MAUD's expert-annotated data can learn to answer 92 reading comprehension questions from the 2021 ABA Study, given extracted deal point text from merger agreements.
- By answering these questions, models interpret the meaning of specialized legal language and categorize the different agreements being made by companies in the contract. Entity extraction and reading comprehension are both important and challenging tasks in legal contract review.
- A large-scale expert-annotated entity extraction benchmark for contract review is already available in Hendrycks et al. (2021b). However, to the best of our knowledge, there is no large-scale expert-annotated reading comprehension dataset for contract review or any other legal task in the English language.
- Therefore in this short paper, we focus on the legal reading comprehension task. (Appendix A.12 presents a preliminary benchmark for the extraction task for interested researchers.)

## Related Work
- Due to the high costs of contract review and the specialized skills it requires, understanding legal text has proven to be a ripe area for NLP research.
- One area of contract review research focuses on legal entity extraction and document segmentation.
- Chalkidis et al. (2017) introduce a dataset for extracting basic information from contracts, with follow-up modeling work using RNNs (Chalkidis et al., 2018) and Transformers (Chalkidis et al., 2020).
- Lippi et al. (2019) introduce a small expert-annotated dataset for identifying "unfair" clauses in 50 online terms of services.
- Tuggener et al. (2020) introduce a semiautomatically constructed dataset of legal contracts for entity extraction.
- Leivaditi et al. (2020) introduce an expert-annotated dataset of 2960 annotations for 179 lease agreements.
- Hendrycks et al. (2021b) introduce CUAD, an expert-annotated contract review dataset containing 13,010 annotations for 150 legal contracts.
- Unlike CUAD, which is a entity extraction task for 16 different types of contracts, MAUD is a multiple-choice reading comprehension task focusing on merger agreements.
- Reading Comprehension for Legal NLP.
- Koreeda and Manning (2021) introduce a crowdworker-annotated dataset containing 7191 Natural Language Inference questions about spans of nondisclosure agreements.
- Hendrycks et al. (2021a) propose a question-answering dataset sourced from freely available online materials, containing questions (including legal exam questions) from dozens of specialized areas.
- Zheng et al. (2021) present a multiple-choice reading comprehension dataset with 53,317 annotations automatically extracted from US case law citations.
- Duan et al. (2019) present a Chinese-language legal reading comprehension dataset, with about 50,000 expertgenerated annotations of Chinese judicial rulings.
- In our work we present a legal reading comprehension dataset with 47,457 expert-generated annotations about merger agreements.
- MAUD is the only English-language legal reading comprehension dataset that is both large-scale and expert-annotated.
- Merger Agreement Understanding MAUD consists of 47,457 annotations based on legal text extracted from 152 English-language public merger agreements.
- MAUD's merger agreements were sourced from the EDGAR system maintained by the U.S. Securities and Exchange Commission.
- Terminology.
- Deal points are legal clauses that define when and how the parties in a merger agreement are obligated to complete an acquisition.
- We refer to the text of these clauses (extracted by annotators from merger agreements) as deal point texts.
- One or more predefined deal point questions can be asked about each deal point text.
- Each deal point question can be answered by one or more pre- Task.
- MAUD is a multiple-choice reading comprehension task. The model predicts the correct deal point answer from a predefined list of possible answers associated with each question. (See Figure 1 for an example).
- Several deal point questions in the ABA Study are in fact multilabel questions, but for uniformity we cast all multilabel questions as binary multiple-choice questions. This increases the effective number of questions from 92 to 144.
- Abridged Dataset.
- The abridged dataset contains 14,928 examples with deal point text extracted from 94 of the 152 merger agreements included in the main dataset.
- In the abridged dataset, deal point texts are abridged to delete portions of legal text in the main dataset that are not pertinent to the deal point question.
- Because many texts contain answers to multiple questions, we provide the abridged data to guide a model to recognize the most pertinent text.
- Appendix A.8 compares the difficulty of main and abridged test examples.

### MAUD Datasets and Splits
- The rare answers dataset contains 3,680 examples that have rare answers to a question.
- Legal experts made small edits to texts in the main dataset to create deal points with rare answers.
- See Appendix A.11 for an example edit.
- We introduced the rare answers dataset to ameliorate imbalanced answer distributions in the main dataset.

## Experiments

### Setup
- We use the area under the precision-recall curve (AUPR) as our primary metric
- For every question, we calculate the minorityclass AUPR score for each answer and then average to get a mean AUPR score for the question
- Then we average over all question scores to get an overall AUPR score for a model
- For example, consider a deal point question Q, with three possible answers: A1, A2, and A3, which have 50, 10, and 10 test examples respectively
- For the unique question-answer pair (Q, A1), we first binarize all answers as A1 or ¬A1
- The minority binarized answer is ¬A1, with 20 examples, and so the AUPR score for (Q, A1) is calculated using positive class ¬A1
- To get the AUPR score for question Q, we average the AUPR scores for (Q, A1), (Q, A2), and (Q, A3)
- We fine-tune both single-task and multitask pretrained language models on MAUD using the Transformers library (Wolf et al., 2020)
- In the single-task setting, we evaluate the performance of fine-tuned BERT-base (110M params), RoBERTa-base (125M params), LegalBERTbase (110M params), DeBERTa-v3-base (184M params), and BigBird-base (127M params)
- In the multi-task setting, we evaluate RoBERTabase, LegalBERT-base, and DeBERTa-v3-base
- BERT (Devlin et al., 2019) is a bidirectional Transformer that established state-of-the-art performance on many NLP tasks
- LegalBERT (Chalkidis et al., 2020) pretrains BERT on a legal corpus
- RoBERTa (Liu et al., 2019) improves on BERT, using the same architecture, but pretraining on an order of magnitude more data
- DeBERTa (He et al., 2020) improves upon RoBERTa by using a disentangled attention mechanism and more parameters

### Results
- Our fine-tuned models achieved high AUPR scores in the Remedies, General Information, and Operating & Efforts Covenant categories, but scored lower on other categories
- Our results indicate that there is substantial room for improvement on these three hardest categories, which have the longest text lengths
- In the single-task setting, DeBERTa achieved an overall score of 57.1% AUPR, compared with 55.5% for RoBERTa and 52.6% for BERT
- BigBird achieved the highest score of 57.8% AUPR, slightly outperforming DeBERTa
- Effect of Pretraining on Legal Corpus. In the single-task setting, LegalBERT outperforms BERT and slightly outperforms RoBERTa, which have the same model architecture but are not specialized for law
- In the multi-task setting, LegalBERT also outperforms DeBERTa

## Conclusion
- MAUD is a large-scale expert-annotated dataset
- The dataset can facilitate NLP research on a specialized merger agreement review task
- The dataset can serve as a benchmark for assessing NLP models in legal text understanding
- Fine-tuned Transformer baselines exhibit strong performance on some deal point categories, but there is significant room for improvement on the three hardest categories

### Data Collection
- The data was created by volunteer annotators
- None of the annotators were compensated monetarily for their time
- Among our 36 annotators, 20 were male and 16 were female
- 33 annotators are based in the United States and 3 annotators are based in Europe

### Societal Impact
- Advances in ML contract review can reduce the costs of and increase the availability of legal services.
- In coming years, M&A attorneys would likely benefit from having auxiliary analysis provided by ML models.

### Limitations
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is licensed under CC-BY 4.0.
- MAUD is...

### A.2 Original Merger Agreement Texts
- The BERT, RoBERTA, LegalBERT, DeBERTa-v3, and BigBird pretrained language models that we use in our experiments are available on HuggingFace Hub as bert-base-cased, roberta-base, nlpaueb/legal-bert-base-uncased, microsoft/deberta-v3-base, and google/bigbirdroberta-base.
- In the singletask setting, we train an individual model for every MAUD question.
- In the multi-task setting, we train a single model that answers all questions.
- We fine-tune both single-task and multi-task models using the AdamW optimizer with weight decay 0.01.
- We oversample to give every answer equal proportion.
- For all models except BigBird we truncate deal point texts to 512 tokens.
- To reduce computational costs while fine-tuning BigBird models, we set the maximum input sequence length to the minimum required to encompass all deal point texts associated with the model's deal point question.
- For BERT, RoBERTa, LegalBERT, and DeBERTa-v3 experiments, including the RoBERTa dataset size ablation experiment,we used batch size 16.
- We grid-searched over learning rates {1 × 10 −5 , 3 × 10 −5 , 1 × 10 −4 } and number of updates {100, 200, 300, 400}.
- For BigBird experiments we used batch size 8.
- We grid-searched over learning rates {1 × 10 −5 , 1 × 10 −4 } and number of updates {200, 400, 600, 800}.
- Validation AUPR scores were averaged over 3 runs.
- Multi-Task Grid Search. For all models, we used batch size 16, grid-searched over learning rates {1 × 10 −5 , 3 × 10 −5 , 1 × 10 −4 } and number of epochs {1, 2, 3, 4, 5, 6}.
- Validation AUPR scores were averaged over 3 runs.
- Infrastructure and Computational Costs. We trained BERT and RoBERTa experiments in parallel on A5000 GPUs, using about 12GB of GPU memory.
- Three runs of fine-tuning models for every question with 400 updates took about one GPU-day per learning rate setting.
- We trained DeBERTa-v3 experiments in parallel on A4000 GPUs, using about 20GB of GPU memory.
- Three runs of fine-tuning models for every question with 400 updates took about two GPU- days per learning rate setting.
- We trained BigBird experiments in parallel on A4000, A5000, and A100 GPUs, choosing the minimum GPU size required to accomodate the GPU usage of the model, which varied with the maximimum deal point text length.
- The experiments with the longest deal point text lengths required about 75 GB of GPU memory.
- Three runs of fine-tuning models for every question with 800 updates took 4 to 5 GPU-days per learning rate setting.
- Multi-task models were trained on a A100 GPU.

### A.4 Best-Performing Hyperparameters
- Over 300 combinations of best hyperparameters
- Files are in supplementary materials
- Hyperparameters can be tuned to optimize the model
- Best hyperparameters can be found by tuning the model

### A.5 Evaluation Variability
- The average overall AUPR over three runs for our models can vary by 1-2%.
- The average AUPR over three runs for our models is 1.8%.
- The AUPR over three runs for our models is not consistent.

### A.6 Example Annotations in the Datasets
- The dataset has a table structure with a column for each annotation.
- The dataset has a few example annotations.

### A.7 Other Dataset Statistics
- Table 9 shows that a majority of deal point texts are longer than 512 tokens.
- There are more deal point questions than deal point texts.

### A.8 Main and Abridged Example Subscores
- The mean multi-task AUPR scores over main and abridged examples separately
- A.9 F1 Scores
- Tables 7 and 8 show the average test F1 micro and macro scores for multi-task models
- The law students who annotated MAUD worked in teams of three. Each annotation in the main and abridged datasets was first annotated collectively, by a consensus established by a law student team. This annotation includes both the text extraction and the deal point answers. When the team of law students could not reach an agreement on an annotation, they escalated to an experienced M&A lawyer. Finally, every law student annotation was reviewed by an experienced M&A lawyer for accuracy.
- The main and abridged datasets
- To create the main dataset and the abridged dataset, the law students conducted manual review and labeling of the merger agreements uploaded in eBrevia, an electronic contract review tool.
- On a periodic basis, the law students exported the annotations into reports, and sent them to experienced lawyers for a quality check. The lawyers reviewed the reports or the labeled contracts in eBrevia, provided comments, and addressed student questions. Where needed, reviewing lawyers escalated questions to a panel of 3-5 expert lawyers for discussions and reached consensus.
- Students or the lawyers made changes in eBrevia accordingly.
- The rare answers dataset
- To create the rare answers dataset, legal experts copied an example from the main dataset and minimally edited the deal point text to create an example with a rare answer. These edits were then reviewed by an experienced attorney to ensure accuracy.
- For example, the deal point question "Limitations on Antitrust Efforts" originally had very few examples of "Dollar-based standard" deal point answer. To create examples with this rare answer, the annotators changed phrases in the deal point text similar to "no obligation to divest or take other actions" with language implying a dollar-based standard, such as "Remedy Action or Remedy Actions with assets which generated in the aggregate an amount of revenues that is in excess of USD 50,000,000."

### A.12 Extraction Dataset and Baseline
- MAUD contains 152 merger agreements with deal point texts extracted by expert annotators
- The deal point texts are truncated for display
- The main dataset contains 20,623 examples with original deal point text extracted from 152 merger agreements
- The precision-recall curves for multi-task models, averaged over all MAUD questions, are shown in Figure 2
- RoBERTa-base AUPR as a function of the number of training examples, highlighting the value of our dataset's size, is shown in Figure 3
