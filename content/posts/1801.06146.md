---
title: "Universal Language Model Fine-tuning for Text Classification"
date: 2018-01-18T17:54:52.000Z
author: "Jeremy Howard, Sebastian Ruder"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/1801-06146v5_yAQwFfIml.jpg" # image path/url
    alt: "Universal Language Model Fine-tuning for Text Classification" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1801.06146)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1801.06146).


# Abstract
- Inductive transfer learning has greatly impacted computer vision
- Existing approaches in NLP still require task-specific modifications and
- Training from scratch is still the best option for most tasks
- We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP
- Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets
- Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data

# Paper Content

## Introduction
- Inductive transfer learning has had a large impact on computer vision (CV)
- Applied CV models (including object detection, classification, and segmentation) are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on ImageNet, MS-COCO, and other datasets
- Text classification is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection
- While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge
- Research in NLP focused mostly on transductive transfer (Blitzer et al., 2007)
- For inductive transfer, fine-tuning pretrained word embeddings (Mikolov et al., 2013), a simple transfer technique that only targets a model's first layer, has had a large impact in practice and is used in most state-of-the-art models
- Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017;McCann et al., 2017;Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness
- In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP
- Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability
- We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption
- LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods
- We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans-fer learning approaches on six widely studied text classification tasks.
- On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10× and-given 50k unlabeled examples-with 100× more data.

## Related work

## Universal Language Model Fine-tuning
- We are interested in the most general inductive transfer learning setting for NLP (Pan and Yang, 2010): Given a static source task T S and any target task T T with T S = T T , we would like to improve performance on T T .
- Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies (Linzen et al., 2016), hierarchical relations (Gulordava et al., 2018), and sentiment (Radford et al., 2017).
- In contrast to tasks like MT (McCann et al., 2017) and entailment (Conneau et al., 2017), it provides data in near-unlimited quantities for most domains and languages.
- Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target task, which we show significantly improves performance (see Section 5).
- Moreover, language modeling already is a key component of existing tasks such as MT and dialogue modeling.
- Formally, language modeling induces a hypothesis space H that should be useful for many other NLP tasks (Vapnik and Kotz, 1982;Baxter, 2000).
- We propose Universal Language Model Finetuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques.
- The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.
- In our experiments, we use the state-of-theart language model AWD-LSTM (Merity et al., 2017a), a regular LSTM (with no attention, short-cut connections, or other sophisticated additions) with various tuned dropout hyperparameters.
- Analogous to CV, we expect that downstream performance can be improved by using higherperformance language models in the future.

### General-domain LM pretraining
- Pretraining a language model on a large, diverse corpus improves performance.
- Pretraining is most beneficial for tasks with small datasets.

### Target task LM fine-tuning
- The data of the target task will likely come from a different distribution than the data used for pretraining.
- We propose discriminative fine-tuning and slanted triangular learning rates for fine-tuning the LM.
- Discriminative fine-tuning allows us to tune each layer with different learning rates.
- For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters.
- Using the same learning rate (LR) or an annealed learning rate throughout training is not the best way to achieve this behaviour.
- Instead, we propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it.

### Target task classifier fine-tuning
- language models are trained with backpropagation through time (BPTT)
- BPTT for Text Classification (BPT3C) is used to fine-tune the classifier
- the classifier is averaged over the LM

## Experiments
- Our approach is equally applicable to sequence labeling tasks
- Text classification tasks are important realworld applications
- We focus on text classification tasks

### Experimental setup

### Results
- Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, and the state-of-the-art on both datasets
- On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively
- On TREC-6, our improvement-similar as the improvements of state-of-the-art approaches-is not statistically significant, due to the small size of the 500-examples test set
- Nevertheless, the competitive performance on TREC-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences-in the case of TREC-6to several paragraphs for IMDb

## Analysis
- ULMFiT is able to match the performance of training from scratch on larger datasets
- Fine-tuning the LM is most beneficial for larger datasets
- Regular fine-tuning is not beneficial on the smaller TREC-6
- Fine-tuning the full model (Full) is the most commonly used fine-tuning method and is not beneficial on the smaller datasets
- Discriminative fine-tuning (Discr) and slanted triangular learning rates (Stlr) are necessary on the smaller TREC-6
- Cosine annealing is competitive with slanted triangular learning rates on large data, but under-performs on smaller datasets

## Discussion and future directions
- ULMFiT can achieve state-of-the-art performance on widely used text classification tasks
- Language model fine-tuning will be particularly useful in the following settings compared to existing transfer learning approaches
- NLP for non-English languages, where training data for supervised pretraining tasks is scarce
- New NLP tasks where no state-of-the-art architecture exists
- Tasks with limited amounts of labeled data (and some amounts of unlabeled data)
- One possible direction is to improve language model pretraining and fine-tuning and make them more scalable
- Language modeling can also be augmented with additional tasks in a multi-task learning fashion
- Another direction is to apply the method to novel tasks and models
- More studies are required to better understand what knowledge a pretrained language model captures, how this changes during fine-tuning, and what information different tasks require

## Conclusion
- ULMFiT is an effective and extremely sample-efficient transfer learning method
- Our method significantly outperformed existing transfer learning techniques and the state-of-the-art on six representative text classification tasks.
