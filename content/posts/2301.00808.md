---
title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"
date: 2023-01-02T18:59:31.000Z
author: "Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-00808v1.webp" # image path/url
    alt: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00808)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00808).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/convnext-v2-co-designing-and-scaling-convnets).

# Abstract
- Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s.
- For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios.
- While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance.
- In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data.

# Paper Content

## Introduction
- Large-scale visual recognition has ushered in a new era of large-scale vision models
- Pre-trained, large-scale vision models have become essential tools for feature learning and enabling a wide range of vision applications
- The performance of a visual representation learning system is largely influenced by three main factors: the neural network architecture chosen, the method Atto Femto Pico Nano Tiny Base Large used for training the network, and the data used for training
- In the field of visual recognition, progress in each of these areas contributes to overall improvements in performance

## Related Work
- ConvNets are a type of neural network that were first introduced in the 1980s
- ConvNets have undergone numerous improvements in terms of optimization, accuracy, and efficiency over the years
- Masked autoencoders are a type of neural network that were first introduced in the 1990s
- MCMAE is a type of neural network that was first introduced in the 1990s

## Fully Convolutional Masked Autoencoder
- Our approach is conceptually simple and runs in a fully convolutional manner.
- The learning signals are generated by randomly masking the raw input visuals with a high masking ratio and letting the model predict the missing parts given the remaining context.
- Our framework is illustrated in Figure 2, and we will now describe its main components in more detail.
- Masking. We use a random masking strategy with a masking ratio of 0.6.
- As the convolutional model has a hierarchical design, where the features are downsampled in different stages, the mask is generated in the last stage and upsampled recursively up to the finest resolution.
- To implement this in practice, we randomly remove 60% of the 32 × 32 patches from the original input image.
- We use minimal data augmentation, only including random resized cropping.
- Encoder design. We use ConvNeXt [52] model as the encoder in our approach.
- One challenge in making masked image modeling effective is preventing the model from learning shortcuts that allow it to copy and paste information from the masked regions. This is relatively easy to prevent in transformer-based models, which can leave the visible patches as the only input to the encoder. However, it is more difficult to achieve this with ConvNets, as the 2D image structure must be preserved.
- While naive solutions involve introducing learnable masked tokens in the input side [3,77], these approaches decrease the efficiency of pretraining and result in a train and test time inconsistency, as there are no mask tokens at test time. This becomes especially problematic when the masking ratio is high.
- To tackle this issue, our new insight is to view the masked image from a "sparse data perspective", which was inspired by learning on sparse point clouds in 3D tasks [15,76].
- Our key observation is that the masked image can be represented as a 2D sparse array of pixels. Based on this insight, it is natural to incorporate sparse convolution into our framework to facilitate pre-training of the masked autoencoder.
- In practice, during pre-training, we propose to convert the standard convolution layer in the encoder with the submanifold sparse convolution, which enables the model to operate only on the visible data points [15,27,28].
- We note that the sparse convolution layers can be converted back to standard convolution at the fine-tuning stage without requiring additional handling.
- As an alternative, it is also possible to apply a binary masking operation before and after the dense convolution operation. This operation has numerically the same effect as sparse convolutions, is theoretically more computationally intensive, but can be more friendly on AI accelerators like TPU.
- Decoder design. We use a lightweight, plain ConvNeXt block as the decoder. This forms an asymmetric encoderdecoder architecture overall, as the encoder is heavier and has a hierarchy.
- We also considered more complex decoders such as hierarchical decoders [48,59] or transformers [21,31], but the simpler single ConvNeXt block decoder performed well in terms of fine-tuning accuracy and reduced pre-training time considerably, demonstrated in Table 1.
- We set the dimension of the decoder to 512.
- Reconstruction target. We compute the mean squared error (MSE) between the reconstructed and target images.
- Similar to MAE [31], the target is a patch-wise normalized image of the original input, and the loss is applied only on the masked patches.
- FCMAE. We now present a Fully Convolutional Masked AutoEncoder (FCMAE) by combining the proposals described above.
- To evaluate the effectiveness of this framework, we use the ConvNeXt-Base model as the encoder...

## Global Response Normalization
- Feature collapse: many feature maps are dead or saturated and the activation becomes redundant across channels
- Qualitative analysis: we observe an intriguing "feature collapse" phenomenon in the feature space
- Quantitative analysis: we perform a feature cosine distance analysis and find that the ConvNeXt V1 FCMAE pre-trained model exhibits severe feature collapse behavior
- Approach: there are many mechanisms in the brain that promote neuron diversity and we introduce a new response normalization layer called global response normalization (GRN) to increase the contrast and selectivity of channels
- Pseudocode of GRN in a PyTorch-like style:
- gamma, beta: learnable affine transform parameters
- X: input of shape (N,H,W,C)
- gx = torch.norm(X, p=2, dim=(1,2), keepdim=True)
- nx = gx / (gx.mean(dim=-1, keepdim=True)+1e-6)
- return gamma * (X * nx) + beta + X

## ImageNet Experiments
- The FCMAE pre-training framework and ConvNeXt V2 architecture are co-designed to make masked-based self-supervised pre-training successful.
- The FCMAE pre-training framework has a limited impact on representation learning quality, while the new GRN layer has a rather small effect on performance under the supervised setup.
- However, the combination of the two results in a significant improvement in fine-tuning performance.
- The model scaling behavior is strong, with consistently improved performance over the supervised baseline across all model sizes.
- Compared to previous methods, our approach outperforms the Swin transformer pre-trained with Sim-MIM [77] across all model sizes.
- Compared to the plain ViT pre-trained with MAE [31], our approach performs similarly up to the Large model regime, despite using much fewer parameters (198M vs 307M).
- However, in the huge model regime, our approach slightly lagged behind. This might be because a huge ViT model can benefit more from self-supervised pre-training.
- Our method, using a convolution-based architecture, sets a new state-of-the-art accuracy using publicly available data only (i.e. ImageNet-1K and ImageNet-22K).

## Transfer Learning Experiments
- We evaluate the impact of our co-design, i.e. comparing ConvNeXt V1 + supervised vs. ConvNeXt V2 + FC-MAE.
- We directly compare our approach with Swin transformer models pre-trained with SimMIM.
- We finetune Mask R-CNN on the COCO dataset and report the detection mAP box and the segmentation mAP mask on the COCO val2017 set.
- We see a gradual improvement as our proposals are applied.
- Upon this, the model further benefits from better initialization when moving from supervised to FCMAE-based self-supervised learning.
- The best performances are achieved when both are applied together.

## Conclusion
- Introduces a new ConvNet model family called ConvNeXt V2 that covers a broader range of complexity
- While the architecture has minimal changes, it is specifically designed to be more suitable for self-supervised learning
- Using our fully convolutional masked autoencoder pre-training, we can significantly improve the performance of pure ConvNets across various downstream tasks, including ImageNet classification, COCO object detection, and ADE20K segmentation
- Inference is performed using a multi-scale test with resolutions that are [0.75,0.875,1.0,1.125,1.25] of 512×2048
