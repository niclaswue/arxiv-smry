---
title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"
date: 2023-01-02T18:59:31.000Z
author: "Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "" # image path/url
    alt: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00808)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00808).

# Paper Content
## Introduction
- Building on research breakthroughs in earlier decades, the field of visual recognition has ushered in a new era of large-scale visual representation learning.
- Three main factors influence the performance of a visual representation learning system: the neural network architecture, the training method, and the data used for training.
- Convolutional neural network architectures (ConvNets) have been used for feature learning and enabled a wide range of vision applications.
- The transformer architecture has gained popularity due to its strong scaling behavior.
- Self-supervised pre-training with pretext objectives has become popular for visual representation learning.
- Masked autoencoders have brought success in masked language modeling to the vision domain.
- Combining the design elements of architectures and self-supervised learning frameworks can be challenging with ConvNeXt.
- ConvNeXt V2 is proposed to address the issue of feature collapse, and has improved performance across various tasks.

## Related Work
- ConvNets were first introduced in the 1980s and have been improved over time.
- Supervised training on the Im-ageNet dataset has been used to discover many of these improvements.
- Self-supervised pre-text tasks such as rotation prediction and colorization have been used for architecture search.
- ConvNeXt has been used to demonstrate that pure ConvNets can be as scalable as vision transformers.
- ConvNeXt V2 model has been developed, powered by self-supervised learning, to provide a boost in performance.
- Masked autoencoders are a self-supervised learning strategy which has been adapted for use with ConvNets.
- MCMAE has used convolutional blocks as input tokenizers.
- There are no pretrained models that show self-supervised learning can improve upon the best ConvNeXt supervised results.

## Fully Convolutional Masked Autoencoder
- Approach is conceptually simple and runs in a fully convolutional manner
- Model predicts missing parts given context
- Masks randomly remove 60% of 32 x 32 patches from the original image
- Encoder uses ConvNeXt with sparse convolution
- Decoder uses single ConvNeXt block with 512 dimensions
- Computes mean squared error between reconstructed and target images
- Evaluates effectiveness with end-to-end fine-tuning performance
- Pre-training provides better initialization than random baseline

## Global Response Normalization
- Introduce Global Response Normalization (GRN) with ConvNeXt architecture
- Feature collapse observed in dimension-expansion MLP layers in ConvNeXt
- Feature cosine distance analysis to quantify feature collapse
- GRN includes global feature aggregation, feature normalization, and feature calibration
- GRN incorporated in ConvNeXt V2 model
- GRN mitigates feature collapse issue
- GRN improves representation quality without additional parameter overhead
- GRN compared with other normalization layers and feature gating methods
- GRN important to keep in both pre-training and fine-tuning

## ImageNet Experiments
- FCMAE pre-training framework and ConvNeXt V2 architecture designed to make masked-based self-supervised pre-training successful
- Synergizing of designs provides strong foundation for scaling
- Comparisons to previous masked auto-encoder methods
- Strong model scaling behavior, improved performance over supervised baseline
- Sets new state-of-the-art accuracy using publicly available data

## Transfer Learning Experiments
- Evaluating the impact of co-design on transfer learning performance
- Comparing ConvNeXt V1 + supervised and ConvNeXt V2 + FC-MAE
- Comparing with Swin transformer models pre-trained with SimMIM
- Object detection and segmentation on COCO
- Gradual improvement from V1 to V2
- Model benefits from better initialization with FCMAE
- ConvNeXt V2 pre-trained on FCMAE outperforms Swin transformer counterparts
- Semantic segmentation on ADE20K
- Similar trend to object detection experiments
- Final model significantly improves over V1 counterparts
- On par with Swin transformer in base and large model regimes
- Outperforms Swin in huge model regime

## Conclusion
- ConvNeXt V2 is a new ConvNet model family
- It is designed for self-supervised learning
- It can improve performance of pure ConvNets on various tasks, like ImageNet classification, COCO object detection, and ADE20K segmentation
- Uses mixup, cutmix, drop path, head init, and EMA
- End-to-end IN-1K fine-tuning setting for Atto, Femto, Pico, and Nano models

## Complete comparisons with V1
- Tables 14 and 15 present detailed experiment-level comparisons between ConvNeXt V1 and V2.
- Table 14 shows ImageNet-1K fine-tuning results using eight models.
- Performance is improved when architecture is upgraded from V1 to V2 and self-supervised learning framework FCMAE is used.
- Table 15 compares the performance of the Base V2 model with the baseline ResNet-50 model on downstream tasks.

## Further Analyses
- Sparse convolution used in FCMAE framework to block flow of information and facilitate pre-training
- Computational and memory efficiency improved during pre-training
- Benchmark experiments conducted to understand pre-training efficiency
- Class selectivity index analysis conducted on FCMAE pre-trained weights
- Distribution of class selectivity index closely matched between V1 and V2 in early stages, but diverges in deep layers
- V2 tends to include more class-generic features than V1
- Feature normalization not preceded by global aggregation, both operations work together to make GRN effective

## Additional Experiments
- Masking ratios of 0.5 to 0.7 produce the best results with a masking ration of 0.6 being the highest
- Performance declines when removing or leaving 90% of the input information
- Comparing the performance of two dominant self-supervised learning approaches: contrastive learning and masked image modeling
- FCMAE leads to better representation quality than MoCo V3 and also outperforms the supervised baseline
