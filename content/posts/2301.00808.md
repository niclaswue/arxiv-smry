---
title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"
date: 2023-01-02T18:59:31.000Z
author: "Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-00808v1_P5uJR8oBi.jpg" # image path/url
    alt: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00808)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00808).


# Abstract
- Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s.
- For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios.
- While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance.
- In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data.

# Paper Content

## Introduction
- Large-scale visual recognition learning is aided by pre-trained, large-scale vision models
- Convolutional neural network architectures have had a significant impact on computer vision research
- In recent years, the transformer architecture [68], originally developed for natural language processing, has also gained popularity
- Masked autoencoders (MAE) [31] have recently brought success in masked language modeling to the vision domain
- To address issue of feature collapse at MLP layer when training ConvNeXt directly on masked input, we propose adding a Global Response Normalization layer

## Related Work
- ConvNets are a type of neural network that have been improved over the years
- The design of ConvNets has been improved through the use of supervised training on the Im-ageNet dataset
- Masked autoencoders are a type of neural network that have been improved over the years
- MCMAE is a neural network pre-training framework that uses a few convolutional blocks as input tokenizers

## Fully Convolutional Masked Autoencoder
- The encoder is a fully convolutional model that uses a random masking strategy with a masking ratio of 0.6.
- The encoder is designed to be lightweight and simple, and is implemented as a single ConvNeXt block.
- The decoder is a simple ConvNeXt block that is dimensioned 512.
- The reconstruction target is a patch-wise normalized image of the original input, and the loss is applied only on the masked patches.
- FCMAE is a proposed framework that combines the proposals described above.

## Global Response Normalization
- Global feature aggregation: G(X) is a scalar that aggregates the statistics of the i-th channel.
- Response normalization: N (â€¢) is a standard divisive normalization as follows, where ||X i || is the L2-norm of the i-th channel.

## ImageNet Experiments
- The FCMAE pre-training framework and ConvNeXt V2 architecture are co-designed to make masked-based self-supervised pre-training successful
- The FCMAE pre-training framework has a limited impact on representation learning quality
- The combination of the FCMAE pre-training framework and the new GRN layer results in a significant improvement in fine-tuning performance
- The convolution-based architecture set a new state-of-the-art accuracy using publicly available data only (i.e. ImageNet-1K and ImageNet-22K)

## Transfer Learning Experiments
- We evaluate the impact of our co-design, i.e. comparing ConvNeXt V1 + supervised vs. ConvNeXt V2 + FC-MAE.
- We directly compare our approach with Swin transformer models pre-trained with SimMIM [77].
- We finetune Mask R-CNN [33] on the COCO dataset [49] and report the detection mAP box and the segmentation mAP mask on the COCO val2017 set.
- We see a gradual improvement as our proposals are applied.
- Upon this, the model further benefits from better initialization when moving from supervised to FCMAE-based self-supervised learning.
- The best performances are achieved when both are applied together.
- Additionally, our final proposal, ConvNeXt V2 pre-trained on FCMAE, outperforms the Swin transformer counterparts across all model sizes, with the largest gap achieved in the huge model regime.

## Conclusion
- Introduces a new ConvNet model family called ConvNeXt V2 that covers a broader range of complexity
- Uses fully convolutional masked autoencoder pre-training to improve the performance of pure ConvNets across various downstream tasks
- Uses multi-scale test to compare the performance of using model weights after supervised fine-tuning on ImageNet-1K and using self-supervised pre-trained weights directly

## B. Complete comparisons with V1
- In Tables 14 and 15, ConvNeXt V1 and V2 are compared on ImageNet-1K and ImageNet-2K validation sets
- V1 is found to be inaccurate and V2 is found to be more accurate
- This improvement is seen when the ConvNeXt V2 is upgraded to use FCMAE

## C. Further Analyses
- Sparse encoding efficiency is improved by using sparse convolution
- Class selectivity index is improved by using sparse convolution
- Both operations work together to make GRN effective

## D. Additional Experiments
- The masking ratio has a significant impact on the performance of a self-supervised learning model
- The proposed FCMAE framework performs better than MoCoV3
- The success of FCMAE was also made possible with pure ConvNets
