---
title: "TotalSegmentator: robust segmentation of 104 anatomical structures in CT images"
date: 2022-08-11T15:16:40.000Z
author: "Jakob Wasserthal, Manfred Meyer, Hanns-Christian Breit, Joshy Cyriac, Shan Yang, Martin Segeroth"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2208-05868v1.webp" # image path/url
    alt: "TotalSegmentator: robust segmentation of 104 anatomical structures in CT images" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2208.05868)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2208.05868).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/totalsegmentator-robust-segmentation-of-104).

# Abstract
- The code and data for most segmentation algorithms is not publicly available or difficult to use.
- Many segmentation algorithms suffer from the problem that they can only segment one anatomical structure.
- In this work, we publish a new dataset and segmentation toolkit which solves the problem of not being able to segment multiple anatomical structures.
- The new dataset contains a wide range of different pathologies, scanners, sequences and sites.
- We show an improved workflow for the creation of ground truth segmentations which speeds up the process by over 10x.

# Paper Content

## Introduction
- The number of radiological images is increasing
- There is a need for computer-based evaluation methods
- One important building block for the automatic analysis of radiological images is the segmentation of major anatomical structures
- There has been a lot of work on segmenting specific anatomical structures, e.g. pancreas (Simpson et al., 2019), spleen (Simpson et al., 2019), colon (Gayathri Devi & Radhakrishnan, 2015), lung (Hofmanninger et al., 2020) and many more
- There has also been work on segmenting several anatomical structures in one dataset and model (e.g. Chen et al., 2021)
- However, all of those models only cover a small subset of relevant anatomical structures and train on a dataset which is not representative of a typical clinical population.
- To the best of our knowledge, this is the first work trying to segment over 100 classes.
- The closest work is from Chen et al. (Chen et al., 2021) which segments 50 classes. However, they are still missing many classes, the dataset as well as the model is not publically available (the model can only be accessed via uploading data to a web interface) and the dataset is less diverse (the majority of the training data comes from the same scanner and uses the same CT sequence).
- Our approach solves all three of these problems: 1. Our model segments a wide range of anatomical structures. 2. Our model robustly works on clinical data since it was trained on a large clinical dataset. 3. Our model as well as the training data is publicly available and very easy to use.

## Methods

### Data selection
- The dataset was collected from patients with general research consent
- Only images from patients with a general research consent were used
- CT images of legs and hands were excluded
- The dataset contains CT images with different sequences (native, arterial, portal venous, late phase, dual energy), with and without contrast agent, with different bulb voltages, with different slice thicknesses and resolution and with different kernels (soft tissue kernel, bone kernel)
- If the human expert annotator was unsure how to segment certain structures because of high ambiguity (e.g. structures highly distorted by pathologies), the examination was excluded

### Data annotation
- Based on clinical experience, 104 anatomical structures were identified for segmentation
- Based on this experience, a first segmentation was created for 68 classes
- For the remaining 36 classes, manual segmentation was necessary
- Refinement of segmentations and manual segmentation was supervised by a board-certified radiologist
- To be able to segment the different heart subparts on all different CT series, the following approach was taken:
- An inhouse dataset was available with ground truth segmentations of the heart subparts on CT images with contrast agent
- For each image with contrast agent also a native CT image without contrast agent was available
- Since this image was from the same examination it was well aligned with the contrast agent image
- So, we transferred the segmentation to the CT images without contrast agent and trained a U-Net on images with and without contrast agent
- This model was then used to predict the heart subparts in our dataset, resulting in very accurate segmentations of the heart subparts in all kinds of CT sequences
- To speedup manual segmentation we used the Nora imaging platform
- Compared to traditional tools like MITK, Nora provides the following advantages:
- First, subjects do not have to be loaded individually, instead Nora provides a list of all subjects
- Second, segmentation masks do not have to be created and saved individually for each image, but whenever you change the subject in Nora, the correct masks are created, named and saved automatically
- Third, the autoloading as well as the view (e.g. the intensity window) can be configured for each project individually
- Thus, it took us only 17 s in Nora to load the case, create a segmentation mask and save the segmentation mask, whereas with MITK this procedure took us 45 s
- Forth, an even higher speedup is achieved by doing rough segmentations instead of pixel perfect segmentations
- Those rough segmentations were preliminary and not sufficient for the final result
- However, the U-Net is good at learning good segmentations from rough segmentations
- After finishing manual segmentation of the first 5 subjects, a U-Net was trained and only its predictions were corrected
- The U-Net was then retrained again after 20 subjects, 100 subjects and 1000 subjects
- After each iteration, less manual correction was necessary
- Another great speedup was achieved by using 3D renderings (using the python fury library) of the masks to spot errors
- We tested this approach on many subjects and it turns out that if a mask has errors you can typically spot it in the 3D rendering
- Moreover, the loading of the PNG image is instant, instead of several minutes for loading 104 masks

### Model
- The nnU-Net is a U-Net implementation which automatically configures all hyperparameters based on the dataset characteristics.
- When performing larger studies on many thousand subjects the "-fast" option can be helpful to make processing of so many subjects feasible.

### Evaluation
- The dataset of 1204 subjects was randomly split into 1082 training subjects (89.9%), 57 validation subjects (4.7%) and 65 subjects for final testing (5.4%).
- For each class in each subject the respective metric is calculated and then the mean is taken across all classes and subjects (micro-average).
- As additional evaluation we compared our model to a nnU-Net trained on the dataset from the "Multi-Atlas Labeling Beyond the Cranial Vault Challenge" https://doi.org/10.7303/syn3193805 (nnU-Net Task 17).

## Results

### Overall results
- The 1.5 mm model has high accuracy
- The 3 mm model still delivers correct results, but the borders are less precise

### Results per class
- Figure 5 shows the results for all classes independently
- Results vary between 0.87 and 1.0

### Comparison to other model
- Figure 7 shows the comparison of a high resolution model to a model trained on data from the "Multi-Atlas Labeling Beyond the Cranial Vault Challenge".
- The high resolution model achieves a higher dice and a higher NSD score.

### Runtime
- Table 8: Overview of runtime, RAM requirements as well as GPU memory requirements of the high resolution (1.5mm) and the low resolution model (3mm) on three different CT images
- Figure 8: Overview of runtime, RAM requirements as well as GPU memory requirements of the high resolution (1.5mm) and the low resolution model (3mm) on three different CT images

### Typical failure cases
- The model produces highly accurate results
- There are a few typical failure cases which occur in roughly 15% of the subjects
- Being aware of those can be helpful when using the segmentations of the affected classes

### Application example
- The model can be used to study how different anatomical structures change over the lifespan
- The model can be used to study how bone density changes over the lifespan
- The model can be used to study how the density and volume of different anatomical structures changes over the lifespan

## Conclusion
- The model is able to produce accurate segmentations across a very diverse dataset which is directly sampled from clinical routine.
- We made it openly available on github (https://github.com/wasserth/TotalSegmentator) and it can be easily installed via pip.
- It can be easily used to segment anatomical structures, for example, by age, gender, or site of image acquisition.
- The model can be trained to predict all 104 classes at the same time, but it requires a lot of memory and GPU memory.
- If the user chooses the flag "-fast" in our toolkit, then the model can be trained on data that is resampled to 3 mm isotropic resolution.
