---
title: "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"
date: 2022-10-19T07:17:39.000Z
author: "Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2210-10341v1_qiKY5zRPAU.jpg" # image path/url
    alt: "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.10341)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.10341).


# Abstract
- Pre-trained language models have been successful in the general natural language domain
- BioGPT is a domain-specific generative Transformer language model pre-trained on large scale
- BioGPT outperforms previous models on most tasks

# Paper Content

## Introduction
- Text mining and knowledge discovery from biomedical literature play important roles in drug discovery, clinical therapy, pathology research, etc.
- Pre-training models have demonstrated their powerful capability in natural language processing (NLP).
- On the GLUE benchmark, a widely used benchmark for natural language understanding, pre-training based methods outperform nonpre-training methods by a large margin.
- There are two main kinds of pre-training models: (1) the BERT-like models [2,3,4], mainly for language understanding tasks; (2) the GPT-like models [5,6,7], mainly for language generation tasks.
- These models are first pre-trained on large scale corpora collected from the Web via self-supervised learning task (e.g., masked language modeling for BERT, auto-regressive language modeling for GPT), and then fine-tuned on specific donwstream tasks.
- The BERT-like models are widely used in sequence classification and sequence labeling, where we need to encode the complete document.
- In comparison, the GPT-like models are often used in generation tasks (e.g., abstract generation, knowledge triplet generation).
- By witnessing the success of pre-training in general NLP, people explore adapting these techniques into biomedical domain. However, directly applying these models to the biomedical domain leads to unsatisfactory performance due to domain shift.
- A natural solution is to develop pre-training models on biomedical texts (e.g., PubMed).
- BioBERT [10] and PubMedBERT [9]) are two representative BERT-like models pre-trained on biomedical domain, and they obtain superior performances than general pre-trained models on biomedical benchmarks.
- However, previous works mainly focus on BERT models which are more appropriate for understanding tasks, not generation tasks.
- In comparison, GPT models have demonstrated their abilities on generation tasks but demonstrate inferior performance when directly applying to the biomedical domain.
- In this work, we propose BioGPT, a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining.
- BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch.
- We apply BioGPT to six biomedical NLP tasks: end-to-end relation extraction on BC5CDR [13], KD-DTI [14] and DDI [15], question answering on PubMedQA [16], document classification on HoC [17], and text generation.
- To adapt to the downstream tasks, we carefully design and analyze the target sequence format and the prompt for better modeling the tasks.
- Experiments demonstrate that BioGPT achieves better performance compared to baseline methods and other well-performing methods across all the tasks.
- It has proven to be a very successful pattern in deep learning to pre-train models on large scale unlabeled data via careful designed self-supervision tasks and then transfer to downstream tasks by fine-tuning on them.
- Downstream tasks can benefit from the learned representations from the pre-trained models.

### Pre-trained Language Models in Biomedical Domain
- Pre-trained BERT models can be further improved if pre-trained on in-domain text data
- Specifically, [10] and [8] start from the original pre-trained BERT model [2] that are pre-trained on general domain (Wikipedia and BooksCorpus) and continue pre-training on biomedical literature.
- [10] continue pre-training using PubMed abstracts and PubMed Central full text articles and [8] continue pre-training on both PubMed text and clinical notes from MIMIC-III [19].
- As they are initialized from the original BERT that are pre-trained on general domain, they use the same vocabulary as the original BERT, which is quite different from the target biomedical domain.
- [18] pre-train the BERT model from scratch on large corpus of scientific literature (mainly biomedical and computer science literature) where the vocabulary is more suitable for science domain but still contains out-domain information for biomedicine.
- [9] propose that it is a better strategy to pre-train on domain-specific data from scratch where the vocabulary is more suitable for the biomedical domain.
- [20] pre-train on 28M data as in [8] also from scratch, using the more advanced ELECTRA model.
- All these works have shown improved performance on plenty of biomedical literature language processing tasks compared to the original BERT pretrained on general domain, while none of them is for biomedical generation tasks.
- Noticing the powerful generation ability of GPT models, it is quite curious how GPT models perform on biomedical domain which is very different from general domain.
- However, recent works show that GPT models, even much more powerful GPT-3 model, perform poorly on biomedical tasks [11,12].
- A previous work on pre-training GPT on biomedical literature is DARE [21].
- However, they pre-train GPT on very limited amount of data (only 0.5M PubMed abstracts) and use it only for data-augmentation for relation extraction task.
- A recent work on using GPT model is [22], where they design converters for GPT-3 [7] for several unconventional downstream clinical tasks.

### Downstream Tasks
- downstream tasks: text generation / mining tasks
- relation extraction is a key task for biomedicine and life science research
- classical pipeline-based methods [33,34,23] resolve the task into several separate sub-tasks that require additional intermediate annotations and information which may suffer from the lack of intermediate annotated data and error accumulation
- joint extraction aims to jointly extract the entities and the relations between them from the text
- sequence labeling methods tackle the task by labeling the word tokens in the text with different tags to mark out all the entity mentions and then perform the relation classification between them via classifiers [35,36,37,38]
- table filling methods formulate the task as a table constituted by the Cartesian product of itself and predicts the relations between the token pairs [39,40,41]
- text generation methods reframe the task as a sequence-to-sequence learning task, by taking the text as the input sequence and the triplet as the target sequence and employing an encoder-decoder network to learn to generate the triplet from the text [42,43,14,24,25]
- but many joint extraction methods still require additional entity information [38,44]

### Question Answering
- PubMedQA is a biomedical question answering dataset
- We use the original train/validation/text split with 450, 50 and 500 respectively
- We use the continuous embedding with length=9 as the prompt
- We format the data into source sequence and target sequence as described before
- We fine-tune GPT-2 medium and BioGPT for 100 epochs with a peak learning rate 10 −5 and 100 warm-up steps
- We measure and compare the classification accuracy

## Pre-training Method
- where λ is a hyperparameter and ε is a noise term.
- We use the Adam [48] algorithm with a learning rate of 0.001 and a momentum of 0.9.
- We use the dropout technique [49] with a rate of 0.5 to reduce the complexity of the network.
- We use the GPT-2 model architecture [6] as the backbone of our BioGPT, which is a Transformer decoder [47].
- The core component of Transformer as well as our BioGPT is the multihead attention.
- The output of multi-head attention layer is then fed into a feed-forward layer to construct a Transformer layer (or Transformer block).
- We use the Adam [48] algorithm with a learning rate of 0.001 and a momentum of 0.9.
- We use the dropout technique [49] with a rate of 0.5 to reduce the complexity of the network.
- We use the GPT-2 model architecture [6] as the backbone of our BioGPT, which is a Transformer decoder [47].

## Fine-tuning Method
- Adapts pre-trained BioGPT to downstream tasks
- Uses natural language labels instead of special tokens
- Shows detailed implementation for each task and empirically verifies effectiveness

### End-to-end Relation Extraction
- Relation extraction is an important task in information extraction
- Here, we target at the end-to-end relation extraction setting where the model takes the text as the input and directly generates the relational triplets
- We mainly compare to REBEL, a recently proposed end-to-end triplet extraction approach based on sequence-to-sequence model, which employs BART pre-trained model as the backbone model, and further enhances it by pre-training on additional large relational triplet dataset created from Wikipedia as REBEL pt

### Document Classification
- The HoC consists of 1580 PubMed abstracts manually annotated at sentence level
- We use the continuous embedding with length=1 as the prompt
- We finetune GPT-2 medium and BioGPT for 20000 steps with a peak learning rate 10 −5 and 1000 warm-up steps
- Micro-F1 score is measured and reported for comparison
- We can see from the results in Table 6 that BioGPT achieves 85.12% accuracy with 3.28% improvement over general domain GPT-2, and surpasses BioBERT, PubMedBERT and BioLinkBERT with 3.58%, 2.8%, 0.77% improvements respectively

### Prompt-based Fine-tuning
- We have formatted the labels to target sequences.
- A naive way is to concatenate the source and the target sequences together but is difficult for the model to generate during inference as it does not know what to generate for the specific task given the source text input.
- Prompt is recently extensively explored in NLP to elicit the knowledge from a pre-trained language model.
- Prompt is to append task-specific instructions to the input for the model to better generate output that meets the demand of the task.
- GPT-3 [7] uses hard prompts (manually designed discrete language phrases) to generate for different tasks. Though hard prompts can achieve satisfactory performance, designing task specific prompts is laborious and it is found that different prompts lead to different performance.
- In this work, we mainly adopt soft prompts in prefixtuning [49], which leverage continuous embeddings (virtual tokens) to steer the pre-trained language model by directly appending several additional virtual tokens before the text as the prompts.
- Different from [49], we do not append the virtual tokens to the very beginning of the source input, but only before the target sequence (between the source and the target).
- Equipped with the prompt, our final sequence is constructed as [source; prompt; target], as depicted in Fig. 1.
- During the inference, we provide the source text and the prompt as the prefix for the language model to condition on and let the language model to generate the target output as in Fig. 1.

## Experiments
- Pre-train BioGPT on 8 NVIDIA V100 GPUs for 200k steps
- Uses Adam optimizer with a peak learning rate of 2×10 −4 and 20000 warm-up steps
- Uses continuous embeddings with length=9 as prompts and the rel-is target sequence format
- Average the checkpoints of the last 5 epochs for evaluation
- Measures and compares the micro-F1 score

### Text Generation
- GPT, GPT-2 and GPT-3 demonstrate remarkable text generation ability
- Given words, phrases or simple sentences as prefix, they can continue to generate text that are syntactically correct and semantically smooth conditioning on the given text
- We are also curious about the text generation ability of the pre-trained BioGPT in the biomedical domain, and how does general domain GPT-2 perform in the biomedical domain
- We evaluate the biomedical text generation ability of BioGPT and GPT-2 medium . Specially, we extract all the entities within the triplets from the KD-DTI test set (i.e., drugs and targets).
- Then for each drug/target name, we provide it to the language model as the prefix and let the model generate text conditioned on it.
- We then investigate whether the generated text is meaningful and fluent.
- For this task, no objective evaluation metric is reported here. Instead, we provide a few examples here for demonstration.
- From the results in Table 7, we can see that:
- Given relatively common names as input, for example in the first two cases (i.e., Bicalutamide and JAK-3), GPT-2 can generate meaningful and fluent text that is related to the word Bicalutamide, a synthetic bicalutamide derivative, was found to increase the cellular proliferation of C. elegans cells. This effect was not observed with the other synthetic bicalutamide derivatives. This is in line with our previous work which showed that bicalutamide increases the proliferation of C. elegans cells but not in the presence of synthetic bicalutamide.
- When given some uncommon names (e.g., in the Apricitabine and Xylazine cases), GPT-2 cannot generate meaningful descriptions while BioGPT still generates specific descriptions. Especially in the Apricitabine case, GPT-2 seems to generate a piece of text that comes from a specific scientific paper while BioGPT generates more general description.
- When given some very uncommon and domain specific names that even lose semantic information from their surface names (e.g., Psoralen, CP-673451 and BIIB-021), GPT-2 trained on general completely failed to generate any informative text.
- Given Psoralen, GPT-2 treats it as a city name and generates some text though fluent but unrelated to the given name. Given CP-673451, GPT-2 even begins to count numbers. Given BIIB-021, GPT-2 treats it as a name of a pdf document. For these types, BioGPT is still able to generate text that describes the names or is highly related to them.
- Besides these samples, we also manually input several keywords or phrases that are of interest (e.g., COVID-19 related terms) and see what GPT-2 and our BioGPT generate. The results are listed in Table 8, where we input many COVID-19 related key words/phrases as the prefix for the language model to condition on.
- We can see that GPT-2 treats the term "COVID-19" and "SARS-CoV-2" as some codes within a link
- Janus kinase 3 (JAK-3) mediates the conversion of glucose to glycogen in skeletal muscle. The increase in glucose uptake by skeletal muscle is believed to be associated with a reduction in muscle protein breakdown and with the release of ketone bodies, which contribute to the metabolism of glucose.
- Xylazine, the "active ingredient" in "bath salts" (see above) has been linked to numerous deaths, including those of former University of California, Berkeley, professor Andrew Anglin, who posted a video of himself having a bath filled with...

## Ablation Study
- Ablation study is conducted on the prompt design and the target sequence format of the label.
- The study found that the prompt design and the target sequence format of the label have a significant impact on the speed of the learning process.

### Target Sequence Format
- Previous works directly format the labels into structured formats using special tokens.
- Taking the triplet generation task as an example, in [24], the triplets are represented by: <triplet> head entity 1 <subj> tail entity 1 <obj> relation 1 <triplet> head entity 2 <subj> tail entity 2 <obj> relation 2 • • • , where <triplet>, <subj> and <obj> are special tokens to represent the start of the head entity, the tail entity and the relation.
- [24,14,25] use similar method to process the targets. Although these methods achieved promising results in their tasks respectively, such formulation pattern is not the best choice for BioGPT.
- Previous works use an encoder-decoder framework, where two separated modules are leveraged to process the input (by the encoder) and generate the answers (by the decoder).
- In contrast, in BioGPT, we use a unified module to encode context and generate answers. Intuitively, it is better to maintain the format consistency between the inputs and answers.
- Consequently, instead of the structured target format with special tokens as in previous works, we format the label within a natural language sentence for the language model to smoothly learn and generate.
- However, there are also various patterns that can be used to construct the target sentence.
- We explore several target sequence formats, including the structured format, on the KD-DTI dataset for end-to-end relation extraction task.
- We fix the prompt to continuous embeddings with length=9.
- From the results in Table 9 we can see that the formats in natural language perform better than structured format, and that the rel-is format performs the best among all the formats in terms of F1 which provides a more semantically smooth and clear description.
- We also conduct experiments on BC5CDR and DDI to further compare the structure format and the rel-is format. The F1 scores of the structure format on BC5CDR and DDI are 42.85 and 38.60, while those two scores with rel-is format are 44.98 and 40.76, which further verify our conclusion.
- We conduct experiment with manually designed hard prompts and continuous embedding soft prompts on the KD-DTI extraction task. We fix the target format to the rel-is format (i.e., "the relation between head entity and tail entity is relation").
- From the results in Table 10 we can see that the best performing prompt is continuous embeddings with length of 13 virtual tokens. Moreover, we have several observations: (1) Different manually designed hard prompts result in different performance and more instructive and informative prompt (e.g., "we can conclude that") achieve better performance. (2) Generally, continuous embedding soft prompts perform better than manually designed hard prompts. (3) The performance of the continuous embedding soft prompts are roughly irrelevant to the length.

## Conclusion
- BioGPT is a generative pre-trained Transformer language model for biomedical text generation and mining
- BioGPT achieves state-of-the-art results on four benchmarks: BC5CDR, KD-DTI and DDI end-to-end relation extraction task, and PubMedQA question answering task
- BioGPT can be used for biomedical literature text generation and mining
- We study the prompt design and the target sequence design when applying BioGPT to downstream tasks and find that target sequence with natural language semantics are better than structured prompts explored in previous works
