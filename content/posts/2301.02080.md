---
title: "Semantic match: Debugging feature attribution methods in XAI for healthcare"
date: 2023-01-05T14:26:55.000Z
author: "Giovanni Cinà, Tabea E. Röber, Rob Goedhart, Ş. İlker Birbil"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02080v2.webp" # image path/url
    alt: "Semantic match: Debugging feature attribution methods in XAI for healthcare" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02080)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02080).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/semantic-match-debugging-feature-attribution).

# Abstract
- The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology.
- One thread of such debate concerns Explainable AI (XAI) and its promise to render AI devices more transparent and trustworthy.
- A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques and especially feature attribution methods, questioning their use and inclusion in guidelines and standards.
- Despite valid concerns, we argue that existing criticism on the viability of post-hoc local explainability methods throws away the baby with the bathwater by generalizing a problem that is specific to image data.
- We begin by characterizing the problem as a lack of semantic match between explanations and human understanding. To understand when feature importance can be used reliably, we introduce a distinction between feature importance of low- and high-level features. We argue that for data types where low-level features come endowed with a clear semantics, such as tabular data like Electronic Health Records (EHRs), semantic match can be obtained, and thus feature attribution methods can still be employed in a meaningful and useful way.

# Paper Content

## Introduction
- AI and model complexity is increasing,
- There is an increase in interest in explainable AI,
- Techniques can roughly be grouped into local vs. global, and model-specific vs. model-agnostic approaches.
- Local methods aim to explain model outputs for individual samples, while global methods focus on making models more explainable at an aggregate level.
- Model-specific methods are tailored to explain a specific type of model, while model-agnostic methods can be applied to a range of different models.
- Many of the well-known techniques yield post-hoc explanations, meaning that they generate explanations for already trained, so-called 'black-box', models.
- Alternatively, there exist approaches that are considered inherently explainable, also known as white-box (or glass-box) models, such as decision trees and linear regression models.
- A detailed taxonomy is beyond the scope of this paper; for an extensive overview we refer the reader to Carvalho et al. [2019], Molnar [2022], and Ras et al. [2022].
- There is a tension between the desire for machines performing better than humans, and the requirement for machines to provide human-understandable explanations.
- Feature attribution methods are one of the most well-known XAI techniques, and have enjoyed substantial fame.
- However, there is currently no consensus on the reliability of XAI techniques, and several researchers have cast serious doubts on whether XAI solutions should be incorporated into guidelines and standards, or even deployed at all.

## The criticism on local feature attribution methods
- Feature attribution methods present heat maps or colored overlays, indicating the contribution of specific pixels to the prediction of the model on the input at hand.
- The root of the problem, we maintain, lies in the inability of humans to attribute meaning to a sub-symbolic encoding of information.
- What is needed is a systematic way to translate sub-symbolic representations to human-understandable ones, in a way that respects the way we assign meaning.
- This is represented schematically in Figure 3.
- Semantic match, which is a crude simplification of complex cognitive and linguistic phenomena, offers a handy conceptual tool to debug explanations.

## Distinguishing low-and high-level features
- Images are a prime example of 'unstructured data', meaning that a single feature has no intrinsic meaning
- In order to understand the utility of such methods, one needs to step back and consider what are the characteristics of the examples that give rise to the problem, and assess whether they generalize
- Not unsurprisingly, most of the debate revolves around images, given the astounding success of deep learning in the realm of computer vision
- Images are a prime example of 'low-level features', meaning that a single feature has no intrinsic meaning
- In order to extract information from a heatmap, one needs to understand the meaning of the low-level features
- For instance, a value of 180 in the feature corresponding to systolic blood pressure gives us a piece of information that we can understand and process, even without knowing other features of a patient

## Saving feature importance for low-level features
- Low-level features are features that are not associated with a specific concept.
- A post-hoc local feature attribution method can be used appropriately on low-level features when they have a predefined translation.
- For example, suppose a risk prediction model is trained on EHR data.
- When a patient is presented to the model, the model might provide a risk score associated with feature importance values for the patient's lab values.
- Suppose systolic blood pressure is marked as the most important factor increasing the risk of a specific patient.
- In this case, there is no ambiguity: such level of importance is attributed to systolic blood pressure and nothing else, and any user with sufficient training knows what systolic blood pressure is.
- Note that this has nothing to do with how the importance is calculated (e.g. if it takes into account feature interactions) or if it is a sensible level of importance; all we are stating is that the user can unambiguously understand what the importance is attributed to.
- In other words, the importance is attributed to something which semantically matches our concept of 'systolic blood pressure'.
- The user of such a model can then engage with the feature importance and assess whether it is sensible, while addressing questions like "Is this level of risk reasonable given a high importance of systolic blood pressure?"
- Such a question may be answered in the positive, if the clinician believes the value of systolic blood pressure is concerning, or in the negative, if for example, the clinician knows that the value of systolic blood pressure is a byproduct of medications she can control.
- An example of such an approach is displayed in Figure 5, where the top features contributing to clinical risk are displayed along with a color code indicating if they are risk increasing or decreasing.
- Crucially, semantic match allows the user of an explanation to engage with it, and to decide whether it is agreeable or not.
- In contrast, when the same risk prediction is trained on image data, this reasoning falls apart.

## Discussion
- Semantic match cannot be obtained for high-level feature importance.
- Structured data may still benefit from feature attribution, since for this data type low-level features have an in-built semantic match with human concepts.
- When it comes to the limit of this analysis, it is important to remark that, even in the presence of semantic match, explanations can still fail to deliver on their promise.
- In healthcare settings it is crucial to clearly identify user groups and provide proper training.
- To obviate problems deriving from failed semantic match, future work in this area could direct attention to generating explanations that comply with human categories by design, possibly even with special categories employed by the targeted user.
- One direction of future research explores the possibility to capture information on machine behavior in a symbolic manner by means of hybrid models.
- Another option might contemplate combining visual explanation with image segmentation, to fix the semantics of entities used in the explanation.
- Finally, more elaborate explanations may require access to ontologies regulating the relationship between entities (as in e.g. Lecue and Wu [2018], Liartis et al. [2021]), which should themselves match ontologies used -more or less implicitly-by humans.
