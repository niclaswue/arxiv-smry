---
title: "Semantic match: Debugging feature attribution methods in XAI for healthcare"
date: 2023-01-05T14:26:55.000Z
author: "Giovanni Cinà, Tabea E. Röber, Rob Goedhart, Ş. İlker Birbil"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-02080v2_jEM0d97-5.jpg" # image path/url
    alt: "Semantic match: Debugging feature attribution methods in XAI for healthcare" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02080)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02080).


# Abstract
- The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology.
- One thread of such debate concerns Explainable AI (XAI) and its promise to render AI devices more transparent and trustworthy.
- A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques and especially feature attribution methods, questioning their use and inclusion in guidelines and standards.
- Despite valid concerns, we argue that existing criticism on the viability of post-hoc local explainability methods throws away the baby with the bathwater by generalizing a problem that is specific to image data.
- We begin by characterizing the problem as a lack of semantic match between explanations and human understanding. To understand when feature importance can be used reliably, we introduce a distinction between feature importance of low- and high-level features. We argue that for data types where low-level features come endowed with a clear semantics, such as tabular data like Electronic Health Records (EHRs), semantic match can be obtained, and thus feature attribution methods can still be employed in a meaningful and useful way.

# Paper Content

## Introduction
- Artificial Intelligence (AI) is becoming more complex and there is an increase in interest in explainable AI (XAI).
- XAI has already shown to improve clinicians' ability to diagnose and assess prognoses of diseases.
- There is a wide variety of techniques for XAI, and many categorizations have been proposed in the literature.
- Local vs. global, and model-specific vs. model-agnostic approaches are two ways that techniques can be grouped.
- Many of the well-known techniques yield post-hoc explanations, meaning that they generate explanations for already trained, so-called 'black-box', models.
- Alternatively, there exist approaches that are considered inherently explainable, also known as white-box (or glass-box) models, such as decision trees and linear regression models.
- A detailed taxonomy is beyond the scope of this paper; for an extensive overview we refer the reader to Carvalho et al. [2019], Molnar [2022], and Ras et al. [2022].
- Feature attribution methods, such as techniques that assign to each feature a measure of how much it contributes to the calculation of the outcome according to the model, have enjoyed substantial fame.
- Popular techniques produce such explanations in a local fashion, and among the most famous there are SHAP Lundberg and Lee [2017], LIME Ribeiro et al. [2016], saliency maps Selvaraju et al. [2017], and integrated gradients Sundararajan et al. [2017].
- To give a sense of the success of these techniques, it suffices to say that some of them are now integrated as default explainability tools in widespread cloud machine learning services.
- Despite the enthusiasm and a growing community of researchers devoting energy to XAI, there is currently no consensus on the reliability of XAI techniques.
- Semantic match between explanations and human understanding is a difficult task and has been a problem for feature attribution methods in the past.

## The criticism on local feature attribution methods
- Feature attribution methods generate explanations in the form of heat maps or colored overlays, indicating the contribution of specific pixels to the prediction of the model on the input at hand.
- One problem with these explanations is that what looks like a plausible explanation at first may turn out to be ungrounded or spurious explanations when subjected to closer scrutiny.
- A semantic match would provide content of the bottom-left node and a suitable translation in order to obtain a semantic match: the explanation of a certain sub-symbolic representation encoded by the machine should have (i) a clearly defined meaning and (ii) an unambiguous way to translate to human terms with the same (or very similar) meaning.
- Semantic match, which is a crude simplification of complex cognitive and linguistic phenomena, offers a handy conceptual tool to debug explanations.

## Distinguishing low-and high-level features
- Images are a prime example of 'unstructured data',
- In order to understand the utility of such methods, one needs to step back and consider what are the characteristics of the examples that give rise to the problem, and assess whether they generalize
- Most of the debate revolves around images, given the astounding success of deep learning in the realm of computer vision
- Images are a prime example of 'unstructured data', meaning that a single feature has no intrinsic meaning: a certain color value of a pixel means nothing by itself
- Only a pattern of values for a group of features can be attributed meaning, e.g. a cloud of pixels with the shape and color of a dog's head
- In other words, we recognize properties or entities in an image by matching activation patterns with our visual intuitions
- Following a widespread habit in the ML community, we will refer to patterns in groups of features as 'high-level features', while single features, i.e. the entries of the vector representing an input, will be dubbed 'low-level features' by contrast
- With this terminology, we can succinctly state that in images only high-level features can be attributed meaning while low-level features cannot
- On the contrary, in structured data each feature is usually conferred specific meaning; e.g. in EHR data a certain value might contain the information pertaining to the blood pressure of a patient at a certain time
- When such low-level features are not defined with a specific protocol -such as the ones for vital signs measurements in clinical settings -they refer to standard concepts in natural language
- We can, therefore, easily interpret what these low-level features mean regardless of the values of the other features
- For instance, a value of 180 in the feature corresponding to systolic blood pressure gives us a piece of information that we can understand and process, even without knowing other features of a patient
- To be sure, there are also high-level features in the case of structured data. Continuing with the example of EHRs, a high-level feature could be for example a phenotype which is not encoded explicitly as a feature but instead depends on the combination of existing features, say glucose, BMI, and so on
- Hence, the crucial difference between structured and unstructured data is that the former has clear meaning for the low-level features, while on the high-level features the two data types behave similarly

## Saving feature importance for low-level features
- Semantic match allows users to unambiguously understand what the importance is attributed to.
- Low-level features (lab values) have a predefined translation and can be attributed using a post-hoc local feature attribution method.
- When training a risk prediction model on image data, semantic match falls apart and clinicians cannot trust what they see in the image.

## Discussion
- Semantic match cannot be obtained for high-level feature importance
- Structured data may still benefit from feature attribution, since for this data type low-level features have an in-built semantic match with human concepts
- When it comes to the limit of this analysis, it is important to remark that, even in the presence of semantic match, explanations can still fail to deliver on their promise
- To obviate problems deriving from failed semantic match, future work in this area could direct attention to generating explanations that comply with human categories by design
