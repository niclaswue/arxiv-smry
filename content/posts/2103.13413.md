---
title: "Vision Transformers for Dense Prediction"
date: 2021-03-24T18:01:17.000Z
author: "René Ranftl, Alexey Bochkovskiy, Vladlen Koltun"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2103-13413v1.webp" # image path/url
    alt: "Vision Transformers for Dense Prediction" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2103.13413)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2103.13413).


# Abstract
- We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks.
- The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks.
- Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available.

# Paper Content

## Introduction
- Almost all dense prediction architectures are based on convolutional networks
- Downsampling of features in the encoder leads to loss of resolution and granularity in the decoder
- A transformer-based architecture, called the dense prediction transformer, avoids these problems

## Related Work
- Fully-convolutional networks are the prototypical architecture for dense prediction
- Many variants of this basic pattern have been proposed over the years, however, all existing architectures adopt convolution and subsampling as their fundamental elements in order to learn multi-scale representations
- Several works propose to progressively upsample representations that have been pooled at different stages
- Others use dilated convolutions or parallel multi-scale feature aggregation at multiple scales
- More recent architectures maintain a high-resolution repre-sentation together with multiple lower-resolution representations throughout the network
- Attention-based models have been the architecture of choice for learning strong models for natural language processing (NLP)
- Transformers are set-to-set models that are based on the self-attention mechanism
- Transformer models have been particularly successful when instantiated as high-capacity architectures and trained on very large datasets
- There have been several works that adapt attention mechanisms to image analysis
- In particular, it has recently been demonstrated that a direct application of token-based transformer architectures that have been successful in NLP can yield competitive performance on image classification

## Architecture
- The vision transformer maintains the spatial resolution of the initial embedding throughout all transformer stages.
- MHSA is an inherently global operation, as every token can attend to and thus influence every other token.
- Consequently, the transformer has a global receptive field at every stage after the initial embedding.
- The embedding procedure for ViT-Base and ViT-Large projects the flattened patches to dimension D = 768 and D = 1024, respectively.
- Since both feature dimensions are larger than the number of pixels in an input patch, this means that the embedding procedure can learn to retain information if it is beneficial for the task.
- Features from the input patches can in principle be resolved with pixel-level accuracy.
- Similarly, the ViT-Hybrid architecture extracts features at 1 16 the input resolution, which is twice as high as the lowest-resolution features that are commonly used with convolutional backbones.

## Experiments
- DPT can improve accuracy when compared to convolutional networks with a similar capacity
- DPT can be configured to improve accuracy
- DPT can be used for monocular depth estimation and semantic segmentation

### Monocular Depth Estimation
- Monocular depth estimation is typically cast as a dense regression problem.
- Massive metadatasets can be constructed from existing sources of data, provided that some care is taken in how different representations of depth are unified into a common representation and that common ambiguities (such as scale ambiguity) are appropriately handled in the training loss.
- Since transformers are known to realize their full potential only when an abundance of training data is available, monocular depth estimation is an ideal task to test the capabilities of DPT.
- We closely follow the protocol of Ranftl et al. [30].
- We learn a monocular depth prediction network using a scale-and shift-invariant trimmed loss that operates on an inverse depth representation, together with the gradient-matching loss proposed in [22].
- We construct a meta-dataset that includes the original datasets that were used in [30] (referred to as MIX 5 in that work) and extend it with with five additional datasets ( [18,43,44,46,47]).
- Zero-shot cross-dataset transfer is successfully performed on different datasets that were not seen during training.
- Relative performance is computed with respect to the original MiDaS model [30].
- DPT-Hybrid achieves this with a comparable network capacity (Table 9), while DPT-Large is about 3 times larger than MiDaS.
- To ensure that the observed improvements are not only due to the enlarged training set, we retrain the fullyconvolutional network used by MiDaS on our larger metadataset MIX 6.
- While the fully-convolutional network indeed benefits from the larger training set, we observe that both DPT variants still strongly outperform this network. This shows that DPT can better benefit from increased training set size, an observation that matches previous findings on transformer-based architectures in other fields.
- The quantitative results are supported by visual comparisons in Figure 2.

### Semantic Segmentation
- Semantic segmentation is a discrete labeling task
- We use the same backbone and decoder structure as in previous experiments
- We use an output head that predicts at half resolution and upsamples the logits to full resolution using bilinear interpolation
- The encoder is again initialized from ImageNet-pretrained weights, and the decoder is initialized randomly
- We employ a cross-entropy loss and add an auxiliary output head together with an auxiliary loss to the output of the penultimate fusion layer
- We set the weight of the auxiliary loss to 0.2
- Dropout with a rate of 0.1 is used before the final classification layer in both heads
- We use SGD with momentum 0.9 and a polynomial learning rate scheduler with decay factor 0.9
- We use batch normalization in the fusion layers and train with batch size 48
- Images are resized to 520 pixels side length
- We use random horizontal flipping and random rescaling in the range ∈ (0.5, 2.0) for data augmentation
- We train on square random crops of size 480
- We set the learning rate to 0.002
- We use multi-scale inference at test time and report both pixel accuracy (pixAcc) as well as mean Intersectionover-Union (mIoU)

### Ablations
- We examine a number of aspects and technical choices in DPT via ablation studies
- We choose monocular depth estimation as the task for our ablations and follow the same protocol and hyper-parameter settings as previously described
- We use a reduced meta-dataset that is composed of three datasets [45,46,47] and consists of about 41,000 images
- We split each dataset into a training set and a small validation set of about 1,000 images total
- We report results on the validation sets in terms of relative absolute deviation after affine alignment of the predictions to the ground truth [30]
- Unless specified otherwise, we use ViT-Base as the backbone architecture
- Skip connections. Convolutional architectures offer natural points of interest for passing features from the encoder to the decoder, namely before or after downsampling of the representation. Since the transformer backbone maintains a constant feature resolution, it is not clear at which points in the backbone features should be tapped. We evaluate several possible choices in Table 6 (top)
- We observe that it is beneficial to tap features from layers that contain low-level features as well as deeper layers that contain higher-level features. We adopt the best setting for all further experiments.
- We perform a similar experiment with the hybrid architecture in Table 6 (bottom), where R0 and R1 refer to using features from the first and second downsampling stages of the ResNet50 embedding network. We observe that using low-level features from the embedding network leads to better performance than using features solely from the transformer stages.
- Readout token. Table 7 examines various choices for implementing the first stage of the Reassemble block to handle the readout token. While ignoring the token yields good performance, projection provides slightly better performance on average. Adding the token, on the other hand, yields worse performance than simply ignoring it.
- We use projection for all further experiments.
- shown in Table 8. ViT-Large outperforms all other backbones but is also almost three times larger than ViT-Base and ViT-Hybrid. ViT-Hybrid outperforms ViT-Base with a similar number of parameters and has comparable performance to the large backbone. As such it provides a good trade-off between accuracy and capacity. ViT-Base has comparable performance to ResNext101-WSL, while ViT-Hybrid and ViT-Large improve performance even though they have been pretrained on significantly less data.
- Note that ResNext101-WSL was pretrained on a billion-scale corpus of weakly supervised data [25] in addition to ImageNet pretraining. It has been observed that this pretraining boosts the performance of monocular depth prediction [30]
- This architecture corresponds to the original MiDaS architecture.
- We finally compare to a recent variant of ViT called DeIT [38]. DeIT trains the ViT architecture with a more data-efficient pretraining procedure. Note that the DeIT-Base architecture is identical to ViT-Base, while DeIT-Base-Dist introduces an additional distillation token, which we ignore in the Reassemble operation. We observe that DeIT-Base-Dist indeed improves performance when compared to ViT-Base. This indicates that similarly to convolutional architectures, improvements in pretraining procedures for image classification can benefit dense prediction tasks.
- Inference resolution. While fully-convolutional architectures can have large effective receptive fields in their deepest layers, the layers close to the input are local and have small receptive fields. Performance thus suffers heavily when performing inference at an input resolution that is significantly different from the training resolution.
- in every layer. We conjecture that this makes DPT less dependent on inference resolution. To test this hypothesis, we plot the loss in performance of different architectures when performing inference at resolutions higher than the training resolution of 384×384 pixels. We plot the relative decrease in performance in percent with respect to the performance of performing inference at the training resolution in Figure 4. We observe that the performance of DPT variants indeed degrades more gracefully as inference resolution increases.

## Conclusion
- We have introduced the dense prediction transformer, DPT, a neural network architecture that effectively leverages vision transformers for dense prediction tasks.
- Our experiments on monocular depth estimation and semantic segmentation show that the presented architecture produces more fine-grained and globally coherent predictions when compared to fully-convolutional architectures.
- Similar to prior work on transformers, DPT unfolds its full potential when trained on large-scale datasets.
- Figure 1. Left: Architecture overview. The input image is transformed into tokens (orange) either by extracting non-overlapping patches followed by a linear projection of their flattened representation (DPT-Base and DPT-Large) or by applying a ResNet-50 feature extractor (DPT-Hybrid). The image embedding is augmented with a positional embedding and a patch-independent readout token (red) is added. The tokens are passed through multiple transformer stages. We reassemble tokens from different stages into an image-like representation at multiple resolutions (green). Fusion modules (purple) progressively fuse and upsample the representations to generate a fine-grained prediction.
- Center: Overview of the Reassembles operation. Tokens are assembled into feature maps with 1s the spatial resolution of the input image. Right: Fusion blocks combine features using residual convolutional units[23] and upsample the feature maps.
- Figure 2. Sample results for monocular depth estimation. Compared to the fully-convolutional network used by MiDaS, DPT shows better global coherence (e.g., sky, second row) and finer-grained details (e.g., tree branches, last row).
- Figure 3. Sample results for semantic segmentation on ADE20K (first and second column) and Pascal Context (third and fourth column). Predictions are frequently better aligned to object edges and less cluttered.
- Figure 4. Relative loss in performance for different inference resolutions (lower is better). 10.82 (-13.2%) 0.089 (-31.2%) 0.270 (-17.5%) 8.46 (-64.6%) 8.32 (-12.9%) 9.97 (-30.3%)
