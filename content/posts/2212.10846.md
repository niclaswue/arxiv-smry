---
title: "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models"
date: 2022-12-21T08:39:36.000Z
author: "Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, Steven C. H. Hoi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10846v1.webp" # image path/url
    alt: "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10846)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10846).


# Abstract
- Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks
- However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily
- End-to-end training on vision and language data may bridge the disconnections, but is inflexible and computationally expensive.
- To address this issue, we propose Img2Prompt, a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training.
- In order to provide such prompts, we further employ LLM-agnostic models to provide prompts that can describe image content and self-constructed question-answer pairs, which can effectively guide LLM to perform zero-shot VQA tasks.
- Img2Prompt offers the following benefits: 1) It can flexibly work with various LLMs to perform VQA. 2)~Without the needing of end-to-end training, it significantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo~\cite{Deepmind:Flamingo2022} by 5.6\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms few-shot methods by as much as 20\%.

# Paper Content

## Introduction
- Visual question answering is a task that can be used to understand a person's environment
- A diverse set of VQA datasets have been proposed, some focusing on image recognition and others on logical reasoning
- However, human annotations are expensive to obtain and may introduce a variety of human biases
- This has led researchers to zero-shot VQA methods that do not require ground-truth question-answer annotations
- Recently, large language models (LLMs) have demonstrated excellent capabilities to perform tasks with zero in-domain data
- As a result, recent approaches have resorted to leverage LLMs in zero-shot VQA
- However, applying LLMs to VQA tasks is less than straightforward, due to the modality disconnect between vision and language and the task disconnect between language modeling and question answering

## Related Work

### Recent Advances in VQA Methods
- As a multi-modal evaluation benchmark, Visual Question Answering (VQA) that requires the model to answer a natural language question according to the image, had been actively studied.
- With vision-language models [20,67,36,33,34,68,57,51,32,22,14] that are pretrained on large-scale image-text datasets, VQA tasks have advanced rapidly through the fine-tuning of these models in specific VQA dataset.
- However, experimental results in [49] show that these methods still struggle to answer questions requiring reasoning ability.

### LLM for Zero/Few-Shot VQA Tasks

### Answer Extraction
- Extracts noun phrases, verb phrases, adjective phrases, numbers, and boolean-typed words from the current VQA image
- Generates captions for synthetic questions using an off-the-shelf question-relevant caption generation module
- Shows some extracted answer candidates in Figure 2 and Appendix A.5

### Question Generation
- We extract answer candidates from a question and the context around it
- We use different question generation methods, one is based on templates and the other is based on a neural network
- We use question-answer pairs as examples of how to do LLM in-context learning

### Question-relevant Caption Prompt
- In order to generate captions that are relevant to the question, we use Imagegrounded Text Encoder (ITE) to assign a similarity score to any pair of image v and textual question q.
- We use GradCAM to generate a coarse localisation map highlighting matching image regions given a question.
- Briefly, GradCAM qualifies the cross-attention scores from the Transformer network by the gradient of ITE similarity function sim(v, q) with respect to the cross-attention scores.
- Having obtained the patch relevance r, we sample a subset of image patches with probability proportional to patch relevance r.
- After that, we generate captions from the sampled image patches using top-k sampling.
- To generate semantically meaningful captions, a short prompt, "a picture of," is also fed into the text decoder.
- We repeat this process M times for each image to generate M diverse captions, and keep only captions that are not exact substrings of others.
- However, due to the non-deterministic nature of top-k sampling, the caption model may generate noisy captions that have a negative impact on performance.
- To remove noisy captions, we use ITE to calculate the similarity score between the generated caption and sampled question-relevant image patches, and filter captions with less than 0.5 matching scores.

### Prompt Design
- We construct complete prompts for LLM by concatenating the instruction, captions, and QA exemplars.
- The instruction text is "Please reason the answers of question according to the contexts." The caption prompt is formatted as "Contexts: [all captions]". Individual QA exemplars are formatted as "Question: [question] Answer: [answer]" and concatenated.
- To select the most informative prompt, we first count the frequency of the synthetic answer candidates in 100 generated captions. We then select 30 answer candidates with highest frequencies and generate one question for each. Also, we include 30 answers with the lowest frequency and one caption containing each answer.
- See ยง4.5 for analysis of caption selection strategies.

## Experiment
- Validates the efficacy of Img2Prompt by comparing it with other zero-shot and few-shot VQA methods
- Performs ablation studies on important design choices, such as prompt patterns and caption selection strategies, to understand their effect
- Shows qualitative examples and includes discussion on observed failure cases

### Environment Setup

### Main Results
- State-of-the-art results on zero-shot evaluation with plug-in frozen LLMs
- Img2Prompt surpasses PICa, the best prior zero-shot model with frozen LLMs, by a significant margin (45.6 versus 17.7 on OK-VQA), thereby establishing a new state-of-the-art
- In addition, we remark that despite PICa uses frozen LLMs, it requires training samples to build prompts. In contrast, our method generates question-answers with no access to VQA samples, thus fully fulfilling the zero-shot requirements
- Scaling effect of LLMs and their emergent capabilities on VQA
- When increasing the number of parameters of LLMs from 6.7B to 175B, we see a 3-10 points improvement in VQA scores across datasets. This shows that stronger language modeling capabilities help better comprehend the question, thus giving more accurate answers. Such a trend is even more clear and consistent on OK-VQA and A-OKVQA, whose questions demand commonsense reasoning and external knowledge that LLMs excel at providing. This corroborates our belief that LLMs are beneficial to VQA
- Another intriguing phenomenon we observe is that the effect of scaling LLMs becomes obvious only when the model size becomes sufficiently large, for example, when using 30B or larger models, while not entirely predictable on smaller ones (6.7B and 13B)
- Competitive performance with end-to-end pretraining and few-shot models
- Img2Prompt obtains superior performance to most models with end-to-end pretraining, as well as those evaluated in few-shot setups. For example, on VQAv2 our method surpasses Flamingo 80B , which cost over 500K TPU hours and billion-scale datasets to train, by a margin of 5.6 points. On A-OKVQA, Img2Prompt more than doubles the best reported results so far, from ClipClap. The only a few exceptions are on OK-VQA, where our method obtains better results than Flamingo 9B , yet is not able to stay on par with Flamingo 80B . Considering that Img2Prompt is flexible to adapt to updated and stronger LLMs with zero extra training cost, we consider it a more approachable solution to practical adoption of VQA systems, than those trained end-to-end
- We also include comparisons with supervised models in Appendix A.5. Img2Prompt achieves better performance than most supervised models, despite the fact that it uses zero training data and evaluated in a zero-shot setup.

### Analysis on Question Generation Methods
- Agnostic performs the worst
- Neural performs the best
- Agnostic QA pairs contain information irrelevant to the current image and may mislead the LLM
- Template questions feature little linguistic variation and hence cannot demonstrate different QA strategies
- Neural has the most relevant information and the most linguistic diversity
- Answer hit rate (AHR) is defined as the proportion of QA pairs containing the ground-truth answer. Answer noise rate (ANR) is defined as the ratio of ground-truth answers to the total number tokens in the exemplar prompts.
- Exemplar prompts generated from question-relevant captions have a higher AHR, hence enhancing the VQA performance.

### Ablation on Caption Selection
- Table 6 shows the different caption selection strategies
- The Max Frequency strategy does not provide more information than the Min Frequency strategy
- The Min Frequency strategy chooses captions that can provide some information not in the QA pairs, providing a performance boost

### Ablation Study on Prompt Design
- We have two options for constructing LLM prompts
- The first option is to append a synthetic QA pair after the caption that the QA pair is generated from. This can be described as CQA-CQA-CQA, where C, Q, A stand for caption, synthetic question, and synthetic answer respectively.
- Alternatively, we can present all captions at once, followed by all question-answer pairs, which we denote as CCC-QAQAQA.
- Experimentally, the second design performs significantly better than the first.

### Examples and Failure Case Analysis
- The LLM can draw on background knowledge to make predictions
- The LLM is able to make inferences based on qualitative physics, but is not able to predict the right answer for captions that do not provide information about the man's job
- The LLM is able to draw on background knowledge to make predictions, but is not able to make inferences based on qualitative physics when captions do not provide information about the man's job

## Limitation
- The proposed approach incurs extra inference overhead
- On an 8x100 machine, our current implementation brings about 24.4% additional computational time on top of the inference time of 175B OPT
- We note that further reduction of the overhead can be obtained by shortening the prompt, trading accuracy for speed.

## Conclusion
- Img2Prompt is a plug-and-play module designed to exploit the knowledge and reasoning power of large language models (LLMs)
- Concretely, Img2Prompt provides visual information and task guidance to LLMs in the format of easily-digestible prompts
- This eliminates the requirement for the expensive end-to-end vision-language alignment, increasing model deployment flexibility while decreasing model deployment cost
- The experiments show that Img2Prompt enables different LLMs to achieve comparable or even superior zero-shot VQA performance to other methods that require costly end-to-end training.

### A.2 Broader Impact Statement
- The proposed solution achieves comparable or superior performance to other zero-shot VQA methods
- Social-economic biases exist in the datasets, LLMs, and VQA systems presented in this paper, including Img2Prompt
- Future work could assess the magnitude of this bias and mitigate its impact

### A.4 Details about Question-Relevant Caption Generation
- Concretely, we denote features of image patches extracted by ITE as f i v โ R KรD i v and question features as , where i is the number of the layer of ITE, K is the number of images patches, L is the number of token in the given question, D i v is the dimension of patch feature in the i-th layer of ITE network and D i q is the dimension of textual feature in the i-th layer of ITE network.
- For cross-attention head in i-th layer, the cross-attention scores W i between each image patch and each token in question can be calculated directly as where q is the query head and q is the key head in the i-th layer of ITE network.
- With Equation 1, we obtain a cross-attention matrix W i โ R LรK , where each row is the cross-attention scores of each token in the question over all image patches.
- Specifically, the attention matrix W i can be regarded as the patch importance for ITE to calculate the similarity of whole image and question, but it still contains redundancy that contributes only a minor performance loss [7], indicating that some patches are uninformative.
- In order to find these less relevant image patches, we follwing GradCAM and compute the derivative of the cross-attention score from ITE function sim(v, q), i.e., โ sim(v, q)/โW , and multiplying its gradient matrix with the cross-attention scores element-wisely.
- The relevance of the k th image patch with the question, r i k , can be computed as the average over H attention heads and the sum over L textual tokens: where h is the index of attention heads and i is the layer index of ITE.
