---
title: "Amortized Inference for Causal Structure Learning"
date: 2022-05-25T17:37:08.000Z
author: "Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, Bernhard Schölkopf"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2205-12934v4_mEoei1tBZ.jpg" # image path/url
    alt: "Amortized Inference for Causal Structure Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2205.12934)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2205.12934).


# Abstract
- Inferring causal structure poses a combinatorial search problem that typically involves evaluating structures with a score or independence test.
- The resulting search is costly, and designing suitable scores or tests that capture prior knowledge is difficult.
- In this work, we propose to amortize causal structure learning. Rather than searching over structures, we train a variational inference model to directly predict the causal structure from observational or interventional data.
- This allows our inference model to acquire domain-specific inductive biases for causal discovery solely from data generated by a simulator, bypassing both the hand-engineering of suitable score functions and the search over graphs.
- The architecture of our inference model emulates permutation invariances that are crucial for statistical efficiency in structure learning, which facilitates generalization to significantly larger problem instances than seen during training.
- On synthetic data and semisynthetic gene expression data, our models exhibit robust generalization capabilities when subject to substantial distribution shifts and significantly outperform existing algorithms, especially in the challenging genomics domain.
- Our code and models are publicly available at: https://github.com/larslorch/avici.

# Paper Content

## Introduction
- Learning the causal structure among a set of variables is a fundamental task in various scientific disciplines
- Inferring this causal structure from observations of the variables is a difficult inverse problem
- The solution space of potential causal structures, usually modeled as directed graphs, grows superexponentially with the number of variables
- To infer a causal structure, standard methods have to search over potential graphs, usually maximizing either a graph scoring function or testing for conditional independences
- Specifying realistic inductive biases is universally difficult for existing approaches to causal discovery
- Score-based methods use strong assumptions about the data-generating process, such as linearity (Shimizu et al., 2006), specific noise models (Hoyer et al., 2008;Peters and Bühlmann, 2014), and the absence of measurement error (cf. Scheines and Ramsey 2016;Zhang et al. 2017), which are difficult to verify
- Conversely, constraint-based methods do not have enough domain-specific inductive bias
- Even with an arbitrarily large dataset, they are limited to identifying equivalence classes that may be exponentially large
- Moreover, the search over directed graphs itself may introduce unwanted bias and artifacts
- The intractable search space ultimately imposes hard constraints on the causal structure, e.g., the node degree (Spirtes et al., 2000), which limits the suitability of search in real-world domains
- In the present work, we propose to amortize causal structure learning. In other words, our goal is to optimize an inference model to directly predict a causal structure from a provided dataset.
- We show that this approach allows inferring causal structure solely based on synthetic data generated by a simulator of the data-generating process we are interested in
- Much effort in the sciences, for example, goes into the development of realistic simulators for high-impact and yet challenging causal discovery domains, like gene regulatory networks (Schaffter et al., 2011;Dibaeinia and Sinha, 2020), fMRI brain responses (Buxton, 2009;Bassett and Sporns, 2017), and chemical kinetics (Anderson and Kurtz, 2011;Wilkinson, 2018)
- Our approach based on amortized variational inference (AVICI) ultimately allows us to both specify domain-specific inductive biases not easily represented by graph scoring functions and bypass the problems of structure search
- Our model architecture is permutation in-and equivariant with respect to the observation and variable dimensions of the provided dataset, respectively, and generalizes to significantly larger problem instances than seen during training
- On synthetic data and semisynthetic gene expression data, our approach significantly outperforms existing algorithms for causal discovery, often by a large margin
- Moreover, we demonstrate that our inference models induce calibrated uncertainties and robust behavior when subject to substantial distribution shifts of graphs, mechanisms, noise, and problem sizes. This suggests that our pretrained models are not only fast but also both reliable and versatile for future downstream use.

### Causal Structure
- The causal structure of a set of variables is the directed graph over the variables that represents all direct causal effects among the variables.
- A variable has a direct causal effect on another variable if intervening on the first variable affects the outcome of the second variable independent of the other variables.
- We assume causal sufficiency, which means that x contains all common causal parents of the variables.

### Related Work

### Variational Objective
- The data-generating distribution p(D) models the domain in which causal structures are inferred
- The observations D = {x 1 , . . . , x n } are generated by sampling from a distribution over causal structures p(G) and then obtaining realizations from a data-generating mechanism p(D | G)
- The AVICI objective in (3) intentionally targets the forward KL D KL (p q( • ; θ)), which requires optimizing E p(G,D) [log q(G; θ)]. This choice implies that we both model the density q(G; θ) explicitly and assume access to samples from the true data-generating distribution p(G, D). Minimizing the forward KL enables us to infer causal structures in arbitrarily complex domains-that is, even domains where it is difficult to specify an explicit likelihood p(D | G). Moreover, the forward KL typically yields more reliable uncertainty estimates since it does not suffer from the variance underestimation problems common to the reverse KL (Bishop and Nasrabadi, 2006).
- Variational inference usually optimizes the reverse KL D KL (q( p), which involves the reconstruction term et al., 2017). This objective requires a tractable marginal likelihood p(D | G). Unless inferring the mechanism parameters jointly (e.g. Brouillard et al. 2020;Lorch et al. 2021), this requirement limits inference to conjugate models with linear Gaussian or categorical mechanisms that assume zero measurement error (Geiger and Heckerman, 1994;Heckerman et al., 1995), which are not justified in practice (Friston et al., 2000;Schaffter et al., 2011;Runge et al., 2019;Dibaeinia and Sinha, 2020). Furthermore, unless the noise scale is learned jointly, likelihoods can be sensitive to the measurement scale of x (Reisach et al., 2021).

## Inference Model
- The variational distribution q(G; θ) is a choice for the distribution of the unknown function G.
- The inference model f φ predicts θ given D, which is a data set.
- The training procedure for optimizing the model parameters φ and for learning causal graphs with acyclicity constraints is described.

### Variational Family
- Inference model: Factorized variational family
- When interventions or gene knockouts are performed, we set o i j = (x i j , u i j ) and u i j ∈ {0, 1} indicating whether variable j was intervened upon in sample i.

### Model Architecture
- The symmetries inherent to the task of causal structure learning include that f φ should be permutation invariant across the sample dimension (axis n), permutation equivariant across the variable dimension (axis d), and apply to any d, n ≥ 1.
- To maximize statistical efficiency, f φ should satisfy the following three properties: first, f φ should be permutation invariant across the sample dimension (axis n); secondly, shuffling the samples should not influence the prediction; and thirdly, f φ should be permutation equivariant across the variable dimension (axis d).
- To parameterize f φ as a neural network, we first map each o i j to a real-valued vector using a position-wise linear layer, and then operate over a continuous, three-dimensional tensor of n rows for the observations, d columns for the variables, and feature size k.

### Acyclicity
- Cyclic causal effects can occur when modeling stationary distributions of dynamical systems
- However, certain domains may be more accurately modeled by acyclic structures, and this insight can be used to improve the efficiency of the optimization problem
- We use the insight by Lee et al. (2019a) to approximate and differentiate through the largest eigenvalue of f φ (D) (Golub and Van der Vorst, 2000), which is significantly more efficient than using matrix powers

### Optimization
- The algorithm uses stochastic optimization to train the parameters φ of the inference model.
- The expectations over p(G, D) inside L and F are approximated using samples from the datagenerating process of the domain.
- When enforcing acyclicity, causal discovery algorithms often use the augmented Lagrangian method for constrained optimization.

## Experimental Setup
- The evaluation of causal discovery algorithms is difficult
- We deal with this difficulty by using simulated data with known causal structure and by controlling for various aspects of the task
- In Appendix E, we additionally report results on a real-world proteomics dataset.

### Domains and Simulated Components
- We study three domains: two classes of structural causal models (SCMs) as well as semisynthetic single-cell expression data of gene regulatory networks (GRNs).
- In addition to SCMs, we consider the challenging domain of GRNs (GRN) using the simulator of Dibaeinia and Sinha (2020).
- Contrary to SCMs, gene expression samples correspond to draws from the steady state of a stochastic dynamical system that varies between cell types (Huynh-Thu and Sanguinetti, 2019).
- In the GRN domain, we use subgraphs of the known S. cerevisiae and E. coli GRNs and their effect signs whenever known.
- To illustrate the distribution shift from p(D) to p(D), Figure 2 shows a set of graph, mechanism, and noise distribution samples in the RFF domain.

### Evaluation Metrics
- All experiments throughout this paper are conducted on datasets that AVICI has never seen during training
- To assess how well a predicted structure reflects the ground truth, we report the structural Hamming distance (SHD) and the structural intervention distance (SID) (Peters and Bühlmann, 2015)
- While the SHD simply reflects the graph edit distance, the SID quantifies the closeness of two graphs in terms of their interventional adjustment sets
- For these metrics and for single-edge precision, recall, and F1 score, we convert the posterior probabilities predicted by AVICI to hard predictions using a threshold of 0.5
- We evaluate the uncertainty estimates by computing the areas under the precision-recall curve (AUPRC) and the receiver operating characteristic (AUROC) (Friedman and Koller, 2003)

### Inference Model Configuration
- The three inference models are trained on three domains
- The three inference models share the same hyperparameters for the architecture and optimization, except for the dropout rate
- The inference models are trained with different datasets, with some containing 50 interventional samples
- The inference models are tested with different datasets and with the acyclicity constraint for the SCM domains LINEAR and RFF

### Out-Of-Distribution Generalization
- In experiments, study the generalization capabilities of a model across the spectrum of test distributions
- In the first set of experiments, study the generalization capabilities of the model across the spectrum of test distributions described in Section 5.1
- Perform causal discovery from n = 1000 observations in systems of d = 30 variables
- Starting from the training distribution p(D), incrementally introduce the described distribution shifts in the causal structures, causal mechanisms, and finally noise
- The top row of Figure 3 visualizes the results of an empirical sensitivity analysis
- In addition to the metrics in Section 5.2, also report the percentage of predicted graphs that are acyclic
- In the LINEAR domain, AVICI performs very well in all metrics and hardly suffers under distribution shift
- In contrast, GRN is the most challenging problem domain and the performance degrades more significantly for the o.o.d. scenarios
- AVICI can perform better under certain distribution shifts than in-distribution, e.g., in GRN
- This is because AVICI empirically performs better at predicting edges adjacent to large-degree nodes, a common feature of the E. coli and S. cerevisiae graphs not present in the Erdős-Rényi training structures
- In addition, acyclicity is perfectly satisfied for LINEAR and RFF and that AUPRC and AUROC do not suffer as much from distributional shift as the metrics based on thresholded point estimates
- In Appendix E.1, we additionally report results for generalization from LINEAR to RFF and vice versa, i.e., to entirely unseen function classes of causal mechanisms

### Benchmarking
- Next, we benchmark AVICI against existing algorithms.
- Using only observational data, we compare with the PC algorithm (Spirtes et al., 2000), GES (Chickering, 2003), LiNGAM (Shimizu et al., 2006), DAG-GNN (Yu et al., 2019), and GraN-DAG (Lachapelle et al., 2020).
- Mixed with interventional data, we compare with GIES (Hauser and Bühlmann, 2012), IGSP (Wang et al., 2017), and DCDI (Brouillard et al., 2020).
- We tune the important hyperparameters of each baseline on held-out task instances of each domain.
- When computing the evaluation metrics, we favor methods that only predict (interventional) Markov equivalence classes by orienting undirected edges correctly when present in the ground truth.
- Details on the baselines are given in Appendix D.
- The benchmarking is performed on the fully o.o.d. domain distributions p(D), i.e., under distribution shifts on causal graphs, mechanisms, and noise distributions w.r.t. the training distribution of AVICI.
- Table 1 shows the SID and F1 scores of all methods given n = 1000 observations for d = 30 variables.
- We find that the AVICI model trained on LINEAR outperforms all baselines, both given observational or interventional data, despite operating under significant distribution shift.
- Only GIES achieves comparable accuracy. The same holds for RFF, where GraN-DAG and DCDI perform well but ultimately do not reach the accuracy of AVICI.
- In the GRN domain, where inductive biases are most difficult to specify, classical methods fail to infer plausible graphs. However, provided interventional data, AVICI can use its learned inductive bias to infer plausible causal structures from the noisy gene expressions, even under distribution shift.
- This is a promising step towards reliable structure discovery in fields like molecular biology.
- Even without gene knockout data, AVICI achieves nontrival AUROC and AUPRC while classical methods predict close to randomly.

### Ablations
- The importance of key architecture components of the inference network
- The performance of the model drops significantly when attending only over axis d and aggregating information over axis n only once through pooling after the 2L self-attention layers
- We find that the performance of the model is not statistically different to our simpler model in Eq. (6)

## Discussion
- We proposed AVICI, a method for inferring causal structure by performing amortized variational inference over an arbitrary data-generating distribution.
- Our approach leverages the insight that inductive biases crucial for statistical efficiency in structure learning might be more easily encoded in a simulator than in an inference technique.
- This is reflected in our experiments, where AVICI solves structure learning problems in complex domains intractable for existing approaches (Dibaeinia and Sinha, 2020).
- Our method can likely be extended to other typically difficult domains, including settings where we cannot assume causal sufficiency (Bhattacharya et al., 2021).
- Our approach will continually benefit from ongoing efforts in developing (conditional) generative models and domain simulators.
- Using AVICI still comes with several trade-offs. First, while optimizing the dual program empirically induces acyclicity, this constraint is not satisfied with certainty using the variational family considered here.
- Moreover, similar to most amortization techniques (Amos, 2022), AVICI gives no theoretical guarantees of performance.
- Some classical methods can do so in the infinite sample limit given specific assumptions on the data-generating process (Peters et al., 2017). However, future work might obtain guarantees for AVICI that are similar to learning theory results for the bivariate causal discovery case (Lopez-Paz et al., 2015).
- Our experiments demonstrate that our inference models are highly robust to distributional shift, suggesting that the trained models could be useful out-of-the-box in causal structure learning tasks outside the domains studied in this paper.

## A Domain Specification and Simulation
- Linear function (a) weights w Target nodes
- Random 50% of nodes
- Intervention values
- In Erdős-Rényi graphs, each edge is sampled independently with a fixed probability (Erdős and Rényi, 1959).
- Scale-free graphs are generated by a sequential preferential attachment process, where in-or outgoing edges of node i to the previous i − 1 nodes are sampled with probability ∝ deg(j) α (Barabási and Albert, 1999).
- Watts-Strogatz graphs are k-dimensional lattices, whose edges get rewired globally to random nodes with a specified probability (Watts and Strogatz, 1998).
- The stochastic block model generalizes Erdős-Rényi to capture community structure. Splitting the nodes into a random partition of so-called blocks, the inter-block edge probability is dampened by a multiplying factor compared to the intrablock probability, also tuned to result in O(d) edges in expectation (Holland et al., 1983).
- Lastly, geometric random graphs model connectivity based on two-dimensional Euclidian distance within some radius, where nodes are randomly placed inside the unit square (Gilbert, 1961).
- For undirected random graph models, we orient edges by selecting the upper-triangular half of the adjacency matrix.
- The classes of random graph models are sampled in equal proportion when generating a set of evaluation datasets (Tables 3 and 4).
- For the evaluation in the GRN domain, we sample realistic causal graphs by extracting subgraphs from the known E. coli and S. cerevisiae regulatory networks.
- For this, we rely on the procedure by Marbach et al. (2009), which is also used by Schaffter et al. (2011) and Dibaeinia and Sinha (2020).
- Their graph extraction method is carefully designed to capture the structural properties of biological networks by preserving the functional and structural properties of the source network.
- Algorithm
- The procedure extracts a random subgraph of the source network by selecting a subset of nodes V, and then returning the graph containing all edges from the source network covered by V. Starting from a random seed node, the algorithm proceeds by iteratively adding new nodes to V. In each step, this new node is selected from the set of neighbors of the current set V. The neighbor to be added is selected greedily such that the resulting subgraph has maximum modularity (Marbach et al., 2009).
- To introduce additional randomness, Marbach et al. (2009) propose to randomly draw the new node from the set of neighbors inducing the top-p percent of the most modular graphs.
- In our experiments, we adopt the latter with p = 20 percent, similar to Schaffter et al. (2011).
- The original method of Marbach et al. (2009) is intended for undirected graphs. Thus, we use the undirected skeleton of the source network for the required modularity and neighborhood computation.
- Real GRNs
- We take the E. coli and S. cerevisiae regulatory networks as provided by the GeneNetWeaver repository,1 which have 1565 and 4441 nodes (genes), respectively.
- For E. coli, we also know the true signs of a large proportion of causal effects.
- When extracting a random subgraph from E. coli, we take the true signs of the effects and map them onto the randomly sampled interaction terms k ∈ R d×d used by SERGIO; cf. Section A.2.2.
- When the interaction signs are unknown or uncertain in E. coli, we impute a random sign in the interaction terms k of SERGIO based on the frequency of known positive and negative signs in the E. coli graph.
- Empirically, individual genes in E. coli tend to predominantly have either up-or down-regulating effects on their causal children.
- To capture this aspect in S. cerevisiae also, we fit the probability of an up-regulating effect caused by a given gene in E. coli to a Beta distribution.
- For each node j in an extracted subgraph of S. cerevisiae, we draw a probability p j from this Beta distribution and then sample the effect signs for the outgoing edges of node j using p j . As a result, the genes in the sub...

### A.2 Data-Generating Processes
- SCMs are models of data-generating processes that use causal mechanisms to model each causal variable given its parents.
- In the LINEAR and RFF domains, the data-generating processes are modeled by SCMs with causal mechanisms that model each causal variable given its parents as x j ← f j (x pa(j) , j ) = f j (x pa(j) ) + h j (x pa(j) ) j .
- In the GRN domain, our goal is to evaluate causal discovery from realistic gene expression data. There exist several models to simulate the mechanisms, intervention types, and technical measurement noise underlying single-cell expression data of gene regulatory networks (Schaffter et al., 2011;Huynh-Thu and Sanguinetti, 2019;Dibaeinia and Sinha, 2020).
- We use the simulator by Dibaeinia and Sinha (2020) (SERGIO) because it resembles the data collected by modern high-throughput single-cell RNA sequencing (scRNA-seq) technologies.

### B Evaluation Metrics
- We report several metrics to assess how well the predicted causal structures reflect the ground-truth graph.
- We measure the overall accuracy of the predictions and how well-calibrated the estimated uncertainties in the edge predictions are.
- Unless evaluating these edge probabilities, we use a decision threshold of 0.5 to convert the AVICI prediction to a single graph G.
- The structural hamming distance (SHD) (Tsamardinos et al., 2006) reflects the graph edit distance between two graphs, i.e., the edge changes required to transform G into G .
- By contrast, the structural intervention distance (SID) (Peters and Bühlmann, 2015) quantifies the closeness of two DAGs in terms of their valid adjustment sets, which more closely resembles our intentions of using the inferred graph for downstream causal inference tasks.
- SHD and SID capture global and structural similarity to the ground truth, but notions like precision and recall at the edge level are not captured well.
- SID is zero if and only if the true DAG is a subgraph of the predicted graph, which can reward dense predictions (Prop. 8 by Peters and Bühlmann (2015): SID(G, G ) = 0 when G is empty and G is fully connected).
- Conversely, the trivial prediction of an empty graph achieves highly competitive SHD scores for sparse graphs.
- For this reason, we report additional metrics that quantify both the trade-off between precision and recall of edges as well as the calibration of their uncertainty estimates.
- Specifically, given the binary predictions for all d 2 possible edges in the graph G, we compute the edge precision, edge recall, and their harmonic mean (F1-score) for each test case and estimate their means and standard errors across the test cases.
- Since the F1-score is high only when precision and recall are high, both empty and dense predictions are penalized and no trivial prediction scores well, making it a reliable metric for structure learning.
- Edge confidence is computed by measuring the areas under the precision-recall curve (AUPRC) and receiver operating characteristic (AUROC) when converting the probabilities into binary predictions using varying decision thresholds (Friedman and Koller, 2003).
- The AUROC is insensitive to changes in class imbalance (edge vs. no-edge) for a given d.
- However, when the number of variables d in sparse graphs of O(d) edges increases, AUROC increasingly discounts the accuracy on the shrinking proportion of edges present in the ground truth, which makes AUPRC more suitable for comparisons ranging over different d.
- The ECE is a scalar summary of this calibration plot and amounts to the weighted average of the vertical deviation from the perfect calibration line, i.e., where n is the total number of evaluated samples (i.e., edges).
- The calibration plot lines compute the calibration statistics in aggregate over all test cases to reduce the variance of the empirical counts within the bins, thus not showing standard errors.

## C Inference Model Details
- The penalty F(φ) for the acyclicity constraint is estimated using the same minibatch as for L(φ).
- Buffer Since we have access to the complete data-generating process rather than only a fixed dataset, we approximate L(φ) with minibatches that are sampled uniformly randomly from a buffer, which is continually updated with fresh data from p(G, D).
- Specifically, we initialize a first-in-firstout buffer that holds 200 pairs (G, D) for each unique number of variables d considered during training.
- A pool of asynchronous single-CPU workers then constantly generates novel training data and replaces the oldest instances in the buffer using a producer-consumer workflow.
- We implement this buffer using an Apache PyArrow Plasma object store (Apache Licence 2.0).
- During training, we used 128 CPU workers (Appendix E).
- The workers balance the data generation for different buffers to ensure an equal sample-to-insert ratio across d, accounting for the oversampling of higher d as well as the longer computation time needed for generating data D of larger d, for instance, in the GRN domain.
- In addition, the dataset D of each element (G, D) in the buffer contains four times more observations than n = 200 used during training.
- These observations are subsampled to obtain n = 200 each time a given buffer element (G, D) is drawn to introduce additional diversity in the training data in case buffer elements are sampled more than once.
- Parameter updates The primal updates of the inference model parameters φ are performed using the LAMB optimizer with a constant base learning rate 3 • 10 −5 and adaptive square-root scaling by the maximum effective batch size3 (You et al., 2019).
- Gradients with respect to φ are clipped at a global 2 norm of one (Pascanu et al., 2013).
- In all three domains, we optimize φ for a total number of 300,000 primal steps, reducing the learning rate by a factor of ten after 200,000 steps.
- When adding the acyclicity contraint in LINEAR and RFF, we use a dual learning rate of η = 10 −4 and perform a dual update every 500 primal steps.
- The dual learning rate η is warmed up with a linear schedule from zero over the first 50,000 primal steps.
- To reduce the variance in the dual update, we use an exponential moving average of F(φ) with step size 10 −4 maintained during the updates of the primal objective.
- To approximate the spectral radius in Eq. ( 8), we perform t = 10 power iterations initialized at u, v ∼ N (0, I d ).
- As described in Section 4.2, the core of our model consists of L = 8 layers, each containing four residual sublayers.
- Different from the vanilla Transformer encoder, we employ layer normalization before each multi-head attention and feedforward module and after the last of the L layers (Radford et al., 2019).
- The multi-head attention modules have a model size of 128, key size of 32, and 8 attention heads.
- The feedforward modules have a hidden size of 512 and use ReLU activations.
- In held-out tasks of RFF and GRN, we found that dropout in the Transformer encoder does not hurt performance in-distribution, so we increased the dropout rates from 0.0 to 0.1 and 0.3, respectively, to help generalization o.o.d.
- Dropout, when performed, is done before the residual layers are added, as in the vanilla Transformer (Vaswani et al., 2017).
- The position-wise linear layers that map the two-dimensional representation (z 1 , . . . , z d ) ∈ R d×k to u i and v i , respectively, apply layer normalization prior to their transformations.
- We use Kaiming uniform initialization for the weights (He et al., 2015a).
- The bias term inside the logistic function of Eq. ( 6) is initialized at −3 and learned alongside all other parameters φ.
- Likewise, the scale parameter τ is learned but optimized in log space to ensure positivity, i.e., τ = exp(τ log ) where τ log is updated as part of φ and initialized at 2.
- When optimizing models under the acyclicity constraint, we ignore the diagonal predictions θ ii and mask the corresponding loss terms.
- We implement AVICI with Haiku in JAX (Hennigan et al., 2020;Bradbury et al., 2018).
- We converged to the above optimization and architecture specifications through experimentation on held-out instances from...

## E Extended Results
- trained three main AVICI models and several ablations
- optimized for approximately four days
- performed benchmarking experiments
- DCDI required one GPU to ensure a computation time of less than one day per task instance
- inference with AVICI is done on eight CPUs and no GPU
- on LINEAR data, the baselines achieve F1 scores of 0.15 -0.54 with observational and 0.33 -0.74 with interventional data, similar to the RFF AVICI model with 0.19 and 0.45, respectively
- conversely, on RFF data, the baselines achieve F1 scores of 0.22 -0.42 with observational and 0.34 -0.41 with interventional data, which is also matched by the LINEAR AVICI model here with 0.27 and 0.42, respectively

### E.2 In-Distribution Benchmarking Results for d = 30
- The data is generated under additive noise and the parameters of its generative processes are sampled from the training domains of AVICI
- Compared to the o.o.d. setting, most baselines perform roughly the same
- AVICI significantly improves by moving to the easier in-distribution setting, in particular in the SCM domains, which are less noisy
- In the GRN Table 7: Benchmarking results (d = 100 variables). Mean SID (↓) and F1 score (↑) with standard error of all methods on 30 random task instances. Methods in the top section use only observational data, in the bottom section both observational and interventional data. We highlight the best result of each section and those within its 95% confidence interval according to an unequal variances t-test.
- AVICI outperforms all baselines in the nonlinear RFF domain, with and without access to interventional data. Likewise, AVICI is the only method to achieve nontrivial edge accuracy in terms of F1 score on the challenging GRN domain.

### E.4 Benchmarking Results on Real-World Proteomics Data
- AVICI outperforms all baselines in all three domains
- GES and GIES are on par with AVICI in LINEAR, but outperform it in GRN
- AVICI achieves nontrivial accuracy in GRN even without access to gene knockout data
