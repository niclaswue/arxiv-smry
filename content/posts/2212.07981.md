---
title: "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation"
date: 2022-12-15T17:26:05.000Z
author: "Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07981v1.webp" # image path/url
    alt: "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07981)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07981).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/revisiting-the-gold-standard-grounding).

# Abstract

# Paper Content

## Introduction
- Human evaluation plays an essential role in both assessing the rapid development of summarization systems in recent years (Lewis et al., 2020a;Zhang et al., 2020a;Brown et al., 2020;Sanh et al., 2022;He et al., 2022) and in assessing the ability of automatic metrics to evaluate such systems as a proxy *
- Equal contribution for manual evaluation (Bhandari et al., 2020;Fabbri et al., 2022a;Gao and Wan, 2022). However, while human evaluation is regarded as the gold standard for evaluating both summarization systems and automatic metrics, as suggested by Clark et al. (2021) an evaluation study does not become "gold" automatically without proper practices.
- For example, achieving a high inter-annotator agreement among annotators can be difficult (Goyal et al., 2022a), and there can be a near-zero correlation between the annotations of crowd-workers and expert annotators (Fabbri et al., 2022a).
- Also a human evaluation study without a large enough sample size can fail to find statistically significant results due to insufficient statistical power (Card et al., 2020).
- Therefore, we believe it is important to ensure that human evaluation can indeed serve as a solid foundation for evaluating summarization systems and automatic metrics. For this, we propose using a robust human evaluation protocol for evaluating the salience of summaries that is more objective by dissecting the summaries into finegrained content units and defining the annotation task based on those units. Specifically, we introduce the Atomic Content Unit (ACU) protocol for summary salience evaluation ( §3), which is modified from the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols.
- We demonstrate that with the ACU protocol, a high inter-annotator agreement can be established among crowd-workers, which leads to more stable system evaluation results and better reproducibility. We then collect, through both in-house annotation and crowdsourcing, RoSE, a large human evaluation benchmark of human-annotated summaries with the ACU evaluation protocol on recent state-of-the-art summarization systems, which yields higher statistical power ( §4).
- To support evaluation across datasets and domains, our benchmark consists of test sets over three summarization datasets, CNN/DailyMail (CNNDM) (Nalla-  pati et al., 2016), XSum (Narayan et al., 2018), and SamSum (Gliwa et al., 2019), and annotations on the validation set of CNNDM to facilitate automatic metric training.
- To gain further insights into the characteristics of different evaluation protocols, we conduct human evaluation with three other protocols ( §5). Specifically, we analyze protocol differences in the context of both fine-tuned models and large language models (LLMs) in a zero-shot setting such as GPT-3 (Brown et al., 2020). We find that different protocols can lead to drastically different results, and these results can be affected by annotators' prior preferences, highlighting the importance of aligning the protocol with the summary quality intended to be evaluated.
- We note that our benchmark enables a more trustworthy evaluation of automatic metrics ( §6), as shown by statistical characteristics such as tighter confidence intervals and more statistically significant comparisons ( §6.3). We summarize our key findings in Tab. 1. Our contributions are the following:
- We propose the ACU protocol for high-agreement human evaluation of summary salience.
- We curate the RoSE benchmark, consisting of 22k summary-level annotations and requiring over 150 hours of in-house annotation, across three summarization datasets, which can lay a solid foundation for training and evaluating automatic metrics.
- We compare four human evaluation protocols for summarization and show how they can lead to drastically different model preferences.
- We evaluate automatic met-rics across different human evaluation protocols and call for human evaluation to be conducted with a clear evaluation target aligned with the evaluated systems or metrics, such that task-specific qualities can be evaluated without the impact of general, input-agnostic preferences of annotators. We note that the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback (Ouyang et al., 2022) and call for a more rigorous human evaluation of LLM performance.

## Related Work

### Human Evaluation Benchmarks
- Human judgment data must be collected that reflects the dimension to be studied
- Recent efforts have focused on aggregating model outputs and annotating quality dimensions to better assess summarization model and metric progress
- The most relevant work to ours, in terms of summarization quality dimension studied, is (Bhandari et al., 2020)
- This benchmark only covers a single dataset (CNNDM) without a focus on similarly-performing state-of-the-art systems
- On the other hand, we introduce a modified protocol for summarization salience evaluation that exhibits higher interannotator agreement

### Summarization Meta-Evaluation
- Many directions of meta-evaluation exist
- Within metric meta-analysis, several studies have focused on the analysis of ROUGE (Lin, 2004b), and its variations (Rankel et al., 2013;Graham, 2015), across domains such as news (Lin, 2004a), meeting summarization (Liu and Liu, 2008), and scientific articles (Cohan and Goharian, 2016).
- Other studies analyze a broader set of metrics (Peyrard, 2019;Bhandari et al., 2020;Deutsch and Roth, 2020;Fabbri et al., 2022a;Gabriel et al., 2021;Kasai et al., 2022b), including those specific to factual consistency evaluation (Kryscinski et al., 2020;Durmus et al., 2020;Wang et al., 2020;Maynez et al., 2020;Laban et al., 20d;Fabbri et al., 2022b;Honovich et al., 2022;Tam et al., 2022).
- Regarding re-evaluating model performance, a recent line of work has focused on evaluating zeroshot large language models (Goyal et al., 2022a;Liang et al., 2022;Tam et al., 2022), noting their high performance compared to smaller models.
- As for the further understanding of human evaluation, prior work has compared approaches to human evaluation (Hardy et al., 2019), studied annotation protocols for quality dimensions such as linguistic quality (Steen and Markert, 2021) and factual consistency (Tang et al., 2022b), and noted the effects of human annotation inconsistencies on system rankings (Owczarzak et al., 2012).
- The unreliability and cost of human evaluation in certain settings have been emphasized (Chaganty et al., 2018;Clark et al., 2021), with some work noting that thousands of costly data points may need to be collected in order to draw statistically significant conclusions (Wei and Jia, 2021).
- Our meta-analysis focuses on this latter aspect of human evaluation protocol comparison. We study the strength of our ACU protocol with respect to the inter-annotator agreement, statistical power, and correlation analysis and compare it against three other protocols. We further analyze potential confounding factors in evaluation such as length and protocol design, which we study with respect to both small and large zero-shot language models.

## Atomic Content Units for Summarization Evaluation
- We describe an annotation protocol for reference-based summary salience evaluation.
- The protocol uses atomic content units (ACUs).

### Preliminaries
- In this work, we focus on a specific summarization meta-evaluation study on summary salience.
- Salience is a desired summary quality that requires the summary to include all and only important information of the input article.
- The human evaluation of summary salience can be conducted in either reference-free or reference-based manners.
- The former asks the annotators to assess the summary directly based on the input article (Fabbri et al., 2022a), while the latter requires the annotators to assess the information overlap between the system output and reference summary (Bhandari et al., 2020), under the assumption that the reference summary is the gold standard of summary salience.
- Given that reference-based protocols are more constrained, we focus on reference-based evaluation for our human judgment dataset collection.

### ACU Annotation Protocol
- Extracting facts from one text sequence, and
- Checking for the presence of the extracted facts in another sequence.

### ACU Annotation Collection
- We collect human annotations according to our ACU annotation protocol on three summarization datasets
- To reflect the latest progress in text summarization, we collect and annotate the generated summaries of pretrained summarization systems
- The aggregated summary-level agreement score of ACU matching is 0.7571, and the ACU-level agreement score is 0.7528
- These agreement scores are higher than prior collections, such as RealSumm (Bhandari et al., 2020) and SummEval (Fabbri et al., 2022a), which have an average agreement score of crowd-workers 0.66 and 0.49, respectively

## Power Analysis
- The statistical power of a test depends on various factors of the experimental setting.
- For our study, we focus on two factors, namely the number of test examples and the observed system difference.
- Notably, while the sample size of the human evaluation performed in recent work is typically around 50-100,9 such sample size can only reach a power of 0.80 when the ROUGE1 recall score difference is above 5, which is greater than the difference of most system pairs compared in related literature.
- Increasing the sample size can effectively raise the statistical power.
- For example, when the system performance difference is within the range of 1-2 points, the power of a 500-sample set is around 0.50 while a 100-sample set only has a power of around 0.20.

### Summarization System Analysis
- The ACU score has a strong correlation with the summary length
- The average summary length of different systems can greatly vary
- The summary length difference is not always captured by the widely-used ROUGE F1 scores.
- Length as Summary Quality We note that all systems in Tab. 4 have longer summaries than the reference summaries, whose average length is only 54.93.

## Evaluating Annotation Protocols
- The design of the human evaluation protocol can have a large impact on the human evaluation result and the associated evaluation of systems and automatic metrics.
- Apart from ACU annotations, we collect human annotations with three different protocols to better understand their characteristics.
- We summarize and compare different protocols in Tab. 5.
- Specifically, two reference-free protocols are investigated: Prior protocol evaluates the annotators' preferences of summaries without the input document, while Ref-free protocol evaluates if summaries cover the salient information of the input document. Additionally, we consider one referencebased protocol, Ref-based, which evaluates if the generated summaries accurately cover the information in the reference summaries.
- See Appendix D.1 for detailed instructions for each protocol.

### Annotation Collection
- We collected three annotations per summary on a 100-example subset of input documents from the above CNNDM test set in protocol comparison using the same pool of workers from our ACU qualification.
- In all non-ACU settings, the annotators judge all of the summaries within a single HIT with a score from 1 (worst) to 5 (best), similar to the EASL scoring schema proposed in Sakaguchi and Van Durme (2018).
- We first collected annotations of the 12 above systems, and we achieve an inter-annotator agreement, as measured by Krippendorff's alpha, for the (Prior, Ref-free, Ref-based) protocols of (0.3455, 0.2201, 0.2741).
- The Prior protocol achieves the highest agreement despite not being grounded in any other text and suggests that confounding factors may influence preference, which we discuss further below.
- We also collect an additional set of annotations to better understand annotation protocols with re- spect to recently introduced large language models applied to zero-shot summarization.

### Results Analysis
- The system-level protocol correlation when evaluating the fine-tuned models in Tab. 6, and the summarylevel correlation can be found in Appendix D.2.
- The reference-free Prior and Ref-free protocols have higher power than the referencebased protocols.
- The Ref-free protocol and the Ref-based protocol have a near-zero correlation at the summary level and a negative one at the system level while ideally they are supposed to measure similar quality aspects of summaries.
- The Ref-based protocol has a relatively strong correlation with our ACU protocol, likely because the underlying evaluation targets of these two reference-based protocols are similar.
- We conduct an annotator-based case study including 4 annotators who annotated at least around 20 examples in this task, in which we compare two summary-level correlations (Eq. 3) given a specific annotator: (1) the correlation between their own Ref-free protocol results and Prior results; (2) the correlation between their Ref-free results and the Ref-free results averaged over the other 2 annotations on each example. We found that the average value of the former is 0.404 while the latter is only 0.188, suggesting that the annotators' own Prior result is a better prediction of their Ref-free result than the Ref-free result of other annotators.

## Evaluating Automatic Metrics
- We analyze the correlations and robustness of metrics
- We use a representative subset of recent metrics
- We have additional results in Appendix E on 50 automatic metrics

### Metrics
- ROUGE measures the n-gram and sequence overlap between the generated summary and a set of reference summaries.
- METEOR aligns unigrams stemming and synonym matches.
- CHRF calculates character-based overlap between model and reference summaries.
- BERTScore aligns tokens by greedily maximizing the cosine similarity between BERT token embeddings.
- BARTScore scores generated text by its log probability given another text.
- SummaQA applies a BERT question-answering (QA) model to answer cloze input questions using generated summaries.
- QAEval scores a summary according to a QA model's ability to answer reference-generated questions using the summary.
- Lite 2 Pyramid reports an entailment score between semantic triplets from reference summaries and the generated summaries.

### Metric Evaluation with ACU Annotations
- We use the correlations between the automatic metric scores and the ACU annotation scores of system outputs to analyze and compare the performance of automatic metrics.
- In Tab. 8, we show Kendall's correlations at both the system level (Eq. 4) and the summary level (Eq. 5)
- We use the recall score of the automatic metrics when available since the un-normalized ACU scores are used for calculating the correlations.
- We report the full results of more automatic metrics with different correlation coefficients, as well as the results with normalized ACU scores in Appendix E.
- We make the following observations: (1) Several automatic metrics (e.g., ROUGE, BERTScore, BARTScore, QAEval, Lite 2 Pyramid) from the different families of methods are all able to achieve relatively high correlation with the ACU scores, especially at the system level. (2) Similar to the findings in (Bhandari et al., 2020), we find that the automatic metrics perform better at the system level than at the summary level. (3) Metric performance varies across different datasets. In particular, metrics tend to have stronger correlations on the SamSum dataset and weaker correlations on the XSum dataset.
- While it may partly result from the differences in the summarization systems used on different datasets, we hypothesize that another reason is that the reference summaries of the XSum dataset contain more complex structures (both syntactically and semantically).
- Prior work has shown that automatic metrics will decorrelate with the human evaluation at the system level when the performance of evaluated systems becomes similar.

### Analysis
- We analyze metric evaluation through correlation confidence intervals
- The confidence intervals are large
- Having a larger sample size can effectively reduce the confidence interval

### Power Analysis of Metric Comparison
- The power analysis of pair-wise metric comparison found that significant results are difficult to find when the metric performance is similar.
- Increasing the sample size can effectively increase the chance of finding significant results.
- We analyzed the metric correlations under different human evaluation protocols using the annotations we collected in §5. The system-level correlation is shown in Tab. 10, and the results of more metrics with both the systemlevel and summary-level correlations can be found in Appendix E.5.

## Discussion
- The impact of different human evaluation protocols on the results of automatic text summarization
- The claim that reference-based automatic metrics cannot reliably evaluate zero-shot summaries is also valid for summaries generated by the more traditional, fine-tuned systems under the reference-free human evaluation protocol
- For future work, we advocate for targeted evaluation of text summarization

## Conclusion
- RoSE is a benchmark that has a protocol and scale that allows for more robust summarization evaluation across three datasets and encompassing two domains.
- RoSE was used to re-evaluate the current state of human evaluation and its implications for both summarization system and automatic metric development.
- Biases may be present in the data annotator as well as in the data the models were pretrained on.
- As discussed in Appendix D.1, RoSE takes measures to ensure a high quality benchmark.
- There will inevitably be noise in the dataset collection process, either in the ACU writing or matching step, and high agreement of annotations does not necessarily coincide with correctness.
- However, we believe that the steps taken to spot check ACU writing and filter workers for ACU matching allow us to curate a high-quality benchmark.
- Furthermore, we encourage the community to analyze and improve RoSE in the spirit of evolving, living benchmarks.
