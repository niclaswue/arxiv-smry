---
title: "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval"
date: 2022-12-21T02:41:40.000Z
author: "John Wieting, Jonathan H. Clark, William W. Cohen, Graham Neubig, Taylor Berg-Kirkpatrick"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10726v1.webp" # image path/url
    alt: "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10726)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10726).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/beyond-contrastive-learning-a-variational).

# Abstract
- Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well.
- In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs.
- Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation.
- We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches.
- Overall, our Variational Multilingual Source-Separation Transformer (VMSST) model outperforms both a strong contrastive and generative baseline on these tasks.

# Paper Content

## Introduction
- Contrastive learning requires strong negative examples in the data
- In this paper, we propose a generative model for learning multilingual text embeddings which encourages source separation, separating semantic information that is shared between translations from stylistic or language-specific variation
- We find that by filtering this variation into separate variables, performance of the remaining representations, that encode shared semantic information, increases across all downstream tasks
- Our model outperforms these models and is also competitive with the state-of-the-art

## Related Work
- There are a number of approaches proposed for learning bilingual and multilingual text embeddings
- One popular approach is contrastive learning (Hermann and Blunsom, 2014;Singla et al., 2018;Guo et al., 2018;Wieting et al., 2019;Feng et al., 2022) where translation pairs are positive examples and text from other pairs are used as negative examples
- An alternative approach is to use a neural machine translation objective, where the representation from the hidden states of the encoder is used as the sentence embedding
- Other approaches include multi-task learning approaches which often use some type of contrastive learning of parallel text to align representations among languages
- Cross-lingual pretraining (Chi et al., 2022), and model distillation from a large pretrained multilingual model (Reimers and Gurevych, 2020) are also possible

## Model
- The generative process of our underlying probabilistic model and the computation graph of our training objective procedure are depicted in Fig- Linguistic variation specific to languages to Figure 1: The generative process of our model. Latent variables, z li , modeling the variation x li specifically due to language l i , as well as a latent variable modeling the common semantics, z sem , are drawn from a multivariate Gaussian prior.
- In practice, we approximate this model to make learning and inference tractable.
- The objective function for VMSST consists of consists of two terms, the first being ELBO as described earlier: where Z S is the collection of semantic variables, while Z L is the collection of language variables.
- The second term, which we found necessary for strong performance, is the sum of p(x l i |µ sem l j ) and p(x l j |µ sem l i ) which can be interpreted as samples from the mean of the posterior distribution using semantic variables generated from both input sentences.
- When training variational objectives, where the model ignores the latent variables and the learned posterior remains close to the prior.

## Architecture
- The architecture is an encoder-decoder model
- The encoder produces a single representation that is fed into the decoder
- Cross-attention between the encoder and decoder is not used, therefore the decoder has no full sequence visibility and more pressure is applied on the encoder to create a semantically meaningful representation
- Specifically, we follow the approach of Wieting et al. (2020) which uses a Transformer (Vaswani et al., 2017) encoder-decoder model
- The sentence embeddings are used in two places: at each layer of the decoder in place of cross-attention and in the computation of the logits
- Merging operation Figure 2:
- The text for languages l i and l j , with their respective language embeddings, are fed into the encoder acting as their inference networks.
- The text is also fed into the semantic inference network which is a separate encoder.
- The output of these networks are the language variables z li and z lj and semantic variable z sem .
- Each language-specific variable is then concatenated to z sem and used by a single shared decoder to reconstruct the input sentence pair.
- The decoder models p(x l i |z sem , z l i ; θ) for each language i (see right side of Figure 2).
- The inputs to the decoder are the language-specific variable z l i and the semantic variable z sem , which are concatenated and used to condition the decoder to generate the reconstruction of the observed text x l i .
- We use a single decoder for all languages.
- The encoders play an important role in the source separation as well as inference as detailed below.
- In order to motivate the separation of the linguistic and semantic information we split the encoder into two parts, only sharing the embedding table.
- We use one of these encoders to be the semantic inference network, which produces the semantic variable.
- The other encoder represents the N language inference networks and produces the language variables for each language.
- These inference networks are shown on the left side of Figure 2.
- We mean-pool the hidden states followed by a linear projection to produce each variable from the encoders.
- The semantic inference network, which models q(z sem |x l i , x l j ; φ), is a multilingual encoder that encodes each language.
- For each translation pair, we alternate which of the two parallel sentences is fed into the semantic encoder within a batch for the ELBO term in the objective.
- Since the semantic encoder is meant to capture language agnostic semantic information, its outputs for a translation pair should be similar regardless of the language of the input sentence.

## Experiments

### Constructing the Training Data
- We sample our data from Europarl, United Nations (Rafalovitch and Dale, 2009), OpenSub-titles2018 (Lison et al., 2018), Global Voices, Tanzil, and Tatoeba v2021-07-22.
- We use both English and Spanish as pivot languages, so each pair includes at least one English or Spanish sentence, and we use approximately the same amount of data for each language.
- We note that we only have training data for 92 languages instead of the 93 in Artetxe and Schwenk (2019b) due to not having training data for Aymara (ay).

### Evaluation
- The goal of the semantic textual similarity tasks is to predict the degree to which sentences have the same meaning as measured by human judges.
- For the English-only evaluation, we follow Wieting et al. (2016) by averaging the yearly performance on 2012-2016 SemEval Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016)).
- More specifically, for each year of the competition, we average the Pearson's r × 100 for each dataset in that year, and then finally average this result for each year of the competition.
- For the cross-lingual evaluation we use the cross-lingual STS tasks from Se-mEval 2017 (Cer et al., 2017).
- This evaluation contains Arabic-Arabic, Arabic-English, Spanish-Spanish, Spanish-English, and Turkish-English STS datasets.
- For bitext mining, we use the Tatoeba dataset introduced in Artetxe and Schwenk (2019b) and the 2018 Building and Using Parallel Corpora (BUCC) shared bitext mining task (Zweigenbaum et al., 2018).
- The goal of the BUCC task is to find the gold aligned parallel sentences given two corpora (one being very large) in two distinct languages.
- Languages are aligned with English and consist of German (de), French (fr), Russian (ru), and Chinese (zh).
- Typically, only about 2.5% of the sentences are aligned.
- Following Schwenk (2018), we evaluate on the publicly available BUCC data.
- This involves scoring all pairs between the source target sentences and finding the optimal threshold that separates the data.
- Using the threshold, we can compute the precision, recall, and F 1 of the alignments.
- We report F 1 × 100 in our results.
- We compare two different approaches for finding the sentence alignments.
- In the first, BUCC (cosine), we compute the cosine similarity between the non-English source sentences and the English target sentences, selecting the highest scoring English sentence as the match.
- In the second, BUCC (margin), we follow Artetxe and Schwenk (2019a) and use a margin-based scoring approach, where the final score of a sentence pair is both a function of the score between the pair and the scores of each sentence with its nearest neighbors.
- To compute this margin score, we divide the cosine similarity for source sentence s i and target sentence t i by the sum of the scores of the four nearest neighbors of s i with the target sentences and the sum of the scores of the four nearest neighbors of t i with the source sentences.
- Margin-based scoring is designed to alleviate the "hubness problem" (Radovanovic et al., 2010;Radovanović et al., 2010) where the neighborhood around embeddings in a high-dimensional space, like in sentence embeddings, have many neighbors in common. These neighbors can displace the correct mapping in the ordering, hurting performance.

### Baselines
- The first baseline is contrastive learning with the other sentences in the batch as negative examples
- The second baseline is translation objective to learn the representation
- We also explore an alternative to the VMSST, VMSST CONTRASTIVE, by incorporating a contrastive loss

### Experimental Settings
- Uses the Transformer architecture for all settings
- Uses the MT5 vocabulary, which is derived from sentencepiece
- Uses a learning rate schedule that increases linearly for 4,000 steps, after which it is decayed proportionally to the inverse square root of the number of steps
- Sets the peak learning rate to be 0.001 and trains models for 100,000 steps total
- Uses a batch size of 2048 and sets the maximum sequence length of the model to 32
- Uses a dropout rate of 0.1 for CONTRASTIVE models and no dropout for BITRANSLATION, VMSST CONTRASTIVE (with the exception of the randomly initialized 24 layer model which used 0.1), and VMSST
- For VMSST, sets λ, the weight on the VMSST ELBO loss term, to be 0.025 for the pre-trained and 6 layer settings and 0.001 for the randomly initialized 24 layer setting

### Results
- Overall, VMSST has the best performance for all three experimental settings and the best performance on each task on average
- VMSST and BITRANSLATION are especially strong when using more layers
- In fact, for NQ question retrieval with a pretrained model, it performs nearly to that of the model trained specifically for this task on NQ data from Lewis et al. (2021) which has an accuracy of 41.2.
- VMSST and CONTRASTIVE have negligible improvement over CONTRASTIVE which was unexpected -that is, a traditional contrastive loss does not improve further on top of generative loss of VMSST.
- We leave the exploration of different strategies of combining these approaches to future work.

### Comparison to Related Work
- Multilingual embeddings have been compared in different ways, using different training data, models, and metrics
- VMSST has the best overall performance when averaged across different tasks
- There are differences in the data and models used among other models

## Analysis

### Model Ablations
- In Table 4, it is mentioned that if we factor the projection layer, we can reduce the space to d × V + 3d × d.
- If we factor the projection layer, we can save a lot of memory in the model, as that projection layer is 3 × d × V where d is the hidden state dimension size and V is the size of the vocabulary.
- In our first experiment, VMSST (fact.), we simply factor the final projection layer of the decoder.
- This can save a lot of memory in the model, as that projection layer is 3 × d × V where d is the hidden state dimension size and V is the size of the vocabulary.
- If we factor the projection layer, we can reduce the space to d × V + 3d × d.
- In practice, this saves about 509 million parameters for our 24 layer models.
- However from the first row in Table 4, we see that this small change has a significant effect on performance, weakening results on semantic similarity and question retrieval tasks and strengthening results on bitext mining tasks.
- In our second ablation, VMSST (4 enc.), we spread the model capacity of the language-specific encoder to 4 encoders, instead of the single encoder in our previous experiments.
- We allocate the languages randomly to the different encoders.
- We find that this doesn't improve results, perhaps because the 24 layer model has sufficient capacity to model all of the languages in one shared encoder.
- We could allocate languages to encoders based on language families, and perhaps this could fare better, but we leave that for future work.
- Prior work (Wieting et al., 2020) shows that, a decoder that is weaker (i.e. less layers) can lead to stronger embeddings.
- This effect is presumably because there is more pressure on the sentence embedding to fully and clearly capture the semantics since it cannot rely on a strong decoder to fill in gaps.
- We found that that using a weaker single layer decoder (1L dec.), does indeed seem to improve performance.
- We also tried a 12 layer ablation (12L dec.), but that seemed to not have a significant improvement in the results.
- Table 6: Comparison of VMSST with a variation that has no parameter sharing, VMSST (full enc., full dec.).
- We experiment on 4 languages, so we have 5 encoders and 4 decoders.
- The last four ablations investigate different modelling choices.
- In the first we eliminate the KL term (no KL), which has the most significant effect on performance, especially on cross-lingual tasks.
- In the second ablation, we use a single encoder instead of the twin encoders (1 enc.), one for semantic embeddings and one for language embeddings, we find that this has a modest overall effect on performance.
- Lastly, we eliminate the language embeddings.
- First we remove the language embedding inputs to the decoder (no enc. l.e.), then we experiment by removing the input language embeddings to the language-specific encoder (no dec. l.e.).
- We find these language embeddings have a smaller than expected impact on performance, perhaps because the large capacity of the decoder can ascertain the language being input or decoded.
- Parameter sharing was needed in order efficiently perform source separation on N languages.
- Specifically we collapsed the language encoders into a single encoder and we collapsed the decoders into a single decoder.
- The VMSST approximates having N language encoders by using an input embedding to indicate the language being considered.
- The same strategy is applied with the decoders as well, with the first input token to the decoder indicating the language to be generated.

### Zero-Shot Bitext Mining
- The Tatoeba dataset contains parallel sentence pairs of English with 112 languages
- Our model is trained using 93 of these languages, and therefore there are 19 languages we can use for a zeroshot evaluation of bitext mining
- Table 8 summarizes the results of this zero-shot evaluation for the two generation objectives, BITRANSLATION and VMSST considered in this paper
- The results are shown in Table 8
- We also compute ∆ which is the difference between the performance gap of VMSST and BITRANSLATION on the seen and unseen languages
- From the results, we see that VMSST does even better than BITRANSLA-TION on unseen languages than unseen languages

### Effects of Batch Size
- The increased batch size increases the chances of encountering harder negative examples and will generally increase performance up to the point where the negatives become false.
- For the 6 layer model, increasing the batch size equalizes VMSST and CONTRASTIVE overall, however each performs better at different tasks.
- For the 24 layer variations, VMSST is better at every task, with the exception of Tatoeba, and has the highest overall score of any model in the table.

## Conclusion
- VMSST is a generative model that outperforms other models on a variety of tasks
- VMSST is faster and uses less memory during training than other models
- There is no difference in memory or speed requirements for VMSST, BITRANS-LATION, or CONTRASTIVE when only a single encoder is used in inference
- VMSST is faster and uses less memory during training than other models when using gradient checkpointing
