---
title: "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval"
date: 2022-12-21T02:41:40.000Z
author: "John Wieting, Jonathan H. Clark, William W. Cohen, Graham Neubig, Taylor Berg-Kirkpatrick"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10726v1.webp" # image path/url
    alt: "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10726)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10726).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/beyond-contrastive-learning-a-variational).

# Abstract
- Contrastive learning has been used for retrieval of semantically aligned sentences.
- Generative model proposed for learning multilingual text embeddings.
- Model operates on parallel data in $N$ languages.
- Approximation encourages source separation in multilingual setting.
- Comparison between contrastive and generation-based approaches for learning multilingual text embeddings.
- Evaluated on suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval.
- VMSST model outperforms contrastive and generative baseline.

# Paper Content

## Introduction
- Contrastive learning is the dominant paradigm for learning text representations from parallel text
- Finding strong negative examples for contrastive learning can be expensive
- Proposed generative model encourages source separation
- Performance of representations encoding shared semantic information increases
- Model scales to 92 languages
- Model outperforms contrastive and generative baselines
- Model is competitive with state-of-the-art
- Generative model increasingly outperforms contrastive model with more layers and larger batches

## Related work
- Popular approach to learning bilingual and multilingual text embeddings is contrastive learning
- Alternative approach is to use a neural machine translation objective
- Other approaches include multi-task learning, cross-lingual pretraining, and model distillation
- Generative models separate linguistic variation from shared semantic information in translation pairs

## Model
- Generative process of model depicted in Figure 1 and Figure 2
- Latent variables, z li , modeling variation x li due to language l i and a latent variable modeling common semantics, z sem , drawn from multivariate Gaussian prior
- Observed translation in each language conditioned on language-specific variable and z sem
- Approximate model to make learning and inference tractable
- Sample semantic variable z sem for sentence
- Sample language-specific variable z li for each of N languages
- Latent variables z sampled from multivariate Gaussian prior N(0, I k )
- Decoder samples each of N sentences in translation set
- z sem encodes semantic, syntactic, or stylistic information shared in all translations
- z li handles language-specific peculiarities or style differences not central to meaning
- Maximize likelihood of observed X with respect to parameters of decoder θ
- Approximate N-way parallel corpus by sampling translation pairs from large pool of pairs
- ELBO introduces variational approximation q to true posterior of model
- ELBO optimized by gradient ascent using reparameterization trick
- KL term encourages z sem , z li , and z lj to be close to zero-centered Gaussian prior
- Objective function consists of ELBO and sum of p(x li |µ sem lj ) and p(x lj |µ sem li )
- Weight ELBO by λ giving total objective

## Architecture
- Architecture is an encoder-decoder model
- Decoder has no full sequence visibility
- Encoder produces single representation
- Cross-attention not used
- Encoder split into two parts, one for semantic inference and one for language inference

## Experiments

### Constructing the training data
- Training data is taken from Europarl, United Nations, OpenSubtitles2018, Global Voices, Tanzil, and Tatoeba.
- Training data is the same amount as Artetxe and Schwenk (2019b).
- Training data includes English and Spanish as pivot languages.

### Evaluation
- Evaluate on three tasks: semantic similarity, bitext mining, and question retrieval
- Question retrieval is uncorrelated to other tasks
- Different datasets used for each task
- Semantic similarity tasks measure how similar sentences are according to human judges
- English-only and cross-lingual evaluations used
- Bitext mining uses Tatoeba and BUCC datasets
- Question retrieval uses NQ and MKQA datasets
- Overall score is average of six subtasks

### Baselines
- VMSST is compared to two strong baselines
- The first baseline is CONTRASTIVE, which uses contrastive learning with other sentences in the batch as negative examples
- The second baseline is BITRANSLATION, which uses a translation objective to learn the representation
- VMSST CONTRASTIVE is an alternative to VMSST, which incorporates a contrastive loss in a multitask setting

### Experimental settings
- Explored 3 settings for 4 objective functions
- Used Transformer architecture for all settings
- 6 layer encoder-decoder model, 24 layer encoder-decoder model, and 24 layer encoder-decoder initialized with Multilingual T5 (MT5) Large
- Embeddings and hidden states for encoders and decoders set to 1024
- Used MT5 vocabulary of 250,000 tokens from mC4 dataset
- Optimization with Adafactor
- Learning rate increases linearly for 4,000 steps, then decayed
- Batch size of 2048, max sequence length of 32
- Dropout rate of 0.1 for CONTRASTIVE models, no dropout for others
- Evaluated on semantic similarity, bitext mining, and question retrieval

### Results
- VMSST has best performance for all 3 experimental settings
- VMSST and BITRANSLATION perform better with more layers
- CONTRASTIVE benefits more from pretraining
- VMSST CONTRASTIVE has negligible improvement over CONTRASTIVE
- Different tasks favor different models
- VMSST outperforms CON-TRASTIVE on BUCC task with cosine similarity

### Comparison to related work
- Prior work on learning multilingual embeddings has explored a variety of models.
- Comparing approaches is difficult as they differ in many factors.
- The main goal of this paper is to compare contrastive and generative losses systematically and uniformly.
- The best systems are competitive with the current state-of-the-art.
- Results are compared on semantic similarity and the Tatoeba and BUCC bitext mining tasks.
- Five models are compared against: mUSE, LASER, XLM-R, XLM, and LaBSE.
- VMSST does not have the best performance on any single task, but has the best overall performance when averaged.
- Models share much in common, but differ in exact data and models used.

## Analysis

### Model ablations
- Investigated different ablations of VMSST
- Started with 24 layer randomly initialized VMSST
- Changed hyperparameters and model choices to see how it affects performance
- First experiment: factored final projection layer of decoder to save memory
- Results weakened on semantic similarity and question retrieval tasks, strengthened on bitext mining tasks
- Second ablation: spread model capacity of language-specific encoder to 4 encoders, no improvement
- Third ablation: weaker single layer decoder, improved performance
- Fourth ablation: eliminated KL term, most significant effect on performance
- Fifth ablation: used single encoder instead of twin encoders, modest overall effect on performance
- Sixth ablation: eliminated language embeddings, smaller than expected impact on performance
- Seventh ablation: compared VMSST with variation that has no parameter sharing, VMSST closer to full model

### Zero-shot bitext mining
- Tatoeba dataset contains parallel sentence pairs of English with 112 languages.
- Model is trained using 93 of these languages, 19 languages used for zero-shot evaluation.
- Results of zero-shot evaluation shown in Table 8.
- Difference between performance gap of VMSST and BITRANSLATION on seen and unseen languages computed.
- VMSST does better than BITRANSLATION on unseen languages.
- Source-separation loss helps with generalization to new languages.

### Effects of batch size
- VMSST and CONTRASTIVE performance increases with batch size
- Bigger batch sizes improve results in Transformer models
- Parallel data is so numerous that training to convergence is not practical
- VMSST and CONTRASTIVE performance equalizes with batch size of 4096/8192 for 6 layer model
- VMSST outperforms CONTRASTIVE for all tasks except Tatoeba with batch size of 4096/8192
- 24 layer VMSST outperforms 6 layer VMSST for all tasks

## Conclusion
- We present VMSST, a generative massively multilingual text embedding model
- VMSST outperforms strong contrastive and generative baselines on a variety of tasks
- Future work includes alternative pretraining objectives, incorporating monolingual data, synergy between VMSST and contrastive methods, and scaling up
- Experiments require significant computational resources
- Code and model checkpoints will be open sourced
- VMSST and BITRANSLATION require decoding, but no difference in memory or speed during inference
- Evaluated on English semantic similarity, Cross-lingual semantic similarity, question retrieval, and bitext mining
- Results reported using Pearson's r and Spearman's ρ
- Full training data for each language reported in Table 15
- Experimental results for VMSST and VMSST CONTRASTIVE and baselines reported in Table 15
- Comparison of CONTRASTIVE and VMSST using different batch sizes reported in Table 15
