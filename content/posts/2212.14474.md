---
title: "Learning 3D Human Pose Estimation from Dozens of Datasets using a Geometry-Aware Autoencoder to Bridge Between Skeleton Formats"
date: 2022-12-29T22:22:49.000Z
author: "István Sárándi, Alexander Hermans, Bastian Leibe"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14474v1.webp" # image path/url
    alt: "Learning 3D Human Pose Estimation from Dozens of Datasets using a Geometry-Aware Autoencoder to Bridge Between Skeleton Formats" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14474)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14474).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/learning-3d-human-pose-estimation-from-dozens).

# Abstract
- Deep learning-based 3D human pose estimation performs best when trained on large amounts of labeled data
- There is little prior research on how to best supervise one model with such discrepant labels
- We show that simply using separate output heads for different skeletons results in inconsistent depth estimates and insufficient information sharing across skeletons
- As a remedy, we propose a novel affine-combining autoencoder (ACAE) method to perform dimensionality reduction on the number of landmarks
- Our approach scales to an extreme multi-dataset regime, where we use 28 3D human pose datasets to supervise one model, which outperforms prior work on a range of benchmarks, including the challenging 3D Poses in the Wild (3DPW) dataset

# Paper Content

## Introduction
- Research on 3D human pose estimation has gone through enormous progress in recent years
- While semi-supervised and self-supervised approaches are on the rise, best results are still achieved when using as much labeled training data as possible
- However, individual 3D pose datasets tend to be rather small and lacking in diversity, as they are often recorded in a single studio with few subjects
- To provide the best possible models for downstream applications (e.g., action recognition, sports analysis, medical rehabilitation, collaborative robotics), it becomes important to use many datasets in the training process
- Thanks to sustained efforts by the research community, numerous publicly re-1 https://vision.rwth-aachen.de/wacv23sarandi CMU-Panoptic Human3.6M 3DPW (SMPL) BML MoVi Berkeley MHAD 3D latent points Figure 1: Different 3D human pose datasets (e.g., CMU-Panoptic and Human3.6M) provide annotations for different sets of body landmarks (left). To best leverage such discrepant labels for multi-dataset 3D pose estimation, we discover a smaller set of latent 3D keypoints (right), from which the dataset-specific points can be reconstructed. This allows us to capture the redundancy among the different skeletons formats and enhance information sharing between datasets, ultimately leading to improved pose accuracy.
- We train models to jointly estimate 3D human pose according to multiple different skeleton formats so that we can train on many datasets at once
- a) Simply using separate prediction heads on a shared backbone is an insufficient solution to this multi-skeleton learning problem, as we obtain inconsistent outputs along the depth axis
- b) We propose a method to capture and exploit the redundancy among the different skeletons using a novel affine-combining autoencoder-based (ACAE) regularization. This leads to a clear improvement in skeleton consistency
- differences through a handful of individually defined rules (e.g., shrink the hip-pelvis distance by a certain factor [68]), but this does not scale to many keypoints and datasets-we need a more systematic and automatic method
- The question we tackle in this work is therefore: How can we automatically merge dozens of 3D pose datasets into one training process, given the label discrepancies?
- We refer to this task as multi-skeleton 3D human pose estimation.

## Related Work
- 3D Human Pose Estimation
- Handling Discrepancy in Skeleton Formats
- Keypoint Discovery
- Linear Subspace Learning

## Method
- We aim to obtain a strong, monocular RGB-based 3D human pose estimation model
- Suppose we have D skeleton formats, with {J d } D d=1 joints in each, for a total of J = ∑ D d=1 J d joints overall.
- Further, we have a merged dataset with N training examples, each consisting of an image of a person and annotations for a subset of the J body joints in 3D.
- Our proposed workflow consists of three main steps. First, we train an initial model that predicts the different skeletons on separate prediction heads, branching out from a common backbone network.
- With the resulting model, we can run inference and produce a pseudo-ground truth "parallel corpus" of many poses given in every skeleton format.
- From this, the geometric relations between skeleton formats can be captured. We accomplish this in the second step, by training an undercomplete geometry-aware autoencoder.
- Finally, equipped with the trained autoencoder, we rely on its learned latent space to make the model output consistent across skeleton formats through output regularization.

### Initial Model Training
- The first step of the paper is to train an initial pose estimator.
- This estimator predicts all J joints separately, without assuming correspondences or relations between different skeletons.
- The pose loss we minimize is L pose = L meanrel + λ proj L proj + λ abs L abs .
- Since each training example is annotated only with a subset of the J joints, we ignore any unlabeled joints when averaging the loss.
- When visualizing the different skeleton outputs of the trained model, we see inconsistencies among them along the challenging depth axis.

### Pseudo-Ground Truth Generation
- To characterize how the joints of the different skeletons relate to one another, we need pose labels according to all skeleton formats for the same examples, to function as a "Rosetta Stone".
- Since no such ground truth is available (datasets only provide one type of skeletons, rarely two), we generate pseudo-ground truth using the initial separatehead model.
- It is important to use images that the model can handle well in this step, hence we choose a relatively clean, clutter-free subset of the training data for this purpose (H36M and MoVi).
- This yields a set of K pseudo-ground truth poses, with all J joints: P k ∈ R J×3 K k=1.

### Affine-Combining Autoencoder
- To capture the redundancy among the full set of J joints, and ultimately to improve the consistency in estimating them, we introduce a simple but effective dimensionality reduction technique.
- Since the pseudo-GT is more reliable in 2D (the X and Y axes) than in the depth dimension, the transformation to and from the latent representation should be viewpoint-independent, in other words the representation should be equivariant to rotation and translation.
- This equivariance in turn requires the latent representation to be geometric, i.e., to consist of a list of L latent 3D points Q k ∈ R L×3 (L < J).
- This makes intuitive sense: the way the different skeletons relate to each other is only dependent on how joints are defined on the human body, not on the camera angle.
- The latent points are then responsible for spanning the overall structure of a pose. Specific skeleton formats can then be computed in relation to these latents.
- Further, the latent points should only have sparse influence on the joints, e.g., some latent points should be responsible for the positioning of the left arm and these should have no influence on the right leg's pose.
- We find that these requirements can be fulfilled effectively by adopting a novel constrained undercomplete linear autoencoder structure, which we call affine-combining autoencoder (ACAE).
- Instead of operating on general n-dimensional vectors, an ACAE's encoder takes as input a list of J points p j ∈ R 3 and encodes them into L latent points q l ∈ R 3 by computing affine combinations according to Similarly, the decoder's goal is to reproduce the original points from the latents, again through affine combinations:
- Since affine combinations are equivariant to any affine transformation, our encoder and decoder are guaranteed to be rotation and translation equivariant.
- The learnable parameters of the ACAE are the affine combination weights w enc l,j and w dec j,l , which can also be understood as (potentially negative) generalized barycentric coordinates [29] for the latents w.r.t. the full joint set and vice versa.
- Allowing negative coordinates is necessary, as this allows the latents to spread outwards from the body, similar to a cage used in graphics [62].
- Restricting the encoder and decoder to convex combinations would severely limit its expressiveness. To achieve sparsity in the weights (i.e., spatially localized influence), we use 1 regularization, and this also reduces the amount of negative weights, preferring nearly convex combinations.
- We further adopt the 1 reconstruction loss, as it is robust to outliers which may be present due to noise in the pseudo-GT.

### Consistency Fine-Tuning
- Once our affine-combining autoencoder is trained on pseudo-ground truth, we freeze its weights and use it to enhance the consistency of 3D pose estimation outputs
- Output Regularization. In this case (Fig. 3c), we estimate all J joints P ∈ R J×3 with the underlying pose estimator, but we feed this output through the autoencoder, and apply an additional loss term that measures the consistency of the prediction with the latent space, through an 1 loss, as This encourages that the separately predicted skeletons can be projected to latent keypoints and back without information loss, thereby discouraging inconsistencies between them.
- The pose loss L pose (c.f . Sec. 3.1) is applied on P .
- Direct Latent Prediction. To avoid having to predict a large number of J joints in the base pose estimator, we define an alternative approach where the latents Q ∈ R L×3 are directly predicted and then fed to the frozen decoder (Fig. 4a).
- The last layer is reinitialized from scratch, as the number of predicted joints changes from J to L.
- The pose loss L pose is applied on W dec Q.
- Hybrid Student-Teacher. In a hybrid of the above two variants, we keep the full prediction head and add a newly initialized one to predict the latents Q directly (Fig. 4b).
- To distill the knowledge of the full prediction head to the latent head, we add a student-teacher-like 1 loss which is only backpropagated to the latent predictor (the student).
- During inference, we use W dec Q as the output, to be as lightweight as direct latent prediction.

## Experimental Setup
- Base model: We adopt the recent state-of-the-art Me-TRAbs 3D human pose estimator as the platform for our experiments.
- Training details: We perform 400k training steps with AdamW and batch size 128, with every dataset represented with a fixed number of examples per batch.
- Batch composition and learning rate schedule are specified in the supplementary.
- We use ghost BatchNorm with size 16, as this improved convergence in multi-dataset training, together with switching the BatchNorm layers to inference mode for the last 1000 updates.
- The final fine-tuning phase has 40k iterations with smaller learning rate on the backbone than the heads.
- The autoencoder weights are trained on pseudo-GT obtained with EffV2-L.
- Datasets: See Tab. 1 for an overview of all used datasets, which employ a variety of skeleton formats.
- In some cases, e.g., when annotations are derived through triangulating COCO-like predictions (of e.g., OpenPose), or through fitting a body model (e.g., SMPL), we can assume that multiple datasets use the same convention (indicated in the last column).
- For other datasets, we assume the skeleton is a custom one, yielding 555 distinct keypoints in total.
- As most 3D human datasets contain videos, rather than isolated images, the number of sufficiently different poses is smaller than the total number of annotated frames.
- We hence discard examples where all joints remain within 100 mm of the last stored example.
- Our overall processing ensures that each training example has a person-centered image crop, camera intrinsics, 3D coordinates for some subset of the joints, a bounding box and a segmentation mask.
- We evaluate on four datasets: MuPoTS [59], 3DPW [56], 3DHP [58] and H36M [10,32].

### Benefit of Training on Many Datasets
- The paper discusses the benefits of using multiple large-scale datasets for training machine learning models
- There is a clear trend showing performance improvement when training with more datasets
- The small dataset combination also outperforms single-dataset baselines

### Consistent Multi-Skeleton Prediction
- Merging joints from different skeletons leads to weaker predictions than predicting all joints separately
- H36M is again an outlier, as the prediction with merged joints works well for it
- Since the model can easily recognize that a test image comes from the H36M studio, it can adapt its prediction to match the H36M skeleton format
- When using our proposed ACAE-based regularization (c.f. Fig. 3c), we can see consistent improvements for almost all metrics
- However, the improvement in the qualitative performance of the model is even more striking
- As seen in Fig. 2, the regularized model creates significantly more consistent skeleton predictions
- Especially the depthconsistency is improved, but some errors in the frontal view are also corrected
- More qualitative results in the supplementary show that this observation holds broadly
- Overall, the model that estimates latent keypoints (Fig. 4a) has slightly lower performance than the separatehead baseline, likely because latent keypoints may be placed at less characteristic locations on the body and can thus be harder to localize
- Further, the latent keypoint head's weights are initialized from scratch, whereas the regularization-based method fine-tunes a pretrained head
- The hybrid combination from Fig. 4a performs slightly worse than the model that is only regularized, but in many cases still outperforms the baseline

### Comparison to Prior Works
- Our final results are better than state-of-the-art published works
- We use large-scale multi-dataset training to improve accuracy
- We use chirality equivariance to enforce symmetry
- We use a learning rate schedule that starts at 2.12e-4 and decays by a factor of 3 over 92% of training
- We use a warm restart on the last layer (the prediction head) to ensure that the regularization loss can take effect
