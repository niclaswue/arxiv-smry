---
title: "SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images"
date: 2023-01-12T09:00:42.000Z
author: "Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, Kuniko Saito"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04883v1.webp" # image path/url
    alt: "SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04883)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04883).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/slidevqa-a-dataset-for-document-visual).

# Abstract
- Visual question answering on document images that contain textual, visual, and layout information, called document VQA, has received much attention
- Although many datasets have been proposed for developing document VQA systems, most of the existing datasets focus on understanding the content
- In this study, we propose a new multi-image document VQA dataset, SlideVQA, containing 2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a slide deck
- SlideVQA requires complex reasoning, including single-hop, multi-hop, and numerical reasoning, and also provides annotated arithmetic expressions of numerical answers for enhancing the ability of numerical reasoning
- Moreover, we developed a new end-to-end document VQA model that treats evidence selection and question answering in a unified sequence-to-sequence format
- Experiments on SlideVQA show that our model outperformed existing state-of-the-art QA models, but that it still has a large gap behind human performance. We believe that our dataset will facilitate research on document VQA.

# Paper Content

## Introduction
- Building intelligent agents that can read and comprehend real-world documents, such as webpages, office documents, lecture slides, etc., has been a long-standing goal of artificial intelligence.
- To achieve this goal, machine reading comprehension (MRC), a central task in natural language understanding, has been intensively studied.
- The typical definition of the MRC task is quite simple, wherein given a short natural language text as a context and a question about it, a machine reads the text and then answers the question by extracting a span from the text (Rajpurkar et al. 2016;Rajpurkar, Jia, and Liang 2018).
- However, this definition is far from real-world applications, such as customer service chatbots on e-commerce websites (Cui et al. 2017) and assistant systems for reading professional literature (Hong et al. 2019), in that the context is composed entirely of text, with no graphical elements.
- To this end, visual question answering on document images (document VQA) has received much attention.
- It is a challenging vision and language task that requires methods to reason about document layout, textual content, and visual elements (Mathew, Karatzas, and Jawahar 2021;Tanaka, Nishida, and Yoshida 2021;Mathew et al. 2022).
- When the primary content in a document is text (e.g., e-mails and forms) and the task is to understand it on the basis of its layout information, state-of-the-art models have already achieved nearly human-level performance (Xu et al. 2021;Powalski et al. 2021).
- On the other hand, challenges remain when it comes to handling diverse real-world documents. First and foremost is that current models are not capable of performing reasoning across multiple images since the existing datasets focus on testing reasoning ability on a single image.
- Moreover, compared with humans, document VQA models still have trouble understanding documents that contain visual elements and understanding questions that require numerical reasoning (Mathew et al. 2022).
- To address the above challenges, we introduce a new document VQA dataset, SlideVQA, for tasks wherein given a slide deck composed of multiple slide images and a corresponding question, a system selects a set of evidence images and answers the question.
- Slide decks are one of the most efficient document types that arrange visual and textual elements for communication.
- As shown in Figure 1, Slide-VQA requires complex reasoning over slide images, including single-hop, multi-hop, and numerical reasoning.
- These reasoning skills play essential roles in MRC tasks (Yang et al. 2018;Dua et al. 2019).
- Our main contributions are summarized as follows:
- We introduce a novel task and dataset, SlideVQA, wherein to answer its questions, a machine has to read and comprehend a slide deck.
- It is the largest multiimage document VQA dataset containing 2.6k+ slide decks (each consisting of 20 slides) and 14.5k questions.
- It also provides bounding boxes around textual and visual elements for understanding document layout and arithmetic expressions for numerical reasoning.
- We developed a Multi-Modal Multi-image Document VQA model, M3D, to jointly perform evidence selection and question answering tasks and to enhance numerical reasoning by generating arithmetic expressions.
- The research sheds light on how journalists conceive story ideas. Internal meetings, tip-offs, events and primary research were the most popular sources with 63 percent of journalists relying on these activities for story ideas. Internal brainstorm sessions and editorial meetings were found to be the most preferred sources for generating fresh content-related ideas. Online content and social networks seem to be triggers for the same pie of journalists across all experience levels.
- In an informal interview chat, one of the journalists said that reading and surfing could provide some cues, but that it was sheer hard work when one finally wrote a story. There was no way "one could do desktop stories'', said another journalist. Yet, another journalist felt that the Net could provide a trigger.
- Seasoned journalists, more often than not, develop sustainable relationships with their sources, consult...

## Related Work
- Abstractive: The model outputs the answer without any reference to the slide images.
- Single-span: The model outputs the answer for a single slide image.
- Multi-span: The model outputs the answer for multiple slide images. Figure 1: Answer types for SlideVQA.
- Non-span: The model outputs an answer that does not depend on any of the slide images. The dataset for SlideVQA is 14.5k questions. The dataset is divided into two parts: the first part is for training the model, and the second part is for testing the model. The training dataset contains 10,000 questions and the testing dataset contains 10,000 questions. The questions in the training dataset are divided into two types: abstractive and non-abstractive. The questions in the testing dataset are divided into two types: abstractive and non-abstractive. The abstractive questions are questions that require the model to output the answer without any reference to the slide images. The non-abstractive questions are questions that require the model to output the answer for a single slide image. The model is trained using a multi-task learning algorithm. The model is tested using a cross-validation algorithm.

### Dataset Collection
- 25,327 slide decks were collected
- The first 20 slides were kept
- The slides were filtered to only contain English content and easy to understand
- Bounding boxes and categories were annotated to facilitate understanding of the semantic components of images
- 12,466 QA pairs were created by asking workers to select a single slide from a slide deck
- Arithmetic expression were annotated to provide numerical answers

### Statistics and Analysis
- SlideVQA contains the largest number of images and annotations of any of the datasets shown in Table 1.
- SlideVQA contains 14,484 QA pairs from 2,619 slide decks, consisting of 52,480 slide images annotated with 890,945 bounding boxes.
- We split the dataset into 10,617 questions for training, 1,652 (2,215) questions for development (test), making sure that each deck appears in the same split.
- Images. SlideVQA provides the largest number of images covering broad range of topics among the datasets shown in Table 1. Moreover, SlideVQA provides the largest number of bounding box annotations, where the number of the annotations in SlideVQA is 14.7 times that of VisualMRC.
- Figure 3a shows the distribution of bounding boxes broken down into nine categories, which cover all classes, including visually related ones (Image and Figure ), unlike DocVQA and DocCVQA.
- To analyze the OCR tokens, we extracted the text shown in the images by using the Google Cloud Vision API.
- As a result, the number of OCR tokens the system should consider simultaneously is larger (1488.88 tokens) than those of single-image document VQA datasets; the largest dataset (InfographicVQA) has 217.89 tokens.
- Questions and answers. As shown in Table 1, SlideVQA requires complex reasoning including single/multi-hop, and numerical reasoning.
- Figure 3b shows the diverse distribution of questions related to reasoning types.
- 49.3% of the questions require multi-hop or numerical reasoning. Moreover, SlideVQA provides annotations of arithmetic expressions to improve numerical reasoning.
- Figure 3c shows the distribution of numerical operations.
- 25.5% of the numerical questions require arithmetic operations, which current systems have particular difficulty answering.
- Figure 3d shows that multi-span and non-span account for 32.4% of the answers, indicating systems also need to generate answers as well as extract multiple spans.
- Figure 4 shows the sunburst pattern of the first three words of the questions. "In" and "Regarding" are frequent first words because SlideVQA needs to search for evidence images from a slide deck, which is a special pattern in multitext document QA (Yang et al. 2018).

## Our Model
- Figure 5: Overview of the M3D model
- FiD is a state-of-the-art multi-text encoder-decoder model
- We initialize FiD with a pre-trained T5 model
- We extend FiD to perform the end-to-end SlideVQA task

### Multi-modal Task-Specific Input
- Input token sequence.
- For each image I k , we first use Faster-RCNN (Ren et al. 2015), which was trained on Slide-VQA, to extract N semantic regions (bounding boxes) and their labels (e.g., Title and Image).
- We parse the slide image for each extracted region r by using an OCR engine and apply a sub-word tokenizer to obtain OCR tokens W r k = {w r k,1 , . . . , w r k,n } and corresponding OCR bounding boxes.
- To jointly train the evidence selection and question answering tasks, we add different task prefixes t âˆˆ {Evidence Selection, Question Answering} to the encoder input.
- Specifically, the input sequence is as follows: x k = (task:t question:q page:e k context:c k ), where the sequence concatenates each slide and page number pair (c k , e k ) with the question q and task prefix t.
- To tell the role of each region, we insert region labels [R ri k ], corresponding to the region label of the i-th region r i in k-th page, before the OCR tokens W ri k extracted in r i :
- Input embedding.
- Following LayoutT5 (Tanaka, Nishida, and Yoshida 2021), the input embeddings z of the encoder are defined by utilizing multi-modal information, including token z token , segment z seg , layout z lay , and visual embeddings z vis as follows:
- where LN is a layer normalization (Ba, Kiros, and Hinton 2016), and L and d are the length of the input sequence and a hidden vector size, respectively.
- The segment embedding indicates which regions are included in the input sequence.
- The layout embedding denotes the encoded bounding box coordinates of the token within the image.
- We normalize all coordinates by the size of images and use embedding layers to embed x-axis and y-axis features separately.
- The visual embedding is the appearance feature of each region and the OCR bounding boxes, which were obtained from Faster-RCNN.

### Multi-modal Encoder-Decoder
- Multi-modal encoder: A stack of m Transformer blocks, consisting of a self-attention layer and a fully-connected layer with residual connections.
- Answer/arithmetic-expression decoder: Another stack of m Transformer blocks similar to the multimodal encoder, where each block has an additional layer of cross-attention between the output sequence and X.
- Evidence selector: Shares the weights and the architecture of the answer/arithmetic-expression decoder.
- Training and inference: Our model is trained by minimizing the weighted sum of two losses L = L dec + L sel , where L dec and L sel are the negative log-likelihood between the ground-truth and the prediction regarding the decoder and selector, respectively.
- Evidence selection baselines: We also evaluated the evidence selection task alone.
- Question answering baselines: In addition to the pipeline models, we developed Q-only, which takes only the question into T5.
- Human performance: We asked six crowdworkers (not among those recruited to collect our dataset) to select slide images relevant to the question and answer the question.

### Implementation Details
- We implemented all of the models in PyTorch
- We experimented on eight Tesla V100 32GB GPUs
- The size of CLIP was Large
- The size of the other models was Base
- We fine-tuned the models using AdamW (Loshchilov and Hutter 2017) with a learning rate of 5e-5 and a dropout rate of 10%
- We linearly warmed up the learning rate over 1000 steps
- The batch size was set to 32
- We evaluated models every 500 steps and selected the best one on the development set on the basis of the loss
- We used a maximum length of 200 tokens for each input sequence of M3D, and set the maximum target sequence length to 50
- We trained Faster-RCNN (Ren et al. 2015) with a ResNet-101 (He et al. 2016) backbone by using stochastic gradient descent (SGD) (Ruder 2016) with a learning rate of 1e-3 and batch size of one
- Standard anchor scales of [8,16,32] and anchor ratios of [0.5, 1.0, 2.0] were used

### Experimental Results and Analysis
- Does our model outperform the baselines? Table 2 summarizes the results of the main tasks. As shown in Table 2a, M3D outperformed the baselines on joint EM/F1, where the metrics evaluate the consistency between the predicted evidence and answers.
- For the evidence selection task, Table 2b shows that H-LayoutLMv2 and M3D performed better than the baselines. This indicates that modeling the interaction between multiple slides simultaneously is needed to improve performance.
- For the QA task, Table 2c shows that M3D outperformed the pipeline methods in all metrics. Our end-to-end M3D model is better at ignoring the slides irrelevant to the question than the answer generator in the pipeline methods that strongly depend on the slides narrowed down by the evidence selector.
- However, M3D GT in Table 2a achieved a significant improvement by knowing the ground-truth slides. There is room for improving the correctness of evidence selection.
- What are the characteristics of our dataset? Table 2 shows that adding modality information tended to improve performance in all tasks. This demonstrates that SlideVQA requires methods to have the ability to jointly understand the text, layout, and visual modalities of documents.
- As shown in and Hoi 2020), especially the ability to read texts in images. Tables 2a and 2c show that LayoutT5, a generative model, significantly outperformed LayoutLMv2, an extractive approach. This result is inline with observations on the DROP dataset (Dua et al. 2019), which also has non-span answers (Geva, Gupta, and Berant 2020). Additionally, all of the models performed all of the tasks significantly worse than humans.
- To be specific, Figure 6 illustrates that (i) better multi-hop reasoning over multiple images is needed and (ii) non-span answers to questions involving arithmetic operations have to be improved.
- Table 5: Object detection performance of Faster-RCNN broken down by bounding box categories. We set an intersection-over union (IoU) threshold to 0.5.
- fective. More precisely, the arithmetic expression (AE) generation was influential on the QA and Joint performance, meaning that predicting the arithmetic expression instead of the numerical value enhances the ability to generate answers with numerical reasoning.
- As shown in Figure 6, applying AE prediction increased F1 by a large margin (+10.4%) in the arithmetic type.

## Discussion and Limitations
- SlideVQA is the largest document VQA benchmark that uses multiple images as input and requires multi-hop reasoning; its limitation is that the multi-hop questions created by editing are different from the questions humans might actually ask the system.
- We argue that developing models that can reason over multiple images is an important research direction, and therefore, we employed an editing method  that guarantees multi-hop questions and easily extends the dataset size.
- Also, our model uses cross-attention on all evidence candidates, which may cause a computational problem when there are a lot of input images (e.g., as in the opendomain QA setting like DocCVQA). To remedy this problem, we consider that models that train a two-stage selector that roughly narrows down candidates to a small number of images and then accurately selects evidence images and an answer generator in an end-to-end manner are promising (Sachan et al. 2021a,b).

## Conclusion
