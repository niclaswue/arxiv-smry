---
title: "Improving Unsupervised Video Object Segmentation with Motion-Appearance Synergy"
date: 2022-12-17T06:47:30.000Z
author: "Long Lian, Zhirong Wu, Stella X. Yu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08816v1.webp" # image path/url
    alt: "Improving Unsupervised Video Object Segmentation with Motion-Appearance Synergy" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08816)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08816).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/improving-unsupervised-video-object).

# Abstract
- IMAS is a method that segments the primary objects in videos without manual annotation
- Previous methods in unsupervised video object segmentation (UVOS) have demonstrated the
- However, motion signals may be uninformative or even misleading in cases such as
- In contrast, IMAS achieves Improved UVOS with Motion-Appearance Synergy. Our method has
- Additionally, we propose motion-semantic alignment as a model-agnostic annotation-free
- IMAS greatly improves the segmentation quality on several common UVOS benchmarks. For

# Paper Content

## Introduction
- Video object segmentation is a task that is widely researched
- Many video object segmentation algorithms require pixel-wise manual annotation
- In recent years, popularity has been gained in the task of unsupervised video object segmentation
- Most of these methods learn to segment foreground objects without human annotation

## Method
- IMAS is a unsupervised machine learning algorithm that uses a flexible motion model with a learnable residual pathway to allow implicit appearance learning when motion cues conflict with appearance.
- The first/second stage of IMAS does not require human annotation, which makes it fully unsupervised.
- We present motion-semantic alignment as a model-agnostic unsupervised hyperparam tuner.

## Related Work
- Unsupervised video object segmentation (UVOS) requires segmenting prominent objects from video sequences without human annotation.
- Mainstream benchmarks on UVOS [3,23,34,36] define the task as a figure-ground problem with binary predictions in which salient objects are the foreground.
- Despite the name, several previous UVOS methods require supervised (pre-)training on large-scale images or videos with manual annotation [12,20,24,29,37,49,51,52].
- In contrast, we focus on a line of work named fully unsupervised VOS, which does not rely on any human annotation at either training or inference time.
- For fairness, we compare with previous literature on fully UVOS.
- Motion segmentation aims at exploiting the motion information to separate foreground and background, typically through using optical flow from a pretrained model.
- FTS [35] utilizes motion boundaries for segmentation.
- SAGE [44] additionally considers edges and saliency priors jointly with motion.
- CIS [50] uses independence between foreground and background motion as the goal for foreground segmentation.
- However, such property is not always satisfied in complex real-world motion patterns.
- MG [48] leverages attention mechanisms to group pixels with similar motion patterns.
- SIMO [22] and OCLR [46] propose to generate synthetic data for segmentation supervision, with the latter supporting individual segmentation on multiple objects, but both methods rely on human-annotated sprites for realistic shapes in artificial data synthesis.
- These motion segmentation methods leverage optical flow as input and thus suffer when foreground objects are moving at a similar speed to the background.
- Recently, appearance-based UVOS methods found a way around by using motion only as a supervisory signal for learning the moving tendency.
- AMD [26] proposes to jointly learn a segmentation model and an optical flow model so that flow prediction is constructed to be piecewise constant according to segmentation.
- GWM [8] proposes to match the piecewise affine flow with filtered optical flow generated by a pretrained flow model [42] and is the current state-of-theart of several UVOS tasks.
- Despite the success of appearance-based UVOS, several caveats from motion signals still hold: 1) Articulated or deformable objects often have complex non-rigid motion hard to fit into hand-crafted motion models, causing false negatives. 2) Naturally-occurred common motion (e.g., reflections and shadows) often provides misleading supervision, resulting in false positives.
- To tackle such issues, we use motion cues to discover the foreground objects with a learnable residual pathway that allows implicit appearance learning.
- Subsequently, appearance cues are explicitly leveraged as training signals for misconception correction.

### Problem Setting
- UVOS is a binary segmentation masking algorithm for video object recognition
- The objective of UVOS is to create a binary segmentation mask for each frame in a sequence, with 1 indicating foreground and 0 otherwise
- To evaluate a method on UVOS, the mean Jaccard index J between predicted segmentation mask M t and ground truth G t is computed
- Since UVOS is performing fully unsupervised video object segmentation, ground truth mask G t is assumed to be unavailable and no human-annotated data are used throughout training and inference
- Following appearance-based UVOS work [8], a motion model is used to supervise foreground objectness learning
- However, rather than hand-crafted motion models, a learnable motion model is proposed with a residual pathway
- The residual pathway is composed of a piecewise constant pathway and a learnable residual pathway
- The final flow prediction Ft takes into account relative motion that is within (−λ, λ) for each spatial location

## Methods

## Experiments

### Datasets
- We evaluate our methods on three datasets commonly used to benchmark UVOS
- The DAVIS2016 dataset contains 50 video sequences of 3,455 frames
- The SegTrackv2 dataset contains 14 videos of different resolutions with 976 annotated frames
- The FBMS59 dataset contains 59 videos with 13,860 frames in total and 720 frames annotated
- We adopt mean Jac-card index J (mIoU) as the main evaluation metric

### Unsupervised Video Object Segmentation
- Our architecture is simple and straightforward
- We use a ResNet50 backbone followed by a segmentation head and a residual prediction head
- Both heads only consist of three Conv-BN-ReLU layers with 256 hidden units
- This standard design allows efficient implementation for real-world applications
- We use C = 4 object channels by default, which is found without human annotation, as we will show in Sec.
- The RAFT model that we use is only trained on synthetic FlyingChairs and Fly-ingThings datasets without human annotation
- We refer readers to supp. materials for more details.

### Motion-Semantic Alignment for Hyperparam Tuning
- We use motion-semantic alignment as a metric to tune two key hyperparameters, the number of segmentation masks C and the object channel index c o .
- As in Fig. 5, despite not using any manual annotation, we observe that using motion-semantic alignment is highly effective for hyperparam tuning due to its high correlation to downstream performance evaluated with ground truth.
- For the number of segmentation masks C, we found that increasing the number of channels improves the segmentation quality of our model by increasing the fitting power. However, such an increase saturates at C = 4. Therefore, we use C = 4 for all experiments unless otherwise stated.
- For the object channel index c o , since c o changes w.r.t random initialization by design [26], optimal c o needs to be obtained at the end of each training run. We propose to leverage the redundancy in video sequences and use only the first frame of each video sequence for finding c o . With this adjustment, our tuning method could complete within only 3 seconds for each candidate channel, which allows our tuning method to be performed after the whole training run with negligible overhead to find the object channel c o .
- Our method is robust to uninformative or even misleading motion cues. Comparisons with CIS [50] and AMD [26] show our improvements in challenging scenes with complex foreground motion (a)(b), distracting background motion (a)(c), depth effect from camera motion (c), and fast camera/object motion (c)(d).

### Ablation Study
- Contributions of each component. As in Tab. 3, residual pathway allows more flexibility and contributes 5.4%.
- Feature merging indicates concatenating the feature from the first block and the last block of ResNet for higher feature resolution (96 × 96/98 × 175 in training/inference), which allows an additional 3.4% improvement with negligible increase in model size.
- Note that our prediction resolution after feature merging is still lower than most previous works (e.g., 128×224 in [48] and 192×384 in [50]) and thus our performance gain to other methods is not due to higher output resolutions.
- The explicit appearance refinement from the second stage boosts the performance to 77.7%, with a 10.1% gain in total.
- The CRF post-processing leads to 79.8% in mean Jaccard index, a 12.2% increase to baseline.
- Designing additional pathway. In Tab. 4, we show that robustness loss [25,41] does not effectively relieve from misleading motion. We also implemented a pixel-wise scaling pathway, which multiplies each value of the motion vector by a predicted value. Furthermore, we fit an affine transformation per segmentation channel as the residual. The pixelwise residual performs the best in our setting and is chosen in our model, showing the effectiveness of a learnable and flexible motion model inspired by relative motion.
- Orthogonality of our appearance supervision with postprocessing. The refined masks after our appearance-based refinement have the same resolution as the original exported masks. Therefore, the refinement CRF in stage 2 has an orthogonal effect to the upsampling CRF in post-processing that is mainly used to create high-resolution masks from bilinearly-upsampled ones.
- As shown in Tab. 5, the gains that come from post-processing do not diminish as we apply appearance-based refinement in stage 2, which also shows the orthogonality of our refinement to post-processing.

### Visualizations and Discussions
- IMAS is a method that adapts to challenging cases such as complex non-uniform foreground motion, distracting background motion, and camera motion including rotation.
- However, IMAS still has limitations: IMAS does not work if neither motion nor appearance provides informative signals and thus is misled by the texture of the cow in Fig. 6(e).
- Although IMAS has the ability to recognize multiple foreground objects when moved in sync, it sometimes learns to capture only one object when the objects move in very different patterns.
- Finally, IMAS is not designed to separate multiple foreground objects.

## Summary
- IMAS is an unsupervised video object segmentation method that leverages motion-appearance synergy
- Our method has an object discovery stage with a conflict-resolving learnable residual pathway and a refinement stage with appearance supervision
- We also propose motion-semantic alignment as an annotation-free hyperparam tuning method
- Extensive experiments show our effectiveness and utility in challenging cases
- We present additional visualizations on the three main datasets that we benchmark on [3,23,34,36]
- We demonstrate high-quality segmentation in several challenging cases
- We also discuss some limitations of our method with examples
- As in Fig. 7, the introduction of residual pathway allows our segmentation prediction to better align with the appearance cues rather than exactly correspond to the motion signals, which leads to high quality segmentation especially when appearance and motion cues conflict
- Modeling relative motion and other non-uniform motion patterns in 2D flow such as the depth effect, residual pathway makes our method flexible and robust to objects with complex motion by leveraging synergy in motion and appearance
- As in Fig. 8, our method is much more robust to scenes when motion and appearance conflict and could segment out multiple foreground objects as foreground when they move together, as consistent with human perception
- However, when neither motion nor appearance is informative, our method may be mislead to exclude part of an object
- As in Fig. 9, our method could model the non-uniform 2D flow from object rotation in 3D with residual flow
- Our method also captures multiple objects in a foreground group when they share similar motion
- Our method is robust to misleading common motion naturally occurred in realworld scenes such as reflections and to the camera motion that leads to non-uniform background flow
- However, our method may select to focus on only one of the foreground objects if one has significantly larger motion than the other
- Our method still could not work in scenes with misleading motion and complicated appearance unable to be parsed by the model
- As in Fig. 10, our method is robust to scenes with complicated appearance distractions
- Our method also works with fine details as well as camera motion and adjustment (e.g., zooming in)
- However, when multiple objects exist and one moves significantly faster than the other, our method sometimes focuses on the object with larger movement, setting them as the only foreground.

### Applying Motion-Semantic Alignment on Previous Work
- Our hyperparam tuning method with motion-semantic alignment is model-agnostic
- Our tuning method also works on AMD to find the number of segmentation channels and the object channel index, showing that our tuning method is modelagnostic

### Additional Implementation Details
- Our setting mostly follows previous works
- We treat video frame pair {t, t + 1} as both a forward action from time t to time t+1 and a backward action from time t+1 and t, since they follow similar rules
- We use this to implement a symmetric loss that applies the loss function both forward and backward and sum them up to get the final loss
- Note that this could be understood as a data augmentation that always supplies a pair in forward and backward to the training batch
- However, since our ResNet shares weights for each image input, the feature for each input is reused by forward and backward action, thus the symmetric loss only adds marginal computation
- Our method is much more robust to scenes when motion and appearance conflict (a-c)
- Our method also segments out multiple foreground objects as foreground when they move together, as consistent to what humans perceive
- However, when neither motion or appearance is informative, our method may be mislead to exclude part of an object
- Adds marginal computation and is included in our implementation as well
- Furthermore, following [26], for DAVIS16, we use random crop augmentation at training to crop a square image from the original image
- At test time, we directly input the original image (which is non-square)
- Note that the augmentation makes the image size different for training and test, but as ResNet [14] takes images of different sizes, this does not pose a problem empirically
- In STv2 and FBMS59, the images have very different aspect ratios (some having a height lower than the width) and thus a square crop is no longer feasible to obtain the same size of images for a batch unless we resize
- Therefore, we perform resize rather than random crop to make all images the size of 480p before the standard pipeline
- However, this reduces the randomness in augmentation and may lead to loss in performance, which remains to be addressed as an implementation decision for the future to further improve the performance
- We additionally use pixel-wise photo-metric transformation [9] for augmentation with default hyperparam
- As for the architecture, we found that simply adding a convolutional head to the last layer feature in ResNet provides insufficient detailed information for high-quality output
- Rather than incorporate a heavy segmentation head (e.g., [5] in [8]), to keep our architecture easy to implement, we only change the head in a simple fashion by fusing feature from the first residual block and the third residual block in ResNet
- This allows the feature to jointly capture highlevel information and low-level details, which is termed feature merging
- Since the backbone remains the same, feature merging adds negligible compute
- Note that feature merging is only in segmentation head, and residual prediction is simply bilinearly upsampled
- Due to lower image resolution, no feature merging is performed for STv2
- Following [8], we load self-supervised ImageNet pretrained weights
- Additional visualizations on STv2 [23]

### Per-sequence Results
- We list our per-sequence results on DAVIS16, STv2, FBMS59 in Tab. 6, Tab. 7, and Tab. 8, respectively.
- We use the same per-sequence running time on all three algorithms.
- We use the same number of sequences for all three algorithms.
- We use the same number of threads for all three algorithms.
- We list our per-sequence results on DAVIS16, STv2, FBMS59 in Tab. 6, Tab. 7, and Tab. 8, respectively.
- We use the same per-sequence running time on all three algorithms.
- We use the same number of sequences for all three algorithms.
- We use the same number of threads for all three algorithms.

### Future Directions
- Our method does not impose temporal consistency, information redundancy from neighboring frames is not leveraged effectively.
- Our method could be more robust by using such information to deal with frames with insufficient motion or appearance information.
- Temporal consistency measures such as matching warped predictions as a regularization could be taken care of with an additional loss term or with post-processing as in [50].
- Furthermore, our method currently does not support segmenting multiple parts of the foreground. Methods such as normalized cuts [40] could be used to split the foreground into a number of objects with motion and appearance input for training the model with bootstrapping to provide training or correction signals.
- Figure 1. IMAS generates high-quality segmentation and is robust to uninformative or even misleading motion signals.
- Best viewed in color and zoom in.
- More in Fig. 6.
- †: with post-processing.
