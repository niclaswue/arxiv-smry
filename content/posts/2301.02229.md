---
title: "All in Tokens: Unifying Output Space of Visual Tasks via Soft Token"
date: 2023-01-05T18:55:20.000Z
author: "Jia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai, Kun He, Han Hu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-02229v1.webp" # image path/url
    alt: "All in Tokens: Unifying Output Space of Visual Tasks via Soft Token" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02229)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02229).


# Abstract
- The output space of visual tasks is more complicated than language tasks
- To unify the output space of visual tasks, we propose a single unified model that simultaneously handles two typical visual tasks
- Soft token is used to represent the task output
- Mask augmentation is used to improve the accuracy of task outputs

# Paper Content

## Introduction
- A unified model for various tasks across modalities is one of the most ambitious goals of artificial intelligence
- However, this is challenging due to the diversity and * Equal Contribution.
- Jia, Chen, and Zigang are interns at MSRA.
- Despite the difficulties, large-scale language models in the field of natural language processing (NLP), e.g., GPT-3 [6], have demonstrated impressive capabilities as a general-purpose solver for language tasks.
- Encouraged by the success of NLP, this paper attempts to investigate the possibility of universal models for various computer vision tasks.
- Most existing related research for universal computer vision models focuses on the unification of architectures [19,39] and pre-training [2].
- A few works aim for developing one model for multiple visual tasks, yet they are usually applicable to limited tasks: [1] handles only tasks with language as output; CLIP models [32] and the followups [47,48,51] tackle only retrieval and image classification tasks; [9] deals with tasks that have describable and sequential outputs.
- In this paper, we aim for a truly generalpurpose solver for various vision tasks.
- To this end, we first notice that a key obstacle was overlooked and underexplored: unlike language tasks whose inputs and outputs are represented by language tokens of the same form, the output forms for different computer vision tasks are more diverse.
- For example, the output of object detection is a set of coordinates and labels; the output of semantic segmentation is a discrete label map; the output of depth estimation is an image with floating-point values; and the output of flow estimation is a vector field.
- In this paper, we address this obstacle by unifying the output space of various visual tasks through a general tokenizer to encode the output space into a set of tokens. Specifically, the proposed framework consists of three components: tokenizer, detokenizer, and task solver, as illustrated in Figure 1.
- The tokenizer and detokenizer are instantiated by VQ-VAE [37], which encodes the task output into a set of tokens, which are then reconstructed into the original output by the decoder.
- The task solver for different vision tasks is instantiated using an auto-regressive encoder-decoder model.
- The encoder-decoder model takes images as inputs and predicts a token sequence in a causal mode, which is decoded into the original task-specific output form by the VQ-VAE decoder.
- To improve the effectiveness, we propose several new techniques that take into account the particularity of visual tasks. First, we propose soft token instead of hard ones as used in language tasks or by the original visual VQ-VAE framework. The soft token is represented by probability vectors, with each value in a vector denoting the probability belonging to a codebook. When a soft token is fed to the input of the detokenizer or to the input of the next token prediction network, its input embedding is computed by the weighted average of the codebook embeddings based on the probability values. This indicates that the soft token embedding has spanned an interpolable continuous space, which may better represent the visual output, especially when it's continuous.
- The continuous nature of the soft token also allows the introduction of an auxiliary loss which learns the task output end-to-end, back from the detokenizer output to the task-solver input.
- In experiments, we show that the three usages of the soft token can all lead to better results.
- Second, we propose a mask augmentation technique to deal with visual tasks which have corrupted, undefined, or invalid values in annotations.
- The depth estimation is a typical example of such a task, where the occluded area is not defined [36], as shown in Figure 3.
- The undefined regions make it difficult to the training of VQ-VAE tokenizers and detokenizers because it does not know what to reconstruct in the undefined area. To solve this problem, we randomly mask several patches of the input depth map when training VQ-VAE. Unlike undefined areas, manually masked patches have ground truth annotations, which help train the VQ-VAE network to be able to recover the ground truth for undefined areas.
- In experiments, we show this technique to notably improve the accuracy of depth estimation.
- Third, we systematically study the impact of architectures in the VQ-VAE model. We find that a very lightweight VQ-VAE with up to 5 convolution layers, 2 residual blocks, and 128 codebooks can serve as a very powerful tokenizer.

## Related Works
- T5 [33]/GPT [6] successful in NLP
- A single unified model for various tasks in computer vision has emerged
- Most existing works are focused on the training algorithm or model architectures
- Our study takes more indepth consideration of the particularity of visual problems
- We propose techniques of soft token and mask augmentation, which prove beneficial generally for visual tasks or a part of them
- Vector-Quantization Discretized token output space is widely used in generative models, such as DALL-E [34], VQGAN [14], and VQ-Diffusion [16], to represent highdimensional complex data
- Our approach represents the depth maps as a set of tokens, and unifies it with other visual tasks, i.e. instance segmentation, in a unified network structure and output space
- Monocular Depth Estimation is a fundamental task for 3D perception
- Deep learning dominates the depth estimation since Eigen et al. [13] introduces it into the depth task
- The follow-up works include proposing powerful network [23,24,35], designing novel augmentation [18,20], making use of the geometric constraints [31,46], exploring pairwise relationship [10,22,52], combing with conditional random field [27,43,49]

## Framework
- The goal of this work is to unify the output space of visual tasks into discrete tokens
- In this section, we present the framework to achieve this goal, which is shown in Figure . 1
- The framework consists of three modules: a tokenizer that encodes the task output to the discrete tokens, a detokenizer that decodes tokens to the task output, and a task-solver that predicts tokens from images
- During training, task annotations are first mapped by the tokenizer as discrete tokens and used as supervision to train the task-solver
- In inference, the tokens predicted by the tasksolver are decoded by the detokenizer into task outputs

### Tokenizer and Detokenizer
- VQ-VAE is an encoder-decoder model that was originally proposed to learn discrete representation for natural images
- In this work, we use its encoder and decoder as the tokenizer and detokenizer
- In training, the input image is encoded as a set of contiguous embeddings, and these embeddings are assigned to their nearest latent codes
- In the decoder, the corresponding codes are used as inputs instead of contiguous embeddings and then decoded into the image
- By minimizing the reconstruction loss between the input image and decoded images, the encoder, decoder and latent codes can be trained
- Since we adopt discrete tokens as targets in the tasksolver, the accuracy of reconstruction has an upper-bound on the performance of the whole framework
- Both training and inference of task-solver require the tokenizer and detokenizer, the fast inference speed is also desired
- The original network architecture of VQ-VAE is designed for natural images, which have more complex textures and colors than the task output, making it not the optimal design for us
- Therefore, we have exhaustively studied the effects of VQ-VAE with different design choices in our framework
- Typical reconstruction losses (e.g. l-1 loss, MSE loss, etc.) cannot directly reflect the realistic performance of the task, we use standard evaluation metrics for different tasks to measure VQ-VAE
- As shown in Table 2 and Table 4, we found that a very lightweight VQ-VAE can achieve promising results in depth estimation and instance segmentation
- Since the input value ranges for depth estimation and instance segmentation are different, we train two VQ-VAE models for two tasks separately
- For depth estimation, the encoder consists of 5 convolution layers (kernel size is 3 and stride is 2) and follows 2 residual blocks. The output feature map has a downsample ratio of 32, and the channel dimension is progressively increased from 16 to 256.
- The architecture of the decoder is symmetrical to the encoder, only replacing the convolution layers with the deconvolution layer.
- For instance segmentation, we reduce the convolution and deconvolution of the encoder and decoder from 5 layers to 4 layers and keep all others the same. Subsequently, the downsample ratio is changed to 16.
- In addition, compared to the standard VQ-VAE usually adopts a large codebook size (e.g. 8192), our codebook size can be reduced to 128.
- We note that though larger codebook size consistently improves VQ-VAE reconstruction ability, they show no difference when applied to task-solver

### Task-solver
- The task-solver is an auto-regressive encoder-decoder network
- The encoder is a Swin Transformer with 6 additional standard transformer blocks, each block consists of a self-attention and an FFN
- The decoder has 6 blocks, each block consists of a self-attention, a cross-attention, and an FFN
- The architecture we used is similar to [8]

### Soft Token
- The auto-regressive prediction procedure selects the token with the maximal predicted probability
- However, since the tokens learned by VQ-VAE are not completely independent of each other, the correlation between the tokens may affect the token prediction accuracy
- To leverage the correlation, a soft token technique is presented in the inference: instead of directly using the embedding of a single token, the soft token is the weighted averaged embedding of different tokens by their prediction probability
- In addition to being applied in task-solver to predict the next token more accurately, the same idea can also be used in detokenizor to get better reconstruction results
- The soft token results in the embedding space being spanned to an interpolable continuous space
- Therefore, we can introduce an auxiliary loss that learns the task-specific output targets in an end-to-end manner by backing from the detokenzior output to the task-solver input.

### Mask Augmentation in Depth Estimation
- The ground-truth depth maps in the depth estimation dataset often have some corrupted regions that are not annotated with depth information.
- In the conventional depth estimation frameworks, these regions are ignored during training.
- However, the same solution cannot be applied to our framework.
- There are two challenges: First, while we have ignored these regions in training VQ-VAE as well, the reconstructed regions are still abnormal (see Figure 3) and further affect the training of the task-solver and make the final result also have many artifacts; Second, a token predicted by VQ-VAE corresponds to a 32 2 patch, which may contain both normal and corrupted pixels.
- Therefore, it is hard to deal with this issue by ignoring the tokens.
- As shown in Figure 4, we present to introduce mask augmentation in the training of VQ-VAE to alleviate this challenge.
- Specifically, we randomly mask some regions in the input depth images and then use their original depth information as supervision.
- In this way, the VQ-VAE can complete/recover some corrupted regions with reasonable results.

## Experiments

### Tasks and Datasets
- To examine the generalizability of our framework, we choose depth estimation and instance segmentation, which are two tasks with very different output spaces.
- The instance segmentation requires predicting the location, the class, and the mask of each instance. The COCO2017 benchmark is one of the most challenging datasets for this task. It consists of 117K training images, 5K validation images, and 41K test images. A total of 80 classes annotation are provided.
- In our experiments, we follow the common setting of previous works [17,40,44] that report the performance on the validation set for comparison.
- Depth Estimation Depth estimation is a fundamental problem in computer vision, which requires estimating the depth for each pixel. Unlike segmentation whose output is a binary mask, the depth map is a floating point image. In this work, we use the NYUv2 Depth dataset, which consists of 24K training images and 654 validation images, and the RMSE is used as the major metric.

### Implementation Details
- We train two separate VQ-VAE for depth estimation and instance segmentation
- The input image size used in depth is 480 2
- The Adam optimizer is used with the base learning rate of 1e-3, α 1 = 0.9, α 2 = 0.999
- An exponential learning rate schedule is applied with the learning rate decay of 0.98 and a total of 20 training epochs
- For instance segmentation, the input image size is 64 2 with a batch size of 512
- The Adam optimizer is used with the base learning rate of 3e-4, α 1 = 0.9, α 2 = 0.999
- An exponential learning rate schedule is applied with the learning rate decay of 0.98 and a total of 20 training epochs
- By default, the EMA model update technique is used for all VQ-VAE models
- For the task-solver, we adopt the auto-regressive encoder-decoder architecture, which is similar to Pix2Seq [8]
- Most experiments in the ablation study are separately trained in depth estimation and instance segmentation
- In depth estimation, we use the AdamW optimizer with a base learning rate of 2e-4, α 1 = 0.9, α 2 = 0.999, a weight decay of 0.05, and a layer decay of 0.9
- The total training length is 25 epochs and the batch size is 24
- The step learning rate schedule is used and the learning rate dropped to 2e-5 at the 18th epoch
- For data argumentation, the random cropping of 480 2 and horizontal flip with probability 0.5 are employed
- For instance segmentation, the AdamW optimizer with a base learning rate of 1e-4, alpha 1 = 0.9, alpha 2 = 0.999, a weight decay of 0.05, and a layer decay of 0.85, linear decay learning rate scheduler is applied, and the total training length is 50 epochs with a batch size of 16
- Large-scale jittering with the range of 0.1 to 3.0 and crop size of 640 2 are used

### Ablation Study
- We ablate the key design choices and techniques in this section
- By default, we train the model separately for each task in the ablation study for better illustration, and Swin-B is used as the default backbone
- performance, our evaluation is performed on the validation set of different tasks and adopts mask mAP and RMSE as metrics
- The results are shown in Table 1
- We find that although the large codebook size (e.g. 256) benefits the reconstruction performance, the small codebook size (e.g. 64) can also yield sufficiently good reconstruction performance. Further applying to the task-solver, we found different codebook size has little effect on final performance (see Table 2)
- Using the same evaluation method, we study the effect of the width of VQ-VAE
- Table 3 shows the reconstruction performance and Table 4 shows the performance of applying to task-solver
- Similar to the observation on codebook size, we find that the network width has little effect on the final performance
- The downsample ratio of VQ-VAE is another key that may affect network performance
- We vary the downsample ratio in [8,16,32]
- We note that the instance segmentation cannot support a downsample ratio of 8 because it results in too long sequences
- Table 5 shows the reconstruction performance, as the downsample ratio increases, the reconstruction performance gets worse, satisfying the intuition
- However, we find that better reconstruction performance does not always lead to better performance when applying the VQ-VAE in task-solver
- In Table 6, the best performance is achieved at downsample ratio of 32
- We explain this phenomenon is that a smaller downsample ratio facilitates reconstruction, but it also increases the length of the token sequence, which is detrimental to the task-solver

### Preliminary Study on Parallel Prediction
- The auto-regressive decoder is not as good as the parallel decoder
- The parallel decoder is better than the auto-regressive decoder

### Comparison with Other Unified Frameworks and State-of-the-arts in Depth Estimation
- UViM and Unified-IO are the most relevant works to ours
- Our auto-regressive approach achieves 0.289 RMSE, which is 0.178 and 0.096 better than UViM and Unified-IO
- Our parallel approach further improves the performance to 0.279 RMSE

### One Model for Multiple Tasks
- We use a shared task-solver to train the instance segmentation and depth.
- Table 10 shows that the joint training with shared model weights has marginal performance gradation.

## Conclusion
- We propose a unified representation for the output of a number of vision tasks, including object recognition, 3D reconstruction, and image captioning
- The proposed representation is based on a set of visual tokens
- The proposed unified representation is more efficient and accurate than the existing representations
- The proposed unified representation is more efficient and accurate than the existing representations for a wider range of tasks
- The proposed unified representation is more efficient and accurate than the existing representations for a wider range of tasks when the input data is compressed
- The proposed unified representation is more efficient and accurate than the existing representations when the input data is compressed.

## A. Ablation on the Depths of VQ-VAE
- Table 3 and Table 4 studied how different widths of VQ-VAE affect the performance.
- Table 11 further examines the effect of depth, i.e. different number of residual blocks in VQ-VAE. The result shows that the increasing residual blocks lead to worse performance in VQ-VAE and task-solver.

## B. VQ-VAE vs. Interpolation Tokenizer
- We use a lightweight VQ-VAE as the tokenizer to reduce the resolution and computation for the task-solver.
- For depth estimation, we use a bilinear interpolation method and discretize the floating numbers between 0 to 10 by 128 bins.
- We select the same input and target resolution as our VQ-VAE tokenizer for a fair comparison.

## C. Details for Joint Training
- The AdamW optimizer with a base learning rate of 8e-4, α 1 = 0.9, α 2 = 0.999, a weight decay of 0.05, and a layer decay of 0.85, linear decay learning rate scheduler is applied
- We train the model with 300k iterations and batch size 64
- The decoder loss weights of 5.0 and 1.0 are Figure 1
- Illustration of our unified framework with two stages. In this framework, various vision task outputs are transferred to discrete token space by a VQ-VAE tokenizer. In this way, discrete or continuous visual tasks can be converted into one discrete classified task, for representing coordinates and the class of a bounding box. The coordinates are manually quantized into 2000 bins, and different classes are represented by different tokens (including a background class, e.g. COCO dataset has 81 class tokens in total)
- For the binary mask, we use 4 × 4 tokens to represent one mask. It is worth noting that since the computational complexity of auto-regressive is proportional to the square of the output sequence length, we have to use very few tokens to represent the mask. Nevertheless, benefitting from our powerful detokenizer, these tokens can be decoded to a 64 × 64 mask
- Based on these designs, each instance is represented by a total of 21 tokens (i.e. 4 tokens for coordinates, 1 token for class, 16 tokens for mask), and we use [INS] as the task token of instance segmentation, as shown in Figure 2
- We append the meaningless zero mask for the noise box and do not add loss on those mask tokens during training
- With the well-designed structure and techniques, our model is capable for this task
- Ablation study on codebook size of VQ-VAE on depth and instance segmentation in reconstruction
- Ablation study on codebook size of VQ-VAE on depth and instance segmentation in task-solver
- Ablation study on the width of VQ-VAE on depth and instance segmentation in reconstruction
- Ablation study on the width of VQ-VAE on depth and instance segmentation in task-solver
- Ablation study on the downsample ratio of VQ-VAE on depth and instance segmentation in reconstruction
- Ablation study on the downsample ratio of VQ-VAE on depth and instance segmentation in task-solver
- Ablation on the effectiveness of soft token
- Affects of mask augmentation in training VQ-VAE on depth
- The patch size of all models is set to 16
