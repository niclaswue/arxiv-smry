---
title: "A System-Level View on Out-of-Distribution Data in Robotics"
date: 2022-12-28T18:45:05.000Z
author: "Rohan Sinha, Apoorva Sharma, Somrita Banerjee, Thomas Lew, Rachel Luo, Spencer M. Richards, Yixiao Sun, Edward Schmerling, Marco Pavone"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14020v1.webp" # image path/url
    alt: "A System-Level View on Out-of-Distribution Data in Robotics" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14020)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14020).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-system-level-view-on-out-of-distribution).

# Abstract
- When testing conditions differ from those represented in training data, so-called out-of-distribution (OOD) inputs can mar the reliability of black-box learned components in the modern robot autonomy stack.
- Therefore, coping with OOD data is an important challenge on the path towards trustworthy learning-enabled open-world autonomy.

# Paper Content

## I. INTRODUCTION
- Machine learning (ML) systems are poised for widespread usage in robot autonomy stacks in the near future, driven by the successes of modern deep learning.
- For instance, decision-making algorithms in autonomous vehicles rely on ML-based perception and prediction models to estimate and forecast the state of the environment.
- As we increasingly rely on ML models to contend with the unstructured and unpredictable real world in robotics, it is paramount that we also acknowledge the shortcomings of our models, especially when we hope to deploy robots alongside humans in safetycritical settings.
- In particular, ML models may behave unreliably on data that is dissimilar from the training data -inputs commonly termed out-of-distribution (OOD).
- This poses a significant challenge to deploying robots in the open world, e.g., as autonomous vehicles or home assistance robots, as such robots must interact with complex environments in conditions we cannot control or foresee.
- Coping with OOD inputs remains a key and largely unsolved challenge on the critical path to reliable and safe open-world autonomy.
- However, there is no generally-agreed-upon precise definition of what makes data OOD; instead, its definition is often left implicit and varies between problem formalisms and application contexts.
- In this paper, we concretize the often nebulous notion of the OOD problem in robotics, drawing connections to existing approaches in the ML community.
- Critically, we advocate for a system-level perspective of OOD data in robotics, which considers the impacts of OOD data on downstream decision making and allows for leveraging components throughout the full autonomy stack to mitigate negative consequences.
- To this end, we present robotics research challenges at three timescales crucial to deploying reliable open-world autonomy: (i) real-time decision-making, (ii) episodic interaction with an environment, and (iii) the data lifecycle as learning-enabled robots are deployed, evaluated, and retrained.

## II. RUNNING EXAMPLES
- To better describe the challenges that OOD data creates in learning-enabled robotic systems, we use the two future autonomy systems shown in Figure 1 as running examples in this paper.
- These conceptual examples highlight the plurality of applications and design paradigms used to leverage ML in the design of robotic systems.
- Autonomous Drone Delivery Service: Firstly, we consider an autonomous drone delivering packages in a city. As illustrated in Figure 1, this robot uses several learningenabled components in its autonomy stack. The delivery drone has to make explainable decisions and meet stringent safety requirements by regulatory agencies to be deployed among humans. Crucially, to maintain these reliability requirements in rare and unforeseen circumstances, the drone needs mechanisms to detect and manage OOD inputs.
- Robotic Manipulators Assisting in the Home: Secondly, we consider the deployment of robotic manipulators to assist with various tasks in and around the home, as shown in Figure 1. The manipulators' tasks are so diverse and unstructured that we consider a general manipulation policy trained in an end-to-end fashion in a controlled environment, as commonly considered in the reinforcement learning (RL) community.
- When we deploy these manipulators in people's homes, the environments and contexts that these robots encounter invariably differ from the lab or simulated environments used for training, which can markedly impact the system's performance. Therefore, ensuring reliable performance in Fig. 1. Left: A future drone delivery system. It uses a modular autonomy stack consisting of 1) a perception system that builds an understanding of the drone's state and its environment, 2) a prediction module that makes inferences about the behavior of other agents and objects in the drone's environment, 3) a high-level planning and decision-making module, and 4) low-level controllers that actuate the drone's propellers to control the drone's trajectory. At deployment, the drone must safely navigate many obstacles such as other delivery drones, power lines, and trees, not all of which can be anticipated at design time.
- Right: Robotic manipulators assisting in the home. The manipulators are controlled end-to-end by directly passing the observations of the robot through a policy network that outputs the actions the robot should take. At deployment, the manipulators perform a wide range of household tasks like folding and hanging clothes, washing dishes, and cleaning up trash. Therefore, engineers train the manipulator policy by creating targeted experiments to collect a large and diverse training dataset of many objects to manipulate and tasks to complete.
- OOD test environments is a crucial aspect of the design challenge.

## III. WHAT MAKES DATA OUT-OF-DISTRIBUTION?
- Well-engineered ML pipelines produce models that generalize well to test data sampled from the same distribution as the training data.
- When models fail to generalize at test time, we often attribute this to "OOD data" in a catch-all manner.
- In this section, we illustrate two concepts that structure our perspective on these questions using the notation of a standard supervised learning pipeline.
- Assume access to independent samples D train = {(x, y)} N i=1 drawn from an underlying joint distribution P train with density p train (x, y), where x ∈ X and y ∈ Y .
- In supervised learning, we fit a model f : X → Y on D train and evaluate its performance on a test data set D test drawn from P test with density p test (x, y).
- Distributional Shifts: A distributional shift occurs when test data D test is sampled from a distribution P test that differs from P train , thereby making D test OOD and D train in-distribution (ID).
- Shifts can corrupt the performance of the learned model f since it may no longer capture the relationship between x and y in the test data.
- Distributional shifts can reflect fundamental changes in the underlying data generating process (often termed concept shift).
- Concepts can shift discontinuously, like when important unobserved features change between train and test, or they can slowly drift over time.
- For example, when we use a model-based approach for lower-level control of the delivery drone, slowly degrading actuators can make the predictions of a learned dynamics model dangerously inaccurate.
- Alternatively, shifts can be limited to part of the generative process.
- A covariate shift describes when p(x) changes while p(y | x) remains constant [3].
- For instance, we might train a vision model for the delivery drone on images collected during the day but deploy the delivery drone using the model at night.
- Similarly, label shift occurs when p(y) changes and p(x | y) does not, for example when deploying a pre-trained pedestrian detector in a new country where there are overall more pedestrians [4].
- The language of distributional shifts is particularly suited to quantifying how population level statistics, like the expected loss of a model, are affected by changing conditions.
- Functional uncertainty: Since we do not have access to P test directly and must learn a model f from a finite set of samples D train , we cannot be certain that f will make good predictions at test time.
- This offers a complementary view on the OOD problem; instead of reasoning about distributional differences, we aim to characterize the domain of competence of a particular f , i.e., when and where we can have confidence in its individual predictions, and conversely, when we are uncertain in its predictions.
- Causes of high functional uncertainty are not rooted only in distributional notions; even when P test = P train , the model f may make poor predictions on rare inputs which were not well represented in the finite D train .
- Instead, functional uncertainty may arise from epistemic uncertainty, i.e., when we are unaware of the input-output relations that our models do not capture in the test domain.
- Of course, distributional shifts can increase the likelihood of encountering test data outside the domain of competence of f .
- Importantly, functional uncertainty is linked to how f is used, as evaluating competence requires a measure of test-time performance.
- While this measure can be generic (e.g., KL divergence between the softmax output probabilities of f (x) and p test (y | x)), it can also be tailored to downstream utility functions.

## IV. TRENDS IN OOD IN MACHINE LEARNING
- The OOD problem is an open challenge in the ML community
- Indeed, state-of-the-art models have been shown to be extremely sensitive to subtle distributional shifts
- In this section, we discuss classical and core formulations and techniques from the ML literature that guide the ML community's approach to tackling the OOD challenge, summarized in Figure 2.

## A. Coping with Distributional Shift
- Standard ML techniques are based on the unrealistic assumption that P test = P train .
- Domain generalization considers the capacity of a model to generalize to an unknown test-time data distribution P test .
- To make this problem approachable, we need to make assumptions on how much P test can reasonably differ from P train .
- For example, one salient research direction aims to improve distributional robustness, optimizing the worst-case performance within an envelope of distributional shifts to guarantee OOD performance .
- However, it is often unclear how large distributional uncertainty sets should be when conditions shift, a topic that roboticists working on applications should address.
- To circumvent such ambiguity, it is common to consider the robustness behavior on subpopulations of the training data instead .
- A complementary research direction targets the root cause of poor generalization under distributional shift from a causal inference perspective: Learned models often pick up spurious correlations in D train , rather than the invariant cause and effect relations that govern the underlying process .
- For example, the vision-based robotic manipulator could rely on features in the background of the image to identify object types in the foreground (e.g., cooking items usually appear in kitchens), and thus fail to generalize to reasonable distribution shifts where the background changes (e.g., a pan on a sofa).
- Empirically, domain generalization often improves most when we (pre)train larger models on larger, more diverse datasets and infuse domain knowledge by encoding invariances explicitly or via data augmentations and selfsupervised pretraining tasks .
- Domain adaptation aims to develop algorithms that leverage both the training dataset and some (usually unlabeled) test inputs {x i } M i=1 iid ∼ P test to optimize the learned model f on P test .
- Domain adaptation therefore typically requires a priori availability of some test domain data.
- In particular, continual, or lifelong, learning algorithms seek to adapt f over time in response to evolving distributions .
- Progress on algorithms that adapt models in response to shifted conditions is not limited to the batched setting (a setting that [22] surveys extensively).

## B. Assessing Functional Uncertainty

## C. Evaluation
- Researchers have developed benchmark datasets that contain train/test splits curated for qualitative semantic differences for evaluating OOD performance
- OD test sets can include synthetic corruptions like motion blur, Gaussian noise, and other perturbations
- In addition, many datasets may test robustness to naturally occurring distribution shifts, like when we train the delivery drone's bird detection model only on images of land birds and include images of waterbirds in the test set, with some recent datasets emphasizing tasks relevant to robotics
- Such datasets provide an intuitive foothold to develop algorithms by isolating reliability problems rooted in OOD data.

## V. OPEN CHALLENGES FOR OOD IN ROBOTICS
- Robotics has always been centered on building systems that work well in the real world.
- Therefore, we argue for a system-level perspective on tackling OOD data in learningenabled autonomy: Our ultimate goal is to reason about an ML-enabled autonomous system's reliability and competence when it applies learned models in a feedback loop over time, as it operates in potentially shifted conditions.
- This perspective differs from the model-level paradigms in the ML community aimed at quantifying how a model's accuracy degrades on independently-sampled OOD data because learned models only constitute individual components of a complex autonomy stack.
- Therefore, system-level perspectives present unique challenges for the robotics community related to 1) detecting OOD conditions, 2) responding to them to avert system failures, and 3) improving the robotic system's OOD closed-loop performance as a whole.
- We illustrate these challenges by considering three different timescales at which data-driven robotic systems operate, as shown in Figure 3, each with distinct OOD challenges for robotics.
- We discuss each timescale, drawing connections to methods from the ML community and highlighting key open research questions (RQs) toward autonomous systems that leverage ML while being robust to the OOD conditions they will inevitably encounter.
- In addition, we examine various aspects of the RQs using our running examples and briefly touch upon recent research trends to contextualize the RQs.
- We emphasize that this discussion is not an exhaustive survey of existing literature but rather a brief discussion to underscore the significance of the RQs.

## A. Real-time ML-Enabled Decision Making
- We need to monitor the competence of the full decision-making stack on individual inputs encountered at test time.
- Even though runtime monitoring is commonplace in robotics, its application to mitigate the effect of OOD data on state-of-the-art learned components suggests two key research questions centered around the functional uncertainty viewpoint on OOD.
- RQ 1 (Averting OOD failures through Runtime Monitoring). Can we leverage full-stack sensory information at runtime to detect if a decision system relying on a learned model f will perform poorly, before a failure occurs?
- Because we generally cannot identify all aspects of the robot's environment that affect the reliability of its ML components, provable safety guarantees are virtually unattainable for autonomous systems without making restrictive assumptions [35].
- Indeed, the failures caused by conditions that were not represented at design time -be it in training data or in simulated test scenarios-are generally what we attribute as OOD. Therefore, we view algorithms for monitoring autonomous systems at runtime as a core aspect of maintaining system-level competence in OOD regimes.
- Assessing the functional uncertainty on the model's inputs is an important first step towards this goal, but is not sufficient to monitor the performance of the overall robotics stack.
- Instead, we need to reason about how functional uncertainty propagates through the decision-making system and devise goal-oriented measures of uncertainty on f that capture system-level performance.

## RQ 2 (OOD Aware Decision Making). Can we design decision-making systems compatible with runtime monitors robust to high functional uncertainty?
- A robot must always choose an action to take, even if runtime monitors suggest that a learned component is operating OOD.
- Thus, as roboticists, we must design systems where model uncertainties are assessed and accounted for during decision-making.
- This entails the joint design of real-time runtime monitors, uncertainty-aware decisionmaking algorithms, and fallback strategies.
- Since fallbacks may need to rely on redundancy or alternate sources of information, the problem of ensuring the safety and reliability of the aggregate autonomous system is a significantly more expansive challenge than that of characterizing the functional uncertainty of an ML model in isolation.
- Additional Examples:
- Consider the scenario of the delivery drone's perception network failing to detect OOD object types not seen at training time.
- At high speeds, missing a detection on a single image can make obstacle avoidance impossible.
- Runtime monitors as suggested in RQ 1 that flag when the functional uncertainty of the visual object detector on a specific input is high, signaling that the model outputs are unreliable, can be critical to avoid catastrophic failures.
- However, not every missed detection will affect the same consequences: a missed detection of an OOD bird breed far away is less likely to cause a collision than a missed detection of a nearby tree (complicating RQ 1).
- When the runtime monitor signals that the object detection system is dangerously inaccurate, the drone should land or continue safe operation in a degraded state.
- Certifying that a fallback strategy does not cause additional hazards, such as ensuring the drone does not land on busy roads with limited sensing, requires the system-level considerations outlined in RQ 2.
- Knowing that particular objects are OOD for the robot manipulator can help it decide which objects it can reliably manipulate.
- This knowledge can allow the robot to abstain from handling OOD objects instead of dropping and damaging them.
- In addition, we can build system-level checks around the manipulator policy to sanity-check its decisions.
- For example, we could compare to grasps computed using more classical techniques or leverage additional sensing modalities to estimate when the robot can or cannot successfully manipulate an object.
- Recent Trends:
- As discussed in Section IV, quantifying functional uncertainty of a model on OOD inputs is a lively field of study, including anomaly detection (as surveyed in [25], [26], [36]) and heuristics for predictive uncertainty like approximate Bayesian inference (e.g., [31], [37], [38]) to name a few.
- However, such methods generally only apply to covariate shifts.
- In addition, robotics-focused monitoring methods that leverage additional sensors to learn how models are innacurate or check consistency among modules (as surveyed in [1]), or learn to predict or recognize system failures also show promise [39], [40].
- However, these early approaches are often not goal-oriented, heuristic, or unverifiable under distribution shift.
- Moreover, while many existing control theoretic approaches provide safety filters that interfere with black-box learned policies to correct trajectories in settings where system dynamics and state are known (e.g., [41], [42], certifiably closing the loop on runtime monitors and fallbacks in complex systems like our drone delivery example is a broadly understudied problem.

## B. Episodic closed-loop interaction
- Learning-enabled robots actively interact with their environment to perform tasks, which introduces distinct research challenges for the robotics community.
- RQ 3 asks how we can distinguish consistent model errors induced by OOD conditions from sporadic errors, which may be tolerable.
- Distributional shifts can stem from the fact that the drone needs to use some policy for test deployments to collect data: The delivery drone might use a slow and conservative policy to collect the data to learn interaction models for other agents. If the drone flies very aggressively using these models, the closedloop trajectory distribution will shift, making the interaction models dangerously inaccurate.
- Exploiting the drone's ability to control this shift is RQ 4's focus.

## C. Data lifecycle
- RQ 5: How can we use data collected during operation in diverse tasks and contexts to improve the robustness and quality of learned models?
- RQ 6: How do we select what operational data we should use to efficiently improve our models?
- RQ 7: Recent Trends: Roboticists have already started leveraging some of the ML community trends to improve generalization when distributions vary across deployments beyond simply augmenting datasets with new operational data and retraining or, as surveyed in [48], by applying techniques from continual learning: Some approaches learn consistent patterns across distributions by considering separate losses for each trajectory, for example, by applying meta-learning to more rapidly learn dynamics models [49] or policies [50] across environments, leveraging causal inference techniques to identify generalizable state and task representations (as surveyed in [51], [52]), or through multi-task learning [53].

## VI. CONCLUSION
- Roboticists need to embrace a system-level perspective on the OOD problem
- Investigating how OOD data impacts the reliability of the full autonomy stack and how to leverage the full autonomy stack to mitigate negative consequences are necessary steps towards a future where we can safely and reliably leverage ML to enable true open-world autonomy
