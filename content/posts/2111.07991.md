---
title: "LiT: Zero-Shot Transfer with Locked-image text Tuning"
date: 2021-11-15T18:53:48.000Z
author: "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2111-07991v3.webp" # image path/url
    alt: "LiT: Zero-Shot Transfer with Locked-image text Tuning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2111.07991)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2111.07991).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/lit-zero-shot-transfer-with-locked-image-text).

# Abstract
- Contrastive-tuning is a simple method that employs contrastive training to align image and text models
- In our empirical study, we find that locked pre-trained image models with unlocked text models work best
- We call this instance of contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks

# Paper Content

## Introduction

## Related work
- literature on transfer learning in vision is extensive
- pre-trained models are often used to solve a new task better and faster
- zero-shot transfer is an alternative paradigm that sidesteps the fine-tuning stage
- contrastive learning techniques are another closelyrelated research direction

## Methods

### Contrastive pre-training
- Collections of images (potentially noisily) paired with free-form text descriptions have emerged as a powerful resource for training visual models.
- The key advantage therein is that it is not limited by a finite set of predefined categories and instead describes images using open-ended natural language.
- As a result, models learned from this data can serve as zero-shot learners for a wide range of tasks, e.g. classification and image/text retrieval.
- Contrastive pre-training is one particularly effective approach for training models from image-text data, which was recently proven to work well in practice.
- We take a closer look at this approach and propose a simple, yet highly effective recipe to significantly enhance contrastive pre-training from image-text data.
- The key idea behind the contrastive pre-training approach is to learn two embedding models: an image model and a text model, both of which produce representations of the same dimensionality.
- These models are trained using a contrastive loss. This loss encourages corresponding image-text pairs to have similar embeddings and, conversely, encourages non-corresponding pairs to have distinct embeddings.
- See [46,71] for the detailed discussion of the contrastive loss function.
- An important detail of this loss function is whether the loss is computed on each accelerator device independently and then accumulated or computed jointly across all devices.
- We ablate this design choice (Appendix F) and confirm that the latter [31,46] consistently results in better performance.
- We therefore use the global loss in all our experiments and ablations.

### Contrastive-tuning
- Contrastive pre-training can be viewed as learning two tasks at the same time: (1) learning an image embedding and (2) learning a text embedding to align with the image embedding space.
- When not using contrastive pre-training on image-text data, a standard approach to learning image embeddings is to use a large and relatively clean dataset of (semi)manually labeled images.
- Large scale and and high quality of such data result in state-of-the-art image embeddings.
- Some dataset choices for learning powerful image embeddings are ImageNet-21k [15], JFT-300M [57].
- However, this common approach has a clear weakness: it is limited to a predefined set of categories and, thus, the resulting models can only reason about these categories.
- In contrast, image-text data does not have this limitation, as it learns from the free-form text that potentially spans a broad range of real-life concepts.
- On the other hand, image-text data that is available may be of lower quality (for learning image embeddings) than carefully curated datasets.
- We propose contrastive-tuning to combine advantages of both sources of data.
- One specific way of doing this is to initialise the contrastive pre-training with an image model that was already pre-trained using cleaner (semi-)manually labeled data.
- This way the image-text alignment is learned independently of image embedding, enabling benefit from both data sources.
- Beyond using supervised pre-trained image models, the proposed contrastive-tuning is also flexible enough to integrate any models that can produce meaningful representations.
- We verify this in our experiments using selfsupervised pre-trained image models.

### Design choices and Locked-image Tuning
- Pre-trained models can be used
- There are 3 settings: L (locked weights, initialized from a pre-trained model), U (unlocked/trainable weights, initialized from a pre-trained model), and u (unlocked/trainable weights, randomly initialized)
- The Lu setting (locked pre-trained image model, and unlocked (trainable) randomly initialized text model) works particularly well

## Image-text datasets
- The Conceptual Captions dataset extracts, filters & transforms image & alt-text pairs from web pages
- We use the latest 12 million image-text pair version, i.e. CC12M [6]
- Due to expired URLs, only 10 million imagetext pairs were used for our experiments.
- The Yahoo Flickr Creative Commons dataset [60] contains 100 million media objects
- Of these, 99.2 million are photos that come with rich metadata including camera info, timestamp, title, description, tags, geolocation, and more.
- [46] defines and uses a subset of 15 million images that have been filtered for English text of high quality, which we call YFCC100m-CLIP.

## Experiments
- LiT is better than state-of-the-art image-text models
- LiT can be used to train models for 0-shot ImageNet classification and MSCOCO image retrieval
- learnings from experimental evaluations

### Comparison to the previous state-of-the-art
- The LiT method is more accurate than the previous state-of-the-art methods
- The LiT method is more efficient than the previous state-of-the-art methods
- The LiT method is more robust than the previous state-of-the-art methods

### Evaluation of design choices
- Small-scale thorough investigation. We first perform an in-depth study on various combinations of the image and text towers being initialized with pre-trained weights and locked (L) or unlocked (U) or being randomly initialized and unlocked (u).
- We train each setting many times on the YFCC100m-CLIP dataset, varying the total number of steps from 2 500 to 60 000 in order to understand the setting's trajectory, and sweeping over learning-rates and weightdecays to avoid being misled.
- Figure 3 shows the best result for each setting for each duration, i.e. each point on the curves is a separate full run for that duration.
- It is evident that locking the image tower almost always works best and using a pre-trained image tower significantly helps across the board, whereas using a pre-trained text tower only marginally improves performance, and locking the text tower does not work well.
- This still holds in the near-infinite data regime.
- One may hypothesize that locking the pre-trained image tower only helps because the YFCC100m-CLIP dataset is relatively small (15 million images, compared to 400M [46] or 1.8B [31]), and that a randomly initialized image tower will eventually outperform a locked one on much larger imagetext datasets.
- The trajectory of the Uu and UU settings in   Maybe surprisingly, experimental results show that this is not the case, and locking the image tower provides benefits even when contrastively tuning on a very large dataset of image-text pairs.
- Table 2 shows results of contrastive tuning on our dataset of 4 billion images in three settings: Lu, Uu, and uu.
- Implementation details can be found in Appendix C.
- The from-scratch method uu unsurprisingly achieves better performance than with smaller datasets such as CC12M and YFCC100m-CLIP.
- Initializing the image tower from a pre-trained model provides even better performance and is a relatively straightforward extension of CLIP/ALIGN.
- While potentially counter-intuitive, another perspective is that LiT simply learns a text tower that extracts knowledge from a strong image embedder. This flexible & performant setup can turn existing vision backbones into a zero-shot learners, by attaching a text-embedding tower.
- Why is locked (L) better than unlocked (U)? It is somewhat surprising and counter-intuitive that locking the image tower works better than allowing it to adapt during the contrastive-tuning; Figure 4 gives hints as to why.
- The first row shows that locking the image tower leads to substantially worse (contrastive) loss on the dataset used for LiT, while the loss of the locked image variant is substan- tially better on out-of-distribution (zero-shot) datasets (middle row) and the "representation quality" as measured by linear few-shot evaluation on the pre-logits (bottom row).
- This reveals how the different settings behave, see text for details.
- We design a controlled experiment to verify whether that is the case. We find that on the contrary, more generally pre-trained models are better suited for LiT.
- We select a set of image models that all use the same ViT-B/16 architecture but were pre-trained in various ways: supervised (AugReg [55]) on ImageNet (IN), on the large but narrow Places [39] dataset, on the much broader ImageNet-21k (IN21k), or fully unsupervised (DINO and MoCo-v3).
- All but the Places model achieve...

### Which text model to use?
- While related work has so far focused on the image model, the text model plays an important yet underexplored role in contrastive image-text learning
- We consider four possible transformer-based text models- the transformer from ViT-B [21] which also resembles that used in CLIP [46], T5-base [47], mT5-base [67] BERT-base [17]- and whether to initialise them randomly, or from a pre-trained checkpoint
- BERT uses a WordPiece (WP) tokenizer [50,65], and all others use the Sentence-Piece (SP) tokenizer [35], a component which we also ablate with the ViT model
- Table 4 shows the results of LiT using an AugReg-ViT-B/32 on YFCC100M-CLIP and our dataset using the base sized variant of these text models
- We sweep over various learning-rates and weight-decays separately for each combination to avoid being misled
- Our observations differ slightly between the relatively small YFCC100m-CLIP dataset, and our much larger dataset, we first discuss the former
- First, we see a small but consistent improvement by initializing the text model with pre-trained weights
- Second and somewhat unexpectedly, we find that the BERT model performs significantly better than others, especially for retrieval
- In order to disentangle the contribution of the architecture from the tokenizer, we further apply LiT using a ViT text encoder paired with BERT's WordPiece tokenizer and see no improvement
- We believe that small differences in the architecture, such as initialization and LayerNorm placement, are responsible for the slightly better generalization of BERT that we observe
- However, we also found the BERT model to be less stable to train
- For the large-scale experiments on our dataset, we do not observe this improvement anymore, and favor sticking with the more stable ViT SentencePiece combination
- We report results averaged across three runs.

### Technical advantages of locked image models
- Using a locked image tower reduces the amount of computation required to train an image model
- Precomputing the image model's embeddings reduces the amount of memory required to store the model

### Preliminary multilingual experiments
- It is currently common practice to filter imagetext datasets to English language data only.
- We believe that removing this restriction has the potential to benefit a larger part of the world's population.
- Concurrent work has relied on additional translated text pairs for training the text encoder.
- In contrast, we do not require any translations and purely rely on the pre-trained, locked image model to bridge the language barrier.
- Preliminary experiments show the promise of LiT for multilingual image-text models.
- We apply LiT on an AugReg-i21k ViT-B/32 with the T5 and mT5 base encoders, both with and without the pre-trained checkpoints.
- We do this on both the full YFCC100m dataset, and the reduced English-only CLIP subset, and we use all available text as supervision signal.
- The high-level conclusions are consistent across both evaluations: training on the full dataset improves performance on non-English languages much more than on English, using a multilingual tokenizer (as in mT5) significantly helps languages that do not use the Latin script, and starting from a pre-trained multilingual text model can best Language, by model worst further help.

## Discussion
- Limitations: This work explores only classification and retrieval as zero-shot transfer tasks.
- On cross-modal retrieval tasks, we have not observed as clear a benefit of the Lu setup compared to Uu or UU.
- Our results suggest that the proposed Lu setup can still save computational cost within a fixed budget, but with a large enough budget, it may be useful to also consider the Uu setup if zero-shot classification is not the primary end goal.

## Conclusion
- We present a simple method named contrastive-tuning that allows transferring any pre-trained vision model in a zero-shot fashion.
- LiT makes it possible to turn publicly available models into zero-shot classifiers using pub-licly available data, and rival the performance of previous works which rely on more, proprietary data.
- We observe a substantial quality improvement when using more than 16 tokens in the description field.
