---
title: "Explain to Me: Towards Understanding Privacy Decisions"
date: 2023-01-05T14:25:20.000Z
author: "Gonul Ayci, Pınar Yolum, Arzucan Özgür, Murat Şensoy"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02079v1.webp" # image path/url
    alt: "Explain to Me: Towards Understanding Privacy Decisions" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02079)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02079).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/explain-to-me-towards-understanding-privacy).

# Abstract
- Privacy assistants help users manage their privacy online
- Their tasks could vary from detecting privacy violations to recommending sharing actions for content that the user intends to share
- Recent work on these tasks are promising and show that privacy assistants can successfully tackle them
- However, for such privacy assistants to be employed by users, it is important that they can explain their decisions to users
- Accordingly, this paper develops a methodology to create explanations of privacy
- The methodology is based on identifying important topics in a domain of interest, providing explanation schemes for decisions, and generating them automatically
- We apply our proposed methodology on a real-world privacy data set and evaluate it on a user study

# Paper Content

## INTRODUCTION
- People are worried about their privacy and think twice before using online systems
- Various recent surveys conducted with users of online social networks indicate that people do not even read the privacy policies that they accept
- Privacy assistants that work side by side with humans in a decentralized manner could serve to address this problem
- Privacy assistants have been developed for various privacy assistance, including checking for privacy violations, resolving privacy conflicts among humans, recommending sharing policies, and signaling if a piece of content is private
- While doing these tasks, it is important for the privacy assistant to be able to explain its decisions to the user
- One of the important works of explainability in conjunction with privacy is by Mosca and Such [20], where they develop an agent that uses computational argumentation to resolve disputes and propose a text-based description of the outcome generated by the system
- However, to the best of our knowledge, there does not exist any methodology to generate explanations as to why a given content is private or public
- Existing work on explanation for binary classifications generally consider what features of the classification have been influential for the classification
- Using these saliency methods, for example, heat maps can be generated such that parts of an image are highlighted to demonstrate its effect on the decision
- Lundberg et al. [17] propose a model-agnostic feature relevance explanation model, SHAP (SHapley Additive exPlanations), that is based on a game theoretically Shapley values [22]
- This method computes the contribution of each feature to the prediction output
- Lundberg et al. [16] also propose TreeExplainer that explains predictions of tree-based machine learning models
- TreeExplainer is a variant of SHAP, which provides the computation of local explanations based on Shapley values in polynomial time
- These approaches are important because they provide interpretability for the underlying classifier
- However, they are not meant to provide explanations to the end user as we aim here
- In order to address this problem, we propose a new representation for explaining why an image is considered private or public
- Our representation is made up of visually exhibiting one or more topics that the image is associated with while emphasizing important keywords that put the image in a given topic
- A natural language description accompanies the visuals to describe the relation between the topics
- We provide a methodology to derive these explanations from a dataset where images are labeled as private or public
- We implement our methodology and apply it to a wellknown image dataset for privacy
- We then perform a user study to measure if users actually find these explanations useful and what factors of the explanation or the image affect users' understanding of the decision
- The rest of this paper is organized as follows. Section 2 explains our understanding of explanation, its formalization, and its relation to topic modelling. Section 3 develops our methodology into a system that can be readily used to explain privacy labels of images and evaluates the effectiveness of the extracted topics. Section 4 presents how to generate explanations from topics. Section 5 evaluates our system through a user study. Section 6 discusses our work in relation to related work. Finally, Section 7 concludes our work and provides future directions.

## METHODOLOGY FOR EXPLAINING PRIVACY
- Given an image, we can determine if it is private or public.
- If it is private, we generate an explanation as to why this is so.
- If it is public, we generate an explanation as to why this is so.

## Understanding Explanation
- The explanations that we are interested in generating are meant for end users.
- Hence, even if our explanations are influenced by the features that are used for classification, our aim is not to educate the user about how the underlying classifier works.
- Hence, the explanation should not be too technical.
- At the same time, given that many users do not read long texts on privacy policies for example, we would like the explanation to be visually understandable and supported by a short text.
- Based on these constraints, we propose to formulate an explanation as to whether an image is private or public by a set of topics that the image belongs to.
- These topics are shown as a circle and labeled by the topic name.
- Each image can have one or more topics.
- Additionally, we identify one or more keywords that link this image to each topic and denote them in the corresponding topic circle.
- The intended understanding of this representation is that the image is private or public, because it can be described with these topics and keywords.
- This visual representation is augmented with a short description that falls into a predetermined language structure to explain the visual representation.
- The text is thus supplementary and does not provide addition information.

## Understanding Topics
- Machine learning algorithms are mostly black-box models and use a large number of features while making predictions.
- Thus, the models are not straightforwardly understandable for humans and are not able to make explainable predictions.
- Motivated by this observation, our aim is to understand the model and its predictions and develop a methodology to generate explanations for privacy decisions.
- Thus, for a prediction of a single instance, we need to extract the most important and relevant features from all the features in the decision.
- For this purpose, we propose to uncover groups of keywords (i.e., latent topics) from a collection of textual information that best represents the information in the collection.
- A topic consists of relevant descriptive keywords.
- Each image is associated with topics based on its keywords.
- Topics should be meaningful and interpretable for humans.
- One way of realizing this during computation of topics is to ensure that the topics are coherent.
- Each topic should pertain to images that could be described with similar keywords.
- At the same time, each topic is relatively different from each other.
- We can measure coherence based on two different criteria as follows: (i) Intra-topic similarity: The average semantic similarity between all pairs of the most associated N keywords in the same topic. (ii) Inter-topic similarity: The average semantic similarity of the most associated N keywords from different topics. That is, we can calculate how close the keywords that describe a topic are semantically using intra-topic similarity and how far the topics are semantically apart using inter-topic similarity.

## GENERATING TOPICS
- Topic modelling is a technique that discovers latent topics within a collection of textual information.
- It allows us to extract different topics (features) from keyword sets.

## Topic Modelling
- We use a widely used topic modelling technique, namely, Nonnegative Matrix Factorization (NMF) [14].
- NMF is an approximation to factorize a non-negative matrix of a non-negative image-keyword matrix X, into non-negative matrices W and H as in Figure 3.
- W (features) matrix stores how much each image belongs to a topic and H (components) matrix stores how much each keyword belongs to each topic.
- The W and H matrices are initialized randomly.
- NMF algorithm runs iteratively until it finds a W and H that minimize the Frobenius norm of the matrix, that is, ∥ −  ×  ∥  .
- NMF is suitable for interpretability (components are non-negative) and works better and faster for short texts (a set of keywords) as compared to alternatives such as Latent Dirichlet allocation (LDA) [4].
- In this study, we make use of the term weighting method, namely, the Term Frequency -Inverse Document Frequency (TF-IDF) model to transform keywords into numerical vectors in order to construct an image-keyword (X) matrix.
- TF-IDF assigns weights based on how relevant a keyword is to a given collection of keyword sets.
- We build the NMF model for a different number of topic (k) values, which generates an image-topic (W) matrix and a topic-keyword (H) matrix, and then we use the Random Forest algorithm to make predictions.

## Evaluation of Topics
- We use a balanced subset of the publicly available PicAlert dataset.
- The PicAlert is a well-known and widely used dataset for the privacy prediction problem for images.
- It contains Flickr images that are labeled as private or public by annotators.
- These images are labeled by 81 users between 10 and 59 years of age with different backgrounds.
- We consider an image as private if at least one annotator has annotated it as private and public if all the annotators have annotated it as public.
- The balanced subset we work with contains 32 samples, including 27 Train and 5 Test, which are labeled as private or public.
- Then, we automatically generate 20 different descriptive keywords for each image using Clarifai API 1.
- In the NMF model, we set the number of topics based on the model performance in terms of coherence.
- While calculating intratopic similarity and inter-topic similarity, each keyword is represented by word embedding vectors, namely, word2vec.
- The similarity between two keyword vectors is measured by the Cosine-Similarity metric.
- Semantically similar tags tend to be close to each other in the semantic space.
- Intra-topic similarity values for 20 topics and 10 topics are 0.20 and 0.18, respectively. Additionally, inter-topic similarity values for 20 topics and 10 topics are 0.43 and 0.48.
- We represent keywords as 300−dimensional vectors of the word2vec model trained on Google News when calculating coherence.
- Note that the cosine-similarity values between two vectors for this model are generally low (e.g., the similarity between "person" and "people" is 0.51 and "tree" and "park" is 0.23).
- We named 20 topics that we discover using NMF.
- Figure 4 shows keyword clouds for five different topics (i.e., Nature, Child, Performance, Business, and Fashion) with the top 20 keywords that describe each topic.
- The font size is sensitive to relative significance. That is, the most descriptive keyword is displayed as the largest.
- For instance, the top five descriptive keywords of the topic Nature are {, , , , }.
- Figure 5 shows the percentage of each topic being associated with private and public images.
- Some topics such as the topic People are associated more frequently with the private class, whereas some of them such as Sky are associated more frequently with the public class.
- Note that although some topics are associated more frequently with private and some with public images, the topics do not have an explicit class to which they belong. Therefore, the topic itself does not directly signal a certain class and thus, it is not straightforward to generate an explanation for the decision only by looking at its class.
- To evaluate the representation of the images with the topics extracted using NMF, we trained a Random Forest classifier where the images are represented as TF-IDF vectors of these topics.
- The classifier yields an accuracy of 88.5% on the test set, indicating that the NMF-extracted topics are effective for privacy prediction.

## GENERATING EXPLANATIONS FROM TOPICS
- The TreeExplainer model provides the contributions of each feature in terms of Shapley values, which affect the model output of tree-based algorithms such as Random Forest.
- Not all features have equal contribution to a class prediction: a feature can push the prediction higher (positive Shapley value) or lower (negative Shapley value).
- The machine learning model concludes its prediction by taking into account the contribution of each feature. This is useful in interpreting how the classifier works.
- One way to create explanations would be to display all these values to the user. However, as the number of features increases, it would be cumbersome and confusing to show them all to the end user. Therefore, we start from the TreeExplainer idea, but modify it to match our expectations for explanations, as described in Section 2.
- In this study, each feature corresponds to a topic. We are interested in identifying topics that are useful in explaining the content of the image at hand.
- For a given image, a large positive Shapley value might be assigned to a topic because the image is related to that topic. But, it might also be the case that a large negative value is assigned to a topic that is unrelated to the topic.
- The second category shows that the classifier made a decision based on the fact that the image did not exhibit the properties associated with this topic. While useful to understand the classifier, this information is difficult and possibly unnecessary to show to the user. Hence, we need to carefully decide how to use the Shapley values when creating the explanations.
- Our methodology generates human-understandable explanations through topic reduction in the output of the TreeExplainer.
- Dominant: An image belongs to the Dominant category when the contribution of one topic is decisive for the class prediction. That is, a topic makes a relatively high contribution compared to other topics of the image.
- Figure 6 shows an example image in the Dominant category that has been identified as private by annotators. The generated explanation for this image being assigned to the private class is that it is relevant to the topic Child with the keywords including {ℎ,  , , ℎ,  }.
- Collaborative: An image belongs to the Collaborative category when the contributions of its topics arrive at a consensus about the decision. That is, the images in this category do not have a single decisive topic as in Dominant, but have topics that support each other collaboratively.
- Figure 7 shows an example image that has been identified as private. The generated explanation by our algorithm for this image being assigned to the private class is that it is relevant to the topics People, Fashion, and Room with certain keywords shown in the topic circles. All three topics push the prediction higher.
- The Algorithm 1 extended by N topics is also applicable to finding such images that belong to the collaborative category. That is, the total contributions of N topics are decisive.
- Conflicting: The topics associated with an image do not always agree on whether the image should be private or public. In such situations, the explanation should indicate this.
- An image belongs to the Conflicting category if the image has topics whose magnitudes are almost equal but the contributions to a class prediction are in the opposite direction.
- Making a decision can be difficult when an image has conflicting topics that have opposing forces in the decision.
- Figure 8 shows an example image that has been identified as public by annotators. The generated explanation for this image is that even though it is relevant to the topic People with the specific keywords (i.e. "wear", "man", "people"), it is also relevant to the topic Art/Vintage that pushes the prediction higher and for that reason, it is classified as public.
- Algorithm 2 describes the process of finding images with conflicting topics. In this algorithm,  and  correspond to the number of images and topics, respectively.
- We normalized the Shapley values by dividing each Shapley value by the sum of the absolute values of all topics of the image. _ is the matrix that contains the Shapley values of topics for each image and _ stores the normal...

## EVALUATION
- We performed an online user study to evaluate our proposed explanation model in terms of sufficiency, satisfaction, and understanding.
- We conducted a pilot study (with  = 5 users) before the real study to test whether the study is understandable.
- Based on the comments during the pilot, we improved the initial description of the study and reworded one question.

## User Study
- The first phase is meant to explain the study and a consent form.
- The second phase is meant to explain the study over an example, wherein we show an image, its generated explanation, and the three questions that will be asked to the participant.
- In the third phase, each participant is exposed to 16 images with generated explanations in a random order.
- Two of these images deliberately provide irrelevant explanations so that we can differentiate the participants that are attentive during the survey. Thus, these questions are meant to filter out the participants who are not focused.

## Participants
- A total of 57 people responded to the survey
- Of those, 45 were male and 12 were female
- The highest degree of education of the participants was a Master's degree
- The majority of the participants were male (64%) and a minority were female (36%)
- The participants with the highest degree of education were mostly male (19 out of 45, or 38%)

## Results
- The performed user study shows that generated explanations are useful for and make sense to humans.
- Table 1 demonstrates how confidence levels change based on intervals of mean value.
- Sufficient, Satisfying, and Understandable in Figures 10 and 11 correspond to the question 1, 2, and 3 in Section 5.1, respectively.
- Our results indicate that participants were very confident that explanations were sufficiently detailed ( = 3.88,  = 1.12), found the explanations satisfactory ( = 3.62,  = 1.28), and understood why the images were labeled as private or public ( = 3.8,  = 1.33).

## Interval Level

## DISCUSSION
- Several studies make use of descriptive keywords and visual features to predict image privacy.
- T2P system that automatically recommends privacy policies using the image tags has low accuracy.
- Tonge and Caragea use deep visual semantic and textual features to develop a model to predict the privacy of images as private or public.
- Their proposed system performs well in predicting privacy, even though the personal assistant only has access to a small amount of data.
- Ayci et al. propose a personal privacy assistant called PURE to preserve the privacy of its user.
- PURE is aware of uncertainty by generating an uncertainty value for each prediction of a given image, informing its user about it, and delegating decisions back to the user if it is uncertain about its predictions.
- PURE is able to make personalized predictions by using the personal data of its user.
- However, it is not yet clear how this approach can scale in applications that use large image sets.
- Miller examine studies of explainability within the scope of philosophy, social and cognitive psychology, and cognitive science.
- Their study provides various definitions of explainability, criteria for selecting explanations, evaluating explanations, and useful insights for Explainable Artificial Intelligence.
- They define interpretability of a model as the degree to which the cause of a prediction can be understood.
- Explainability is defined by the interpretability that one can adopt and the understanding of the explanation obtained.
- Justification is provided by explaining why a decision is good.
- Responsibility is one of the criteria for selecting explanations, indicating what caused an event to occur and the minimum number of changes that must be made to prevent that event from occurring.
- Arrieta et al. provide an overview from a broad perspective on XAI by defining interpretability and explainability.
- They define interpretability as the ability to explain meaning in a form that people can understand.
- They associate explainability with explanation as the interface between a human and a decision maker.
- They provide a taxonomy for explainability techniques in machine learning (ML) models.
- They examine XAI in ML, which captures transparent models (i.e., linear regression or Bayesian models) and post-hoc explainability techniques that can be both model-agnostic and model-specific.
- In general, these techniques include model simplification (e.g., rule extraction methods), feature relevance explanation, and visual explanation.
- While they use all features in the explanations, this is not always straightforwardly understandable.
- We develop a powerful methodology that is capable of generating explanations with only relevant topics.

## CONCLUSION
- The paper proposes a novel methodology to understand why a given image is private or public.
- The methodology is able to explore latent topics using topic modelling from descriptive keywords of images.
- The privacy classifier achieves high accuracy, demonstrating the effectiveness of the topic-based representation of images.
- Based on a user study, we show that the generated explanations make sense to people and that participants find the explanations sufficient, satisfying, and understandable.
- Another interesting direction for future work is to be able to get feedback from people and update the explanations.
