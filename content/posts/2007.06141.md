---
title: "Gender Classification and Bias Mitigation in Facial Images"
date: 2020-07-13T01:09:06.000Z
author: "Wenying Wu, Pavlos Protopapas, Zheng Yang, Panagiotis Michalatos"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2007-06141v1.webp" # image path/url
    alt: "Gender Classification and Bias Mitigation in Facial Images" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2007.06141)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2007.06141).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/gender-classification-and-bias-mitigation-in).

# Abstract
- Gender classification algorithms have important applications in many domains
- Recent research showed that algorithms trained on biased benchmark databases could result in algorithmic bias
- However, to date, little research has been carried out on gender classification algorithms' bias towards gender minority subgroups, such as the LGBTQ and the non-binary population, who have distinct characteristics in gender expression
- In this paper, we began by conducting surveys on existing benchmark databases for facial recognition and gender classification tasks
- We discovered that the current benchmark databases lack representation of gender minority subgroups
- We worked on extending the current binary gender classifier to include a non-binary gender class
- We did that by assembling two new facial image databases: 1) a racially balanced inclusive database with a subset of LGBTQ population 2) an inclusive-gender database that consists of people with non-binary gender
- We worked to increase classification accuracy and mitigate algorithmic biases on our baseline model trained on the augmented benchmark database
- Our ensemble model has achieved an overall accuracy score of 90.39%, which is a 38.72% increase from the baseline binary gender classifier trained on Adience

# Paper Content

## INTRODUCTION 1.Gender Classi cation Algorithm and Its Applications
- Automated gender classi cation systems recognize the gender of a person based on their facial characteristics that di erentiate between masculinity and femininity.
- Embedded gender classi cation algorithms in computer so ware systems have a wide arrange of applications in areas such as video surveillance, law enforcement, demographic research, online advertising, and human-computer interaction.
- Gender information falls under the category of so biometrics data.
- Currently, in video surveillance systems, facial so biometrics features are being extracted to achieve gender and ethnicity classi cation to aid in security, and forensic evidence collection.
- When identifying a target of interest, gender identi cation could act as a pre-possessing step to reduce the search time in a large-scale database.
- In social media design, classifying users' gender is considered as part of the digital monetization e ort.
- Users' demographic data is o en times layered on top of coveted behavioral data as part of the market segmentation strategy.
- For example, gender is rede ned through the lens of consumption logic motivated by advertising and revenue generation purposes.
- Advertisements that activate identi cation with one's gender group are more likely to have a favorable impact on future brand and ad judgments.
- In HCI today, a variety of intelligent so ware systems are designed to identify a human's gender a ributes to appear more human-like.

## Algorithms Bias in Gender Classi cation
- Machine learning algorithms can discriminate based on classes such as gender and race -discriminatory decisions could be made even if the computing process is well-intentioned.
- Studies have found substantial disparities in the accuracy rate of classifying gender of dark-skinned females in existing commercial gender classi cation systems [6].
- In facial recognition tasks used by the law enforcement systems, research across 100 police departments revealed that African-American individuals are more likely to be subjected to law enforcement searches than individuals of other ethnicity [7].
- e accuracy rates of facial recognition systems used by law enforcement are systematically lower for people labeled female and black [8].
- Recent research also discovered that machine learning models are likely to further amplify existing societal gender bias [9].

## Unrepresentative Benchmark Databases
- Biased database is a major contributing factor in accuracy disparities among prediction tasks.
- Most benchmark databases are built using facial detection algorithms to rst detect faces from online platforms [10].
- However, any systematic errors in these face detectors will inevitably introduce bias into the benchmark databases.
- Many databases curated using this approach contain gender and racial bias and lack demographic diversity [6].
- For example, Color FERET is a 8.5 GB database released under National Institute of Standards and Technology (NIST) as a benchmark standards for facial recognition under controlled environments [11].
- Color FERET contains very limited number of dark-skinned individuals [6] [11].
- Even though the database claims to be heterogeneous in its racial and gender distribution, the number of African male identities is only 10% of their white male counterparts and the number of African female identities is only 17% of their white female counterparts.

## Benchmark Databases in the Fairness Domain
- Recently, a few benchmark databases have been assembled for facial diversity.
- For instance, Fairface [14] is a novel facial image dataset with 108K images curated to mitigate racial bias in datasets.
- Collected from YFCC-100M Flickr database, Fairface has a balanced representation of 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East and Latino.
- Models trained on Fairface dataset reported higher accuracy rates on novel test datasets and the accuracy rates appear to be consistent among di erent racial and gender subgroups [14].
- PPB [6] is a database assembled to achieve be er intersectional representation on the basis of gender and skin type.
- e database consists of 1270 unique identities of government o cials and represents a balanced composition of light-skinned individuals and dark-skinned individuals.
- e PPB database is annotated using the Fitzpatrick skin type labels [6].
- Diversity in Faces (DiF) [15] is another dataset sampled from YFCC-100M Flickr database.
- DiF contains one million publicly available facial images.
- e dataset is annotated with various statistical analysis of facial coding scheme measures such as craniofacial features, facial symmetry, facial contrast, skin color, pose, resolution and etc.
- DiF sets an example for providing a much needed methodology for evaluating diversity in benchmark facial image databases [15].

## e LGBTQ Population and eir Distinct Characteristics in Gender Expression
- In 2017, a controversial study was conducted by Wang and Kosinski that used deep neural networks to detect white males' sexuality.
- e controversial research implied that facial images of the LGBTQ population have distinct characteristics from the heterosexual population -LGBTQ population tends to have atypical gender expression and grooming styles.
- A more recent study [17] released in 2016 estimate the proportion of Americans who identify as transgender to be between 0.5% and 0.6%.
- Transgender facial recognition still remains a challenging area today because gender transformation results in signi cant face variations over time, both in shape and texture.
- Gender afrming surgeries o en a ect the face distribution due to hormone replacement therapy, and depending upon the type of the gender transformation, the changes vary and are subtle to notice.
- e lack of representation of the LGBTQ population in current benchmark databases leads to a false sense of universal progress on gender classi cation and facial recognition tasks in machine learning.
- Many current Automatic Gender Recognition (AGR) systems consistently depict gender in a trans-exclusive way.
- e existence of these systems further reinforces the erasure of the transgender population within our society.

## Limitations of Binary Gender Classi cation
- Systems and e Social Distress of Being Misgendered
- e binary gender classi cation system has created many disadvantages for the sexuality minorities and for those who fall outside the binary gender categories.
- e presumption that gender is binary, stagnant, and based on physiology, additionally harms the non-binary gendered population.
- Misgendering refers to the act of using the language to describe somebody that does not align with their a rmed gender identities.
- A recent study showed that being misgendered can have negative consequences on one's self-con dence and overall mental health [19].
- Gender expression refers to the the way we communicate our gender to others through physical appearances and behaviors such as clothing, hairstyles, and mannerism.
- e LGBTQ population, whose social gender behavior may not conform to the normative gender roles, are likely to be misgendered.
- Being misgendered is a form of mockery, imposing a literal challenge to the cis-gender person's stated gender and increasing one's perception of being socially marginalized.
- Misgendering transgender people could result in decreased selfworth and self-esteem, increased level of dysphoria, which could take many in this community to experience depression and even suicidal thoughts [20].
- Another recent study [17] also showed that being misgendered by AGR (Automatic Gender Recognition) is worse than being misgendered by human beings.
- Being misgendered by AGR could lead to a greater insult due to perceived objectivity and nitivity of computer systems.
- Being misgendered by AGR is a reinforcement of gendered standards and another source of invalidation to injure those that were interpreted the wrong gender label.

## METHODS
- The study found that current benchmark databases lack representation of dark-skin individuals and contain almost no information on the percentage of the LGBTQ population.
- The study hypothesizes that by assembling a racially-balanced, LGBTQ and non-binary gender inclusive database, they will be able to improve gender classi cation algorithms' accuracy on minority faces.
- The study plans to make their assembled non-binary database publicly available for other researchers.

## Creation of Inclusive Benchmark Database
- We created an inclusive benchmark database with images from di erent online platforms to achieve better representation of dark skin individuals and LGBTQ individuals.
- Our current inclusive database contains 12,000 images of 168 unique identities.
- Our database contains racially diverse pro les from di erent geographic locations as well as 21 unique LGBTQ identities which make up for 9% of the database.
- Our inclusive database contains 29 white male identities, 25 white female identities, 23 Asian male identities, 23 Asian female identities, 33 African male identities, and 35 African female identities.

## Creation of Non-binary Gender Benchmark Database
- To extend the binary gender classifier, they assembled a non-binary gender benchmark database.
- The non-binary gender benchmark database was based off of a list of people with non-binary gender identities on Wikipedia.
- The non-binary gender benchmark database contained 2000 images of 67 unique identities.
- Figure 2, Figure 3 and Figure 4 show samples of images from the non-binary gender benchmark database.
- Table 1 shows the gender identity distribution of the non-binary benchmark database.
- Table 2 shows a comparison of the annotated facial features of the non-binary benchmark database and other benchmark databases.

## e Baseline Model Trained on a Biased Benchmark Database
- The Adience database is a benchmark facial image database assembled for machine learning research tasks on age and gender classi cation
- The Adience database contains images of individuals of various appearance and postures, with di erent levels of noise and under di erent lighting conditions
- Among the 2284 unique identities of the Adience database, only 302 of these identities are dark skinned subjects
- Dark-skinned people are highly underrepresented in the Adience benchmark database.

## Labeling Process (Adience Benchmark Database
- Gender labels were already annotated in the Adience benchmark dataset.
- For skin-type labels, one author labeled each image with three ways.
- 'ImageDataGenerator' operates by computing the internal data stats related to the data dependent transformations.
- Rough tuning key parameters and customizing 'ImageData-Generator' library, we were able to increase the percentage of darkskin male from 1.3% to 15.21% and dark-skin female from 2.5% to 16.03%.

## Training Baseline Model. A er resizing each image to 227
- 227 pixels
- We split the augmented Adience benchmark dataset into 23,004 samples for training, 1,279 samples for validation and 1,278 samples for testing.
- Inspired by the CNN trained on Adience benchmark database, we trained our baseline model on augmented Adience images using 6 convolutional layers, each followed by a recti ed linear operation and pooling layer.
- e rst four layers are also followed by batch normalization and dropouts to prevent the model from over ing.
- e rst two convolutional layers contain 96 7x7 kernels, the third and fourth convolutional layers contain 256 5x5 kernels.
- e h and nal convolutional layers contain 384 3x3 kernels. Additionally, two fully-connected layers are added, each containing 512 neuron.

## Baseline Model Accuracy Evaluation.
- Our baseline model is trained to predict gender on two classes: male and female.
- The model achieved an overall accuracy score of 94.37% on the Adience benchmark test set (1278 images).
- Figure 8 shows a sample of the misclassified images from the baseline and Figure 9 shows the learning curve of our baseline model.

## Extension of binary gender classi er
- The accuracy of the baseline model dropped to 51.67% when predicting gender on images from the inclusive database and non-binary database
- To improve the accuracy of the model, we supplied training samples from the inclusive and non-binary databases
- We also used images from the inclusive and non-binary databases for validation and testing

## Oversampling.
- Over-sampling technique was used to increase the percentage of male population from 29.89% to 36.51%.
- Accuracy rates of the training sets were around 10% higher than that of the validation set.
- Various ensemble techniques were used to make the nal prediction outcomes more robust.

## Logistic Regression
- We constructed a logistic regression model by assembling the baseline ne-tuned model, the baseline feature extraction model and the VGG16 feature extraction model.
- By horizontally stacking the probabilities of the three classes output by the list of models, we constructed a matrix of dimension 3778 x 9 which we used for training.
- A er applying logistic regression cross validation, we trained a logistic regression model and achieved an accuracy score of 96.03 % on the training set.
- Applying the trained model on the test set (1260 images), we received an accuracy score of 90.39%.

## Adaboost Ensemble.
- We constructed an adaboost model by assembling the baseline ne-tuned model, the baseline feature extraction model and the VGG16 feature extraction model
- By horizontally stacking the predicted class output by each of the models, we constructed a matrix of dimension of 3778 x 3
- We used for training
- A er applying grid search to cross validate the parameters, we trained an adaboost mega-model and achieved an accuracy score of 96.03%
- Applying the trained model on the test set (1260 images), we received an accuracy score of 90.24%

## Evaluation Metric
- We use selection rate as a measure of algorithmic bias
- An increase in selection rate is considered to be bias mitigation in which case the di erences between the highest accuracy rate and the lowest accuracy rate has been reduced
- Applying the 80% threshold, we consider any selection rate below 80% as an indication of disparate impact
- Disparate impact can result in a disparate proportion of individuals from a protected group (e.g., race, gender, religion, etc.) being subjected to unintended discrimination
- Using the 80% rule, our baseline model is subject to disparate impact since the accuracy rate for non-binary gender population is 0%
- A er applying transfer learning techniques however, disparate impact is removed
- Transfer learning has produced impressive results in improving classi cation accuracy on the non-binary gender class and mitigating algorithmic biases
- Our Adaboost ensemble model achieved a selection rate of 98.46%, a signi cant increase from our baseline model.

## RESULTS AND DISCUSSION
- We found that our baseline model is subject to disparate impact on the non-binary population under the 80% rule.
- A er applying various machine learning techniques such as oversampling, transfer learning and ensemble, we were able to mitigate algorithmic bias and increase classi cation accuracy from our baseline model.
- We assembled three of our transfer learning models: baseline feature extraction, baseline ne-tuned, and VGG16 feature extraction model using logistic ensemble -our mega model achieved an overall accuracy rate of 90.39%.

## LIMITATIONS AND FUTURE WORK 4.1 Limitations
- Our study extended the current binary gender classi cation system to include a non-binary gender class.
- By assembling 2 di erent databases (a racially balanced and LGBTQ inclusive database and a non-binary gender database), we worked with various machine learning techniques to teach the classi er to recognize non-binary gender class and increase gender classi cation accuracy for the nonbinary population.
- However, our algorithm is limited in terms of providing speci c gender labels within the non-binary class, such as gender uid, gender-queer, gender nonconforming, agender, demigender and etc.
- Among the misclassi ed images using our best performing classi er -logistic regression ensemble model, we noticed that people of color, as well as the LGBTQ population are more likely to be misclassi ed compared to the other subclasses.
- Figure 14 shows a sample of misclassi ed images using our logistic ensemble model.
- And Figure 15 shows a sample of the misclassi ed images using our adaboost ensemble model.
- e classi cation accuracy is also a ected by di erent masks on face. For example, we noticed that people who are wearing glasses, or heavy makeups are more likely to be misclassi ed.

## Future Work: Modeling Gender as a Continuum
- The body becomes its gender through a set of actions which are renewed, revised, and consolidated through time.
- Gender is a complex socio-cultural construct and an internal identity that is not necessarily tied to physical appearances.
- Gender identity has its multifaceted aspects that a simple label could not categorize.
- Gender expressions also vary in di erent social contexts.
- A recent study found that non-binary people are inclined to respond to others' perception of their gender entering di erent social contexts.
- Some people shi their gender expression to appear more feminine or masculine in order to t into the social context.
- According to a recent survey by the LGBTQ advocacy organization GLAAD, around 20% of the millennials identi ed themselves other than strictly cis-gendered.
- Gender expressions that go beyond the xed labeling terms are moving from the margins to the mainstream.
