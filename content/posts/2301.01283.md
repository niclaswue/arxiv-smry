---
title: "Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection"
date: 2023-01-03T18:36:52.000Z
author: "Junjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li, Tiancai Wang, Xiangyu Zhang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-01283v1.webp" # image path/url
    alt: "Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01283)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01283).


# Abstract
- Cross Modal Transformer (CMT) is a robust 3D detector for end-to-end 3D multi-modal detection
- CMT takes image and point clouds tokens as inputs and outputs accurate 3D bounding boxes
- Spatial alignment of multi-modal tokens is performed implicitly, by encoding the 3D points into multi-modal features

# Paper Content

## Introduction
- Multi-sensor fusion has shown its great superiority in autonomous driving system
- Different sensors usually provide the complementary information for each other.
- For instance, the camera captures information in a perspective view and the image contains rich semantic features while point clouds provide much more localization and geometry information.
- Taking full advantage of different sensors helps reduce the uncertainty and makes accurate and robust prediction.
- Sensor data of different modalities usually has large discrepancy in distribution, making it hard to merge the multi-modalities.
- State-of-the-art (SoTA) methods tend to fuse the multi-modality by constructing unified bird's-eyeview (BEV) representation [20,24,28] or querying from tokens [1,8].
- For 3D object detection, one intuitive way is to concatenate the image and point cloud tokens together for further interaction with object queries. However, the concatenated tokens are disordered and unaware of their corresponding locations in 3D space. Therefore, it is necessary to provide the location prior for multi-modal tokens and object queries.
- In this paper, we propose Cross-Modal Transformer (CMT), a simple yet effective end-to-end pipeline for highperformance 3D object detection.
- First, we propose the Coordinates Encoding Module (CEM), which produces position-aware features, by encoding 3D points set implicitly into multi-modal tokens.
- Specifically, for camera images, 3D points sampled from frustum space are used to indicate the probability of 3D positions for each pixel.
- While for LiDAR, the BEV coordinates are simply encoded into the point cloud tokens.
- Next, we introduce the position-guided queries. Each query is initialized as a 3D reference point following PETR [26]. We transform the 3D coordinates of reference points to both image and Li-DAR spaces, to perform the relative coordinates encoding in each space. Moreover, for faster convergence, we introduce the inductive bias of locality, by extending Query Denoising [19] to a point-based formulation.
- The proposed CMT framework brings many advantages compared to existing methods.
- Firstly, our method is a simple and end-to-end pipeline and can be easily extended.
- The 3D positions are encoded into the multi-modal features implicitly, which avoids introducing the bias caused by explicit cross-view feature alignment.
- Secondly, our method only contains basic operations, without the feature sampling or complex 2D-to-3D view transformation on multimodal features.
- Thirdly, the robustness of our CMT is much stronger than other existing approaches. Extremely, under the condition of LiDAR miss, our CMT with only image tokens can achieve similar performance compared to those visual 3D object detectors [23,26] (see Fig. 2).

## Related Work

### Camera Based 3D Object Detection
- Camera-based 3D object detection is one of the basic tasks in computer vision
- Early works [41,42] mainly follow the dense prediction pipeline. They first localize the objects on image plane and then predict their relevant 3D attributes, such as depth, size and orientation
- However, with the surrounding cameras, the perspective-view based design requires elaborate post-processes to eliminate the redundant predictions of the overlapping regions
- Recently, 3D object detection under the BEV has attracted increasing attention. The BEV representation provides a unified coordinate to fuse information from multiple camera views.
- LSS [32], BEVDet [15] and BEVDepth [21] predict the depth distribution to lift the image features to 3D frustum meshgrid. Besides, inspired by DETR [4], DETR3D [45] and BEV-Former [23] project the predefined BEV queries onto images and then employ the transformer attention to model the relation of multi-view features.
- The above methods explicitly project the local image feature from 2D perspective view to BEV. Different from them, PETR [26,27] and Spa-tialDETR [9] adopt the positional embedding that depends on the camera poses, allowing the transformer to implicitly learn the projection from image views to 3D space.

### LiDAR Based 3D Object Detection
- Point-based methods extract features from raw point clouds and predict 3D bounding boxes
- PointNet is the first architecture to process the point cloud in an end-to-end manner, which preserves the spatial characteristics of the point cloud
- Other methods project the unordered, irregular Li-DAR point clouds onto a regular feature space such as 3D voxels, feature pillars, and range images
- VoxelNet first divides the raw point clouds into regular voxel grids, and then uses PointNet network to extract features from the points in each voxel grid

### Multi-modal 3D Object Detection
- Multi-sensor fusion in 3D detection has gained great attention in recent years
- State-of-the-art (SoTA) methods tend to find a unified representation for both modalities, or define object queries to fuse the features for further prediction
- For example, BEVFusion [24,28] applies a lift-splatshoot (LSS) operation to project image feature onto BEV space and concatenates it with LiDAR feature.
- UVTR [20] generates a unified representation in the 3D voxel space by deformable attention [56].
- While for query-based methods, FUTR3D [8] defines the 3D reference points as queries and directly samples the features from the coordinates of projected planes.
- TransFusion [1] follows a two-stage pipeline. The proposals are generated by LiDAR features and further refined by querying the image features.

### Transformer-based Object Detection
- DETR [3] proposes a transformer-based detector paradigm without any hand-craft components
- DETR-like methods usually suffer from the slow convergence
- To this end, many works are proposed to improve the training efficiency
- Other improvements in 2D detection mainly focus on modifying the transformer layers
- Deformable DETR [56] proposes the de-formable attention
- SAM-DETR [52] presents a semantic aligner between object queries and encoded features
- To alleviate the instability of bipartite matching, DAB-DETR [25] formulates the object queries as dynamic anchor boxes
- DN-DETR [19] auxillarily reconstructs the ground-truths from the noisy ones
- DINO [53] further improves the denoising anchor boxes via a contrastive way

## Method
- The proposed CMT is a system that can extract multi-modal tokens from images and LiDAR data.
- The multimodal tokens are encoded into 3D coordinates by the coordinates encoding.
- The queries from the position-guided query generator are used to interact with the multi-modal tokens in the transformer decoder and then predict the object class and 3D bounding boxes.
- Point-based query denoising is introduced to accelerate the training convergence.

### Coordinates Encoding Module
- The coordinates encoding module (CEM) is used to encode the 3D position information into multi-modal tokens.
- It generates both the camera and BEV position encodings (PEs), which are added to image tokens and point cloud tokens respectively.
- With the help of CEM, multi-modal tokens can be implicitly aligned in 3D space.
- Let P (u, v) be the 3D points set corresponding to the feature map F (u, v) of different modalities. Here (u, v) indicates the coordinate in the feature map.
- Specifically, F is the image feature for camera while BEV feature for LiDAR.
- Suppose the output position embedding of CEM is Γ(u, v), its calculation can be formulated as: where ψ is a multi-layer perception (MLP) layer.
- CE for Images. Since the image is captured from a perspective view, each pixel can be seen as an epipolar line in 3D space.
- Inspired by PETR [26], for each image, we encode a set of points in camera frustum space to perform the coordinates encoding.
- Given the image feature F im , each pixel can be formulated as a series of points .., d} in the camera frustum coordinates.
- Here, d is the number of points sampled along the depth axis. The corresponding 3D points can be calculated by: where T l ci ∈ R 4×4 is the transformation matrix from the ith camera coordinate to the LiDAR coordinate.
- K i ∈ 4 × 4 is the intrinsic matrix of i-th camera.
- The position encoding of pixel (u, v) for image is formulated as: ) CE for Point Clouds.
- We choose VoxelNet [48,54] or PointPillar [17] as backbone to encode the point cloud tokens F pc .
- Intuitively, the point set P in Eq. ( 1) can be sampled along the Z-axis.
- Suppose (u, v) is the coordinates in BEV feature map, the sampled point set is then p k (u, v) = (u, v, h k , 1) T , where h k indicates the height of k-th points and h 0 = 0 as default.
- The corresponding 3D points of BEV feature map can be calculated by: where (u d , v d ) is the size of each BEV feature grid.
- To simplify, we only sample one point along the height axis. It is equivalent to the 2D coordinate encoding in BEV space.
- The position embedding of point cloud can be obtained by:
- The coordinates encoding module (CEM) is used to encode the 3D position information into multi-modal tokens.
- It generates both the camera and BEV position encodings (PEs), which are added to image tokens and point cloud tokens respectively.
- With the help of CEM, multi-modal tokens can be implicitly aligned in 3D space.
- Let P (u, v) be the 3D points set corresponding to the feature map F (u, v) of different modalities. Here (u, v) indicates the coordinate in the feature map.
- Specifically, F is the image feature for camera while BEV feature for LiDAR.
- Suppose the output position embedding of CEM is Γ(u, v), its calculation can be formulated as: where ψ is a multi-layer perception (MLP) layer.
- CE for Images. Since the image is captured from a perspective view, each pixel can be seen as an epipolar line in 3D space.
- Inspired by PETR [26], for each image, we encode a set of points in camera frustum space to perform the coordinates encoding.
- Given the image feature F im , each pixel can be formulated as a series of points .., d} in the camera frustum coordinates.
- Here, d is the number of points sampled along the depth axis. The corresponding 3D points can be calculated by: where T l ci ∈ R 4×4 is the transformation matrix from the ith camera coordinate to the LiDAR coordinate.
- K i ∈ 4 × 4 is the intrinsic matrix of i-th camera.
- The position encoding of pixel (u, v) for image is formulated as: ) CE for Point Clouds.
- We choose VoxelNet [48,54] or PointPillar [17] as backbone to encode the point cloud tokens F pc .
- Intuitively, the point set P in Eq. ( 1) can be sampled along the Z-axis.
- Suppose (u, v) is the coordinates in BEV feature map, the sampled point set is then p k (u, v) = (u, v, h k , 1) T , where h k indicates the height of k-th points and h 0 = 0 as default.
- The corresponding 3D points of BEV feature map can be calculated by: where (u d , v d ) is the size of each BEV feature grid.
- To simplify, we only sample one point along the height axis. It is equivalent to the 2D coordinate encoding in BEV space.
- The position embedding of point cloud can be obtained by:
- The coordinates encoding module (CEM) is used to encode the 3D position information into multi-modal tokens.
- It generates both the camera and BEV position encodings (PEs), which are added to image tokens and point cloud tokens respectively.
- With the help of CEM, multi-modal tokens can be implicitly aligned in 3D space.
- Let P (u, v) be the 3D points set corresponding to the feature map F (u, v) of different modalities. Here (u, v) indicates the coordinate in the feature map.
- Specifically, F is the image feature for camera while BEV feature for LiDAR.
- Suppose the output position embedding of CEM is Γ(u, v), its calculation can be formulated as: where ψ is a multi-layer perception (MLP) layer.
- CE for Images. Since the image is captured from a perspective view, each pixel can be seen as an epipolar line in 3D space.
- Inspired by PETR [26], for each image, we encode a set of points in camera frustum space to perform the coordinates encoding.
- Given the image feature F im , each pixel can be formulated as a series of points .., d}...

### Position-guided Query Generator
- Initialize queries with n anchor points sampled from uniform distribution between [0, 1]
- Transform anchor points into 3D world space by linear transformation: (x min , y min , z min , x max , y max , z max )
- Project anchor points to different modalities and encode point sets by CEM
- Generate positional embedding Γ q of object queries by: (A pc and A im ) = (x min , y min , z min , x max , y max , z max ) + (query content embedding)

### Point-based Query Denoising (PQD)
- DN-DETR is extended to 3D object detection
- Different from DN-DETR, noisy anchor points are generated by center shifting
- For each noise query, it is a positive query if λ 2 1 + λ 2 2 + λ 2 3 < ξ

### Decoder and Loss
- As for the decoder, we follow the original transformer decoder in DETR [46] and use L decoder layers.
- For each decoder layer, the position-guided queries interact with the multi-modal tokens and update their representations.
- Two feed-forward networks (FFNs) are used to predict the 3D bounding boxes and the classes using updated queries.
- We formulate the prediction process of each decoder layer as follows:
- bi where Ψ reg and Ψ cls respectively represent the FFN for regression and classification.
- Q i is the the updated object queries of the i-th decoder layer.
- For set prediction, the bipartite matching is applied for one-to-one assignment between predictions and ground- truths.
- We adopt the focal loss for classification and L1 loss for 3D bounding box regression:
- where ω 1 and ω 2 are the hyper-parameter to balance the two loss terms.

### Masked-Modal Training for Robustness
- Security is the most important concern for autonomous driving
- Recently, BEVFusion [24] has explored the robustness of LiDAR sensor failure
- In this paper, we try more extreme failures, including single camera miss, camera miss and Li-DAR miss
- It is consistent with the actual scene and ensures the safety of autonomous driving
- To improve the robustness of the model, we propose a training strategy, called masked-modal training

## Experiments

### Datasets and Metrics
- The dataset has 1000 scenes
- Each scene has 20s video frames with 12 FPS
- 3D bounding boxes are annotated every 0.5s
- We only use these key frames
- In each frame, nuScenes provides images from six cameras
- LiDAR. NuScenes provides a 32-beam LiDAR with 20 FPS
- The key frames are also annotated every 0.5s
- We follow the common practice to transform the points from the past 9 frames to the current frame for training and evaluation
- Metrics. We follow the nuScenes official metrics

### Implementation Details
- We use ResNet or VoVNet as image backbone
- The C5 feature is upsampled and fused with C4 feature to produce P4 feature
- We use VoxelNet or PointPillars as the backbone to extract the point-cloud features
- We set the region-of-interest (RoI) to [−54.0m, 54.0m] for X and Y axis, and [−5.0m, 3.0m] for Z axis
- The 3D coordinates in the world space are normalized to [0, 1]
- All the feature dimension is set to 256
- Six decoder layers are adopted in transformer decoder
- Voxel size of 0.075 and image size of 1600 × 640 are adopted as default in our experiments
- Our model is trained with the batch size of 16 on 8 A100 GPUs
- It is trained for total 20 epochs with CBGS
- We adopt the AdamW optimizer for optimization
- The initial learning rate is 1.0×10−4 and we follow the cycle learning rate policy
- The mask ratios η 1 and η 2 are both set to 0.25 for masked-modal training
- The threshold ξ is set to 0.75 to divide the noise queries into positives and negatives for training
- The tolerance λ that controls the noise scale is set to 1.
- GT sample augmentation is employed for the first 15 epochs and closed for the rest epochs

### State-of-the-Art Comparison
- As shown in Tab. 1, CMT achieves comparable results compared to several state-of-the-art methods on nuScenes test set.
- Our LiDAR-only baseline, named CMT-L, achieves the 70.1% NDS, which is a nearly SoTA performance among all existing LiDAR-only methods.
- Our multi-modal method CMT achieves 73.0% NDS and 70.4% mAP and outperforms existing SoTA BEVFusion.
- Benefits from the large receptive field, CMT gains better results on some metrics like mAVE.
- We also compare the performance with other SoTA methods on nuScenes val set (see Tab. 2). It shows that our proposed CMT with multi-modal fusion outperforms the BEVFusion by 0.5% NDS and 0.9% mAP.
- CMT introduces large performance improvements compared to our LiDAR-only CMT-L by 2.9%/5.8% and 3.3%/7.0% NDS/mAP on test and validation set, showing that camera images bring complementary information.

### Strong Robustness
- Evaluates the robustness of the framework under various harsh environments
- Shows that the framework is robust when any one of the cameras fails
- Shows slight performance drop when any one of the cameras fails, indicating tolerable single camera miss

### Ablations
- All the experiments are conducted for 20 epochs
- Removing PC PE introduces a 7.4%/8.70% NDS/mAP performance drop
- PQD can greatly improve the overall performance by 4.3%/5.7% NDS/mAP
- Scaling the image size increases more mAP than the voxel number (+3.2% vs. +1.7%)

### Analysis
- CMT is a direct and easy pipeline for multi-modal fusion
- CMT shows strong robustness under sensor miss conditions
- We present some attempted experiments in this section
- Data extension. Multi-frame is now a common setting in camera-based 3D object detection
- Using multi frames often outperforms the single frame by a clear margin
- We follow the multi-frame alignment in PETRv2
- Considering the high memory cost of multi-frames, we conduct our experiment with a 800 × 320 image resolution
- As shown in Tab. 5, adding image frame only improves the NDS/mAP by 0.2%/0.7%. More image frames rarely improve the performance with multi-modal fusion.
- Radar has advantages in long range detection and the robustness of extreme weather
- Following FUTR3D, we stack points from 5 radars together to generate the point cloud
- We use several MLP layers to perform coordinates encoding on Radar features, the same as LiDAR
- Tab. 5 shows that adding Radar data to our pipeline degrades the performance by 0.9% NDS and 0.6% mAP

## Conclusions
- Cross-modal transformer (CMT) paradigm implicitly encodes 3D coordinates into tokens in images and point clouds
- With coordinates encoding, the simple yet effective DETR pipeline can be adopted for multi-modal fusion and end-to-end learning
- With masked-modal training, our multi-modal detector can be learned with strong robustness, even if one of multi-modalities are missed
- Limitation: Though our CMT brings some advantages compared to those existing approaches, it also reveals some limitations
- The computation cost is relatively larger due to the large number of multi-modal tokens and the global attention employed in transformer decoder
- To solve this problem, some efforts in two directions maybe taken
- The first one is to reduce the redundancy of multi-modal tokens
- The second one is to replace the global attention with other efficient attentions
