---
title: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"
date: 2022-11-14T18:59:52.000Z
author: "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2211-07636v2.webp" # image path/url
    alt: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2211.07636)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2211.07636).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/eva-exploring-the-limits-of-masked-visual).

# Abstract
- We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale
- EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches
- Via this pretext task, we can efficiently scale up EVA to one billion parameters
- We observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models

# Paper Content

## Introduction
- Scaling up pre-trained language models (PLMs) has revolutionized natural language processing (NLP) in the past few years.
- The key to this success lies in the simple and scalable self-supervised learning task of masked signal prediction, with which Transformer models can be scaled up to billions of parameters.
- MIM is somewhat only adopted as an initialization stage before the heavily supervised pre-training.
- Through a pilot empirical study, we find that simply using image-text aligned (i.e., CLIP [73]) vision features as the prediction targets in MIM scales up well and achieves satisfactory performances on a broad range of downstream benchmarks.

### The Feature Instrumentality Project
- The task is a MIM vision task with compelling transfer performance
- Based on previous literature, we study two promising candidates: recovering the masked out tokenized semantic vision features and feature distillation from strong pre-trained representation
- Both of them exploit pre-trained image-text aligned vision features (CLIP features)
- Via a series of pilot experiments, we find that: (i) the (additional) CLIP feature tokenization process is unnecessary for achieving good downstream performance, and (ii) feature distillation fails to provide consistent performance gain
- Instead, we find that simply reconstructing the masked out CLIP vision features conditioned on visible image patches is highly performant

### Pre-training
- EVA is a vanilla ViT with 1.0B parameters
- The shape of her follows ViT giant and the vision encoder of BEiT-3
- We do not use relative positional embeddings and layer-scale during pre-training
- Pre-training objective is to reconstruct the masked out image-text aligned vision features conditioned on visible image patches
- We corrupt the input patches with MASK tokens and use block-wise masking with a masking ratio of 40%
- The target for MIM pre-training is from the publicly available 1 Ope-nAI CLIP-L/14 vision tower trained on 224Ã—224 pixel images
- The output feature of EVA is first normalized and then projected to the same dimension as the CLIP feature via a linear layer
- We use negative cosine similarity as the loss function
- Pre-training data is summarized in Table 3b
- ImageNet-21K and Ob-ject365 image data are also used
- The merged dataset for pre-training has 29.6 million images in total
- The CLIP features we used as MIM prediction targets are trained on a 400 million image-text dataset in a selfsupervised manner
- So during pre-training EVA also implicitly exploits the knowledge from this dataset to some extent
- Meanwhile, these CLIP features is also widely used in other state-of-the-art representation learning & pre-training works such as the BEiT family, AI generated content, and large scale dataset filtering
- EVA is optimized via Adam with decoupled weight decay of 0.05
- The peak learning rate is 1e-3 and decays according to a cosine learning rate schedule
- We employed stochastic depth with a rate of 0.1 for regularization and RandResizeCrop (0.2, 1) for data augmentation
- Color jitter is not used

### Evaluation on Downstream Tasks
- Evaluates pre-trained EVA on several representative benchmarks
- EVA achieves state-of-the-art performance on a broad range of downstream tasks

### Image Classification
- EVA is a fast and accurate image classification model that is comparable to state-of-the-art models
- EVA is robust and generalizes well on different image datasets
- EVA can be fine-tuned using only publicly available data

### Object Detection & Instance Segmentation

### Semantic Segmentation

## Related Work
- Masked image modeling (MIM) learns rich visual representations via predicting masked visual contents conditioned on visible context.
- ViT [33] and iGPT [18] report the first meaningful MIM pre-training results.
- The BEiT family [5,70,104] greatly improves MIM's performance via masked visual token prediction.
- Recent work [4,21,32,34,40,106,116,130] (re-)explore pixel / feature regression in MIM, but only in a relatively small model and data scales.
- In this work, we explore the limits of large scale MIM pre-training via masked image-text aligned feature prediction [44,109].

## Conclusion
- EVA is a one billion parameter vanilla ViT encoder that explores the limits of masked visual representation learning
- EVA scales well on an architecture with minimal vision priors
- EVA bridges the gap between vision and language study via masked modeling
- EVA is able to achieve excellent results in a representative & diverse set of downstream tasks
