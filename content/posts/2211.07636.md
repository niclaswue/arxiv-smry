---
title: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"
date: 2022-11-14T18:59:52.000Z
author: "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2211-07636v2_5PBrte_g8p.jpg" # image path/url
    alt: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2211.07636)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2211.07636).


# Abstract
- We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale
- EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches
- Via this pretext task, we can efficiently scale up EVA to one billion parameters
- We observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models

# Paper Content

## Introduction
- Scaling up pre-trained language models (PLMs) has revolutionized natural language processing (NLP) in the past few years.
- The key to this success lies in the simple and scalable self-supervised learning task of masked signal prediction, with which Transformer models can be scaled up to billions of parameters.
- With further scaling on compute, data, and model sizes, PLMs have led to not only continuous performance improvements, but also a surprising emergence of in-context learning capability.
- Motivated by the success of model scaling in NLP, it is appealing that we can also translate this success from language to vision, i.e., to scale up a vision-centric foundation model that is beneficial for both vision & multi-modal downstream tasks.
- Recently, masked image modeling (MIM) has boomed as a viable approach for vision model pretraining and scaling. However, the most competitive billionsized vision pre-trained models still heavily rely on supervised or weakly-supervised training with hundreds of millions of (often publicly inaccessible) labeled data.
- MIM is somewhat only adopted as an initialization stage before the heavily supervised pre-training.
- Through a pilot empirical study, we find that simply using image-text aligned (i.e., CLIP [73]) vision features as the prediction targets in MIM scales up well and achieves satisfactory performances on a broad range of downstream benchmarks.
- This pre-training task draws the benefits from both the high-level semantic abstraction of image-text contrastive learning as well as the good capture of geometry & structure in masked image modeling, which typically covers the information needed for most visual perception tasks.
- Via this MIM pretext task, we can efficiently scale up a vanilla ViT encoder [33], dubbed EVA, to one billion parameters with strong visual representations that transfers well to a wide range of downstream tasks (Fig. 1).

### The Feature Instrumentality Project
- The task is a MIM vision task with compelling transfer performance
- Based on previous literature, we study two promising candidates: recovering the masked out tokenized semantic vision features and feature distillation from strong pre-trained representation
- Both of them exploit pre-trained image-text aligned vision features (CLIP)
- Via a series of pilot experiments, we find that: (i) the (additional) CLIP feature tokenization process is unnecessary for achieving good downstream performance, and (ii) feature distillation fails to provide consistent performance gain
- Instead, we find that simply reconstructing the masked out CLIP vision features conditioned on visible image patches is highly performant
- This MIM pretext task is not originally proposed by us

### Pre-training
- EVA is a vanilla ViT with 1.0B parameters
- EVA's architecture is in Table 3a
- EVA is pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches
- The target for MIM pre-training is from the publicly available 1 Ope-nAI CLIP-L/14 vision tower trained on 224Ã—224 pixel images
- The output feature of EVA is first normalized and then projected to the same dimension as the CLIP feature via a linear layer
- We use negative cosine similarity as the loss function
- Pre-training data is summarized in Table 3b
- ImageNet-21K and Ob-ject365 image data are also used
- The merged dataset for pre-training has 29.6 million images in total
- The CLIP features we used as MIM prediction targets are trained on a 400 million image-text dataset in a selfsupervised manner
- EVA is optimized via Adam with decoupled weight decay of 0.05
- Stochastic depth with a rate of 0.1 for regularization and RandResizeCrop (0.2, 1) for data augmentation are employed
- Color jitter is not used

### Evaluation on Downstream Tasks
- EVA achieves state-of-the-art performance on a broad range of downstream tasks
- Evaluation shows that EVA is effective at generalizing from training data
- Pre-trained EVA is effective at performing a variety of tasks

### Image Classification
- EVA is evaluated on ImageNet-1K (IN-1K) [30] validation set
- EVA is also evaluated on ImageNet-V2 (IN-V2) [81], ImageNet-ReaL (IN-ReaL) [7], ImageNet-Adversarial (IN-Adv.) [43], ImageNet-Rendition (IN-Ren.) [42], ImageNet-Sketch (IN-Ske.) [102]
- Training Settings: Following the conventional setting [5,70,104], we first perform intermediate fine-tuning on ImageNet-21K [30] for 60 epochs with an image resolution of 2242 , then EVA is further fine-tuned on ImageNet-1K training set for 10 epochs
- Different from [120,123] that use multi-head attention pooling and BEiT-3 that exploits an additional pretrained giant language tower as the image classification task layer, we simply adopt a linear layer as the classifier [33]
- Results: Table 4 compares EVA with some state-of-the-art models on ImageNet-1K validation set
- EVA achieves 89.6% top-1 accuracy with 336 2 inputs, comparable to BEiT-3
- Using a larger image resolution of 560 2 can further boost the top-1 accuracy to 89.7%
- Notice that BEiT-3 treats image classification as an image-to-text retrieval task. Therefore they leverage an additional one billion parameters pre-trained language encoder along with 35 million image-text data (21M pairs from CC12M, CC3M, SBU, COCO, VG and 14M pairs from ImageNet-21K) as well as 160GB text data in total. Meanwhile, we simply use a linear classifier on top of EVA with only ImageNet-21K image-tag data used for additional finetuning
- With only publicly available data, EVA creates a new state-of-the-art image classification result on ImageNet-1K with a much neater architecture

### Object Detection & Instance Segmentation

### Semantic Segmentation

## Related Work
- Masked image modeling (MIM) learns rich visual representations via predicting masked visual contents conditioned on visible context.
- ViT [33] and iGPT [18] report the first meaningful MIM pre-training results.
- The BEiT family [5,70,104] greatly improves MIM's performance via masked visual token prediction.
- Recent work [4,21,32,34,40,106,116,130] (re-)explore pixel / feature regression in MIM, but only in a relatively small model and data scales.
- In this work, we explore the limits of large scale MIM pre-training via masked image-text aligned feature prediction [44,109].
- Vision foundation models. ConvNets [56] have long been the de-facto standard visual architecture ab initio.
- Since AlexNet [55], ConvNets have rapidly evolved and become deeper, wider and larger [41,45,65,91,93,96,114].
- However, at sufficient model and data scales, ConvNets lag behind ViTs [33] due to a lack of scalable pre-training tasks and the built-in inductive biases.
- Entering the 2020s, large pretrained ViTs [33,123] such as SwinV2-G [64] with hierarchical architectures as well as BEiT-3 [104] with multi-modal representations started to demonstrate various vision benchmarks.
- In this work, we show by leveraging unlabeled images, vanilla ViT can be efficiently scaled up to billion-scale parameters, and stands out in various downstream tasks.

## Conclusion
- EVA is a one billion parameter vanilla ViT encoder
- EVA scales well on an architecture with minimal vision priors
- Masked feature modeling is a visual learning pretext task
- EVA bridges the gap between vision and language study via masked modeling

### A.3. Object Detection & Instance Segmentation
- The model is trained with a batch size of 128 for 380k iterations
- To accelerate the training process, the input resolution is lifted to 1280 2 for a better adaptation to the fine-tuning of COCO and LVIS
- For fine-tuning COCO and LVIS, the learning rate is initialized as 2.5e-5 and step by a factor of 10 for the last 5k iterations
- As shown in Table 18, the only difference in training is that we train the model for 45k steps on COCO, while a longer 75k step on LVIS
