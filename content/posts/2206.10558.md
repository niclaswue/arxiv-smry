---
title: "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine"
date: 2022-06-21T17:36:15.000Z
author: "Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, Zhongwen Xu, Shuicheng Yan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2206-10558v2.webp" # image path/url
    alt: "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.10558)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.10558).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/envpool-a-highly-parallel-reinforcement).

# Abstract
- Significant progress has been made in developing reinforcement learning (RL) training systems
- Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the
- In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel
- With a curated design for paralleling RL environments, we have improved the RL
- On a high-end machine, EnvPool achieves one million frames per second for the environment
- Moreover, great compatibility with existing RL training libraries has been demonstrated in

# Paper Content

## Introduction
- Deep Reinforcement Learning has made remarkable progress in the past years
- Notable achievements include Deep Q-Network (DQN), AlphaGo, AlphaStar, OpenAI Five
- On the other hand, academic research has been accelerated dramatically by the shortened training time
- To the best of our knowledge, it is often the slowest part of the whole system but has received little attention in previous research
- The unique technical difficulty in RL systems is the interaction between the agents and the environments
- Our contribution is to optimize the environment execution for general RL environments, including video games and various applications of financial trading, recommendation systems, etc.
- Performance highlights of the EnvPool system include:
- With 256 CPU cores on an NVIDIA DGX-A100 machine, EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation
- On a laptop with 12 CPU cores, EnvPool obtains a speed 2.8× of the Python implementation

## Related Works
- Most implementations in RL systems use Python-level parallelization, e.g., For-loop or subprocess [4], in which we can easily run multiple environments and obtain the interaction experience in a batch.
- This issue might be because, in a distributed setup, Ray and RLlib have to trade-off the communication costs with other components and are not specifically optimized for environment execution.
- Sample Factory [24] focuses on optimizing the entire RL system for a single-machine setup instead of a distributed computing architecture.
- To achieve high throughput in the action sampling component, they introduce a sophisticated, fully asynchronous approach called Double-Buffered Sampling, which allows network forwarding and environment execution to run in parallel but on different subsets of the environments.
- Though having improved the overall throughput dramatically over other systems, the implementation complexity is high, and it is not a standalone component that can be plugged into other RL systems.
- In contrast, EnvPool has both properties of high throughput and great compatibility with existing APIs and RL algorithms.
- A few recent works, e.g., Brax [9], Isaac Gym [21], and WarpDrive [18], use accelerators like GPUs and TPUs for the environment engine.
- Due to the highly parallel nature of the accelerators, numerous environments can be executed simultaneously.
- The intrinsic drawback of this approach is that the environments must be purely compute-based, i.e., matrix operations so that they can be easily accelerated on GPUs and TPUs.
- They cannot handle general environments like common video games, e.g., Atari [1], ViZDoom [17], StarCraft II [35], and Dota 2 [2].
- In contrast, EnvPool uses the asynchronous execution mode as a default to avoid slowing down due to any single environment instance.
- Moreover, it is not tied to any specific computing architectures. We have run EnvPool on both GPU machines and Cloud TPUs.

## Methods
- The EnvPool system contains three key components: the ActionBufferQueue, the ThreadPool, and the StateBufferQueue.
- The ActionBufferQueue is optimized in C++, and uses pybind11 to expose the user interface to Python.
- The ThreadPool is optimized in C++, and uses a priority queue to manage the threads.
- The StateBufferQueue is optimized in C++, and uses a vector to store the states of the agents.

### Overview
- The central way to interact with RL environments in gym and dm_env is through the step function.
- The typical approach is to replicate it in multiple threads or processes, but in systems that prioritize throughput (such as web services), the asynchronous event-driven pattern often achieves better overall throughput.
- EnvPool follows the asynchronous event-driven pattern visualized in Figure 1, instead of providing a synchronous step function.
- In each interaction, EnvPool receives a batched action through the send function.
- The send function only puts these actions in the ActionBufferQueue, and returns immediately without waiting for environment execution.
- Independently, threads in the ThreadPool take action from the ActionBufferQueue and perform the corresponding environment execution.
- The execution result is then added to the StateBufferQueue, which is pre-allocated as blocks.
- A block in StateBufferQueue contains a fixed number (batch_size in the next section) of states.
- Once a block in the StateBufferQueue is filled with data, EnvPool will pack them into NumPy arrays.
- The RL algorithm receives a batch of states by taking from the StateBufferQueue via the recv function.

### Synchronous vs. Asynchronous
- The synchronous step is inefficient for scaling out
- We introduce a new concept batch_size which allows for efficient scaling out
- Asynchronous send/recv has a considerable advantage when the environment execution time has a large variance

### ThreadPool
- To minimize context switch overhead, the number of threads in ThreadPool is usually limited by the number of CPU cores.
- To further speed up ThreadPool execution, we can pin each thread to a pre-determined CPU core. This further reduces context switching and improves cache efficiency.
- We recommend setting num_env N to be 2−3× greater than the number of threads to keep the threads fully loaded when part of the envs are waiting to be consumed by the RL algorithm.

### Adding New RL Environments
- EnvPool is a platform for adding new reinforcement learning environments
- The process is well-documented and straightforward for C++ developers
- First, developers need to implement the RL environment in a C++ header file
- Next, they need to write a Bazel BUILD file to manage dependencies
- They can then use these C++ source files to generate a dynamically linked binary
- Finally, they need to register the environment in Python side and write rigorous unit tests for debugging

## Experiments
- The experiments are divided into two parts
- In the first part, we evaluate the simulation performance of the reinforcement learning environment execution engines
- This isolated benchmark allows us to measure the performance of the engine component without the added complexity of agent policy network inference and learning
- In the second part of our experiments, we assess the impact of using EnvPool with existing RL training frameworks
- We test EnvPool with CleanRL [14], rl_games [20], and DeepMind's Acme framework [12] to see how it can improve overall training performance
- Overall, our experiments demonstrate the value of EnvPool as a tool for improving the efficiency and scalability of RL research

### Pure Environment Simulation
- We first evaluate the performance of EnvPool against a set of established baselines on the RL environment execution component.
- Three hardware setups are used for the benchmark: the Laptop setting, the Workstation setting, and the NVIDIA DGX-A100 setting.
- Evaluating EnvPool on these two configurations can demonstrate its effectiveness with small-scale experiments.
- An NVIDIA DGX-A100 has 256 CPU cores with 8 NUMA nodes.
- Note that running multi-processing on each NUMA node not only makes the memory closer to the processor but also reduces the thread contention on the ActionBufferQueue.
- As for the RL environments, we experiment on two of the most used RL benchmarks, namely, Atari [1] with Pong and MuJoCo [33] with Ant.
- In the experiments of pure environment simulation, we obtain a randomly sampled action based on the action space definition of the games and send the actions to the environment executors.
- The number of frames per second is measured with a mean of 50K iterations, where the Atari frame numbers follow the practice of IMPALA [7] and Seed RL [6] with frameskip set to 4, and MuJoCo sub-step numbers set to 5.
- We compare several concrete implementations extensively, which are described below.
- Among them, Subprocess is the most popular implementation currently and, to the best of our knowledge, Sample Factory is the best performing general RL environment execution engine at the time of publication.
- • For-loop: execute all environment steps synchronously within only one thread;
- • Subprocess [4]: execute all environment steps synchronously with shared memory and multiple processes.
- • Sample Factory [24]: pure asynchronous step with a given number of worker threads; we pick the best performance over various num_envs per worker.
- • EnvPool (sync): synchronous step execution in EnvPool.
- • EnvPool (async): asynchronous step execution in EnvPool; given several worker threads for batch_size, pick the best performance over various num_envs.
- • EnvPool (numa+async): use all NUMA nodes, each launches EnvPool individually with asynchronous execution to see the best performance of EnvPool.
- To demonstrate the scalability of the above methods, we conduct experiments using various numbers of workers for the RL environment execution.
- The experiment setup ranges from a couple of workers (e.g., 4 cores) to using all the CPU cores in the machine (e.g., 256 cores).
- Our EnvPool system outperforms all of the strong baselines with significant margins on all hardware setups of the Laptop, Workstation, and DGX-A100 (Figure 3 and Table 1).
- The most popular Subprocess implementation has extremely poor scalability with an almost flat curve. This indicates a small improvement in throughput with the increased number of workers and CPUs.
- The poor scaling performance of Python-based parallel execution confirms the motivation of our proposed solution.
- The second important conclusion is that, even if we use a single environment in EnvPool, we can get a free ∼2× speedup.
- Complete benchmarks on Atari, MuJoCo, and DeepMind Control can be found in Appendix C.
- The third observation is that synchronous modes have significant performance disadvantages against asynchronous systems. This is because the throughput of the synchronous mode execution is determined by the slowest single environment instance, where the stepping time for each environment instance may vary considerably, especially when there is a large number of environments.

## Conclusion
- EnvPool outperforms existing environment executors
- With a curated design dedicated to the RL use case, we leverage techniques of a general asynchronous execution model
- For data organization and outputting batch-wise observations, we designed BufferQueue tailored for the RL environments
- We conduct an extensive study with various setups to demonstrate the scale-up ability of the proposed system and compare it with both the most popular implementation gym and highly optimized system Sample Factory
- The conclusions hold for both Atari and MuJoCo, two of the most popular RL benchmark environments
- In addition, we have demonstrated significant improvements in existing RL training libraries' speed when integrated with EnvPool in a wide variety of setups, including different machines, different RL environments, different RL algorithms, etc.
