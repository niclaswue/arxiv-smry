---
title: "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine"
date: 2022-06-21T17:36:15.000Z
author: "Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen Liu, Yufan Song, Ting Luo, Yukun Jiang, Zhongwen Xu, Shuicheng Yan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2206-10558v2_jtpEevd62.jpg" # image path/url
    alt: "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.10558)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.10558).


# Abstract
- Significant progress has been made in developing reinforcement learning (RL) training systems
- Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the
- In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel
- With a curated design for paralleling RL environments, we have improved the RL
- On a high-end machine, EnvPool achieves one million frames per second for the
- Moreover, great compatibility with existing RL training libraries has been

# Paper Content

## Introduction
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2× improvement over the current popular Python implementation [4] (i.e., 72K frames per second / 163K physics steps per second for the same hardware setup).
- EnvPool achieves a simulation throughput of one million frames per second on Atari and three million physics steps per second on MuJoCo environments, which is 14.9× / 19.2...

## Related Works
- Most implementations in RL systems use Python-level parallelization, e.g., For-loop or subprocess [4], in which we can easily run multiple environments and obtain the interaction experience in a batch.
- The straightforward Python approaches are plugged easily with existing Python libraries and thus widely adopted, but they are computationally inefficient compared to using a C++-level thread pool to execute the environments.
- In contrast, EnvPool uses the asynchronous execution mode as a default to avoid slowing down due to any single environment instance.
- Moreover, it is not tied to any specific computing architectures.

## Methods
- EnvPool is a C++ library that contains three key components: the ActionBufferQueue, the ThreadPool, and the StateBufferQueue.
- The ActionBufferQueue is optimized for performance and uses pybind11 to expose the user interface to Python.
- The ThreadPool is optimized for concurrency and uses a custom thread scheduler.
- The StateBufferQueue is optimized for memory usage and uses a custom allocator.

### Overview
- The central way to interact with RL environments in gym and dm_env is through the step function.
- To increase the throughput of this interaction, the typical approach is to replicate it in multiple threads or processes. However, in systems that prioritize throughput (such as web services), the asynchronous event-driven pattern often achieves better overall throughput.
- This is because it avoids the context switching costs that arise in a simple multi-threaded setting.
- EnvPool follows the asynchronous event-driven pattern visualized in Figure 1.
- Instead of providing a synchronous step function, in each interaction, EnvPool receives a batched action through the send function.
- The send function only puts these actions in the ActionBufferQueue, and returns immediately without waiting for environment execution.
- Independently, threads in the ThreadPool take action from the ActionBufferQueue and perform the corresponding environment execution.
- The execution result is then added to the StateBufferQueue, which is pre-allocated as blocks.
- A block in StateBufferQueue contains a fixed number (batch_size in the next section) of states.
- Once a block in the StateBufferQueue is filled with data, EnvPool will pack them into NumPy [10] arrays.
- The RL algorithm receives a batch of states by taking from the StateBufferQueue via the recv function.
- Details on the ActionBufferQueue and StateBufferQueue can be found in Appendix D.

### Synchronous vs. Asynchronous
- The synchronous step is inefficient for scaling out
- We introduce a new concept batch_size which allows for more efficient scaling out
- Asynchronous send/recv has a considerable advantage when the environment execution time has a large variance

### ThreadPool
- To minimize context switch overhead, the number of threads in ThreadPool is usually limited by the number of CPU cores.
- To further speed up ThreadPool execution, we can pin each thread to a pre-determined CPU core. This further reduces context switching and improves cache efficiency.
- We recommend setting num_env N to be 2−3× greater than the number of threads to keep the threads fully loaded when part of the envs are waiting to be consumed by the RL algorithm.

### Adding New RL Environments
- EnvPool is a highly extensible and developer-friendly platform for adding new reinforcement learning environments
- The process is well-documented and straightforward for C++ developers
- First, developers need to implement the RL environment in a C++ header file
- Next, they need to write a Bazel BUILD file to manage dependencies
- They can then use these C++ source files to generate a dynamically linked binary
- Finally, they need to register the environment in Python side and write rigorous unit tests for debugging

## Experiments
- Evaluate the performance of reinforcement learning environment execution engines
- Impact of using EnvPool with existing RL training frameworks

### Pure Environment Simulation
- For-loop: execute all environment steps synchronously within only one thread;
- Subprocess: execute all environment steps synchronously with shared memory and multiple processes;
- Sample Factory: pure asynchronous step with a given number of worker threads;
- EnvPool (sync): synchronous step execution in EnvPool;
- EnvPool (async): asynchronous step execution in EnvPool;
- EnvPool (numa+async): use all NUMA nodes, each launches EnvPool individually with asynchronous execution to see the best performance of EnvPool.
- We compare several concrete implementations extensively, which are described below.
- Among them, Subprocess is the most popular implementation currently and, to the best of our knowledge, Sample Factory is the best performing general RL environment execution engine at the time of publication.

## Conclusion
- Introduced a highly parallel reinforcement learning environment execution engine EnvPool
- This engine significantly outperforms existing environment executors
- With a curated design dedicated to the RL use case, we leverage techniques of a general asynchronous execution model
- For data organization and outputting batch-wise observations, we designed BufferQueue tailored for the RL environments
- We conduct an extensive study with various setups to demonstrate the scale-up ability of the proposed system
- The conclusions hold for both Atari and MuJoCo, two of the most popular RL benchmark environments
- In addition, we have demonstrated significant improvements in existing RL training libraries' speed when integrated with EnvPool

## B CPU Specifications for Pure Environment Simulation
- The laptop has 12 Intel CPU cores
- The workstation has 32 AMD CPU cores
- The laptop has a Intel Core i7-8750H CPU @ 2.20GHz
- The workstation has an AMD Ryzen 9 5950X 16-Core Processor
- Evaluating EnvPool on these two configurations can demonstrate its effectiveness
- An NVIDIA DGX-A100 has 256 CPU cores with an AMD EPYC 7742 64-Core Processor and 8 NUMA nodes

## C Speed Improvements on Single Environment
- EnvPool manages to reduce overhead compared to the Python counterpart
- ActionBufferQueue is the queue that caches the actions from the send function, waiting to be consumed by the ThreadPool
- Many open-source general-purpose thread-safe event queues can be used for this purpose
- In this work, we observe that in our case the total number of environments N, the batch_size M, and the number of threads are all pre-determined at the construction of EnvPool
- The ActionBufferQueue can thus be tailored for our specific case for optimal performance
- We implemented ActionBufferQueue with a lock-free circular buffer
- A buffer with a size of 2N is allocated
- We use two atomic counters to keep track of the head and tail of the queue
- The counters modulo 2N is used as the indices to make the buffer circular
- When one environment finishes its step inside ThreadPool, the corresponding thread will acquire a slot in StateBufferQueue to write the data
- When all slots are written, a block is marked as ready (see yellow slots in Figure 1)
- By pre-allocating memory blocks, each block in StateBufferQueue can accommodate a batch of states
- Environments will use slots of the pre-allocated space in a first come first serve manner
- When a block is full, it can be directly taken as a batch of data, saving the overhead for batching
- Both the allocation position and the write count of a block are tracked by atomic counters
- When a block is ready, it is notified via a semaphore
- Therefore the StateBufferQueue is also lock-free and highly performant
- The popular Python vectorized environment executor performs memory copies at several places that are saved in EnvPool
- In EnvPool, these copies are saved thanks to the StateBufferQueue because:
- We pre-allocate memory for a batch of states, the pointer to the target slot of memory is directly passed to the environment execution and written from the worker thread
- The ownership of the block of memory is directly transferred to Python and converted into NumPy arrays via pybind11 when the block of memory is marked as ready
- Currently, EnvPool supports jitting for CPU and GPU when used with JAX [3]
- All jittable functions are implemented via XLA's custom call mechanism [8]
- When the environment code is jitted, the control loop of the actor is lowered from Python code to XLA's runtime, allowing the entire control loop to run on a native thread and freeing the Python Global Interpreter Lock (GIL)

## E Jitting for JAX
- CleanRL's PPO closely matches the performance and implementation details of openai/baselines' PPO
- The source code is made available publicly
- The hardware specifications for conducting the CleanRL's experiments are as follows: • OS: Pop!_OS 21.10 x86_64 • Kernel: 5.17.5-76051705-generic • CPU: AMD Ryzen 9 3900X (24) @ 3.800GHz • GPU: NVIDIA GeForce RTX 2060 Rev. A • Memory: 64237MiB
- CleanRL's Atari experiment's hyperparameters and learning curves can be found in Table 3 and Figure 7.
- CleanRL's MuJoCo experiment's hyperparameters and learning curves can be found in
- Note that [28] uses N envs = 1 so we needed to find an alternative set of hyperparameters.

### F.3 Acme-based Training Results
- Integrates EnvPool with Acme for experiments of PPO in MuJoCo tasks
- EnvPool is argued to be more efficient than other vectorized environments such as Stable Baseline's DummyVecEnv
- All the experiments were performed on a standard TPUv3-8 machine on Google Cloud
- Under the same environment interaction budget, tuning the num_envs can greatly reduce the training time while maintaining similar sample efficiency
