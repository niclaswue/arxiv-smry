---
title: "Policy learning 'without'' overlap: Pessimism and generalized empirical Bernstein's inequality"
date: 2022-12-19T22:43:08.000Z
author: "Ying Jin, Zhimei Ren, Zhuoran Yang, Zhaoran Wang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09900v1.webp" # image path/url
    alt: "Policy learning without'' overlap: Pessimism and generalized empirical Bernstein's inequality" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09900)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09900).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/policy-learning-without-overlap-pessimism-and).

# Abstract
- offline learning relies on a uniform overlap assumption
- this assumption can be unrealistic in many situations
- we propose a new algorithm that optimizes lower confidence bounds
- these bounds are constructed using knowledge of the behavior policies for collecting the offline data

# Paper Content

## Introduction
- Policy learning aims to construct an optimal individualized decision rule that achieves the best overall outcomes for a given population
- Pre-collected data is often used in policy learning, and the uniform overlap assumption is often violated in practice
- Policy evaluation and policy optimization are used to construct policy value estimators and optimize them to return the optimal policy, but it is unclear how to efficiently learn the optimal policy when the uniform overlap assumption is violated

### A pessimism-based framework
- The pessimism principle: optimizes confidence lower bounds instead of point estimates
- Derived a generalized empirical Bernstein inequality to construct the LCBs
- For both batched and adaptively collected data, proves a generic data-dependent upper bound for the performance of the algorithm
- For adaptively collected data, achieves polynomial learning rate once the propensity for the optimal policy is decaying at a polynomial rate
- Supports matching minimax lower bound

## Background
- The space of contexts is X and the space of rewards is Y.
- A contextual bandit model is specified by an action set, a context distribution, and a set of reward distributions.
- Policy learning is done from an offline dataset that is collected a priori.

### Data collecting process
- At each time t = 1, 2, . . . , T , a context X t is independently drawn from P X .
- Then the experimenter takes an action A t ∈ A according to the behavior policy, and receives a reward Y t = µ(X t , A t ) + ǫ t , where ǫ t are independent mean-zero noise.
- Throughout, we denote We use e t (•, • | H t ) to additionally emphasize that the behavior policy is potentially adaptive.
- Closer to the causal inference setting (Imbens and Rubin, 2015), A t can be understood as the treatment option taken for X t , and the reward is Y t = Y t (A t ), where {Y t (a)} a∈A are potential outcomes with (X t , Y t (a) a∈A ) following a population joint distribution, and we implicitly assume the conditional independence condition: Y t (a) ⊥ ⊥ A t | X t , H t .
- The two types of data generating processes we introduced in Section 1 are formally defined as follows. (a) Batched data: the behavior policy is fixed and known, where (b) Adaptively collected data: at each time t ∈ [T ], the action is taken according to a history-dependent rule, where As the behavior policy may depend on previous observations, {(X t , A t , Y t )} T t=1 are no longer i.i.d.

### Policy learning and performance metric
- Let Π be a collection of policies.
- Each element in Π is a mapping π : X → A.
- For any π ∈ Π, we define its average policy value under the contextual bandit model C as where the expectation is taken with respect to the context distribution.
- The optimal policy π * ∈ Π maximizes the policy value, i.e.,
- Our goal is to learn a policy π ∈ Π and measure the learning performance by suboptimality of π, defined as
- The above notations emphasize the dependence of our performance measure on the underlying contextual bandit C and policy class Π.

### Policy class complexity
- We use the Natarajan dimension (Natarajan, 1989) to quantify the complexity of the policy class Π, which plays an important role in characterizing the performance of policy learning algorithms.
- Definition 2.2 (Natarajan dimension). Given policy class Π = {π : X → A}, we say {x 1 , x 2 , . . . , x m } is shattered by Π if there exist two functions f 1 , f 2 : {x 1 , x 2 , . . . , x m } → A such that the following holds: (1) For any j ∈ [m], f 1 (x j ) = f 2 (x j ); (2) For any subset S ⊆ [m], there exists some π ∈ Π such that π(x j ) = f 1 (x j ) for all j ∈ S and π(x j ) = f 2 (x j ) for all j / ∈ S.
- The Natarajan dimension of Π, denoted as N-dim(Π), is the largest size of a set of points shattered by Π.
- The Natarajan dimension is a generalization of the well-known Vapnik-Chervonenkis (VC) dimension (Vapnik and Chervonenkis, 2015) in the multi-action classification problem.
- Finite upper bounds for the Natarajan dimension have been established for many commonly used policy classes including linear function classes, reduction trees, decision trees, random forests and neural networks.

### Related work
- Policy learning from a fixed class
- The overlap assumption is not necessary for efficient policy learning
- Pessimism in offline RL and bandits
- The principle of pessimism was initially proposed in offline reinforcement learning (RL) (Buckman et al., 2020;Jin et al., 2021b) for learning an optimal policy in Markov decision processes (MDPs) using pre-collected datasets (Zanette et al., 2021;Xie et al., 2021a;Chen and Jiang, 2022;Rashidinejad et al., 2021;Yin and Wang, 2021;Shi et al., 2022;Yan et al., 2022;Xie et al., 2021b;Uehara and Sun, 2021) and has been applied to contextual bandits as a special case of MDPs.
- To the best of our knowledge, this work is the first to introduce and instantiate the pessimism principle for learning individualized decision rules, which requires a distinct set of techniques.
- To deal with the technical challenges in this setting, we develop a novel empirical Bernstein's inequality to provide uncertainty quantification for inverse-weighted estimators.
- Variance regularization in ERM
- The idea of pessimism has also appeared in statistical learning and empirical risk minimization (ERM) that takes into account the estimation uncertainty in empirical risks.
- Among them, Maurer and Pontil (2009) was the first to propose to optimize a variance-regularized empirical risk, and was later extended by Duchi and Namkoong (2016) to a computationally tractable formulation.
- The variance regularization is justified by an empirical Bernstein's inequality studied by a series of works (Bartlett et al., 2005(Bartlett et al., , 2006;;Koltchinskii, 2006).
- Recently, Xu and Zeevi (2020) studied instance-dependent risk without explicitly using a regularization term.
- Our method extends the setting in this line of work to unbounded random variables and adaptively collected hence mutually dependent data.
- We develop a novel self-normalized-type concentration inequality for empirical risks, which can be viewed as a generalization of the empirical Bernstein's inequality (Maurer and Pontil, 2009).
- Closer to our setting, Swaminathan and Joachims (2015) studies counterfactual risk minimization based on i.i.d. data and unbounded weights. However, they used clipping to exclude extremely small weights and reduced the analysis to bounded random variables.
- The clipping causes bias and misses the "oracle" property of pessimism, i.e., the sole dependence on the optimal instance.

### Pessimistic policy learning
- To eliminate the reliance on the uniform overlap assumption, we make an algorithmic change to the greedy learning paradigm by introducing the pessimism principle (Jin et al., 2021b)
- For every policy π ∈ Π, we complement the point estimator Q(π) for the policy value with a policydependent regularization term R(π), and optimize the pessimistic policy value
- Roughly speaking, we expect R(π) to reflect the estimation error of Q(π), and therefore (3) can be viewed as optimizing the lower confidence bounds (LCBs) for the policy values
- The core merit of the pessimism principle is captured by the following proposition: Proposition 3.1
- Let π be obtained as in (3), and let π * = π * (C, Π) be the optimal policy among Π. Then holds on the event

### Construction of the estimator and the regularizer
- We instantiate the generic idea in Section 3.2 into a concrete algorithm by constructing the estimator Q(π) and the regularizer R(π) for all π ∈ Π.
- We begin with an Augmented Inverse Propensity Weighting (AIPW)-type estimator (Robins et al., 1994): and we set Q With batched data, we assume for simplicity that is an estimator that is independent of the data. This is only for the convenience of exposition; in practice, it can be replaced by a cross-fitted version (Schick, 1986;Chernozhukov et al., 2018) in Appendix A with the same theoretical guarantee.
- With adaptively collected data, we allow µ t (x, a) to be constructed using . The estimator ( 5) is standard in the policy learning literature.
- Taking µ(•, •) ≡ 0 reduces (5) to the inverse-propensity-weighting (IPW) estimator used in Zhao et al. (2012); Kitagawa and Tetenov (2018); Swaminathan and Joachims (2015); Bibaut et al. (2021).
- For general µ t (•, •), it is similar to the AIPW-type estimator in Athey and Wager (2021); Zhou et al. (2022); Zhan et al. (2021).
- The second ingredient is the policy-wise regularizer R(π). We construct R(π) = β • V (π) for a scaling constant β > 0 to be decided later, and for the three data-dependent components . As we will see later, the scaling factor β > 0 depends on the complexity of the policy class Π which arises in the uniform concentration over π ∈ Π, and slightly differs for batched and adaptively collected data.
- The uncertainty term V (π) can be interpreted as a proxy for the standard deviation of Q(π). Intuitively, the knowledge of the behavior policy allows us to conjecture the fluctuation of Q(π) around Q(π).
- To be more specific, V s (π) roughly upper bounds the uncertainty of Q(π) conditional on all but {Y t } T t=1 : Similarly, V p (π) approximately upper bounds the uncertainty conditional on {X t } T t=1 : Finally, V h (π) upper bounds the standard deviation of V s (π) conditional on {X t } T t=1 , and is a higher-order term dominated by V p (π) in general, in the sense that V h (π) ≤ V p (π) if e(x, a) ≥ 1/T for all (x, a) ∈ X × A.

## Learning from a fixed behavior policy
- An upper bound on the suboptimality of our pessimistic learning algorithm
- Provides a matching lower bound

### Suboptimality upper bound
- The theorem provides a data-dependent upper bound for the suboptimality of pessimistic policy learning
- The theorem's proof can be found in Section 6.2
- Assumption 5.1 is mild, so we use it for the convenience of exposition
- The theorem does not require any assumption on the adaptive data collecting process other than Assumption 5.1
- Under Assumption 5.5, we show in the following corollary that the upper bound on the suboptimality of pessimistic policy learning is polynomial in T
- The proof of the corollary is in Appendix C.5

### Pessimistic policy learning is minimax optimal
- Pessimistic policy learning is the "best effort" given the fixed behavior policy
- To be specific, given an action set A, a policy class Π, a (possibly T-dependent) constant C * > 0, and a sample size T ∈ N + , we let R(C * , T, A, Π) denote the set of instances we consider -each element of R(C * , T, A, Π) corresponds to a contextual bandit model C defined on A, a behavior policy e(•, which together generates the offline data Here π * = π * (C, Π) is the optimal policy among Π for the contextual bandit model C defined in (1).
- Theorem 4.4 establishes the minimax lower bound for policy learning in Π from any behavior policy obeying (8).

## Learning from adaptive behavior policies
- The behavior policy is allowed to change over time depending on previous observations
- The adaptive setting is significantly more challenging than the batched case due to the dependence between observations and the diminishing propensities
- Throughout this section, we assume an additional mild condition that the sampling probability has a (nearly-vacuous) deterministic lower bound

### Minimax optimality
- The minimax lower bound for policy learning with adaptive behavior policy is captured by the sampling propensities for the optimal policy.
- The minimax lower bound is implied by the lower bound in Zhan et al. (2021) which states that greedy learning is minimax optimal (with essentially the same rate as ours) under the uniform lower bound assumption that inf x,a e t (x, a | H t ) ≥ c • t −γ .

## Proof sketch of main results
- Proof sketches for the main results
- Upper bound in the fixed policy case
- Proof sketch for the adaptive policy case

### Proof of upper bound for the fixed policy case
- We start by showing that the (rescaled) estimation error for any (x, a) ∈ X × A, and we decompose the error as follows. We show in steps I and II how to bound term (i), and establish the upper bound for term (ii) in step III.
- Step I: Symmetrization via a Rademacher process. Based on these independent copies, we define We are to use the exchangeability between {A t , Y t } T t=1 and {A ′ t , Y ′ t } T t=1 conditional on {X t } T t=1 to turn the uniform concentration to tail bounds on a Rademacher process. To this end, we let x = (x 1 , . . . , x T ), a = (a 1 , . . . , a T ), a ′ = (a ′ 1 , . . . , a T ), y = (y 1 , . . . , y T ) and , and {Y ′ t } T t=1 , respectively. We then define as the corresponding realization of Γ t (π) and Γ ′ t (π).
- The following lemma bounds the probability of term (i) exceeding a constant ξ ≥ 4 by the tail probability of a Rademacher process, and its proof is in Appendix B.1.
- Lemma 6.1. For any constant ξ ≥ 4, it holds that for all π ∈ Π , where P ε is the probability over i.i.d. Rademacher random variables ε 1 , . . . , ε T i.i.d. ∼ Unif{±1} while holding other quantities fixed at their realized values.
- Step II: Uniform concentration. The next lemma establishes a uniform upper bound for the tail probability of the Rademacher process in terms of the sample size T and the complexity of Π, whose proof is provided in Appendix B.2.
- Lemma 6.2. For any fixed δ ∈ (0, 1) and any fixed realizations x, a, a ′ , y, y , and {Y ′ t } T t=1 , the event , for all π ∈ Π holds with probability at least 1 − δ over the randomness of ε 1 , . . . , ε T . Taking ξ = 16 2 N-dim(Π) log(T K 2 ) + log(8/δ) , combining Lemmas 6.1 and 6.2, we know that with probability at least 1 − δ/2. This controls term (i).
- Step III: Controlling term (ii). Next, we use similar techniques as in Step I and Step II to develop a uniform bound for term (ii). The proof of the next lemma is deferred to Appendix B.3.
- Lemma 6.3. For any δ ∈ (0, 1), with probability at least 1 − δ, there is Finally, applying a union bound to (10) and Lemma 6.3, we have with probability at least 1 − δ that which concludes the proof of the first claim. Combining this result with Lemma 3.1 by taking R(π) = β•V (π), we conclude the proof of Theorem 4.1.

### Proof of upper bound for the adaptive policy case
- As before, we begin with the triangle inequality , for which we will control terms (i) and (ii) separately.
- With adaptive behavior policy, bounding term (ii) is similar to the i.i.d. case while controlling term (i) is much more challenging.
- The general tool for controlling term (i) is still symmetrization. Due to the adaptivity of the data-collecting processes, however, we are to use a decoupled tangent sequence (De la Pena and Giné, 2012; Rakhlin et al., 2015) which possesses a "weaker" symmetry property than the symmetric copies generated in the i.i.d. case.
- The decoupled tangent sequence allows us to turn the tail bound on term (i) to that of a tree-Rademacher process (Rakhlin et al., 2015), whose definition will be provided shortly; from there, we shall utilize a conditional symmetry property of the tree-Rademacher process to invoke a self-normalized concentration inequality to control term (i).
- Step I: Symmetrization. For every t ∈ [T ], we let (A ′ t , Y ′ t ) be an independent copy of (A t , Y t ) conditional on {X t } ∪ {X i , A i , Y i } t−1 i=1 , and {A ′ t , Y ′ t } T t=1 are conditionally independent of each other given {X t , A t , Y t } T t=1 .
- This can be achieved as follows. For t = 1, 2, . . . , T , we sequentially sample | X=x for a = A ′ t and x = X t , all independently of everything else. We then define For notational simplicity, for any t ∈ [T ], we also define
- The following lemma moves the supremum over π ∈ Π out of the probability, and turns the tail bound of term (i) into that of a process symmetric in (A t , Y t ) and (A ′ t , Y ′ t ).
- The proof is in Appendix C.1.
- Lemma 6.4. Let Π(X) = (π(X 1 ), . . . , π(X T )) ∈ A T : π ∈ Π be the set of all possible actions on {X t } T t=1 from π ∈ Π.
- For any ξ ≥ 4 that may depend on {X t } T t=1 , it holds that It then suffices to control the probability in (11) for any fixed π ∈ Π.
- In the following, we are to turn the upper bound in Lemma 6.4 into the tail probability of a tree Rademacher process (Rakhlin et al., 2015).
- Step II: Tree Rademacher process. We define a superset for all possible realizations z t = (a, a ′ , y, y ′ , f, h) for any t ∈ [T ] as where F e is the set of all possible sampling rule, i.e., all mappings of the form f (•, •) : X × A → [0, 1] such that a∈A f (x, a) = 1 for all x ∈ X and log f (x, a) ≥ −(log T ) α for all (x, a) ∈ X × A; F µ is the set of all fitted value functions, i.e., all mappings of the form h(•, •) : because in...

## Discussion
- The proposed algorithm promises efficient learning as long as the optimal policy is sufficiently covered by the offline data set
- The theoretical guarantee builds upon self-normalized processes
- Extension to ERM is an interesting future research avenue

## A Cross-fitted pessimistic policy learning
- Cross-fitted version of the algorithm is used
- Estimator is independent of input data
- Error in estimator is bounded

## B Proof of fixed-policy results
- The proof of Lemma 6.1: To begin with, for any ξ ≥ 4, we define the event we wish to control as where we define We assume this supremum is attained; otherwise, a limiting argument yields exactly the same conclusion.
- The proof of Lemma 6.2: For each t ∈ [T ], we let X ′ t be an i.i.d. copy of ] is the policy value.
- The proof of Lemma 6.3: For each t ∈ [T ], we let X ′ t be an i.i.d. copy of ] is the policy value.
- The proof of Lemma 6.4: For any η > 1/ √ 2, by Markov's inequality we have where the second line uses the fact that Q(X ′ t , π) are i.i.d., and that µ( , by ( 16) we have Then for any η ≥ 1/ √ 2, we have where the next-to-last step is due to the triangle inequality.
- The proof of Lemma 6.5: Fix any η > 1/ √ 2 and let Π be the set of all policy values. Then, by the exchangeability of X t and X ′ t , we have where x = (x 1 , . . . , x T ) denotes any realization of {X t } T t=1 .
- The proof of Lemma 6.6: For any x, similar to our proof of Lemma 6.2, we have where the last step is due to the triangle inequality.

## C Proof of adaptive results

## E Auxiliary lemmas
