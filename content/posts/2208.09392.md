---
title: "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise"
date: 2022-08-19T15:18:39.000Z
author: "Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2208-09392v1.webp" # image path/url
    alt: "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2208.09392)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2208.09392).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/cold-diffusion-inverting-arbitrary-image).

# Abstract
- Standard diffusion models involve adding Gaussian noise
- The generative behavior of diffusion models is not strongly dependent on the choice of image degradation
- An entire family of generative models can be constructed by varying the choice of image degradation

# Paper Content

## Introduction
- Diffusion models have recently emerged as powerful tools for generative modeling
- Diffusion models come in many flavors, but all are built around the concept of random noise removal
- At test time, the denoising network is used to convert pure Gaussian noise into a photo-realistic image using an update rule that alternates between applying the denoiser and adding Gaussian noise.
- When the right sequence of updates is applied, complex generative behavior is observed.
- The origins of diffusion models, and also our theoretical understanding of these models, are strongly based on the role played by Gaussian noise during training and generation.
- Diffusion has been understood as a random walk around the image density function using Langevin dynamics
- The walk begins in a high temperature (heavy noise) state, and slowly anneals into a "cold" state with little if any noise.
- Another line of work derives the loss for the denoising network using variational inference with a Gaussian prior.
- In this work, we examine the need for Gaussian noise, or any randomness at all, for diffusion models to work in practice.
- We consider generalized diffusion models that live outside the confines of the theoretical frameworks from which diffusion models arose.
- Rather than limit ourselves to models built around Gaussian noise, we consider models built around arbitrary image transformations like blurring, downsampling, etc.
- We train a restoration network to invert these deformations using a simple p loss.
- When we apply a sequence of updates at test time that alternate between the image restoration model and the image degradation operation, generative behavior emerges, and we obtain photo-realistic images.

## Background
- Generative models exist for a range of modalities spanning natural language and images
- Both the Langevin dynamics and variational inference interpretations of diffusion models rely on properties of the Gaussian noise used in the training and sampling pipelines
- From the score-matching generative networks perspective, noise in the training process is critically thought to expand the support of the low-dimensional training distribution to a set of full measure in ambient space
- The noise is also thought to act as data augmentation to improve score predictions in low density regions, allowing for mode mixing in the stochastic gradient Langevin dynamics (SGLD) sampling
- Kingma et al. propose a method to learn a noise schedule that leads to faster optimization
- Using a classic statistical result, Kadkhodaie and Simoncelli show the connection between removing additive Gaussian noise and the gradient of the log of the noisy signal density in deterministic linear inverse problems
- Here, we shed light on the role of noise in diffusion models through theoretical and empirical results in applications to inverse problems and image generation.

## Generalized Diffusion
- Standard diffusion models are built around two components: an image degradation operator that contaminates images with Gaussian noise, and a trained restoration operator.
- In this work, we consider the construction of generalized diffusions built around arbitrary degradation operations. These degradations can be randomized (as in the case of standard diffusion) or deterministic.

### Model components and training
- Given an image x 0 ∈ R N , consider the degradation of x 0 by operator D with severity t, denoted x t = D(x 0 , t).
- The output distribution D(x 0 , t) of the degradation should vary continuously in t, and the operator should satisfy D(x 0 , 0) = x 0 .
- In the standard diffusion framework, D adds Gaussian noise with variance proportional to t.
- In our generalized formulation, we choose D to perform various other transformations such as blurring, masking out pixels, downsampling, and more, with severity that depends on t.
- We explore a range of choices for D in Section 4.
- We also require a restoration operator R that (approximately) inverts D. This operator has the property that R(x t , t) ≈ x 0 .
- In practice, this operator is implemented via a neural network parameterized by θ. The restoration network is trained via the minimization problem where x denotes a random image sampled from distribution X and • denotes a norm, which we take to be 1 in our experiments.

### Sampling from the model
- After choosing a degradation D and training a model R to perform the restoration, these operators can be used in tandem to invert severe degradations by using standard methods borrowed from the diffusion literature.
- For small degradations (t ≈ 0), a single application of R can be used to obtain a restored image in one shot. However, because R is typically trained using a simple convex loss, it yields blurry results when used with large t.
- Rather, diffusion models [Song et al., 2021a, Ho et al., 2020] perform generation by iteratively applying the denoising operator and then adding noise back to the image, with the level of added noise decreasing over time. This corresponds to the standard update sequence in Algorithm 1.
- When the restoration operator is perfect, i.e. when R(D(x 0 , t), t) = x 0 for all t, one can easily see that Algorithm 1 produces exact iterates of the form x s = D(x 0 , s). But what happens for imperfect restoration operators?
- In this case, errors can cause the iterates x s to wander away from D(x 0 , s), and inaccurate reconstruction may occur.
- We find that the standard sampling approach in Algorithm 1 works well for noise-based diffusion, possibly because the restoration operator R has been trained to correct (random Gaussian) errors in its inputs.
- However, we find that it yields poor results in the case of cold diffusions with smooth/differentiable degradations as demonstrated for a deblurring model in Figure 2.
- We propose Algorithm 2 for sampling, which we find to be superior for inverting smooth, cold degradations. This sampler has important mathematical properties that enable it to recover high quality results. Specifically, for a class of linear degradation operations, it can be shown to produce exact reconstruction (i.e. x s = D(x 0 , s)) even when the restoration operator R fails to perfectly invert D.
- We discuss this in the following section.

### Properties of Algorithm 2
- For small values of x and s, Algorithm 2 is extremely tolerant of error in the restoration operator R.
- To see why, consider a model problem with a linear degradation function of the form D(x, s) ≈ x + s • e for some vector e.
- While this ansatz may seem rather restrictive, note that the Taylor expansion of any smooth degradation D(x, s) around x = x 0 , s = 0 has the form D(x, s) ≈ x + s • e + HOT where HOT denotes higher order terms.
- Note that the constant/zeroth-order term in this Taylor expansion is zero because we assumed above that the degradation operator satisfies D(x, 0) = x.
- For a degradation of the form (3.3) and any restoration operator R, the update in Algorithm 2 can be written
- By induction, we see that the algorithm produces the value x s = D(x 0 , s) for all s < t, regardless of the choice of R.
- In other words, for any choice of R, the iteration behaves the same as it would when R is a perfect inverse for the degradation D.
- By contrast, Algorithm 1 does not enjoy this behavior. In fact, when R is not a perfect inverse for D, x 0 is not even a fixed point of the update rule in Algorithm 1 because x 0 = D(R(x, 0), 0) = R(x, 0).
- If R does not perfectly invert D we should expect Algorithm 1 to incur errors, even for small values of s.
- Meanwhile, for small values of s, the behavior of D approaches its first-order Taylor expansion and Algorithm 2 becomes immune to errors in R.

## Generalized Diffusions with Various Transformations
- In this section, we take the first step towards cold diffusion by reversing different degradations and hence performing conditional generation.
- We will extend our methods to perform unconditional (i.e. from scratch) generation in Section 5.
- We emprically evaluate generalized diffusion models trained on different degradations with our improved sampling Algorithm 2.
- We perform experiments on the vision tasks of deblurring, inpainting, super-resolution, and the unconventional task of synthetic snow removal.
- We perform our experiments on MNIST [LeCun et al., 1998], CIFAR-10 [ Krizhevsky, 2009], and CelebA [Liu et al., 2015].
- In each of these tasks, we gradually remove the information from the clean image, creating a sequence of images such that D(x 0 , t) retains less information than D(x 0 , t − 1).
- For these different tasks, we present both qualitative and quantitative results on a held-out testing dataset and demonstrate the importance of the sampling technique described in Algorithm 2.
- For all quantitative results in this section, the Frechet inception distance (FID) scores [Heusel et al., 2017] for degraded and reconstructed images are measured with respect to the testing data.

### Deblurring
- We consider a generalized diffusion based on a Gaussian blur operation (as opposed to Gaussian noise)
- The forward process given the Gaussian kernels {G s } and the image x t−1 at step t − 1 can thus be written as
- We train a deblurring model by minimizing the loss (1), and then use Algorithm 2 to invert this blurred diffusion process for which we trained a DNN to predict the clean image x0
- Qualitative results are shown in Figure 3 and quantitative results in Table 1.
- Qualitatively, we can see that images created using the sampling process are sharper and in some cases completely different as compared to the direct reconstruction of the clean image.
- Quantitatively, we can see that the reconstruction metrics such as RMSE and PSNR get worse when we use the sampling process, but on the other hand FID with respect to held-out test data improves.
- The qualitative improvements and decrease in FID show the benefits of the generalized sampling routine, which brings the learned distribution closer to the true data manifold.

### Inpainting
- We define a schedule of transforms that progressively grays-out pixels from the input image.
- We remove pixels using a Gaussian mask as follows: For input images of size n × n we start with a 2D Gaussian curve of variance β, discretized into an n × n array.
- We normalize so the peak of the curve has value 1, and subtract the result from 1 so the center of the mask as value 0.
- We randomize the location of the Gaussian mask for MNIST and CIFAR-10, but keep it centered for CelebA.
- We denote the final mask by z β .
- Input images x 0 are iteratively masked for T steps via multiplication with a sequence of masks {z βi } with increasing β i .
- We can control the amount of information removed at each step by tuning the β i parameter.
- In the language of Section 3, D(x 0 , t) = x 0 • t i=1 z βi , where the operator • denotes entry-wise multiplication.
- Figure 4 presents results on test images and compares the output of the inpainting model to the original image.
- The reconstructed images display reconstructed features qualitatively consistent with the context provided by the unperturbed regions of the image.
- We quantitatively assess the effectiveness of the inpainting models on each of the datasets by comparing distributional similarity metrics before and after the reconstruction. Our results are summarized in Table 2.

### Super-Resolution
- For each value of t, the degradation operator downsamples the image by a factor of two in each direction
- After each down-sampling, the lowerresolution image is resized to the original image size, using nearest-neighbor interpolation

### Snowification

## Cold Generation
- Diffusion models can successfully learn the underlying distribution of training data
- We will first discuss deterministic generation using Gaussian noise and then discuss in detail unconditional generation using deblurring
- Finally, we provide a proof of concept that the Algorithm 2 can be extended to other degradations

### Generation using deterministic noise degradation
- Noise-based image generation is studied
- Two methods are proposed: deterministic interpolation or deterministic noise calculation
- Results are presented for CelebA and AFHQ datasets

### Image generation using blur
- The forward diffusion process in noise-based diffusion models has the advantage that the degraded image distribution at the final step T is simply an isotropic Gaussian.
- One can therefore perform (unconditional) generation by first drawing a sample from the isotropic Gaussian, and sequentially denoising it with backward diffusion.
- When using blur as a degradation, the fully degraded images do not form a nice closed-form distribution that we can sample from. They do, however, form a simple enough distribution that can be modeled with simple methods.
- Note that every image x 0 degenerates to an x T that is constant (i.e., every pixel is the same color) for large T . Furthermore, the constant value is exactly the channel-wise mean of the RGB image x 0 , and can be represented with a 3-vector.
- This 3-dimensional distribution is easily represented using a Gaussian mixture model (GMM).
- This GMM can be sampled to produce the random pixel values of a severely blurred image, which can be deblurred using cold diffusion to create a new image.
- Our generative model uses a blurring schedule where we progressively blur each image with a Gaussian kernel of size 27x27 over 300 steps.
- The standard deviation of the kernel starts at 1 and increases exponentially at the rate of 0.01.
- We then fit a simple GMM with one component to the distribution of channel-wise means.
- To generate an image from scratch, we sample the channel-wise mean from the GMM, expand the 3D vector into a 128 × 128 image with three channels, and then apply Algorithm 2.
- Empirically, the presented pipeline generates images with high fidelity but low diversity, as reflected quantitatively by comparing the perfect symmetry column with results from hot diffusion in Table 5.
- We attribute this to the perfect correlation between pixels of x T sampled from the channel-wise mean Gaussian mixture model.

### Generation using other transformations
- Generation can be extended to other transformations
- Preliminary results show that generation can be used to inpainting, super-resolution, and animorphosis
- To use the Gaussian mask transformation for generation, the masking routine is modified so the final degraded image is completely devoid of information
- One might think a natural option is to send all of the images to a completely black image, but this would not allow for any diversity in generation
- To get around this maximally non-injective property, we instead make the mask turn all pixels to a random, solid color
- This still removes all of the information from the image, but it allows for recovery of different samples from the learned distribution via Algorithm 2

## Conclusion
- Deblurring: experiments were trained on different datasets for 700,000 gradient steps.
- For the deblurring experiments, we use the Adam [Kingma and Ba, 2014] optimizer with learning rate 2 × 10 −5 .
- The training was done on the batch size of 32, and we accumulate the gradients every 2 steps.
- Our final model is an Exponential Moving Average of the trained model with decay rate 0.995 which is updated after every 10 gradient steps.
- For the MNIST dataset, we blur recursively 40 times, with a discrete Gaussian kernel of size 11x11 and a standard deviation 7.
- In the case of CIFAR-10, we recursively blur with a Gaussian kernel of fixed size 11x11, but at each step t, the standard deviation of the Gaussian kernel is given by 0.01 * t + 0.35.
- The blur routine for CelebA dataset involves blurring images with a Gaussian kernel of 15x15 and the standard deviation of the Gaussian kernel grows exponentially with time t at the rate of 0.01.
- For the inpainting transformation, models were trained on different datasets with 60,000 gradient steps.
- The models were trained using Adam [Kingma and Ba, 2014] optimizer with learning rate 2×10 −5 .
- We use batch size 64, and the gradients are accumulated after every 2 steps.
- The final model is an Exponential Moving Average of the trained model with decay rate 0.995.
- We update the EMA model every 10 gradient steps.
- For all our inpainting experiments we use a randomized Gaussian mask and T = 50 with β 1 = 1 and β i+1 = β i + 0.1.
- To avoid potential leakage of information due to floating point computation of the Gaussian mask, we discretize the masked image before passing it through the inpainting model. This was done by rounding all pixel values to the eight most significant digits.
- Figure 11 shows nine additional inpainting examples on each of the MNIST, CIFAR-10, and CelebA datasets.
- Figure 10 demonstrates an example of the iterative sampling process of an inpainting model for one image in each dataset.
- We train the super-resolution model per Section 3.1 for 700,000 iterations.
- We use the Adam [Kingma and Ba, 2014] optimizer with learning rate 2 × 10 −5 .
- The batch size is 32, and we accumulate the gradients every 2 steps.
- Our final model is an exponential moving average of the trained model with decay rate 0.995.
- We update the EMA model every 10 gradient steps.
- For CIFAR-10 we use T = 50 and for CelebA we use T = 20.
- We illustrate our recolorization results in Figure 14.
- We present testing examples, as well as their grey scale images, from all the datasets, and compare the recolorization results with the original images.
