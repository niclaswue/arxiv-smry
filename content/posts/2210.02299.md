---
title: "SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations"
date: 2022-10-05T14:38:49.000Z
author: "Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-02299v1.webp" # image path/url
    alt: "SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.02299)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.02299).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/shine-mapping-large-scale-3d-mapping-using).

# Abstract
- Accurate mapping of large-scale environments is an essential building block of most outdoor autonomous systems
- Challenges of traditional mapping methods include the balance between memory consumption and mapping accuracy
- This paper addresses the problems of achieving large-scale 3D reconstructions with implicit representations using 3D LiDAR measurements
- We learn and store implicit features through an octree-based hierarchical structure, which is sparse and extensible
- The features can be turned into signed distance values through a shallow neural network
- We leverage binary cross entropy loss to optimize the local features with the 3D measurements as supervision
- Based on our implicit representation, we design an incremental mapping system with regularization to tackle the issue of catastrophic forgetting in continual learning
- Our experiments show that our 3D reconstructions are more accurate, complete, and memory-efficient than current state-of-the-art 3D mapping methods.

# Paper Content

## I. INTRODUCTION
- Localization and navigation in large-scale outdoor scenes is a common task of mobile robots, especially for self-driving cars
- Most systems use or maintain a model of their surroundings
- For outdoors, dense 3D maps are built based on range sensors such as 3D LiDAR
- Due to the limited memory of most mobile robots performing all computations onboard, maps should be compact but, at the same time, accurate enough to represent the environment in sufficient detail
- Current large-scale mapping methods often use spatial grids or various tree structures as map representation
- For these models, it can be hard to simultaneously satisfy both desires, accurate and detailed 3D information but not requiring substantial memory resources
- Recently, neural network-based representations attracted significant interest in the computer vision and robotics communities
- By storing information about the environment in the neural network implicitly, these approaches can achieve remarkable accuracy and high-fidelity reconstructions using a comparably compact representation
- Due to such advantages, several recent works have used implicit representation for 3D scene reconstruction built from images data or RGB-D frames
- Comparably, little has been done in the context of LiDAR data
- Furthermore, most of these methods only work in relatively small indoor scenes, which is difficult to use in robot applications for large-scale outdoor environments
- In this paper, we address this challenge and investigate effective means of using an implicit neural network-based map representation for outdoor robots using range sensors such as LiDARs
- Our approach uses an octree-based sparse data structure to store incrementally learnable feature vectors and a shared shallow MLP as the decoder to transfer local features to signed distance values
- We design a binary cross entropy-based loss function for efficient and robust local feature optimization
- By interpolating among local features, our representation enables us to query geometric information at any resolution
- We use point clouds as observation to opti-
- mize local features online and achieve incremental mapping through the extension of the octree structure
- Our approach yields an incremental mapping system based on our implicit representation

## II. RELATED WORK
- 3D LiDAR point clouds are a core requirement for localization and navigation
- Various representations of the environment, like surfel-based, triangle meshes, octree-based occupancy representations, and implicit neural representations, are common choices
- The seminal work of Newcombe et al. popularized the use of truncated signed distance function for real-time reconstruction
- By combining a shallow MLP with optimizable local feature grids, NICE-SLAM and Go-SURF can achieve more accurate and faster surface reconstruction in larger scenes
- However, the most notable shortcoming of these methods is the enormous memory cost of dense voxel structures
- Our method leverages the octree-based sparse feature grid to reduce memory consumption significantly.

## III. OUR APPROACH -SHINE-MAPPING
- We propose a framework for large-scale 3D mapping based on an implicit neural representation taking point clouds from a range sensor such as LiDAR with known poses as input.
- Our implicit map representation uses learnable octree-based hierarchical feature grids and a globally shared MLP decoder to represent the continuous signed distance field (SDF) of the environment.
- As illustrated in Fig. 2, we optimize local feature vectors online to capture the local geometry by using direct measurements to supervise the output signed distance value from the MLP.
- Using this learned implicit map representation, we can generate an explicit geometric representation in the form of a triangle mesh by marching cubes [11] for visualization and evaluation.

## A. Implicit Neural Map Representation
- Our implicit map representation has to store spatially located features in the 3D world.
- The SDF values will then be inferred from these features through the neural network.
- Our network will not only use features from one spatial resolution but combine features at H different resolutions.
- The H hierarchical levels would always double the spatial extend in x, y, z-direction from one level to the next.
- We found that H equals 3 or 4 is sufficient for good results.
- Spatial Model.
- We first have to specify our data structure. To enable large-scale mapping with implicit representation, one could choose an octree-based map representation similar to the one proposed in NGLOD [33].
- In this case, one would store features per octree node corner and consider several levels of the tree when combining features to feed the network.
- However, NGLOD is designed to model a single object, and the authors do not target the incremental extension of the mapped area.
- Extending the map is vital for the online robotic operation as the robot's workspace is typically not known beforehand, especially when navigating through large outdoor scenes.
- Based on the NGLOD, we build the octree from the point cloud directly and take a different approach to store our features using hash tables, one table for each level, such that we maintain H hash tables during mapping.
- For addressing the hash tables, while still being able to find features of upper levels quickly, we can exploit unique Morton codes.
- Morton codes, also known as Z-order or Lebesgue curve, map multidimensional data to one dimension, i.e., the spatial hash code, while preserving the locality of the data points.
- This setup allows us to easily extend the map without allocating memory beforehand while still being able to maintain the H most-detailed levels.
- Thus, this provides an elegant and efficient, i.e., average case O(1) access to all features.
- Features.
- In our representation, we store the feature, a one-dimensional vector of length L, in each corner of the tree nodes.
- The MLP will take the same length vector as the input and compute the SDF values.
- For that operation, the feature vectors from at most H levels of our representation are combined to form the network's input.
- For these features stored in corners, we randomly initialize their values when created and then optimize them until convergence by using the training pairs sampled along the rays.
- Fig. 2 illustrates the overall process of training our implicit map representation, and the right-hand side of the image illustrates the combination of features from different levels.
- For clearer visualization, we depict only two levels, green and blue.
- For any query location, we start from the lowest level (level 0) and compute a trilinear interpolation for the position x. This yields the feature vector at level 0.
- Then, we move up the hierarchy to level 1 to repeat the process.
- We combine features through summation for up to H levels.
- Next, we feed the summed-up feature vector into a shared shallow MLP with M hidden layers to obtain the SDF value.
- As the whole process is differentiable, we can optimize the feature vectors and the MLP's parameters jointly through backpropagation.
- To ensure our shared MLP generalizes well at various scales, we do not stack the query point coordinates under the final feature vector as done by Takikawa et al. [33].
- Our MLP does not need to be pre-trained if mapping operates in batch mode, i.e., all range measurements taken at known poses are available.
- In case we map incrementally, we, in contrast, use a pre-trained MLP and keep it fixed to minimize the effects of catastrophic forgetting.

## B. Training Pairs and Loss Function
- Range sensors such as LiDARs typically provide accurate range measurements.
- We obtain training pairs by sampling points along the ray and directly use the signed distance from sampled point to the beam endpoint as the supervision signal.
- This signal is often referred to as the projected signed distance along the ray.
- For SDF-based mapping, the regions of interest are the values close to zero as they define the surfaces.
- Therefore, sampled points closer to the endpoint should have a higher impact as the precise SDF value far from a surface has very little impact.
- Thus, instead of using an L2 loss, as, for example, used by Ortiz et al. [23], we map the projected signed distance to [0, 1] before feeding to the actual loss through the sigmoid function: S(x) = 1/(1 + e x/σ ), where the σ is a hyperparameter to control the function's flatness and indicates the magnitude of the measurement noise.
- Given the sampled data point x i ∈ R 3 , we calculate its projected signed distance to the surface d i , then use the value after sigmoid mapping l i = S(d i ) as supervision label and apply the binary cross entropy (BCE) as our loss function: where o i = S(f θ (x i )) represents the signed distance output of our model f θ (x i ) after the sigmoid mapping, which can be regarded as an indicator for the occupancy probability.
- The effect of BCE loss is visualized in Fig. 3.
- Additionally, the sigmoid function also realizes soft truncation of signed distance, and the truncation range can be adjusted by changing the σ in the sigmoid function.

## C. Incremental Mapping Without Forgetting
- catastrophic forgetting happens when we use the data from frame T 0 to train and update the feature V 0 , V 1 , V 2 , and V 3
- to prevent this problem, we add a regularization term to the loss function

## IV. EXPERIMENTAL EVALUATION
- Uses a sparse hierarchical feature grid
- Uses a neural decoder
- Capabilities assessed in this section

## A. Experimental Setup
- Evaluates the model qualitatively and quantitatively on two publicly available outdoor LiDAR datasets
- Uses near ground truth data from a terrestrial scanner
- Validates the scalability for incremental mapping
- Shows high-fidelity 3D reconstruction indoors

## B. Mapping Quality
- Evaluates the mapping quality in terms of accuracy and completeness on the MaiCity and the Newer College dataset
- Compares our approach against three other mapping systems: two state-of-the-art TSDF fusion-based methods, namely Voxblox [21] and VDB Fusion [35] as well against the Possion surface-based reconstruction system Puma [34]
- Obtains better performance compared with the other methods on the more noisy Newer College dataset with the same 10 cm voxel size

## C. Memory Efficiency
- Fig. 7 depicts the memory usage in relation to the mapping quality.
- The results indicate that our method can create map with smaller memory consistently for all settings.
- Meanwhile, our mapping quality hardly gets  worse with a lower feature grid resolution, while the mapping error of Voxblox and VDB Fusion increases significantly.
- Our representation using hash tables storing features allows for efficient memory usage.

## D. Scalable Incremental Mapping
- SHINE-Mapping is able to reconstruct a driving sequence over a distance of about 4 km
- The regularization-based continual learning strategy manages to clearly reconstruct the structures that may vanish or distort as the consequences of forgetting during the incremental mapping.

## E. Indoor Mapping and Filling Occluded Areas
- Our model does not explicitly store signed distance values in a voxel grid.
- Instead, it uses an octree-based implicit representation consisting of features stored in hash tables and which can, through a neural network, be turned into SDF values.
- The network and the features can be learned end-to-end from range data.
- We evaluated our approach on both simulated and realworld datasets and show our reconstruction approach has advantages over current state-of-the-art mapping systems.
