---
title: "Fine-Tuning Language Models from Human Preferences"
date: 2019-09-18T17:33:39.000Z
author: "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1909-08593v2.webp" # image path/url
    alt: "Fine-Tuning Language Models from Human Preferences" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1909.08593)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1909.08593).


# Abstract
- Reward learning enables the application of reinforcement learning to tasks where reward is defined by human judgment
- Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks
- In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets
- For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.

# Paper Content

## Introduction
- We would like to apply reinforcement learning to complex tasks defined only by human judgment, where we can only tell whether a result is good or bad by asking humans.
- To do this, we can first use human labels to train a model of reward, and then optimize that model.
- While there is a long history of work learning such models from humans through interaction, this work has only recently been applied to modern deep learning, and even then has only been applied to relatively simple simulated environments.
- By contrast, real world settings in which humans need to specify com-* Equal contribution. Correspondence to paul@openai.com. plex goals to AI agents are likely to both involve and require natural language, which is a rich medium for expressing value-laden concepts.
- Natural language processing has seen substantial recent advances. One successful method has been to pretrain a large generative language model on a corpus of unsupervised data, then fine-tune the model for supervised NLP tasks.
- This method often substantially outperforms training on the supervised datasets from scratch, and a single pretrained language model often can be fine-tuned for state of the art performance on many different supervised datasets.
- In some cases, fine-tuning is not required: Radford et al. (2019) find that generatively trained models show reasonable performance on NLP tasks with no additional training (zero-shot).
- There is a long literature applying reinforcement learning to natural language tasks. Much of this work uses algorithmically defined reward functions such as BLEU for translation (Ranzato et al., 2015;Wu et al., 2016), ROUGE for summarization (Ranzato et al., 2015;Paulus et al., 2017;Wu and Hu, 2018;Gao et al., 2019b), music theory-based rewards (Jaques et al., 2017), or event detectors for story generation (Tambwekar et al., 2018).
- Nguyen et al. (2017) used RL on BLEU but applied several error models to approximate human behavior. Wu and Hu (2018) and Cho et al. (2019) learned models of coherence from existing text and used them as RL rewards for summarization and long-form generation, respectively.
- Gao et al. (2019a) built an interactive summarization tool by applying reward learning to one article at a time. Experiments using human evaluations as rewards include Kreutzer et al. (2018) which used off-policy reward learning for translation, and Jaques et al. (2019) which applied the modified Q-learning methods of Jaques et al. (2017) to implicit human preferences in dialog.
- Yi et al. (2019) learned rewards from humans to fine-tune dialog models, but smoothed the rewards to allow supervised learning.
- We refer to Luketina et al. (2019) for a survey of RL tasks involving language as a component, and for RL results using transfer learning from language.
- RL is not the only way to incorporate ongoing human feedback: Hancock et al. (2019) ask humans what a dialogue system should have said instead, then continue supervised training.

## Methods
- We define a vocabulary, Σ, and a language model, ρ, which defines a probability distribution over sequences of tokens
- We will apply this model to a task with input space X = Σ ≤m, data distribution D over X, and output space Y = Σ n
- For example, x ∈ X could be an article of up to 1000 words and y ∈ Y could be a 100-word summary
- ρ defines a probabilistic policy for this task via ρ(y|x) = ρ(xy)/ρ(x): fixing the beginning of the sample to x and generating subsequent tokens using ρ
- We initialize a policy π = ρ, and then fine-tune π to perform the task well using RL
- If the task was defined by a reward function r : X × Y → R, then we could use RL to directly optimize the expected reward: However, we want to perform tasks defined by human judgments, where we can only learn about the reward by asking humans
- To do this, we will first use human labels to train a reward model, and then optimize that reward model
- Following Christiano et al. (2017), we ask human labelers to pick which of several values of y i is the best response to a given input x.
- 1 We ask humans to choose between four options (y 0 , y 1 , y 2 , y 3 ); considering more options allows a human to amortize the cost of reading and understanding the prompt x. Let b ∈ {0, 1, 2, 3} be the option they select. Having collected a dataset S of (x, y 0 , y 1 , y 2 , y 3 , b) tuples, we fit a reward model r : X × Y → R using the loss x,yi)
- Since the reward model needs to understand language, we initialize it as a random linear function of the final embedding output of the language model policy ρ following Radford et al. (2018) (see section 4.2 for why we initialize from ρ rather than π).
- To keep the scale of the reward model consistent across training, we normalize it so that it has mean 0 and variance 1 for x ∼ D, y ∼ ρ(•|x).
- Now we fine-tune π to optimize the reward model r. To keep π from moving too far from ρ, we add a penalty with expectation β KL(π, ρ) (see table 10 for what happens without this). That is, we perform RL on the modified reward
- We either choose a constant β or vary it dynamically to achieve a particular value of KL(π, ρ); see section 2.2
- This term has several purposes: it plays the role of an entropy bonus, it prevents the policy from moving too far from the range where r is valid, and in the case of our style continuation tasks it also is an important part of the task definition: we ask humans to evaluate style, but rely on the KL term to encourage coherence and topicality. Our overall training process is:
- 1. Gather samples (x, y 0 , y 1 , y 2 , y 3 ) via x ∼ D, y i ∼ ρ(•|x). Ask humans to pick the best y i from each.
- 2. Initialize r to ρ, using random initialization for the final linear layer of r. Train r on the human samples using loss (1).
- 3. Train π via Proximal Policy Optimization (PPO, Schulman et al. (2017)) with reward R from (2) on x ∼ D. We use a 774M parameter version of the GPT-2 language model in Radford et al. (2019) trained on their WebText dataset and their 50,257 token invertible byte pair encoding to preserve capitalization and punctuation (Sennrich et al., 2015). The model is a Transformer with 36 layers, 20 heads, and embedding size 1280 (Vaswani et al., 2017).
- For stylistic continuation tasks we perform supervised finetuning of the language model to the BookCorpus dataset of Zhu et al. (2015) prior to RL fine-tuning; we train from scratch on WebText, supervised fine-tune on BookCorpus, then RL fine-tune to our final task.
- To improve sample quality, we use a temperature of T < 1 for all experiments; we modify the initial language model by dividing logits by T , so that future sampling and RL with T = 1 corresponds to a lower temperature for the unmodified pretrained model.

### Human labeling
- Scale AI is used to collect labels
- The Scale API accepts requests of the form (x, y 0 , y 1 , y 2 , y 3 ) and returns selections b ∈ {0, 1, 2, 3}.
- We describe the task to Scale through a combination of instructions (appendix A) and a dataset of about 100 example comparisons from the authors.
- Unlike many tasks in ML, our queries do not have unambiguous ground truth, especially for pairs of similar outputs (which play a large role in our training process, since we train r on pairs of labels sampled from a single policy π).
- This means that there is significant disagreement even between labelers who have a similar understanding of the task and are trying to rate consistently.
- On 4-way comparisons for sentiment and TL;DR summarization, authors of this paper agree about 60% of the time (vs. 25% for random guessing).
- This low rate of agreement complicates the quality control process for Scale; the authors agree with Scale Figure 2: Learning curves for a 124M-parameter model with mock sentiment reward, targeting a KL of 8 nats.
- Lines and shaded areas show mean and range for 5 seeds.
- Early on the reward model sometimes speeds up training, a phenomenon also observed by Christiano et al. (2017).
- For final evaluation of two models A and B, we generate either 2-way comparisons between pairs (a ∼ A, b ∼ B) or 4-way comparisons with quadruples (a 0 , a 1 ∼ A, b 0 , b 1 ∼ B), randomize the order in which samples are presented, and present these comparisons to Scale.

## Experiments

### Summarization
- Truncate to 500 tokens
- Add a "TL;DR:" suffix
- Let the policy respond with up to 75 tokens
- Set the temperature of the pretrained model to T = 0.5 for CNN/Daily Mail and T = 0.7 for TL;DR
- For CNN/Daily Mail, use a fixed KL coefficient β = 0.1; for TL;DR, use β = 0.03
- For RL fine-tuning, train online data collection mod- els with 15k, 30k, and 60k human labels
- For CNN/Daily Mail, use a rejection sampling method to ensure there is a newline between tokens 55 and 75
- For TL;DR, use a rejection sampling method to ensure there is a newline between tokens 55 and 75 and truncate at that newline
- Combine supervised and RL fine-tuning: performing human RL fine-tuning starting with the supervised fine-tuned model

### Online data collection is hard
- Online data collection was difficult to debug
- Software complexity: Our online system interleaves data gathering, reward model training, and RL finetuning.
- Machine learning complexity: Online experiments were difficult to debug
- Quality control issues: Significant work was required on Scale's part to make their data quality mechanisms work in the low latency, online setting.
- Regressions were often not detected until after (or well after) training runs were complete.
- Since evaluation of labeler performance was online, by the time a worker was detected as poor some of their data might already been reported back and used for reward model training.
- We believe the right middle ground between offline and online data collection is batched data collection, and plan to use this setting in future work.

### Sharing parameters between reward model and policy causes overfitting
- We initialize the reward and policy models separately
- We try joint training, but it doesn't work because the reward model has only 60,000 samples while the policy has 2 million
- We hope future work will overcome this challenge

### Ambiguous tasks make labeling hard
- Evaluation of a summary is both subjective and multidimensional
- A single human labeler may have a clear notion of whether a given sample is separately accurate, grammatical, nonredundant, or covers all important topics, but in our experiments a labeler will often be asked to choose between samples each of which has some deficiencies
- In choosing which of four samples is the best, a labeler must trade off between different desiderata
- This makes consistent labeling difficult for honest labelers (including the authors!), and makes it difficult to quickly detect problematic labelers
- It also makes the research more difficult to present and interpret: during our experiments we routinely checked the performance of models by having authors label results, since we knew the authors would attempt to do the task honestly, but were epistemically uneasy about reporting these numbers in the paper
- One could hope to cope with such "noise" by simply getting more labels and averaging them, but this does not resolve all the practical difficulties with ambiguity
- When possible, it seems better to design less ambiguous labeling tasks that get at the same information

### Bugs can optimize for bad behavior
- The code refactor introduced a bug which flipped the sign of the reward.
- This bug was remarkable since the result was not gibberish but maximally bad output.
- The authors were asleep during the training process, so the problem was noticed only once training had finished.

## Conclusion
- We have demonstrated RL fine-tuning of language models to four NLP tasks: stylistic continuation with high sentiment or physically descriptive language, and summarization on the CNN/Daily Mail and TL;DR datasets.
- Rather than building task-specific techniques, we achieve our results by straightforwardly applying reward learning to language generation.
- We extend previous reward learning work with pretrained models and KL regularization to prevent the policy from diverging too far from natural language.
- Our results are mixed. On the continuation tasks we achieve good results vs. the zero-shot baseline as evaluated by humans with very few samples: 2.5k for sentiment and 5k for descriptiveness. However, for both summarization tasks our policies are only "smart copiers" (extractive rather than abstractive): they copy from the input text but skip over irrelevant preamble.
- The advantage of copying is truthfulness: by comparison the zero-shot and supervised models produce natural, plausible-looking summaries that are often lies.
- We believe the limiting factor in our experiments is data quality, in particular exacerbated by the online data collection setting, and plan to ameliorate this with batched data collection in future.
- In the supervised case NLP models are trained using human data; if we want RL fine-tuning we need human data too.
- On the AI safety side, interactive communication between humans and ML models is a requirement for scalable reward learning methods such as amplification, debate, and recursive reward modeling (Christiano et al., 2018;Irving et al., 2018;Leike et al., 2018), and natural language is how humans communicate complex ideas.
- Although language models are unlikely to be ready for these tasks in their full generality, Perez et al. (2019) demonstrates that debate al-ready improves generalization for question-answering when debaters quote from a source text.
- Using direct human preferences for language tasks is a step in the direction of scalable reward learning for language, and we believe further steps are possible.
