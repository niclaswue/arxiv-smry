---
title: "Fine-Tuning Language Models from Human Preferences"
date: 2019-09-18T17:33:39.000Z
author: "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1909-08593v2.webp" # image path/url
    alt: "Fine-Tuning Language Models from Human Preferences" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1909.08593)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1909.08593).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/fine-tuning-language-models-from-human).

# Abstract
- Reward learning enables the application of reinforcement learning to tasks where reward is defined by human judgment
- Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks
- In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets
- For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics

# Paper Content

## Introduction
- We would like to apply reinforcement learning to complex tasks defined only by human judgment, where we can only tell whether a result is good or bad by asking humans.
- To do this, we can first use human labels to train a model of reward, and then optimize that model.
- While there is a long history of work learning such models from humans through interaction, this work has only recently been applied to modern deep learning, and even then has only been applied to relatively simple simulated environments (Christiano et al., 2017;Ibarz et al., 2018;Bahdanau et al., 2018).
- By contrast, real world settings in which humans need to specify com-* Equal contribution. Correspondence to paul@openai.com. plex goals to AI agents are likely to both involve and require natural language, which is a rich medium for expressing value-laden concepts.
- Natural language is particularly important when an agent must communicate back to a human to help provide a more accurate supervisory signal (Irving et al., 2018;Christiano et al., 2018;Leike et al., 2018).
- Natural language processing has seen substantial recent advances. One successful method has been to pretrain a large generative language model on a corpus of unsupervised data, then fine-tune the model for supervised NLP tasks (Dai and Le, 2015;Peters et al., 2018;Radford et al., 2018;Khandelwal et al., 2019).
- This method often substantially outperforms training on the supervised datasets from scratch, and a single pretrained language model often can be fine-tuned for state of the art performance on many different supervised datasets (Howard and Ruder, 2018).
- In some cases, fine-tuning is not required: Radford et al. (2019) find that generatively trained models show reasonable performance on NLP tasks with no additional training (zero-shot).
- There is a long literature applying reinforcement learning to natural language tasks. Much of this work uses algorithmically defined reward functions such as BLEU for translation (Ranzato et al., 2015;Wu et al., 2016), ROUGE for summarization (Ranzato et al., 2015;Paulus et al., 2017;Wu and Hu, 2018;Gao et al., 2019b), music theory-based rewards (Jaques et al., 2017), or event detectors for story generation (Tambwekar et al., 2018).
- Nguyen et al. (2017) used RL on BLEU but applied several error models to approximate human behavior.
- Wu and Hu (2018) and Cho et al. (2019) learned models of coherence from existing text and used them as RL rewards for summarization and long-form generation, respectively.
- Gao et al. (2019a) built an interactive summarization tool by applying reward learning to one article at a time.
- Experiments using human evaluations as rewards include Kreutzer et al. (2018) which used off-policy reward learning for translation, and Jaques et al. (2019) which applied the modified Q-learning methods of Jaques et al. (2017) to implicit human preferences in dialog.
- Yi et al. (2019) learned rewards from humans to fine-tune dialog models, but smoothed the rewards to allow supervised learning.
- We refer to Luketina et al. (2019) for a survey of RL tasks involving language as a component, and for RL results using transfer learning from language.
- RL is not the only way to incorporate ongoing human feedback: Hancock et al. (2019) ask humans what a dialogue system should have said instead, then continue supervised training.

## Methods
- We begin with a vocabulary Σ and a language model ρ which defines a probability distribution over sequences of tokens
- We will apply this model to a task with input space X = Σ ≤m , data distribution D over X, and output space Y = Σ n .
- For example, x ∈ X could be an article of up to 1000 words and y ∈ Y could be a 100-word summary.
- ρ defines a probabilistic policy for this task via ρ(y|x) = ρ(xy)/ρ(x): fixing the beginning of the sample to x and generating subsequent tokens using ρ.
- We initialize a policy π = ρ, and then fine-tune π to perform the task well using RL.
- If the task was defined by a reward function r : X × Y → R, then we could use RL to directly optimize the expected reward: However, we want to perform tasks defined by human judgments, where we can only learn about the reward by asking humans.
- To do this, we will first use human labels to train a reward model, and then optimize that reward model.
- Following Christiano et al. (2017), we ask human labelers to pick which of several values of y i is the best response to a given input x.
- 1 We ask humans to choose between four options (y 0 , y 1 , y 2 , y 3 ); considering more options allows a human to amortize the cost of reading and understanding the prompt x.
- Let b ∈ {0, 1, 2, 3} be the option they select.
- Having collected a dataset S of (x, y 0 , y 1 , y 2 , y 3 , b) tuples, we fit a reward model r : X × Y → R using the loss x,yi)
- Since the reward model needs to understand language, we initialize it as a random linear function of the final embedding output of the language model policy ρ following Radford et al. (2018) (see section 4.2 for why we initialize from ρ rather than π).
- To keep the scale of the reward model consistent across training, we normalize it so that it has mean 0 and variance 1 for x ∼ D, y ∼ ρ(•|x).
- Now we fine-tune π to optimize the reward model r.
- To keep π from moving too far from ρ, we add a penalty with expectation β KL(π, ρ) (see table 10 for what happens without this).
- That is, we perform RL on the modified reward
- We either choose a constant β or vary it dynamically to achieve a particular value of KL(π, ρ); see section 2.2.
- This term has several purposes: it plays the role of an entropy bonus, it prevents the policy from moving too far from the range where r is valid, and in the case of our style continuation tasks it also is an important part of the task definition: we ask humans to evaluate style, but rely on the KL term to encourage coherence and topicality.
- Our overall training process is:
- 1. Gather samples (x, y 0 , y 1 , y 2 , y 3 ) via x ∼ D, y i ∼ ρ(•|x).
- Ask humans to pick the best y i from each.
- 2. Initialize r to ρ, using random initialization for the final linear layer of r.
- Train r on the human samples using loss (1).
- 3. Train π via Proximal Policy Optimization (PPO, Schulman et al. ( 2017)) with reward R from (2) on x ∼ D.
- We...

### Human labeling
- Scale AI is used to collect labels
- The Scale API accepts requests of the form (x, y 0 , y 1 , y 2 , y 3 ) and returns selections b ∈ {0, 1, 2, 3}.
- We describe the task to Scale through a combination of instructions (appendix A) and a dataset of about 100 example comparisons from the authors.
- Unlike many tasks in ML, our queries do not have unambiguous ground truth, especially for pairs of similar outputs (which play a large role in our training process, since we train r on pairs of labels sampled from a single policy π).
- This means that there is significant disagreement even between labelers who have a similar understanding of the task and are trying to rate consistently.
- On 4-way comparisons for sentiment and TL;DR summarization, authors of this paper agree about 60% of the time (vs. 25% for random guessing).
- This low rate of agreement complicates the quality control process for Scale; the authors agree with Scale Figure 2: Learning curves for a 124M-parameter model with mock sentiment reward, targeting a KL of 8 nats.
- Lines and shaded areas show mean and range for 5 seeds.
- Early on the reward model sometimes speeds up training, a phenomenon also observed by Christiano et al. (2017).
- For final evaluation of two models A and B, we generate either 2-way comparisons between pairs (a ∼ A, b ∼ B) or 4-way comparisons with quadruples (a 0 , a 1 ∼ A, b 0 , b 1 ∼ B), randomize the order in which samples are presented, and present these comparisons to Scale.

## Experiments
- In section 3.1.1, we test our approach to RL fine-tuning of language models by using a mock labeler (a sentiment model trained on a review classification problem) as a stand-in for human labels.
- We show that RL fine-tuning is effective at optimizing this complex but somewhat artificial reward.
- In section 3.1.2, we show that we can optimize language models from human preferences on stylistic continuation tasks (sentiment and physical descriptiveness) with very little data, and that in the sentiment case the results are preferred to optimizing the review sentiment model.
- In section 3.2 we apply RL fine-tuning to summarization on the CNN/Daily Mail and TL;DR datasets, show that the resulting models are essentially "smart copiers", and discuss these results in the context of other summarization work.
- Figure 3: Allowing the policy π to move further from the initial policy ρ as measured by KL(π, ρ) achieves higher reward at the cost of less natural samples.
- Here we show the optimal KL vs. reward for 124M-parameter mock sentiment (as estimated by sampling), together with results using PPO.
- Runs used 2M episodes, except for the top series.
- We release code 2 for reward modeling and fine-tuning in the offline data case.
- Our public version of the code only works with a smaller 124M parameter model with 12 layers, 12 heads, and embedding size 768.
- We include fine-tuned versions of this smaller model, as well as some of the human labels we collected for our main experiments (note that these labels were collected from runs using the larger model).
- In this mock sentiment experiment, we simulate human judgments by assuming that the "human" always selects the continuation with the higher reward according to r s , and ask how many queries we need to optimize r s .
- Figure 2 shows how r s evolves during training, using either direct RL access to r s or a limited number of queries to train a reward model.
- With a constraint on the KL divergence, our policies continue to receive higher rewards for larger KL divergences.

### Summarization
- We applied a method to summarize two datasets, CNN/Daily Mail and TL;DR.
- We sampled articles or Reddit posts, truncated to 500 tokens, and added a "\n\nTL;DR:" suffix (and for CNN/Daily Mail, a "Article:\n\n" prefix).
- We set the temperature of the pretrained model to T = 0.5 for CNN/Daily Mail and T = 0.7 for TL;DR.
- We ensured articles consist of whole sentences by truncating to the last newline character.
- We sampled summaries that will be shown to a human, and used rejection sampling to ensure there is a newline between tokens 55 and 75.
- We trained online data collection mod- els with 15k, 30k, and 60k human labels, and an offline data collection ablation with 60k labels.
- We also show zero-shot performance of the pretrained model, a supervised fine-tuned baseline using the same pretrained model as starting point, and a lead-3 baseline which copies the first three sentences of the context.
- We combine supervised and RL fine-tuning: performing human RL fine-tuning starting with the supervised fine-tuned model.

### Online data collection is hard
- Online data collection was necessary to achieve the best results on summarization
- Fully online data collection-where each label comes from an up-to-date version of the policy which has already learned from almost all previous labels-had major disadvantages: 1. Software complexity: Our online system interleaves data gathering, reward model training, and RL finetuning. The resulting distributed system was significantly more complicated than if each task was kept separate, slowing the software development process. Moreover, a bug in any one of the tasks would break the entire training process. 2. Machine learning complexity: Online experiments were difficult to debug, as it was hard to iterate on one piece of the ML system at a time. We could often debug an online job only by switching briefly to offline, such as by launching a standalone reward model training run, but then would switch back to online once debugging was complete (until the next cycle). 3. Quality control issues: Significant work was required on Scale's part to make their data quality mechanisms work in the low latency, online setting. However, even after this work it was difficult to maintain high data quality over a long period of time, and regressions were often not detected until after (or well after) training runs were complete. Since evaluation of labeler performance was online, by the time a worker was detected as poor some of their data might already been reported back and used for reward model training. We believe the right middle ground between offline and online data collection is batched data collection, and plan to use this setting in future work.
- Collect a batch of data from the pretrained policy ρ, train the reward model r on this batch, then fine-tune the policy π with r frozen. Once complete, collect another batch of data sampled from π, and iterate. The latency for each batch can be far longer than the online case, simplifying quality control. As in the fully online setting, we can always retrain the reward model from scratch on all data collected so far; human data is expensive so the total volume will be low.
- Removing the interleaved training of r and π simplifies software architecture and diagnosis of ML issues, and allows iteration on just one component (say r in isolation) if problems occur.

### Sharing parameters between reward model and policy causes overfitting
- The reward model and policy are initialized with different values, but joint training is not successful
- The problem comes from the imbalance of data: we have at most 60k samples for the reward model, but 2M episodes for the policy

### Ambiguous tasks make labeling hard
- Evaluation of a summary is both subjective and multidimensional
- A single human labeler may have a clear notion of whether a given sample is separately accurate, grammatical, nonredundant, or covers all important topics; but in our experiments a labeler will often be asked to choose between samples each of which has some deficiencies.
- In choosing which of four samples is the best, a labeler must trade off between different desiderata. This makes consistent labeling difficult for honest labelers (including the authors!), and makes it difficult to quickly detect problematic labelers.
- It also makes the research more difficult to present and interpret: during our experiments we routinely checked the performance of models by having authors label results, since we knew the authors would attempt to do the task honestly, but were epistemically uneasy about reporting these numbers in the paper (table 8 is the one exception).
- One could hope to cope with such "noise" by simply getting more labels and averaging them, but this does not resolve all the practical difficulties with ambiguity.
- When possible, it seems better to design less ambiguous labeling tasks that get at the same information.

### Bugs can optimize for bad behavior
- The code refactor introduced a bug which flipped the sign of the reward.
- The bug also flipped the sign of the KL penalty.
- Since the authors were asleep during the training process, the problem was noticed only once training had finished.

## Conclusion
- We have demonstrated RL fine-tuning of language models to four NLP tasks: stylistic continuation with high sentiment or physically descriptive language, and summarization on the CNN/Daily Mail and TL;DR datasets.
- Rather than building task-specific techniques, we achieve our results by straightforwardly applying reward learning to language generation.
- We extend previous reward learning work with pretrained models and KL regularization to prevent the policy from diverging too far from natural language.
- Our results are mixed. On the continuation tasks we achieve good results vs. the zero-shot baseline as evaluated by humans with very few samples: 2.5k for sentiment and 5k for descriptiveness. However, for both summarization tasks our policies are only "smart copiers" (extractive rather than abstractive): they copy from the input text but skip over irrelevant preamble.
- The advantage of copying is truthfulness: by comparison the zero-shot and supervised models produce natural, plausible-looking summaries that are often lies.
- We believe the limiting factor in our experiments is data quality, in particular exacerbated by the online data collection setting, and plan to ameliorate this with batched data collection in future.
- On the capability side, purely supervised training is insufficient to correct mistakes that arise when sampling from trained policies, and RL training to programmatic reward functions such as BLEU or ROUGE is insufficient: Paulus et al. (2017) conclude that "optimizing for single discrete evaluation metric[s] such as ROUGE with RL can be detrimental to the model quality."
- Interactive tasks such as dialogue are particularly relevant: it is difficult to define the goal of a dialogue without the human participant, and the length of dialogue makes it more likely that supervised learned models will go off distribution.
- In the supervised case NLP models are trained using human data; if we want RL fine-tuning we need human data too.
- On the AI safety side, interactive communication between humans and ML models is a requirement for scalable reward learning methods such as amplification, debate, and recursive reward modeling (Christiano et al., 2018;Irving et al., 2018;Leike et al., 2018), and natural language is how humans communicate complex ideas.
- Although language models are unlikely to be ready for these tasks in their full generality, Perez et al. (2019) demonstrates that debate al-ready improves generalization for question-answering when debaters quote from a source text.
- Using direct human preferences for language tasks is a step in the direction of scalable reward learning for language, and we believe further steps are possible.
