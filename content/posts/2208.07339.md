---
title: "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
date: 2022-08-15T17:08:50.000Z
author: "Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2208-07339v2.webp" # image path/url
    alt: "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2208.07339)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2208.07339).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/llm-int8-8-bit-matrix-multiplication-for).

# Abstract

# Paper Content

## Introduction
- Higher quantization precision at scales beyond 1B parameters
- Explicitly represent the sparse but systematic large magnitude outlier features that ruin quantization precision once they emerge in all transformer layers starting at scales of 6.7B parameters
- Recover the output of the matrix multiplication by denormalizing by the outer product of column and row normalization constants before we perform the next operation.
- To scale beyond 6.7B parameters without performance degradation, it is critical to understand the emergence of extreme outliers in the feature dimensions of the hidden states during inference.
- To this end, we provide a new descriptive analysis which shows that large features with magnitudes up to 20x larger than in other dimensions first appear in about 25% of all transformer layers and then gradually spread to other layers as we scale transformers to 6B parameters.
- At around 6.7B parameters, a phase shift occurs, and all transformer layers and 75% of all sequence dimensions are affected by extreme magnitude features.
- These outliers are highly systematic: at the 6.7B scale, 150,000 outliers occur per sequence, but they are concentrated in only 6 feature dimensions across the entire transformer.
- Setting these outlier feature dimensions to zero decreases top-1 attention softmax probability mass by more than 20% and degrades validation perplexity by 600-1000% despite them only making up about 0.1% of all input features.
- In contrast, removing the same amount of random features decreases the probability by a maximum of 0.3% and degrades perplexity by about 0.1%.
- To support effective quantization with such extreme outliers, we develop mixed-precision decomposition, the second part of our method.
- We perform 16-bit matrix multiplication for the outlier feature dimensions and 8-bit matrix multiplication for the other 99.9% of the dimensions.
- We name the combination of vector-wise quantization and mixed precision decomposition, LLM.int8().
- We show that by using LLM.int8(), we can perform inference in LLMs with up to 175B parameters without any performance degradation.

## Background
- Scale transformer models to break quantization techniques
- Study high-precision asymmetric quantization and symmetric quantization
- Zeropoint quantization offers high precision but is impractical
- Absolute maximum quantization is the most commonly used technique

### 8-bit Data Types and Quantization
- Absmax quantization scales inputs into the 8-bit range [−127, 127] by multiplying with s x f 16 which is 127 divided by the absolute maximum of the entire tensor.
- For an FP16 input matrix X f 16 ∈ R s×h Int8 absmax quantization is given by: where indicates rounding to the nearest integer.
- Zeropoint quantization shifts the input distribution into the full range [−127, 127] by scaling with the normalized dynamic range nd x and then shifting by the zeropoint zp x .
- With this affine transformation, any input tensors will use all bits of the data type, thus reducing the quantization error for asymmetric distributions.
- For ReLU outputs, in absmax quantization all values in [−127, 0) go unused, whereas in zeropoint quantization the full [−127, 127] range is used.
- Zeropoint quantization is given by the following equations:
- To use zeropoint quantization in an operation we feed both the tensor X i8 and the zeropoint zp xi16 into a special instruction4 which adds zp xi16 to each element of X i8 before performing a 16-bit integer operation.
- For example, to multiply two zeropoint quantized numbers A i8 and B i8 along with their zeropoints zp ai16 and zp bi16 we calculate: where unrolling is required if the instruction multiply i16 is not available such as on GPUs or TPUs: where A i8 B i8 is computed with Int8 precision while the rest is computed in Int16/32 precision.
- As such, zeropoint quantization can be slow if the multiply i16 instruction is not available.

## Int8 Matrix Multiplication at Scale
- The main challenge with quantization methods is that a single outlier can reduce the quantization precision of all other values.
- We improve upon one of the most common ways of blocking quantization, row-wise quantization, by using vector-wise quantization.
- To handle the large magnitude outlier features that occur in all transformer layers beyond the 6.7B scale, vector-wise quantization is no longer sufficient.
- For this purpose, we develop mixedprecision decomposition, where the small number of large magnitude feature dimensions (≈0.1%) are represented in 16-bit precision while the other 99.9% of values are multiplied in 8-bit.
- Since most entries are still represented in low-precision, we retain about 50% memory reduction compared to 16-bit.

### Vector-wise Quantization
- Matrix multiplication is viewed as a sequence of independent inner products
- Each row and column in the matrix are given a different scaling constant
- To dequantize the matrix, each inner product is denormalized by 1/(scaling constant for row x column)

### The Core of LLM.int8(): Mixed-precision Decomposition
- Large magnitude features (columns) are important for transformer performance and require high precision quantization
- Vector-wise quantization, our best quantization technique, quantizes each row for the hidden state, which is ineffective for outlier features
- Fortunately, we see that these outlier features are both incredibly sparse and systematic in practice, making up only about 0.1% of all feature dimensions
- As such, we propose mixed-precision decomposition for matrix multiplication where we separate outlier feature dimensions into the set O = {i|i ∈ Z, 0 ≤ i ≤ h}, which contains all dimensions of h which have at least one outlier with a magnitude larger than the threshold α
- In our work, we find that α = 6.0 is sufficient to reduce transformer performance degradation close to zero.

### Experimental Setup
- We measure the robustness of quantization methods as we scale the size of several publicly available pretrained language models up to 175B parameters.
- The key question is not how well a quantization method performs for a particular model but the trend of how such a method performs as we scale.
- We use two setups for our experiments. One is based on language modeling perplexity, which we find to be a highly robust measure that is very sensitive to quantization degradation.
- We use this setup to compare different quantization baselines. Additionally, we evaluate zeroshot accuracy degradation on OPT models for a range of different end tasks, where we compare our methods with a 16-bit baseline.
- For the language modeling setup, we use dense autoregressive transformers pretrained in fairseq (Ott et al., 2019) ranging between 125M and 13B parameters.
- These transformers have been pretrained on Books (Zhu et al., 2015), English Wikipedia, CC-News (Nagel, 2016), OpenWebText (Gokaslan and Cohen, 2019), CC-Stories (Trinh and Le, 2018), and English CC100 (Wenzek et al., 2020).
- For more information on how these pretrained models are trained, see Artetxe et al. (2021).
- To evaluate the language modeling degradation after Int8 quantization, we evaluate the perplexity of the 8-bit transformer on validation data of the C4 corpus (Raffel et al. 2019) which is a subset of the Common Crawl corpus.
- We use NVIDIA A40 GPUs for this evaluation.

### Main Results
- Table 1: Main language modeling perplexity results on the 125M to 13B Int8 models evaluated on the C4 corpus
- Absmax, row-wise, and zeropoint quantization fail as we scale, where models after 2.7B parameters perform worse than smaller models.
- Zeropoint quantization fails instead beyond 6.7B parameters.
- Our method, LLM.int8(), is the only method that preserves perplexity.
- As such, LLM.int8() is the only method with a favorable scaling trend.
- When we look at the scaling trends of zeroshot performance of OPT models on the EleutherAI language model evaluation harness in Figure 1, we see that LLM.int8() maintains full 16-bit performance as we scale from 125M to 175B parameters.
- On the other hand, the baseline, 8-bit absmax vector-wise quantization, scales poorly and degenerates into random performance.
- Although our primary focus is on saving memory, we also measured the run time of LLM.int8().
- The quantization overhead can slow inference for models with less than 6.7B parameters, as compared to a FP16 baseline.
- However, models of 6.7B parameters or less fit on most GPUs and quantization is less needed in practice.
- LLM.int8() run times is about two times faster for large matrix multiplications equivalent to those in 175B models.

## Emergent Large Magnitude Features in Transformers at Scale
- As we scale transformers, outlier features with large magnitudes emerge and strongly affect all layers and their quantization.
- Given a hidden state X ∈ R s×h where s is the sequence/token dimension and h the hidden/feature dimension, we define a feature to be a particular dimension h i .
- Our analysis looks at a particular feature dimension h i across all layers of a given transformer.
- We find that outlier features strongly affect attention and the overall predictive performance of transformers.
- While up to 150k outliers exist per 2048 token sequence for a 13B model, these outlier features are highly systematic and only representing at most 7 unique feature dimensions h i .
- Insights from this analysis were critical to developing mixed-precision decomposition.

### Finding Outlier Features
- The difficulty with the quantitative analysis of emergent phenomena is two-fold: we aim to select a small subset of features for analysis such that the results are intelligible and not to complex while also capturing important probabilistic and structured patterns;
- We use an empirical approach to find these constraints;
- We define outliers according to the following criteria: the magnitude of the feature is at least 6.0, affects at least 25% of layers, and affects at least 6% of the sequence dimensions.
- More formally, given a transformer with L layers and hidden state X l ∈ R s×h , l = 0...L where s is the sequence dimension and h the feature dimension, we define a feature to be a particular dimension h i in any of the hidden states X li .
- We track dimensions h i , 0 ≤ i ≤ h, which have at least one value with a magnitude of α ≥ 6 and we only collect statistics if these outliers occur in the same feature dimension h i in at least 25% of transformer layers 0...L and appear in at least 6% of all sequence dimensions s across all hidden states X l .
- Since feature outliers only occur in attention projection (key/query/value/output) and the feedforward network expansion layer (first sub-layer), we ignore the attention function and the FFN contraction layer (second sub-layer) for this analysis.
- Our reasoning for these thresholds is as follows: we find that using mixed-precision decomposition, perplexity degradation stops if we treat any feature with a magnitude 6 or larger as an outlier feature.
- For the number of layers affected by outliers, we find that outlier features are systematic in large models: they either occur in most layers or not at all. On the other hand, they are probabilistic in small models: they occur sometimes in some layers for each sequence.
- As such, we set our threshold for how many layers need to be affected to detect an outlier feature in such a way as to limit detection to a single outlier in our smallest model with 125M parameters.
- This threshold corresponds to that at least 25% of transformer layers are affected by an outlier in the same feature dimension.
- The second most common outlier occurs in only a single layer ( 2% of layers), indicating that this is a reasonable threshold.
- We use the same procedure to find the threshold for how many sequence dimensions are affected by outlier features in our 125M model: outliers occur in at least 6% of sequence dimensions.
- We test models up to a scale of 13B parameters.
- To make sure that the observed phenomena are not due to bugs in software, we evaluate transformers that were trained in three different software frameworks.
- We evaluate four GPT-2 models which use OpenAI software, five Meta AI models that use Fairseq (Ott et al., 2019), and one EleutherAI model GPT-J that uses Tensorflow-Mesh (Shazeer et al., 2018).
- More details can be found in Appendix C.

### Measuring the Effect of Outlier Features
- The number of layers affected by emergence increases rapidly from 6B to 6.7B parameters
- The number of sequence dimensions affected increases rapidly from 35% to 75%
- Median outlier feature magnitude rapidly increases once outlier features occur in all layers of the transformer
- The large magnitude of outliers features and their asymmetric distribution disrupts Int8 quantization precision
- Regular 16-bit floating point training becomes unstable due to outliers beyond the 6.7B scale
- The number of outliers features increases strictly monotonically with respect to decreasing C4 perplexity

### Interpretation of Quantization Performance
- Outliers are ubiquitous in large transformers
- These outliers are critical for transformer performance
- Absmax quantization methods fail quickly after emergence
- However, almost all outliers have a strict asymmetric distribution
- This makes zeropoint quantization particularly effective for these outliers

## Related work
- There is closely related work on quantization data types and quantization of transformers, as described below.
- Appendix B provides further related work on quantization of convolutional networks.
- 8-bit Data Types. Our work studies quantization techniques surrounding the Int8 data type, since it is currently the only 8-bit data type supported by GPUs.
- Other common data types are fixed point or floating point 8-bit data types (FP8). These data types usually have a sign bit and different exponent and fraction bit combinations.
- For example, a common variant of this data type has 5 bits for the exponent and 2 bits for the fraction (Wang et al., 2018;Sun et al., 2019;Cambier et al., 2020;Mellempudi et al., 2019) and uses either no scaling constants or zeropoint scaling.
- These data types have large errors for large magnitude values since they have only 2 bits for the fraction but provide high accuracy for small magnitude values.
- Jin et al. (2022) provide an excellent analysis of when certain fixed point exponent/fraction bit widths are optimal for inputs with a particular standard deviation.
- We believe FP8 data types offer superior performance compared to the Int8 data type, but currently, neither GPUs nor TPUs support this data type.
- Outlier Features in Language Models. Previous work proved the theoretical relationship between outlier appearance in transformers and how it relates to layer normalization and the token frequency distribution (Gao et al., 2019). Similarly, Kovaleva et al. (2021) attribute the appearance of outliers in BERT model family to LayerNorm, and Puccetti et al. (2022) show empirically that outlier emergence is related to the frequency of tokens in the training distribution.
- We extend this work further by showing how the scale of autoregressive models relates to the emergent properties of these outlier features, and showing how appropriately modeling outliers is critical to effective quantization.
- Quantization. There are two methods that were developed in parallel to ours: nuQmm (Park et al., 2022) and ZeroQuant (Yao et al., 2022). Both use the same quantization scheme: group-w2ise quantization, which has even finer quantization normalization constant granularity than vector-wise quantization.
- Both nuQmm and ZeroQuant aim to accelerate inference and reduce the memory footprint while we focus on preserving predictive performance under an 8-bit memory footprint.
- The largest models that nuQmm and ZeroQuant evaluate are 2.7B and 20B parameter transformers, respectively. ZeroQuant achieves zero-degradation performance for 8-bit quantization of a 20B model.
- We show that our method allows for zero-degradation quantization of models up to 176B parameters.
- Both nuQmm and ZeroQuant suggest that finer quantization granularity can be an effective means to quantize large models. These methods are complementary with LLM.int8().
- Another parallel work is GLM-130B which uses insights from our work to achieve zero-degradation 8-bit quantization (Zeng et al., 2022). GLM-130B performs full 16-bit precision matrix multiplication with 8-bit weight storage.

## Discussion and Limitations
- We have demonstrated for the first time that multi-billion parameter transformers can be quantized to Int8 and used immediately for inference without performance degradation.
- We achieve this by using our insights from analyzing emergent large magnitude features at scale to develop mixed-precision decomposition to isolate outlier features in a separate 16-bit matrix multiplication.
- In conjunction with vector-wise quantization that yields our method, LLM.int8(), which we show empirically can recover the full inference performance of models with up to 175B parameters.
- The main limitation of our work is that our analysis is solely on the Int8 data type, and we do not study 8-bit floating-point (FP8) data types.
- Since current GPUs and TPUs do not support this data type, we believe this is best left for future work.
- However, we also believe many insights from Int8 data types will directly translate to FP8 data types.
- Another limitation is that we only study models with up to 175B parameters.
- While we quantize a 175B model to Int8 without performance degradation, additional emergent properties might disrupt our quantization methods at larger scales.
- A third limitation is that we do not use Int8 multiplication for the attention function.
- Since our focus is on reducing the memory footprint and the attention function does not use any parameters, it was not strictly needed.
- However, an initial exploration of this problem indicated that a solution required additional quantization methods beyond those we developed here, and we leave this for future work.
- A final limitation is that we focus on inference but do not study training or finetuning.
- We provide an initial analysis of Int8 finetuning and training at scale in Appendix E.

## Broader Impacts
- Large models that couldn't fit into GPU memory before can now be accessed
- This enables research and applications which were not possible before due to limited GPU memory
- However, our work also enables resource-rich organizations with many GPUs to serve more models on the same number of GPUs, which might increase the disparities between resource-rich and poor organizations
- In particular, we believe that the public release of large pretrained models, for example, the recent Open Pretrained Transformers (OPT) (Zhang et al., 2022), along with our new Int8 inference for zeroand few-shot prompting, will enable new research for academic institutions that was not possible before due to resource constraints
- The widespread accessibility of such large-scale models will likely have both beneficial and detrimental effects on society that are difficult to predict
