---
title: "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
date: 2022-08-15T17:08:50.000Z
author: "Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2208-07339v2.webp" # image path/url
    alt: "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2208.07339)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2208.07339).


# Abstract

# Paper Content

## Introduction
- Large pretrained language models require significant memory for inference
- One way to reduce the size of the parameters is to quantize them to less bits and use low-bit-precision matrix multiplication
- With this goal in mind, 8-bit quantization methods for transformers have been developed
- While these methods reduce memory use, they degrade performance, usually require tuning quantization further after training, and have only been studied for models with less than 350M parameters
- Degradation-free quantization up to 350M parameters is poorly understood
- In this paper, we present the first multi-billion-scale Int8 quantization procedure for transformers that does not incur any performance degradation
- Our procedure makes it possible to load a 175B parameter transformer with 16 or 32-bit weights, convert the feed-forward and attention projection layers to 8-bit, and use the resulting model immediately for inference without any performance degradation
- We achieve this result by solving two key challenges: the need for higher quantization precision at scales beyond 1B parameters and the need to explicitly represent the sparse but systematic large magnitude outlier features that ruin quantization precision once they emerge in all transformer layers starting at scales of 6.7B parameters
- With the first part of our method, vector-wise quantization, it is possible to retain performance at scales up to 2.7B parameters
- For vector-wise quantization, matrix multiplication can be seen as a sequence of independent inner products of row and column vectors. As such, we can use a separate quantization normalization constant for each inner product to improve quantization precision
- We can recover the output of the matrix multiplication by denormalizing by the outer product of column and row normalization constants before we perform the next operation
- To scale beyond 6.7B parameters without performance degradation, it is critical to understand the emergence of extreme outliers in the feature dimensions of the hidden states during inference
- To this end, we provide a new descriptive analysis which shows that large features with magnitudes up to 20x larger than in other dimensions first appear in about 25% of all transformer layers and then gradually spread to other layers as we scale transformers to 6B parameters
- At around 6.7B parameters, a phase shift occurs, and all transformer layers and 75% of all sequence dimensions are affected by extreme magnitude features
- These outliers are highly systematic: at the 6.7B scale, 150,000 outliers occur per sequence, but they are concentrated in only 6 feature dimensions across the entire transformer. Setting these outlier feature dimensions to zero decreases top-1 attention softmax probability mass by more than 20% and degrades validation perplexity by 600-1000% despite them only making up about 0.1% of all input features
- In contrast, removing the same amount of random features decreases the probability by a maximum of 0.3% and degrades perplexity by about 0.1%

## Background
- Scale transformer models to study quantization techniques
- At which scale quantization techniques fail
- How quantization precision relates to scale
- Study high-precision asymmetric quantization and symmetric quantization

### 8-bit Data Types and Quantization
- Absmax quantization scales inputs into the 8-bit range [−127, 127] by multiplying with s x f 16 which is 127 divided by the absolute maximum of the entire tensor.
- For an FP16 input matrix X f 16 ∈ R s×h Int8 absmax quantization is given by:
- Zeropoint quantization shifts the input distribution into the full range [−127, 127] by scaling with the normalized dynamic range nd x and then shifting by the zeropoint zp x .
- With this affine transformation, any input tensors will use all bits of the data type, thus reducing the quantization error for asymmetric distributions.
- For ReLU outputs, in absmax quantization all values in [−127, 0) go unused, whereas in zeropoint quantization the full [−127, 127] range is used.
- Zeropoint quantization is given by the following equations:
- To dequantize C i32 , we divide by the scaling constants nd a f 16 and nd b f 16 .

## Int8 Matrix Multiplication at Scale
- Quantization methods that use a single scaling constant per tensor can lead to reduced quantization precision
- We improve upon one of the most common ways of blocking quantization, row-wise quantization, by using vector-wise quantization
- To handle the large magnitude outlier features that occur in all transformer layers beyond the 6.7B scale, vector-wise quantization is no longer sufficient, so we develop mixedprecision decomposition

### Vector-wise Quantization
- Matrix multiplication is viewed as a sequence of independent inner products
- Each row and column of the matrix are given a different scaling constant
- To dequantize the matrix, each inner product is denormalized by 1/(scaling constant for row x column)

### The Core of LLM.int8(): Mixed-precision Decomposition
- Large magnitude features (columns) are important for transformer performance and require high precision quantization
- Vector-wise quantization, our best quantization technique, quantizes each row for the hidden state, which is ineffective for outlier features
- Fortunately, we see that these outlier features are both incredibly sparse and systematic in practice, making up only about 0.1% of all feature dimensions
- We propose mixed-precision decomposition for matrix multiplication where we separate outlier feature dimensions into the set O = {i|i ∈ Z, 0 ≤ i ≤ h}, which contains all dimensions of h which have at least one outlier with a magnitude larger than the threshold α
- In our work, we find that α = 6.0 is sufficient to reduce transformer performance degradation close to zero.

### Experimental Setup
- We measure the robustness of quantization methods as we scale the size of several publicly available pretrained language models up to 175B parameters
- The key question is not how well a quantization method performs for a particular model but the trend of how such a method performs as we scale
- We use two setups for our experiments. One is based on language modeling perplexity, which we find to be a highly robust measure that is very sensitive to quantization degradation. We use this setup to compare different quantization baselines. Additionally, we evaluate zeroshot accuracy degradation on OPT models for a range of different end tasks, where we compare our methods with a 16-bit baseline.
- For the language modeling setup, we use dense autoregressive transformers pretrained in fairseq (Ott et al., 2019) ranging between 125M and 13B parameters. These transformers have been pretrained on Books (Zhu et al., 2015), English Wikipedia, CC-News (Nagel, 2016), OpenWebText (Gokaslan and Cohen, 2019), CC-Stories (Trinh and Le, 2018), and English CC100 (Wenzek et al., 2020).
- For more information on how these pretrained models are trained, see Artetxe et al. (2021).
- To evaluate the language modeling degradation after Int8 quantization, we evaluate the perplexity of the 8-bit transformer on validation data of the C4 corpus (Raffel et al., 2019) which is a subset of the Common Crawl corpus.
- We use NVIDIA A40 GPUs for this evaluation.

### Main Results
- The main language modeling perplexity results on the 125M to 13B Int8 models evaluated on the C4 corpus can be seen in Table 1
- We see that absmax, row-wise, and zeropoint quantization fail as we scale, where models after 2.7B parameters perform worse than smaller models
- Zeropoint quantization fails instead beyond 6.7B parameters
- Our method, LLM.int8(), is the only method that preserves perplexity. As such, LLM.int8() is the only method with a favorable scaling trend
- When we look at the scaling trends of zeroshot performance of OPT models on the EleutherAI language model evaluation harness in Figure 1, we see that LLM.int8() maintains full 16-bit performance as we scale from 125M to 175B parameters
- On the other hand, the baseline, 8-bit absmax vector-wise quantization, scales poorly and degenerates into random performance
- Although our primary focus is on saving memory, we also measured the run time of LLM.int8(). The quantization overhead can slow inference for models with less than 6.7B parameters, as compared to a FP16 baseline
- However, models of 6.7B parameters or less fit on most GPUs and quantization is less needed in practice
- LLM.int8() run times is about two times faster for large matrix multiplications equivalent to those in 175B models.

## Emergent Large Magnitude Features in Transformers at Scale
- As we scale transformers, outlier features with large magnitudes emerge and strongly affect all layers and their quantization
- Given a hidden state X ∈ R s×h where s is the sequence/token dimension and h the hidden/feature dimension, we define a feature to be a particular dimension h i
- Our analysis looks at a particular feature dimension h i across all layers of a given transformer
- We find that outlier features strongly affect attention and the overall predictive performance of transformers
- While up to 150k outliers exist per 2048 token sequence for a 13B model, these outlier features are highly systematic and only representing at most 7 unique feature dimensions h i
- Insights from this analysis were critical to developing mixed-precision decomposition.

### Finding Outlier Features
- The difficulty with the quantitative analysis of emergent phenomena is two-fold
- We use an empirical approach to find these constraints
- We define outliers according to the following criteria: the magnitude of the feature is at least 6.0, affects at least 25% of layers, and affects at least 6% of the sequence dimensions
- We track dimensions h i , 0 ≤ i ≤ h, which have at least one value with a magnitude of α ≥ 6 and we only collect statistics if these outliers occur in the same feature dimension h i in at least 25% of transformer layers 0...L and appear in at least 6% of all sequence dimensions s across all hidden states X l
- Since feature outliers only occur in attention projection (key/query/value/output) and the feedforward network expansion layer (first sub-layer), we ignore the attention function and the FFN contraction layer (second sub-layer) for this analysis
- We evaluate four GPT-2 models which use OpenAI software, five Meta AI models that use Fairseq (Ott et al., 2019), and one EleutherAI model GPT-J that uses Tensorflow-Mesh (Shazeer et al., 2018)
- We also perform our analysis in two different inference software frameworks: Fairseq and Hugging Face Transformers (Wolf et al., 2019)

### Measuring the Effect of Outlier Features
- The number of layers affected by emergence increases rapidly from 6B to 6.7B parameters
- The number of sequence dimensions affected increases rapidly from 35% to 75%
- Median outlier feature magnitude rapidly increases once outlier features occur in all layers of the transformer
- Outliers are critical for transformer performance and if they are removed, the mean top-1 softmax probability is reduced from about 40% to about 20%, and validation perplexity increases by 600-1000%

### Interpretation of Quantization Performance
- Outliers are ubiquitous in large transformers
- These feature dimensions are critical for transformer performance
- Absmax quantization methods fail quickly after emergence
- Zeropoint quantization is particularly effective for these outliers
- However, at the 13B scale, even zeropoint quantization fails due to accumulated quantization errors

## Related work
- There is closely related work on quantization data types and quantization of transformers
- Appendix B provides further related work on quantization of convolutional networks
- 8-bit Data Types. Our work studies quantization techniques surrounding the Int8 data type, since it is currently the only 8-bit data type supported by GPUs.
- Other common data types are fixed point or floating point 8-bit data types (FP8). These data types usually have a sign bit and different exponent and fraction bit combinations.
- For example, a common variant of this data type has 5 bits for the exponent and 2 bits for the fraction (Wang et al., 2018;Sun et al., 2019;Cambier et al., 2020;Mellempudi et al., 2019) and uses either no scaling constants or zeropoint scaling.
- These data types have large errors for large magnitude values since they have only 2 bits for the fraction but provide high accuracy for small magnitude values.
- Jin et al. (2022) provide an excellent analysis of when certain fixed point exponent/fraction bit widths are optimal for inputs with a particular standard deviation.
- We believe FP8 data types offer superior performance compared to the Int8 data type, but currently, neither GPUs nor TPUs support this data type.
- Large magnitude outlier features in language models have been studied before (Timkey and van Schijndel, 2021;Bondarenko et al., 2021;Wei et al., 2022;Luo et al., 2021).
- Previous work proved the theoretical relationship between outlier appearance in transformers and how it relates to layer normalization and the token frequency distribution (Gao et al., 2019).
- Similarly, Kovaleva et al. (2021) attribute the appearance of outliers in BERT model family to LayerNorm, and Puccetti et al. (2022) show empirically that outlier emergence is related to the frequency of tokens in the training distribution.
- We extend this work further by showing how the scale of autoregressive models relates to the emergent properties of these outlier features, and showing how appropriately modeling outliers is critical to effective quantization.
- Quantization. There are two methods that were developed in parallel to ours: nuQmm (Park et al., 2022) and ZeroQuant (Yao et al., 2022).
- Both use the same quantization scheme: group-w2ise quantization, which has even finer quantization normalization constant granularity than vector-wise quantization.
- This scheme offers higher quantization precision but also requires custom CUDA kernels.
- Both nuQmm and ZeroQuant aim to accelerate inference and reduce the memory footprint while we focus on preserving predictive performance under an 8-bit memory footprint.
- The largest models that nuQmm and ZeroQuant evaluate are 2.7B and 20B parameter transformers, respectively.
- ZeroQuant achieves zero-degradation performance for 8-bit quantization of a 20B model.
- We show that our method allows for zero-degradation quantization of models up to 176B parameters.
- Both nuQmm and ZeroQuant suggest that finer quantization granularity can be an effective means to quantize large models.
- These methods are complementary with LLM.int8().
- Another parallel work is GLM-130B which uses insights from our work to achieve zero-degradation 8-bit quantization (Zeng et al., 2022).
- GLM-130B performs full 16-bit precision matrix multiplication with 8-bit weight storage.

## Discussion and Limitations
- We have demonstrated for the first time that multi-billion parameter transformers can be quantized to Int8 and used immediately for inference without performance degradation.
- We achieve this by using our insights from analyzing emergent large magnitude features at scale to develop mixed-precision decomposition to isolate outlier features in a separate 16-bit matrix multiplication.
- In conjunction with vector-wise quantization that yields our method, LLM.int8(), which we show empirically can recover the full inference performance of models with up to 175B parameters.
- The main limitation of our work is that our analysis is solely on the Int8 data type, and we do not study 8-bit floating-point (FP8) data types.
- Since current GPUs and TPUs do not support this data type, we believe this is best left for future work.
- However, we also believe many insights from Int8 data types will directly translate to FP8 data types.
- Another limitation is that we only study models with up to 175B parameters.
- While we quantize a 175B model to Int8 without performance degradation, additional emergent properties might disrupt our quantization methods at larger scales.
- A third limitation is that we do not use Int8 multiplication for the attention function.
- Since our focus is on reducing the memory footprint and the attention function does not use any parameters, it was not strictly needed.
- However, an initial exploration of this problem indicated that a solution required additional quantization methods beyond those we developed here, and we leave this for future work.
- A final limitation is that we focus on inference but do not study training or finetuning.
- We provide an initial analysis of Int8 finetuning and training at scale in Appendix E.

## Broader Impacts
- Our work enables access to large models that previously could not fit into GPU memory.
- This enables research and applications which were not possible before due to limited GPU memory, in particular for researchers with the least resources.
- See Table 3 for model/GPU combinations which are now accessible without performance degradation.
- However, our work also enables resource-rich organizations with many GPUs to serve more models on the same number of GPUs, which might increase the disparities between resource-rich and poor organizations.
- In particular, we believe that the public release of large pretrained models, for example, the recent Open Pretrained Transformers (OPT) (Zhang et al., 2022), along with our new Int8 inference for zeroand few-shot prompting, will enable new research for academic institutions that was not possible before due to resource constraints.
- The widespread accessibility of such large-scale models will likely have both beneficial and detrimental effects on society that are difficult to predict.

## B Additional Related Work
- Quantization of transformers has been focused on sub-billion parameter masked language models (MLMs), including BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019).
- Versions of 8-bit BERT/RoBERTa include Q8BERT (Zafrir et al., 2019), QBERT (Shen et al., 2020), product quantization with quantization noise (Fan et al., 2020), TernaryBERT (Zhang et al., 2020), and BinaryBERT (Bai et al., 2021).
- Work by Zhao et al. (2021) performs both quantization and pruning.
- All these models require either quantization-aware finetuning or post-training quantization to make the model usable in low-precision.
- In contrast with our methods, the model can be used directly without performance degradation.
- If one views matrix multiplication as 1x1 convolution, vector-wise quantization is equivalent to channel-wise quantization for convolution combined with row quantization (Khudia et al., 2021).
- For matrix multiplication, this was used by Wu et al. (2020) for BERT-sized transformers (350M parameters), while we are the first to study vector-wise quantization for autoregressive and large-scale models.
- The only other work that we are aware of that quantizes transformers other than BERT is Chen et al. (2020), which uses post-training quantization with zeropoint quantization in the forward pass and zeropoint-row-wise quantization in the backward pass.
- However, this work is still for sub-billion parameter transformers.
- We compare with both zeropoint and row-wise quantization in our evaluations and do not require post-training quantization.
- Low-bitwidth and Convolutional Network Quantization
- Work that uses less than 8-bits for data types is usually for convolutional networks (CNNs) to reduce their memory footprint and increase inference speed for mobile devices while minimizing model degradation.
- Methods for different bit-widths have been studied: 1-bit methods (Courbariaux and Bengio, 2016;Rastegari et al., 2016;Courbariaux et al., 2015), 2 to 3-bit (Zhu et al., 2017;Choi et al., 2019), 4-bits (Li et al., 2019), more bits (Courbariaux et al., 2014), or a variable amount of bits (Gong et al., 2019).
- For additional related work, please see the survey of Qin et al. (2020).
- While we believe that lower than 8-bit width with some performance degradation is possible for billion-scale transformers, we focus on 8-bit transformers that do not degrade performance and that can benefit from commonly used GPUs that accelerates inference through Int8 tensor cores.
- Another line of work that focuses on convolutional network quantization is to learn adjustments to the quantization procedure to improve quantization errors.
- For example, using Hessian information (Dong et al., 2019), step-size quantization (Esser et al., 2019), soft quantization (Gong et al., 2019), mixedprecision via linear programming optimization (Yao et al., 2021), and other learned quantization methods (Zhang et al., 2018;Gholami et al., 2021).

## C Detailed Outlier Feature Data
- Outlier feature analysis was conducted on transformer data
- Quartiles of the most common outlier were found in each transformer
- Outliers that are one-sided are more common in transformers with asymmetric distributions

## D Inference Speedups and Slowdowns

### D.1 Matrix Multiplication benchmarks
- cuBLASLt is faster than cuBLAS for matrix multiplication when the model size is 5140 or larger
- Adding mixed precision decomposition slows inference further
- Existing custom CUDA kernels are much faster than when using default PyTorch and NVIDIA-provided kernels for quantization
- Matrix multiplication is the only operation where Int8 is faster than 16-bit

## E Training Results
- We tested Int8 training on a variety of training settings and compared to 32-bit baselines
- We tested separate settings for running the transformer with 8-bit feed-forward networks with and without 8-bit linear projections in the attention layer, as well at the attention iteself in 8-bit and compared against 32-bit performance
- We tested two tasks (1) language modeling on part of the RoBERTa corpus including Books (Zhu et al., 2015), CC-News (Nagel, 2016), OpenWebText (Gokaslan and Cohen, 2019), and CC-Stories (Trinh and Le, 2018); and (2) neural machine translation (NMT) (Ott et al., 2018) on WMT14+WMT16 (Macháček and Bojar, 2014;Sennrich et al., 2016)
- The results are shown in Table 7 and Table 8
- We can see that for training, using the attention linear projections with Int8 data types and vector-wise quantization leads to degradation for NMT and for 1.1B language model but not for 209M language modeling
- The results improve slightly if mixed-precision decomposition is used but is not sufficient to recover full performance in most cases
- These suggests that training with 8-bit FFN layers is straightforward while other layers require additional techniques or different data types than Int8 to do 8-bit training at scale without performance degradation

## F Fine-tuning Results
