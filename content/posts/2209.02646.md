---
title: "A Survey on Generative Diffusion Model"
date: 2022-09-06T16:56:21.000Z
author: "Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-02646v8.webp" # image path/url
    alt: "A Survey on Generative Diffusion Model" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.02646)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.02646).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-survey-on-generative-diffusion-model).

# Abstract
- Deep learning shows excellent potential in generation tasks thanks to deep latent representation
- Generative models are classes of models that can generate observations randomly concerning certain implied parameters
- Recently, the diffusion Model has become a rising class of generative models by its power-generating ability
- Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this field. However, the diffusion model has its genuine drawback of a slow generation process, single data types, low likelihood, and the inability for dimension reduction. They are leading to many enhanced works. This survey makes a summary of the field of the diffusion model. We first state the main problem with two landmark works -- DDPM and DSM, and a unified landmark work -- Score SDE. Then, we present improved techniques for existing problems in the diffusion-based model field, including speed-up improvement
- For model speed-up improvement, data structure diversification, likelihood optimization, and dimension reduction. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to specific NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this field together with limitations and further directions.

# Paper Content

## INTRODUCTION
- Deep generative models, e.g., VAE, EBM, GAN, have shown great potential in creating new patterns that humans cannot properly distinguish.
- Inspired by the so-far successes of the diffusion model in these popular domains, applying diffusion models to generation-related tasks of the other domains would be a favorable path for exploiting powerful generative capacity.
- However, the diffusion model has the inherent drawback of plenty of sampling steps and a long sampling time compared to Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs).
- Since diffusion models leverage a Markov process to convert data distribution via tiny perturbations, a large number of diffusion steps are required in both the training and inference phases.
- Thus, it takes more time to sample from a random noise until it eventually alters to high-quality data similar to the prior.
- Additionally, other problems such as likelihood optimization and the inability of dimension reduction also count.
- Hence, lots of works aspired to accelerate the diffusion process along with improving sampling quality.

## •
- Diffusion models are widely used in computer vision, natural language processing, bioinformatics, and speech processing.
- Diffusion models are used to solve problems in these domains, and have been found to be effective in achieving desired results.
- There are some limitations to diffusion models that need to be addressed, and further proof is needed to show the effectiveness of diffusion models in these fields.

## PROBLEM STATEMENT

## Notions and Definitions

## State
- States are a set of data distributions that describe the whole process of diffusion models.
- The noise is gradually injected into the starting distribution, called starting state 0 .
- With enough steps of noise injection, the distribution finally comes into a known noise distribution (mostly Gaussian), which is called the prior state   (Discrete)/ 1 (Continuous).
- Then, the other distributions between the starting state and the prior state are called intermediate states   .

## Process & Transition Kernel
- The forward/diffusion process transforms the starting state into the tractable noise
- The reverse/denoised process samples the noise gradients step by step into the samples
- In either process, the interchange between any two states is achieved by the transition kernel
- The forward process consists of plenty of forward steps which are the forward transition kernels
- The reverse process conducts similarly: Different from the discrete case, for any time 0 ≤  <  ≤ 1, the forward process is defined: where   ,   are the forward and reverse transition kernel at time  with the variables intermediate state  −1 &   and the noise scale
- The most frequently used kernel is the Markov kernel since it ensures randomness and tractability in the forward process and the reverse process
- The difference between this expression and normalizing flow is the variable noise scale, which controls the randomness of the whole process
- The pipeline consists of a number of stages, each of which transforms the data in a certain way

## Discrete and continuous
- Taking the perturbation kernel to sufficiently small will result in an infinite number of steps in the discrete process.
- To tackle the mechanism behind this situation, the continuous process starting from time 0 and ending at time 1 is used.
- Compared to a discrete process, the continuous one enables the extraction of any information from any time state.
- Further, assuming the change of perturbation kernel is slight enough, the continuous process enjoys better theoretical support.

## Training Objective
- The diffusion model follows the same training objective as variational autoregressive-encoder and normalizing flow.
- This is implemented by maximizing the log-likelihood.

## Problem Formulation

## Denoised Diffusion Probabilistic Model
- The DDPM forward process selects a sequence of noise coefficients for Markov transition kernels
- Different noise schedules have no clear effects in experiments, so the DDPM forward steps are defined as a sequence of diffusion steps
- The DDPM reverse process starts at (0) = N(0;0) and given the forward process, defines the reverse process with learnable Gaussian transitions parameterized by
- The minimization problem for the DDPM can be formulated as: 0] =:

## Score Matching Formulation
- The score matching model aims to solve the original data distribution estimation problem by approximating the gradient of data ∇   ().
- The main approach of score matching is to train a score network   to predict the score [69], [70], which is obtained using perturbing data with different noise schedules.
- The scorematching process is defined as: Score Perturbation Process & Kernel: The perturbation process consists of a sequence of perturbation steps with increasing noise scales  1 , ...,   . The Gaussian perturbation kernel is defined as   ( x|) := N x | ,  2  . For each noise scale   , the score is equivalent to the gradient of the perturbation kernel.
- If we treat this increasing noise perturbation as a discrete process, the transition kernel between two neighbor states is where  is the length of the noise scale sequence, and  is random noise.

## Score Matching Process:

## DDPM & DSM
- where is the standard Wiener process/Brownian Motion
- (•, ) is the drift coefficient of (), and (•) is the simplified version of diffusion coefficient of (), which is assumed not dependent on
- denotes the data distribution and probability density of (). denotes the original prior distribution which gains no information from
- when the coefficients are piece-wise continuous, the forward SDE equation admits a unique solution
- similar to the discrete case, the forward transition in the SDE framework is derived as:
- in contrast to the Forward SDE Process, the Reversed SDE Process is defined with respect to the reverse-time Stochastic Differential Equation
- ∇  log   () is the score to be matched

## Score SDE Training Objective:
- The training objective of score SDE employs a weighting scheme in the score loss compared to denoised score matching.
- The weighting scheme is where () (0) are corresponding continuous time variables of   .

## SDE-based DDPM & DSM:
- The transition kernel of DDPM and DSM can be expressed as a function of the continuous-time variable of discrete noise scales and
- Probability Flow ODE (Diffusion ODE) is the continuous-time ODE that supports the deterministic process which shares the same marginal probability density with SDE.

## Training Strategy

## Denoising Diffusion Training Strategy
- The only item we can be used to train is 1:−1
- By parameterizing the posterior (−1|,0) using Baye's rule, we have:
- where   is defined as 1−  , ᾱ is defined as  =1   . Mean and variance schedules can be expressed as:
- Keeping above parameterization as well as reparameterizing   as  (0,), −1 can be regarded as an expectation of L2-loss between two mean coefficients:
- Simplifying −1 by reparameterizing   w.r.t  , we obtain the simplified training objective named    :
- Most diffusion models until now use the training strategy of DDPMs. But there exist some exceptions. DDIM's training objective [77] can be transformed by adding a constant from DDPM's although it is independent of Markovian step assumption;
- Training pattern of Improved DDPM [61] named as  ℎ  is to combine training object of DDPM    and a term with variational lower bound   . However,    still takes the main effect of these training methods.

## Score Matching Training Strategy
- Maximize the difference between the estimated and true scores
- Use a kernel function to minimize the cost function
- Use a noise distribution that is Gaussian with a mean and variance
- Traditional score-matching techniques require massive computation cost for Hessian of log density function.
- To fix this problem, advanced methods find approaches to avoid Hessian computing.
- Implicit score matching (ISM) [73] treat the real score density as a non-normalized density function that can be optimized by neural network.
- Sliced score matching (SSM) [78] provide a unperturbed score estimation method through reverse-mode auto-differentiation by projecting score onto random vectors.
- However, because of the low-manifold problem in real data as well as the sampling problem in the low-density region, denoised score matching could be the better solution for improving score matching.
- Denoised score matching (DSM) [69] transforms the original score matching into a perturbation kernel learning by perturbing a sequence of increasing noise.

## Sampling Algorithm
- Unconditional sampling is used to extract the gradient in each time step
- There are three types of unconditional sampling algorithms- landmark works, effective conditional sampling, and general conditional sampling
- In Appendix, effective conditional sampling algorithms are presented

## Unconditional Sampling

## Ancestral Sampling
- Ancestral sampling is a method for reconstructing an initial idea
- Gradient of inverse Markovian step by step is used to reconstruct the initial idea
- Predictor-corrector (PC) Sampling is inspired by a type of ODE black-box ODE solver
- Sampling procedure comprises a predictor sampler and a corrector sampler

## Langevin Dynamics Sampling

## Conditional Sampling
- Labeled Condition Sampling with labeled conditions provides gradient guidance in each sampling step.
- Usually, an additional classifier with UNet Encoder architecture for generating condition gradients for specific labels is needed.
- The labels can be text & categorical label [84], [85], [86], [87], [88], binary label [89], [90], or extracted features [26], [91], [92].
- It is firstly presented by [84], and current conditional sampling methods are similar in theory.
- Unlabeled Condition Except for label-guidance sampling, unlabeled condition sampling only takes self-information as guidance.
- Conducting in a self-supervised manner [93], [94], it is often applied in denoising [95], resolution [96], and inpainting [41] tasks.

## ALGORITHM IMPROVEMENT
- Conditional diffusion with strong guidance can achieve high fidelity samples within 10 steps
- Unconditional sampling speed remains incomparable to GAN and VAE
- Handling diverse data distribution, optimizing loglikelihood, and dimension reduction techniques still count

## Speed-up Improvement
- Diffusion models have high-fidelity generation, but low sampling speed limits.
- To improve this situation, advanced techniques can be divided into four categories: training scheme enhancement, training-free accelerated sampling, mix-modeling design, and score-diffusion unification design.

## Training Schedule
- Recent studies have shown that the key factors in training schemes influencing learning patterns and models' performance are knowledge distillation, diffusion scheme learning, and noise scale designing.
- Knowledge distillation is the process of extracting knowledge from data.
- Diffusion scheme learning is the process of learning a diffusion model.
- Noise scale designing is the process of designing a noise scale.

## Knowledge Distillation

## Diffusion Scheme Learning
- Forward diffusion is an encoder that projects data into many latent spaces
- Compared to VAE, the diffusion model encodes data onto latent spaces with the same dimension to achieve high expressiveness in a more complex way
- For encoding degree optimization methods, CCDF [95] and Franzese et al., [103] establish an optimization problem where the number diffusion step is treated as a variable for minimizing ELBO from a theoretical perspective [158], [159]
- Another approach is based on truncation, which conducts a trade-off between generating speed and sample fidelity. Truncating patterns samples from less diffused data generated by GAN and VAE in a one-step manner. TDPM [99] truncates both diffusion and sampling processes by sampling from implicit generative distribution learned by GAN and conditional transport (CT) [160]
- Similarly, Early Stop (ES) DDPM [101] learns from latent space to generate implicit distributions.

## Noise Scale Designing
- Traditional diffusion process: each transition step is determined by the injected noise, which is equivariant to a random walk on the forward and reversed trajectories.
- Existing methods treat noise scale as a learnable parameter throughout the whole process.
- Among the forward noise design methods, VDM [67] parameterizes the noise scalar as signal-to-noise ratio for connecting noise scale and training loss and model types.
- FastDPM [105] obtains forward noise from the discretetime variables or variance scalar, connecting noise design to ELBO optimization.
- In the reverse noise design methods, improved DDPM [61] learns the reverse noise scale implicitly by training a hybrid loss containing    and   .
- Besides, San Roman et al. employs a noise prediction network to update the reverse noise scale directly before conducting ancestral sampling in each step.

## Training-Free Sampling
- Analytical methods: (1) Pre-training the gradient of data in a diffusion model can achieve training-free methods. (2) Flow-based: DiffFlow [117], Expressiveness Mixture LSGM [118], PDM [120], INDM [121], ScoreEBM [122], Score-Diffusion Unification Unification Reformulation: Score SDE [66], gDDIM [110], VDM [67], FastDPM [105], f-DM [124]. (3) Connection: Cold Diffusion [125], GGDM [115], DMM [126], [127], [128], [129].
- Claims: (1) Training enhancement methods focus on changing the training pattern and noise schemes for sampling speed-up. (2) Based on the fact that the gradient of data is stored in the pre-trained diffusion models, training-free methods apply pre-trained information directly to the advanced sampling algorithms with fewer steps and higher fidelity. (3) In this subsection, we divide them into four categories: analytical methods, Flow-based: DiffFlow [117], Expressiveness Mixture LSGM [118], PDM [120], INDM [121], ScoreEBM [122], Score-Diffusion Unification Unification Reformulation: Score SDE [66], gDDIM [110], VDM [67], FastDPM [105], f-DM [124].

## Continuous Space

## Analytical Method
- Existing training-free sampling methods take reverse covariance scales as a hand-crafted sequence of noises without considering them dynamically.
- Starting from KL-divergence optimization, analytical methods set the reverse mean and covariance as optimal solutions.
- Analytic-DPM [107] and extended Analytic-DPM [108] jointly propose optimal reverse solutions under correction for each state.
- Analytical methods enjoy a theoretical guarantee for the approximation error, but they are limited in particular distributions due to the pre-assumptions.

## Implicit Sampler
- The implicit sampler follows the jump-step pattern using knowledge from the pre-trained diffusion model.
- FastDPM [105] conducts accelerated sampling discretely by re-designing the reverse noise schedule.
- Song et al. [77] proposes DDIM, which follows the discrete pattern of probability flow ODE with Neural ODE formulation [75]: where   is parameterized by and x is parameterized as / √ .
- Besides, the probability can be treated as one kind of Score SDE, which is derived from the discrete formulation:
- Besides, the implicit sampler is actually one type of neural ODE solver.
- On the one hand, part of the methods employ advanced ODE solvers, such as PNDM [76], edm [112], DEIS [161], gDDIM [110], and DPM-Solver [64].
- On the other hand, Watson et al. proposed dynamic programming based jump-step method for sampling the optimal implicit route along the reversed trajectory.
- Further improved works with strong theoretical support, like manifold hypothesis and sparsity, are expected.

## Differential Equation Solver Sampler

## Dynamic Programming Adjustment
- Dynamic programming (DP) is a memory-based optimization technique that can find the optimized solution in a reduced time
- DP-based methods take O  2 of computational cost
- The optimal solution is found by optimizing the sum of ELBO losses

## Mixed-Modeling
- Mixed-modeling applies fast-sampling, and highexpressiveness generative models in diffusion pipeline.
- For diffusion mixed-modeling, diffusion models take the virtue of high-speed sampling of others (such as adversarial training network and autoregressive encoder) and high expressiveness (such as normalizing flow).
- Thus, designing mixed models not only performs a promising enhancement but also helps perceive connections between diffusion models and others.

## Acceleration Mixture
- Acceleration mixture aims to apply high-speed generation of VAEs and GANs to save plenty of steps on sampling less perturbed data from random noise.
- One type of models generate predicted 0 with VAE [116] and GAN [63].
- Another type of model like ES-DDPM [101] reconstructs intermediate samples as the starting points of the denoised process, which can be viewed as the early stop technique.

## Expressiveness Mixture
- Expressiveness mixture support diffusion models on expressing data or noise in a different pattern.
- Highexpressiveness data combined with fast-sampling generative models achieve speed-up by obtaining mean and variance more accurately.
- The high expressiveness methods can be divided into noise modulation, space projection, and kernel expressiveness.
- As for noise modulation, DiffFlow [117] employs a flow function in each SDE-based diffusion step for noise modulation through a minimizing process w.r.t. KL-Divergence.
- Benefiting from specific spaces' properties, space projection methods leverage NFs into data transformation.
- Besides, Score-Flow [119] conducts the diffusion process onto the dequantization field using NFs to solve the mismatching between continuous density and discrete data [173].
- By taking the energy function as the transition kernel in the reverse process, kernel expressiveness method [122], [174] bridges the gap between non-normalized probability density data and diffusion sampling.
- GGDM [111] and DMM [126] generalizes diffusion models with non-Markovian samplers and a vast range of marginal variance to explore formulations of a wider diffusion family.
- Cold Diffusion [125] proposes a unified training and inference framework available for any transition kernels and data distributions.

## Data Structure Diversification
- Diffusion methods are mostly used for image generation, which limits the fidelity of generated images.
- Diffusion methods have been proven to work in inter-disciplinary tasks with diverse data types.
- The traditional patterns of diffusion based on Gaussian perturbed kernels and Gaussian noise prior are expected to be extended for universal practices.

## Non-linear Space
- Existing denoising and super-resolution methods don't work well with non-linear perturbations.
- Kawar et al. and DPS use a pseudo-inverse operator and posterior sampling approximation to solve these problems.

## Image & Point Cloud
- Point cloud generation is first proposed by Luo et al.
- Luo et al. uses latent space transformation to obtain high-quality 3d shapes.
- Some slight improvements used in latent space transformation include canonical map, condition feature extraction sub-nets, and point-voxel representation.

## Latent Space
- Most current methods project data into continuous space, obtaining promising performance with the aid of high-quality generation power of diffusion models such as EDM and antigen-diffusion.
- Latent space processing should be a beneficial pattern utilized in new application fields.

## Function
- Traditional diffusion processes conducted by Gaussian distribution are limited for some real-world tasks.
- Dutordoir et al., [177] proposes the first diffusion model sampling on the function space.

## Others
- Score-flow [119] employs a flow function to project RGBimage into dequantization space, achieving diffusion techniques for generating accurate samples.
- Cold diffusion [125] proposes algorithms for projecting data into random distributions with the support of reconstructing correction.

## Discrete Space
- Deep generative models have been successful in natural language processing, multimodal learning, and AI for science
- Processing discrete data such as sentences, residue, atom, and vector-quantized data is necessary for eliminating inductive bias
- So, based on the previous success, it seems that conducting relevant tasks with diffusion models is promising

## Text & Categorical
- D3PM promoted diffusion algorithm onto discrete space to deal with discrete data like sentences and images
- Cat(): Similar to D3PM, multi-nomial diffusion [134] and ARDM [135] extended the categorical diffusion into multi-nomial data for generating language text & segmentation map and Lossless Compression.

## Vector-Quantized
- Vector-quantized data is used to combine data from different fields into the codebook.
- This achieves great performance in autoregressive encoders.

## Constrained Space
- Graph-based neural networks can step over traditional data constraints.
- Constrained space extension methods are based on the Riemann manifold and graph.

## Manifold Space
- Most current data structures, such as images and video, are defined in a flat-geometry manifold (Euclidean space).
- However, there exists a series of data in the field of robotics [190], geoscience [191], and protein modeling [192] defined in Riemannian manifold [193], where current methods for Euclidean space cannot capture the high-dimensional Riemann feature.
- Thus, recent methods RDM [145], RGSM [144], and Boomerang [146] applied diffusion sampling into the Riemannian manifold based on score SDE framework [66].
- Besides, there are relevant theoretical works [76], [147] providing comprehensive support for manifold sampling.

## Graph
- Graph-based neural networks are becoming increasingly popular due to the high expressiveness in the human pose, molecules, and proteins
- In EDP-GNN, Pan et al. [151], and GraphGDP [149], graph data is processed through adjacency matrices for capturing the graph's permutation invariance.

## Likelihood Optimization
- Variational methods [183], [196] and diffusion methods [68] train models by the principle of variational evidence lower bound (ELBO)
- However, sometimes the log-likelihood still needs to be competitive because the variational gap between ELBO and log-likelihood is not minimized simultaneously.
- Thus, several methods [61], [119] directly focus on the likelihood optimization problem to solve this problem.
- And, the solutions can be classified into two classes -improved ELBO and variational gap optimization.

## Improved ELBO

## Score Connection
- Inspired by [197], [198], score connection methods provide a new connection between ELBO optimization and score matching, solving the likelihood optimization problems via improved score training.
- Score-flow [119] treats the forward KL divergence in ELBO as optimizing a score-matching loss with a weighted scheme.
- Huang et al. [129] treated Brownian motion as a latent variable to track the loglikelihood estimation explicitly, and it builds the bridge between the estimation and weighted score matching in the variational framework.
- Analytic-DPM [107] enhances ELBO by analyzing the KL Divergence and reverse covariance & mean.
- Similarly, NCSN++ [152] bridges the theoretical gap by introducing a truncation factor to ELBO.

## Re-Design
- Compared to loss transformation techniques, re-Design methods directly tighten the ELBO
- VDM [67] and DDPM++ [152] connect the advanced training objectives concerning signalto-noise ratio and truncate factors, respectively, optimizing ELBO via finding optimal factors
- Improved DDPM [61] and D3PM [65] propose hybrid loss functions based on ELBO with a weighted scheme for improving ELBO

## Variational Gap Optimization
- Variational gap optimization has been successful in the VAE field
- INDM is a variational gap optimization algorithm
- Apart from minimizing the variational gap, minimizing the energy is still one approach to maximize the loglikelihood

## Dimension Reduction
- Inferencing on the highdimensional dataset is extremely consuming.
- However, diffusion models enjoy high expressiveness from equaldimension transitions considering that dimension reduction may cause the information to be missing.
- Actually, diffusing on the low-dimensional manifold has wide applications in graph-based representations.
- Thankfully, reduceddimension diffusion can be achieved with the aid of latent projection and dimension projection techniques.

## Latent Projection
- Several mix-modeling methods project training data onto the latent space with a lower dimension by flow function and VAE-encoder
- LSGM [118], INDM [121], and PDM [120] follow the pattern to learn smoother models in a smaller space, triggering fewer network evaluations and faster sampling

## Dimension Projection
- Dimension projection aims to reduce spatial redundancy on image manifolds
- DVDP conducts subspace inference during perturbation and reconstruction
- Down-sampling and up-sampling steps can be seen as a mixture of DDPM and VAE

## APPLICATION
- diffusion models are widely used in computer vision
- diffusion models can generate realistic samples
- diffusion models can be used for a variety of purposes

## Computer vision
- Low-level vision CMDE empirically compared score-based diffusion methods in modeling conditional distributions of visual image data and introduced a multi-speed diffusion framework.
- By leveraging the controllable diffusion speed of the condition, CMDE outperformed the vanilla conditional denoising estimator [69] in terms of FID scores in in-painting and super-resolution tasks.
- DDRM [201] proposed an efficient, unsupervised posterior sampling method served for image restoration.
- Motivated by variational inference, DDRM demonstrated successful applications in super-resolution, deblurring, inpainting, and colorization of diffusion models.
- Palette [97] further developed a unified diffusion-based framework for low-level vision tasks such as colorization, inpainting, cropping, and restoration.
- With its simple and general idea, this work demonstrated the superior performance of diffusion models compared to GAN models.
- DiffC [202] proposed an unconditional generative approach that encoded and denoise corrupted pixels with a single diffusion model, which showed the potential of diffusion models in lossy image compression.
- SRDiff [29] exploited the diffusion-based single-image super-resolution model and showed competitive results.
- RePaint [203] was a free-form inpainting method that directly employed a pre-trained diffusion model as the generative prior and only replaced the reverse diffusion by sampling the unmasked regions using the given image information.
- Though there was no modification to the vanilla pre-trained diffusion model, this method was able to outperform autoregressive and GAN methods under extreme tasks.

## High-level vision
- FSDM was a few-shot generation framework based on conditional diffusion models
- Leveraging advances in vision transformers and diffusion models, FSDM can adapt quickly to various generative processes at test-time and performs well under few-shot generation with strong transfer capability
- Though approaching supervised learning from a conditional generation perspective and training with objectives indirectly related to the evaluation metrics, CARD presented a strong ability in uncertainty estimation with the help of diffusion models
- Motivated by CLIP [204], GLIDE [85] explored realistic image synthesis conditioned on the text and found that diffusion models with classifier-free guidance yielded high-quality images containing a wide range of learned knowledge
- DreamFusion [205] extends GLIDE's achievement into 3D space
- To obtain expressive generative models within a smooth and limited space, LSGM [118] built a diffusion model trained in the latent space with the help of a variational autoencoder framework
- SegDiff [206] extended diffusion models for performing image-level segmentation by summing up feature maps from a diffusionbased probabilistic encoder and an image feature encoder
- Video diffusion [26], on the other hand, extended diffusion models in the time axis and performed video-level generation by utilizing a typically designed reconstruction-guided conditional sampling method
- VQ-Diffusion [32] improved vanilla vector quantized diffusion by exploring classifierfree guidance sampling for discrete diffusion models and presenting a high-quality inference strategy
- Diff-SCM [209] built a deep structural model based on the generative diffusion model. It achieved counterfactual estimation by inferring latent variables with deterministic forward diffusion and intervening in the backward process

## 3D vision
- [33] was an early work on diffusion-based 3D vision tasks.
- [210] was a concurrent work on diffusionbased point cloud generation but performed unconditional generation without additional shape encoders, while a hybrid and point-voxel representation was employed for processing shapes.
- [34] proposed a paradigm for diffusionbased point cloud completion that applied a diffusion model to generate a coarse completion based on the partial observation and refined the generated output by another network.
- [35] introduced a neural network to estimate the score of the distribution and denoised point clouds by gradient ascent.

## Video modeling
- Video diffusion introduced the advances in diffusion-based generative models into the video domain.
- RVD employed diffusion models to generate a residual to a deterministic next-frame prediction conditioned on the context vector.
- FDM applied diffusion models to assist long video prediction and performed photo-realistic videos.
- MCVD proposed a conditional video diffusion framework for video prediction and interpolation based on masking frames in a blockwise manner.
- RaMViD extended image diffusion models to videos with 3D convolutional neural networks and designed a conditioning technique for video prediction, infilling, and upsampling.

## Medical application
- It is a natural choice to apply diffusion models to medical images.
- Score-MRI [38] proposed a diffusion-based framework to solve magnetic resonance imaging (MRI) reconstruction. [213] was a concurrent work but provided a more flexible framework that did not require a paired dataset for training.
- With a diffusion model trained on medical images, this work leveraged the physical measurement process and focused on sampling algorithms to create image samples that are consistent with the observed measurements and the estimated data prior.
- R2D2+ [214] combined diffusionbased MRI reconstruction and super-resolution into the same network for end-to-end high-quality medical image generation. [215] explored the application of the generative diffusion model to medical image segmentation and performed counterfactual diffusion.

## Sequential modeling

## Natural language processing
- Diffusion-LM took advantage of continuous diffusions to iteratively denoise noisy vectors into word vectors
- Bit Diffusion proposed a diffusion model for generating discrete data and was applied to image caption tasks.

## Time series
- CSDI used score-based diffusion models conditioned on observed data
- Inspired by masked language modeling, a self-supervised training procedure was developed
- SSSD further introduced structured state space models to capture long-term dependencies in time series data
- CSDE proposed a probabilistic framework to model stochastic dynamics and introduced Markov dynamic programming and multi-conditional forward-backward losses to generate complex time series

## Audio

## AI for science

## Molecular conformation generation
- ConfGF was an early work on diffusion-based molecular conformation generation models.
- While preserving rotation and translation equivariance, ConfGF generated samples by Langevin dynamics with physically inspired gradient fields.
- However, ConfGF only modeled local distances between the first-order, the second-order, and the third-order neighbors and thus failed to capture long-range interactions between non-bounded atoms.
- To tackle this challenge, DGSM [53] proposed to dynamically construct molecular graph structures between atoms based on their spatial proximity.
- GeoDiff [54] found that the model was fed with perturbed distance matrices during diffusion learning, which might violate mathematical constraints.
- Thus, GeoDiff introduced a roto-translational invariant Markov process to impose constraints on the density.
- EDM [21] further extended the above methods by incorporating discrete atom features and deriving the equations required for loglikelihood computation.
- Torsional diffusion [55] operated on the space of torsional angles and produced molecular conformations according to a diffusion process limited to the most flexible degrees of freedom.
- Based on previous geometric deep learning methods, DiffDock [271] conducts denoised score matching on transition, rotation, and torsion angle to generate drug conformation in protein-ligand complexes.

## Material design
- CDVAE explored the periodic structure of stable material generation
- Inspired by the recent success of antibody modeling, the recent work developed a diffusion-based generative model that explicitly targeted specific antigen structures and generated antibodies
- The proposed method jointly sampled antibody sequences and structures and iteratively generated candidates in the sequence-structure space
- ProteinSGM formulated protein design as an image inpainting problem and applied conditional diffusion-based generation to precisely model the protein structure
- DiffFolding generates protein backbone concentrating on internal angles by traditional DDPM idea

## CONCLUSIONS & DISCUSSIONS
- The diffusion model is important in a wide range of fields
- The review provides insights on attitudes and improved algorithms
- The survey serves as a guide for readers on diffusion model enhancement

## LIMITATIONS & FURTHER DIRECTIONS
- Most existing improvements and application algorithms are based on the original setting as DDPM.
- However, many aspects are ignored by researchers concerning the generalized setting of diffusionbased models.
- Further meaningful works that explore prior distribution, transition kernel, sampling algorithm, and diffusion schemes are expected.
- Diffusion models should be viewed as a class, but not a brunch of DDPM-based models.
- Training objective & evaluation metric:
- Most diffusionbased models set training objectives as evidence of lower bound (ELBO) of negative log-likelihood.
- However, we have no clear theory that ELBO and NLL are optimized simultaneously.
- Therefore, the inconsistency may lead to a hidden mismatch between the real goal and the practical refinement of designing.
- Consequently, further analytical approaches linking log-likelihood optimization to existing variables or creating novel training objectives consistent with the likelihood may guide a significant enhancement of the model's performance.
- Furthermore, current evaluation metrics like FID and IS scores cannot perfectly match the primary goals since data distributions are not equivariant to likelihood matching.
- The ideal evaluation metric should test the sample diversity as well as the recovery effect of diffusion models.
- A diversity score considering enough classes like CLIP [204] may be an available solution.
- A recovery score considering real-world data on the manifold for distribution distance will describe the model's generating ability more accurately and comprehensively.

## APPENDIX B EVALUATION METRIC B.1 Inception Score (IS)
- The inception score is built on valuing the diversity and resolution of generated images based on the ImageNet dataset
- It can be divided into two parts: diversity measurement and quality measurement
- Diversity measurement denoted by    is calculated w.r.t. the class entropy of generated samples: the larger the entropy is, the more diverse the samples will be.
- Quality measurement denoted by    is computed through the similarity between a sample and the related class images using entropy.
- It is because the samples will enjoy high resolution if they are closer to the specific class of images in the ImageNet dataset.

## B.2 Frechet Inception Distance (FID)
- The Inception Score is proposed to solve the bias from the specific reference datasets.
- The score shows the distance between real-world data distribution and the generated samples using the mean and the covariance.

## B.3 Negative Log Likelihood (NLL)
- Negative log-likelihood is a common evaluation metric
- Lots of works on normalizing flow field, VAE field, and diffusion models use NLL as one of the choices for evaluation

## APPENDIX C BENCHMARKS
- The benchmarks of landmark models along with improved techniques corresponding to FID score, Inception Score, and NLL are provided on diverse datasets which includes CIFAR-10 [290], ImageNet [207], and CelebA-64 [291].
- In addition, some dataset-based performances such as LSUN [292], FFHQ [293], and MINST [294] are not presented since there is much less experiment data.
- The selected performance are listed according to NFE in descending order to compare for easier access. [259] 2021 Audio Diffusion Text-to-sound Generation tasks [code] EdiTTS [47] 2022 Audio SDE fine-grained pitch, content editing [code] Guided-TTS [48] 2022 Audio SDE Conditional Speech Generation -Guided-TTS2 [49] 2022 Audio SDE Conditional Speech Generation -Levkovitch's Model [266] 2022 Audio SDE Spectrograms-Voice Generation [code] SpecGrad [50] 2022 Audio Diffusion Spectrograms-Voice Generation [code] ItoTTS [24] 2022 Audio SDE Spectrograms-Voice Generation -ProDiff [32] 2022 Audio Diffusion Text-to-Speech Synthesis [code] BinauralGrad [260] 2022 Audio Diffusion Binaural Audio Synthesis -AI For Science ConfGF [268] 2021 Molecular Score Conformation Generation [code] DGSM [53] 2022 Molecular Score Conformation Generation, Sidechain Generation -GeoDiff [54] 2022 Molecular Diffusion Conformation Generation [code] EDM [21] 2022 Molecular SDE Conformation Generation [code] Torsional Diff [55] 2022 Molecular Diffusion Molecular Generation [code] DiffDock [271] 2022 Molecular&protein Diffusion Conformation Generation, molecular docking [code] CDVAE [269] 2022 Protein Score Periodic Material Generation [code] Luo's Model [56] 2022 Protein Diffusion CDR Generation -Anand's Model [57] 2022 Protein Diffusion Protein Sequence and Structure Generation -ProteinSGM [270] 2022 Protein SDE de novo protein design -DiffFolding [275] 2022 Protein Diffusion Protein Inverse Folding [code]

## C.1 Benchmarks on CelebA-64
