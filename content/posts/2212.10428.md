---
title: "HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios"
date: 2022-12-20T17:06:32.000Z
author: "HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Hannah Schieber, Pengyuan Wang, Giulia Rizzoli, Hongcheng Zhao, Sven Damian Meier, Daniel Roth, Nassir Navab, Benjamin Busam"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10428v2.webp" # image path/url
    alt: "HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10428)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10428).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/housecat6d-a-large-scale-multi-modal-category).

# Abstract
- Estimating the 6D pose of objects is one of the major fields in 3D computer vision
- Since the promising outcomes from instance-level pose estimation, the research trends are heading towards category-level pose estimation for more practical application scenarios
- However, unlike well-established instance-level pose datasets, available category-level datasets lack annotation quality and provided pose quantity.
- We propose the new category level 6D pose dataset HouseCat6D featuring 1) Multi-modality of Polarimetric RGB+P and Depth, 2) Highly diverse 194 objects of 10 household object categories including 2 photometrically challenging categories, 3) High-quality pose annotation with an error range of only 1.35 mm to 1.74 mm, 4) 41 large scale scenes with extensive viewpoint coverage, 5) Checkerboard-free environment throughout the entire scene. We also provide benchmark results of state-of-the-art category-level pose estimation networks.

# Paper Content

## Introduction
- 6D pose estimation is one of the cornerstones in many computer vision tasks, especially for interactions like robotic grasping [51,54] or augmented reality [14].
- Most of the methods focus on instance-level where each network is trained and tested on a single object instance [34,48].
- However, generalization and applicability are limited, as the object mesh is required, and an individual network needs to be trained for each instance.
- Recent  --> 1k GNU GPL 3.0 LabelFusion [38] -12 138 BSD 3-Clause Toyota Light [22] -21 21 MIT YCB [4,52] -21 92 MIT Linemod [2,20] -15 15 CC BY 4.0 GraspNet-1Billion [15] -88 190 CC BY-NC-SA 4.0 T-LESS [21] -30 20 CC BY 4.0 HomebrewedDB [28] -33 13 CC0 1.0 Universal ITODD [12] () -28 800 CC BY-NC-SA 4.0 StereoOBJ-1M [32] ++++ ++++ +++ -18 183 MIT kPAM [37] 2 91 362 MIT CAMERA25 [49] () 6 42 30 MIT REAL275 [49] 6 42 13 MIT TOD [33] 3 20 10 CC BY 4.0 Wild6D [16] ()
- pose estimation [7,31,35,36,49] by training on multiple objects within one category. They can later generalize to unseen objects from the same category. However, a significant limitation blocking further progress is the lack of datasets for training and evaluation that fulfills all criteria like: large-scale, accurate and realistic.
- Existing categorylevel datasets only comply partly, e.g. high quantity and low quality [49], or high quality but insufficient quantity [50].
- To this end, we propose a new category-level dataset HouseCat6D. It consists of high-quality ground-truth annotations on greatly diverse objects acquired by multiple sensor modalities with extensive viewpoint coverage.
- Our dataset includes 194 objects from 10 different categories, including photometrically challenging classes such as glass and cutlery (Fig. 1), and 3 sensor modalities, i.e. RGB, depth, and polarimetric images, with a total of 23.5k frames and approx. 160k annotated object poses.
- Our dataset recording relies on an accurate external infrared tracking system and additional subsequent post-processing through sparse bundle adjustment to avoid errors induced by timestamp offsets and motion blur of the freely moving camera rig [41,42].
- Specifically, we conduct three calibrations, i.e. pivot calibration, timestamp calibration, and hand-eyecalibration.
- For the timestamp calibration, we adopt existing methods [13,24] adjusted to our setup with an ICPbased method.
- For the hand-eye-calibration, we improve the calibration from recent work [50], by aggregating multiple measurements of a ChArUcO [1] calibration board (cf. Sec. Compared to the recent PhoCal dataset [50] that relies on a robotic end-effector to estimate poses and thus has limited viewpoint coverage and backgrounds, our method provides accurate object pose annotation and wide viewpoint coverage while providing pose annotations with similar quality.
- We use active stereo as depth maps which is more reliable on different surface materials [26,50].
- In addition to the typical RGB and depth, we provide polarimetric images with four different filter angles. Recent investigations have shown that this modality is especially suitable for tasks such as depth and surface normal estimation [27,29,47], and 6D pose estimation [17], especially for photometrically challenging objects or surfaces.
- In summary, our main contributions are: (1) We propose HouseCat6D a large scale category-level object pose dataset comprising, which is multi-modal (RGBD + RGBP), comprising 194 high-quality 3D models of household objects including transparent and reflective objects in 41 sequences with broad viewpoint coverage and realistic scenarios without markers in the scene.
- We develop a novel pipeline for annotation, recording, and accuracy-dependent postprocessing to achieve comparable accuracy to a robotic GT [50], but with a handheld multi-camera rig to enable realistic camera trajectories.
- We detail the pose annotation pipeline with a commonly used external IR tracking system together with calibration details and its evaluation, which makes the high-quality 6D pose dataset annotation accessible to the community.
- ...

## Related Work
- There are two sub-fields of 6D pose estimation: Instance-level 6D pose estimation and category-level 6D pose estimation.
- The recent state-of-the-art methods are mostly data-driven approaches.
- A common need for these methods are datasets for both training and evaluation.

### Instance-level 6D Object Pose Dataset
- Early 6D pose datasets started from a single image
- LineMOD [20] and LM-Occlusion [2] are amongst the most popular and commonly used datasets, for instance, level pose estimation
- An RGBD camera is used to annotate the pose of the objects, and checkerboards are used to track the camera pose
- Although these two datasets served an essential role in the early time, they have limitations in terms of their annotation quality
- Later, HomebrewedDB [28] is released with 33 high-quality 3D objects scanned by a commercial handheld scanner
- Although HomebrewedDB [28] features better quality for object mesh and pose annotation, it lacks scene variety
- Only 13 scenes are provided, and checkerboards are included in the scenes
- A variety of pose datasets share similar principal [11,43]
- RGBD-based annotation is typically used with checkerboard-based camera localization, sometimes with different objects of interest [21]
- Other works use humanpowered annotation [40] or a rotating table [12] instead of checkerboards
- Lately, more datasets have provided image sequences with camera and object pose annotations
- With video sequences, the pose estimation problem can be simplified to pose tracking [3,18,30]
- One of the most widely used 6D pose datasets with video sequences is YCB [52]
- The authors annotate the pose by leveraging an RGBD cam-era and structure from motion (Label Fusion [38])
- While this makes large-scale annotation possible, the dataset suffers from depth-induced error [3,50]
- In comparison, the Laval 6DOF dataset [18] uses a different approach, which attaches small markers on the objects for tracking purposes while the camera is fixed
- This results in high-quality annotations without checkerboards
- However, marker-induced depth artifacts need a depth map post-correction
- On the other hand, StereoOBJ-1M [32] uses structure from motion in a more precise way together with the use of checkerboards to ensure quality and quantity
- This allows the dataset to be the largest available pose dataset in scale with high-quality annotation (cf. Tab. 2)

### Category-level Object Poses and Dataset
- Category-level pose estimation has been proposed to address generalizability in 6D pose estimation over multiple objects of the same category.
- Wang et al. [49] present NOCS, an approach, and dataset for category-level 6D pose and size estimation.
- In REAL275, the poses are aligned using checkerboards.
- For CAMERA25, ShapeNetCore [5] objects are placed in tabletop scenarios.
- To estimate the pose on NOCS, they adapt Mask R-CNN [19] to predict the normalized object coordinate space (NOCS) maps and instance mask.
- This prediction is combined with the depth image to lift the prediction to 3D.
- Several approaches have been proposed on NOCS data to tackle the challenge of 6D pose estimation (i.e. FS-Net [8] utilizes graph convolutions to encode the object shapes, and predicts the pose using a decoupled rotation mechanism. GPV-Pose [10] further improves FS-Net [8] by adding geometry awareness through a geometric consistency loss which optimizes between 3D bounding boxes, reconstruction, and poses.
- While they rely on bounding box predictions or semantic segmentation masks, Centersnap [25] is free of bounding boxes or segmentation masks and predicts in a single-stage Figure 3. Tracking System. ARTTRACK2 tracking system and sets of infrared marker bodies we used for our setup.
- Once at least four infrared spheres are detected from at least two cameras, the tracking system provides the pose of the marker body as transformation from tracker system base to marker body base.
- The 3D shape, 6D pose, and size of the objects in one network.
- A dataset focusing more on the robotic field is kPam which uses keypoints. Manuelli et al. [37] capture kPam using a similar approach as [38].
- They perform 3D reconstruction before manually labeling the keypoints on the 3D reconstruction.
- The dataset results in 117 training sequences and 245 testing sequences.
- While NOCS and kPAM contain solid objects, TOD [33] and PhoCal [50] specifically focus on either translucent or transparent and reflective objects. TOD [33] is captured with a robotic arm and annotated keypoints and focuses on stereo images. Wang et al. [50] introduce a category-level dataset including polarimetric images besides RGBD only.
- For annotation, a robotic arm is used to tip individual objects with a calibrated pointer. Annotations are refined via ICP.
- Instead of using a robotic arm, Wild6D [16] is annotated via tracking.
- Every 50th keyframes is annotated and then registered via TEASER++ [53] and colored ICP [39].
- The training dataset is label-free, and only the test dataset contains annotations.
- For recording, multiple iPhones are used to capture RGB images, depth, and point cloud.
- As a baseline, Fu et al. [16] introduce RePoNet, which estimates pose, NOCS map, and shape via pose and shape networks connected with a differentiable rendering module.

## Dataset
- The dataset has a size of 34 training scenes, 5 test scenes and 2 validation scenes.
- The dataset is composed of 194 objects from 10 household categories.
- The dataset features multiple modalities, namely RGB images, polarimetric images and depth maps.

### Objects Mesh Acquisition
- For our dataset, we choose 10 household categories to represent typical household scenarios.
- All objects are scanned with high quality EinScan-SP 3D Scanner (SHINING 3D Tech. Co., Ltd., Hangzhou, China).
- For the photometrically challenging categories, we use self-vanishing 3D scanning spray (AESUB Blue, Aesub, Recklinghausen, Germany) to enable scanning.

### Hardware
- External tracking system composed of four ARTTRACK2 cameras
- Average 0.67 mm / 0.12°error in the static case and 0.92 mm / 0.16°error in the dynamic tracking scenario
- Used tracking bodies for the annotation pipeline
- Evaluated the accuracy of the tracking setup as translation and rotational error [18]
- This results in an average 0.67 mm / 0.12°error in the static case and 0.92 mm / 0.16°error in the dynamic tracking scenario
- Used an external tracking signal provided by a Raspberry Pi

### Object Pose Annotation
- Annotating the 6D pose of the object is, without a doubt, the most crucial part of a 6D pose dataset.
- In our dataset, we adopt the highly accurate object pose annotation pipeline from [50] by replacing the robotic end-effector pose with an IR tracking body.
- This ensures reliable tracking quality while covering a more extensive working volume.
- The annotation step follows tool tip calibration, 3D points mea-surement of the objects, and point correspondence with ICP-based refinement.
- In this subsection, we describe the details of each step.
- Tip Calibration. The poses of the object meshes are annotated by measuring the 3D point using the tool tip.
- Thus, calibrating the location of the tip from the tracking body is essential to ensure the accuracy of the annotation.
- We use an NDI Active 4-Marker Planar Rigid Body (Northern Digital, Ontario Canada) as the measurement tip (Fig. 3 (b)).
- The tip is calibrated by fixing the tip while pivoting the tracking body and finding the optimal location of the point to minimize the variance of the fixed point (pivot calibration).
- The most common way to evaluate the quality of the pivot calibration is by measuring the variance of the fixed pivot point.
- We carefully calibrated with 18 points, with the final variance of the tip location of ε = 0.040 mm.
- Pose Annotation. After the tip is calibrated from the tracking body, it can measure accurate 3D points in the space in the world coordinates of the tracker system.
- We measure points for the initial point correspondence and ICP refinement as in [50] while covering around three times more points measurements with various surfaces of the object, thanks to the enlarged working space without the constraint given by using a robot arm [50].
- We evaluate the quality of the pose annotation step by simulating the pose annotation pipeline on randomly selected three objects with the addition of pivot calibration error (Sec. 3.3) and static tracking error (Sec. 3.2), which gives an average RMSE of 0.32 mm in translation and 0.43 • in rotation.

### Camera Trajectory Annotation
- The object poses are annotated from the center of the tracker system, not from the center of the individual camera.
- In order to obtain the 6D pose of the object from the camera center, the camera pose from the tracker system base has to be applied.
- There are multiple ways to perform Hand-Eye-Calibration, one of which is by detecting the checkerboard via camera while tracking the body of the camera from an external source.
- To solve the issue of accuracy, we propose a new hand-eye-calibration that takes into account multiple images captures.
- The RMSE for this calibration is measured as 0.27 mm for the translation and 0.42 • for the rotation.
- Another important aspect of using the external IR tracking system is the timestamp calibration between the tracking system and the camera image acquisition time.
- To tackle this issue, we use the RGB input to minimize the reprojection error with multiview images.
- We use structure from motion with given initial poses and carefully selected fixed frames.
- The initial poses are used for initial feature matching and structure reconstruction.
- The fixed frames are excluded in the later bundle adjustment stage.
- Our annotation quality is lower than robotic acquisition, however, we cover significantly more viewpoints and provide more accurate annotaitons than checkerboard-based datasets.

### Annotation Quality Evaluation
- For the evaluation of the annotation quality, we report the point-wise RMSE between objects and the camera center with and without the consideration of three systematic errors: tracking system error (Sec. 3.2), pose annotation error (Sec. 3.3) and hand-eye-calibration error (Sec. 3.4).
- In the upper bound, we report the number with the object annotation error and the static tracking error, assuming no synchronization error.
- In the lower bound, we include all tree mentioned systematic errors with dynamic tracking error as the tracking system error.
- Our method achieves low RMSE from 1.35 mm to 1.73 mm.

### Scene Statistics
- HouseCat6D has the most diverse number of instances and categories
- Compared to other category-level datasets, our dataset covers the most diverse number of instances and categories
- Class-wise evaluation of 3D IoU (at 25%/ at 50%) for NOCS [49], FS-Net [8], and GPV-Pose [10] on the test split of HouseCat6D

### Pose Coverage
- Figure 9: Qualitative baseline comparisons and NOCS map.
- Visualization of the baseline predictions on the test set of HouseCat6D for FS-Net [8], GPV-Pose [10] and NOCS [49] together with predicted NOCS map with GT NOCS map the object.
- They usually focus on the upper hemisphere, even for large-scale dataset variants like SterOBJ-1M [32]. In contrast, HouseCat6D provides very dense and welldistributed poses (cf. Fig. 7).

## Benchmark and Experiments
- RGB-D approaches like NOCS [49] are commonly used in 6D pose estimation
- RGB provides information about the object itself (the class), however, categories can show a high intra-class variance
- First, the RGB image is used to identify the object and depth is leveraged consecutively to learn more about the shape and object boundaries
- NOCS [49] predicts NOCS maps in 2D image space and utilizes depth information and ICP for 3D prediction
- GPV-Pose first segments the object in the RGB image and then applies back-projection of the segmented depth map to the 3D space where it predicts the pose using geometry-guided point-wise voting
- FS-Net [8] extracts the 3D point cloud from the depth image based on 2D RGB object detection
- From this, latent features are extracted using a residual network, and object size and translation are estimated
- For our experiment using FS-Net [8], we follow the implementation of Di et al. [10]

### Evaluation Pipeline & Results
- Our dataset consists of 34 training sequences, 2 validation sequences, and 5 test sequences.
- For the baseline results, we report the results on the test split. We report the intersection over union (IoU) result with a threshold of 25% and 50%.
- NOCS [49] results in a mean average precision (mAP) of 37.8 for 3D IoU at 25%.
- The geometry-guided approach GPV-Pose [10] gives 68.3% mAP for 3D IoU at 25%.
- FS-Net [8] achieves the best average results with an mAP of 69.4% for 3D IoU at 25%.
- Class-wise comparisons are listed in Tab. 3.
- Compared to the widely used NOCS [49] dataset, our data contains cluttered scenes which lead to occluded object parts and objects in close proximity to each other as illustrated in Fig. 9.
- Current category-level 6D pose estimation pipelines mostly consider cases without occlusion. This leaves room for improvement as can be seen by this first evaluation.

## Discussion and Conclusion
- We have introduced HouseCat6D, a large-scale 6D pose dataset acquired with a custom multi-modal camera rig with an external tracking system.
- With this setup, we solve issues of established datasets by providing realistic scenes without markers for which object poses are well distributed in realistic scenarios.
- Additionally, our annotation approach allows us to include photometrically challenging objects without texture and with translucent material.
- However, our dataset has a few drawbacks from the limitation of the tracking system. First, it requires a manual annotation that is labor-intensive and time-consuming. This makes scaling the magnitude of the dataset difficult. Second, our tracking system limits the scene recording to indoor scenes as the system does work under strong sunlight.

## Object Meshes and Orientation
- The HouseCat6D dataset features 194 highly diverse objects from 10 household object categories with different textures, sizes, and shapes.
- In this section, we show the meshes of the objects in each category and the descriptions of their orientation.
- Glass HouseCat6D aligns the symmetry axis with the y axis for the (partially) symmetric objects.
- Glass objects in our dataset are fully symmetric around y axis in accordance with [3] who also align y axis and symmetry axis.
- The x and z axes serve as any orthogonal axes around the y axis as exemplified in Fig. 1 (a).
- Bottle Unlike the glass objects, bottle objects in our dataset are sometimes not fully symmetric (i.e. frontal surface is wider than the side) as in Fig. 1 (b).
- In this case, we define the x axis perpendicular to the surface of larger area.
- Can Similar to the bottle objects, can objects in our dataset sometimes are not fully symmetric (i.e. some cans are more square and one side is wider than the other side) as shown in Fig. 1 (c).
- Like the bottle objects, we define the x axis perpendicular to the wider side.
- Tube Tube objects in our dataset are partially symmetric in shape, such that one side is round at the end while flat on the other side as shown in Fig. 1 (d).
- As in the can and bottle category, we define the x axis perpendicular to the wider side.
- Teapot In general, teapots have the shape of one (partially) symmetric body with a handle and tip where the liquid comes out. In our dataset, we use the y axis for the direction of the symmetric body and x axis for the direction from the handle to the tip as shown in Fig. 2 (a).
- Cup For the cup category, we only use cups with handles that have the shape of one symmetric body with a handle. Thus, similar to the Teapot category, we align the y axis to the direction of the symmetric body and x with the direction from the handle to the other side of the body as shown in Fig. 2
- Shoe Shoes, in general, have a long, flat and nonsymmetric shape. For this category, we use only the right
- Unlike the other categories, the sides of the box are rather defined by their texture. To allow networks to generalize in this category, we orient the meshes by their side length. We set y,x,z as direction of first, second and third longest side.
- All the objects are rendered in the same scale to highlight the size variance among the same category.

## External Tracking System Evaluation
- The paper discusses how they evaluated their IR-based external tracking system ARTTRACK2
- They first co-calibrated the robot and the tracking system
- They then ran an example trajectory to calculate the difference between the trajectory obtained from the robot and the tracking system for error evaluation
- They used a robotic arm to evaluate the quality of the tracking system

### Robot-Tracker Co-Calibration
- The first step to evaluate the tracking system with a robot is to co-calibrate the base of the robot and the tracking system.
- For this, we attach the calibrated IR tracking body on the robotic End-Effector (EE) as shown in Fig. 5 (a).
- We then acquire one trajectory from two different coordinate bases, one from the Robot base and the other one from the Tracker base.
- Similar to hand-eye calibration, we extract the static transformation between the two trajectories using the method of Horn [2].
- In this case, the static transformation matrix is the transformation between Tracker Base and Robot Base (marked red in Fig. 5 (a)).

### Trajectory Error Evaluation
- After co-calibration, we keep the tracking body on the robotic EE and make an evaluation trajectory that replicates the trajectory in one of the scenes.
- We repeat the trajectory twice, once with the robot stopping at every capturing position and once with the robot not stopping during the pose capture.
- The first trajectory serves as an evaluation for the tracking system accuracy in the static case, and the later trajectory serves as an evaluation in the dynamic case.
- As it is possible to obtain the pose of the tracking body from both, robot and tracking system, in the same coordinate frame using the co-calibration matrix, the error of the tracking system is calculated as the pose difference between the pose from the robotic arm and the pose from the tracking system.
- We measure an error of 0.67 mm / 0.12 • in the static case and 0.92 mm / 0.16 • in the dynamic case.

## Ablation Study
- We train FS-Net on data of our dataset HouseCat6D but with a similar data distribution as in PhoCal in terms of reduced number of scenes, trajectories and pose coverage.
- Tab. 1 summarizes the results from the reduced dataset against the result from the full dataset. The ablation confirms our motivation for the need to provide an extensive 6D object pose dataset with comprehensive pose coverage. Objects with a certain degree of overall symmetry in shape (like Box, Can or Glass) only suffer to a relatively small extent from the reduced pose coverage during training. In contrast, objects with a distinct non-symmetric feature, e.g. the handle of a cup or teapot, show a significant drop in accuracy, especially in the more constrained 3D50.
- Figure 2 shows the dataset acquisition pipeline. (a): Pre-scanning 3D models. (b): Pivot calibration to calibrate measurement tip from the tracking body. (c): Pose annotation of objects using measurement tip. (d): Hand-Eye-Calibration to calibrate camera center of tracking body. (e): Camera trajectory recording (f): Post-processing step to reduce synchronization-induced trajectory error.
