---
title: "StyleTRF: Stylizing Tensorial Radiance Fields"
date: 2022-12-19T09:50:05.000Z
author: "Rahul Goel, Sirikonda Dhawal, Saurabh Saini, P. J. Narayanan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09330v1.webp" # image path/url
    alt: "StyleTRF: Stylizing Tensorial Radiance Fields" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09330)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09330).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/styletrf-stylizing-tensorial-radiance-fields).

# Abstract
- Stylized view generation of scenes captured casually using a camera has received much attention recently
- The geometry and appearance of the scene are typically captured as neural point sets or neural radiance fields in the previous work
- An image stylization method is used to stylize the captured appearance by training its network jointly or iteratively with the structure capture network
- The state-of-the-art SNeRF method trains the NeRF and stylization network in an alternating manner. These methods have high training time and require joint optimization.
- In this work, we present StyleTRF, a compact, quick-to-optimize strategy for stylized view generation using TensoRF.
- The appearance part is fine-tuned using sparse stylized priors of a few views rendered using the TensoRF representation for a few iterations. Our method thus effectively decouples style-adaption from view capture and is much faster than the previous methods.

# Paper Content

## ABSTRACT
- Stylized view generation of scenes captured casually using a camera has received much attention recently
- The geometry and appearance of the scene are typically captured as neural point sets or neural radiance fields
- An image stylization method is used to stylize the captured appearance by training its network jointly or iteratively with the structure capture network
- The state-of-the-art SNeRF [29] method trains the NeRF and stylization network in an alternating manner.
- These methods have high training time and require joint optimization.
- In this work, we present StyleTRF, a compact, quick-to-optimize strategy for stylized view generation using TensoRF [6]
- The appearance part is finetuned using sparse stylized priors of a few views rendered using the TensoRF representation for a few iterations.
- Our method thus effectively decouples style-adaption from view capture and is much faster than the state-of-the-art methods.

## INTRODUCTION
- Stylizing entire 3D scenes has applications in the world of augmented reality (AR) and virtual reality (VR).
- Stylizing image content is a harder problem as it needs temporal consistency along with stylizing across the frames of the video.
- Combining stylization with new view generation takes the game one step further.
- Radiance Fields have become the dominant method of capturing a scene using a few images and then generating its views from other positions.
- NeRF [28] and its successors exploit the neural representations to encode the whole scene and render novel views at test time.
- They use a small MLP-based architecture to represent the radiance field (typically using 8 layers of 256 neurons), which has a small memory footprint of 5MB.
- They, however, need very high training time, ranging between 16hrs to 1 day per scene.
- Plenoxels [9] leverage the traditional voxel space to optimize for the feature vectors associated per voxel and rely on standard tri-linear interpolation to regress the radiance field vectors for the continuous volumetric space.
- The voxel-based approach benefits from the adaptive upsampling and pruning techniques, thereby reducing the optimization times to about 15-minutes.
- Plenoxels has a large memory footprint, often running into GBs.
- The recently introduced TensoRF method [6] uses the Tensor decomposition based on BTD and provides a way to reduce both memory and training times simultaneously.
- TensoRF can optimize a scene in 10-20 minutes while maintaining a memory footprint of just 5-10MB.
- Recent stylized new view generation methods use NeRF to represent the scene. They jointly or iteratively optimize NeRF with a stylization technique while rendering stylized new views [7,19,29].
- The latest method SNeRF proposed by Nguyen-Phuoc et al. [29] uses an iterative training strategy to optimize for each style using Gatys et al. [10].
- Adapting to a new style is thus a heavy process, taking days of processing.
- In this work, we build on and modify the ideas of SNeRF [29] and present a simple yet powerful method for stylized new view generation.
- Our method nearly decouples the style-adaption from view capture and makes it much faster than methods like SNeRF.

## RELATED WORK
- Stylizing images has been a well-studied problem in the vision community for a long time.
- The method proposed by Gatys et al. [10] optimizes white noise to match the content of one image while transferring the style from the other.
- Johnson et al. [20] proposed to use simple feed-forward architecture, which produces stylized content in real-time.
- While being quick at producing results, Johnson et al. [20] require a separate network for each style.
- Several works like [18,23,33] have addressed this issue of style-dependent training.
- More recently, Svoboda et al. [35] have been able to decompose an image into its style and content codes.
- While these works have provided a strong basis for stylizing 2D content, most of the aforementioned works do not faithfully stylize temporal data, let alone stylizing 3D content.
- Temporal Stylization of Videos:
- The stylization of temporaldata-like videos necessitate temporal-consistent stylization.
- To address these issues, various methods have tried incorporating temporal-consistency losses across frames, namely Ruder et al. [31].
- Further works like Huang et al. [16] concentrate on the real-time generation of stylization in addition to temporal-consistent stylization.
- Recent work in this line by Wang et al. [38] has relaxed the objective function to be optimized and derived a new regularization strategy.
- This allowed them to stylize video content for any arbitrary given style without any re-training or fine-tuning.
- Though these methods provide temporally-consistent stylization, they can not produce novel stylized views.
- This requires scene understanding at the geometric and as well as at the appearance levels.
- Consequently, temporal consistency methods proposed by video stylization works cannot be extended to novel view stylization.
- Stylizing Geometric Appearance:
- As discussed earlier, obtaining stylized novel views require an understanding of the geometric content of the scene.
- Methods like Cao et al. [5] have proposed stylizing geometry represented as 3D-point-cloud referenced on another point-cloud or image.
- However, stylizing point-cloud do not fully address the problem of novel view stylization as they are often accompanied by undesirable noise and missing surface information.
- These noise points and missing geometries can only be filled up using neural rendering-based frames works.
- Works like NPBG [1] used U-Net-based architecture to fill the missing gaps in geometry and alleviate the noisy data.
- Moving in this direction Huang et al. [17] introduced consistent 3D point cloud stylization through a learned linear transformation matrix to change the appearance.
- They build up on NPBG [1] to account for the sparsity and noisy nature of point clouds for the rendering of stylized novel views.
- Though the method proposed by Huang et al. [17] produces promising stylization of novel views, the geometric representation and appearance captured by the underlying U-Net-based pipelines are not accurate.
- Novel View Synthesis(NVS) using Implicit Neural Representation:
- The Novel View Synthesis(NVS) has been a challenging field that has been tackled for decades with different approaches like Lumigraph [13], Multi-Plane Imaging [37], and Light Field Fusion [27].
- Recently the field of NVS has taken a large leap forward due to the introduction of NeRF [28].
- NeRF(Neural Radiance Fields) proposed by Mildenhall et al. [28], learns the radiance fields using simple MLPs coupled with positionally encoded input features to regress radiance at novel viewpoints, given a sparse set...

## METHOD
- Given a set of posed images and a reference style image, our aim is to render stylized novel views of the scene.
- We achieve this by fine-tuning the appearance of a TensoRF.

## Preprocessing Stylization Module
- Uses stylization method presented by Johnson et al. [20], which produces stylized content in real time
- Takes 20 minutes for per-style training and produces 30 images/sec
- Uses Johnson et al. [20] as underlying stylization module over Gatys et al. [10], unlike others [29]
- Provides stable stylization across adjacent views, but is not temporally consistent

## Scene Representation using TensoRF
- Radiance fields encode geometry and radiance in their mapped representation
- TensoRF is a compact, accurate, and fast-to-optimize representation of Radiance Fields
- We use a specifically VM-48 variant of TensoRF which optimizes a scene within a timeframe of 15-20 minutes while maintaining a memory footprint of 10-15MB
- The TensoRF representation utilizes a Tensor decomposition known as VM-decomposition which in itself is a special case of BT-decomposition
- This scene optimization can be done independently for every scene irrespective of style
- In a later phase, we alter the appearance to adapt to the reference style in a short period of 40-50 seconds

## Stylizing TensoRF representation
- Optimizing radiance fields
- Rendering a sparse set of novel views
- Stylizing these novel renders with a pre-trained Stylization Module

## Stylizing
- We utilize the sparse set of stylized novel views generated using the per-style optimized Johnson et al. [20] module
- During the process of optimization, we ensure that the density terms are frozen, and only the appearance is altered
- We explicitly chose to freeze density as we have observed that stylizing looks pleasing and free from artifacts when density is kept frozen
- This fine-tuning only takes a nominal time of downwards of 40 secs.
- Once the fine-tuning is done, we obtain a geometric scene represented as Tensorial Radiance fields, which can be used to render stylized novel views with consistent appearance across the viewpoints.

## IMPLEMENTATION DETAILS 4.1 Optimizing TensoRF
- COLMAP is used to obtain camera poses for images
- TensoRF is optimized on input images for 15 iterations
- In each iteration, 4096 rays are shot into the voxel grid
- Radiance is obtained using volumetric rendering and optimized grid iteratively
- Adam optimizer is initialized to a learning rate of 0.02 and is re-initialized to 0.02 after upsampling
- The voxel grid is initialized with an effective resolution of 128 3 and iteratively upsampled every 1000 iterations
- Finally, an effective voxel-grid resolution of 640 3 is reached for real-world scenes and 300 3 for synthetic scenes

## Stylization
- We train multiple models for each desired style
- We use this to create stylized priors from the views generated in the previous phase
- While optimizing for the style we freeze the density parameters in the TensoRF representation and optimize only the appearance parameters for a small number of iterations
- This style adaption only takes a nominal time of 40-50 seconds

## EXPERIMENTS
- The various experiments with the proposed StyleTRF are conducted on a workstation PC equipped with an AMD Ryzen-5800x and an NVidia RTX-3090 GPU.
- In Sec. 5.1 we discuss the choice of stylization module used to stylize the sparse prior used in our approach. We also experimentally compare and contrast between temporal-stylization of smooth  trajectories of ground truth 3D content vs Actual 3D-Stylization.
- In Sec. 5.3 we show the effect of different optimization strategies used to stylize the 3D content.

## Stylization Module
- Johnson et al. [20] is a more reliable stylization method than Gatys et al. [10]
- Nguyen-Phuoc et al. [29] requires multiple epochs to reflect the style in appearance

## Video Stylization v/s 3D Stylization
- 3D content can be stylized by using a camera trajectory
- Temporally consistent stylization frameworks like ReReVST can be used to generate stylized novel views, but they don't capture the style information well
- StylizedNeRF captures better style than ReReVST

## Optimization Strategies for Style Adaptation
- For the stylization of Radiance fields, three strategies are presented: (1) S1: Optimizing a TensoRF from scratch directly using the stylized priors, (2) S2: Pre-optimizing a TensoRF on the original ground truth data and adapting for style using sparse stylized priors without freezing any parameters (both density and appearance), and (3) S3: Pre-optimizing a TensoRF on the original ground truth data and adapting for style using sparse stylized priors while freezing the density.
- When following the S1 approach, geometric artifacts are observed because the stylized priors generated may not share the exact geometry with the ground truth.
- In contrast, when following the S2 approach, most of the geometric prior is learned from the pre-optimization phase which uses groundtruth images to obtain scene properties, and the stylization is generally more appealing.
- However, when following the S3 approach, geometric components are frozen and the stylization is more crisp.

## RESULTS
- We compare our results against both 3D-stylization techniques and temporally-consistent stylization techniques.
- We find that our results are better than both 3D-stylization techniques and temporally-consistent stylization techniques.

## Qualitative Results
- Uses smooth trajectories which are similar to a video to compare with stylization techniques
- Similarity enables us to compare with temporally-consistent stylization techniques alongside 3D-Stylization
- For 3D stylized novel view synthesis, though there exists Huang et al. [17], they do not hold an accurate representation of geometry and rely on explicit geometry as input.
- Although other methods like Chiang et al. [7] do not rely on explicit geometry input, they suffer from patch border artifacts as discussed in [19] and Sec. 2.
- Hence, we compare our results with the latest 3D radiance field-based stylization method StylizedNeRF [19] and temporallyconsistent stylization technique of ReReVST Wang et al. [38], on similar lines as [19].
- Comparison amongst different stylization Techniques: We have observed that though the novel views generated using Styl-izedNeRF are reasonably consistent, the geometry after stylization in the case of StylizedNeRF has been greatly affected as seen in Fig. 5(c).
- It produces blurry geometry in the case of TRex stylized using udnie along with the missing greenish tints present in the udnie, and produces extremely noisy results for mediterranean applied on Fern .
- Contrary to this, our method distinctively transfers different colors to different parts of the scene as seen in TRex stylized using udnie and captures the complete color palette in mediterranean adapted onto Fern .
- Our method also consistently preserves geometry across all scenes and styles.
- The geometric noise in the StylizedNeRF can partly be attributed to the combined density and appearance encoded of a single MLP, which hinders the disentanglement of geometry and radiance as observed by [29].
- In the case of the temporal-consistent style transfer technique ReReVST, we have observed that although the method is robust to adapt to new styles on the fly and produces results in real-time, it lacks proper capture of style information.
- This can be observed Fig. 5(b), ReReVST fails to capture the prevalent blue tint in santamaria in the case of Flower , this could also be observed in the case of StylizedNeRF and vaguely captures the color palette of mediterranean in Fern .
- For udnie applied on TRex , it does not account for distinct colors present in the style and the resultant image contains an unappealing blend of colors throughout the image.

## Quantitative Results
- We generate a smooth trajectory similar to SNeRF
- The rendered views along the trajectory are used to evaluate the consistency
- We obtained better metrics than both temporal stylization [38] and implicit geometric stylization [19]
- We estimated the consistency by calculating the optical flow O between the non-stylized frame and the stylized frame

## CONCLUSIONS
- StyleTRF is a compact and quick-tooptimize stylization technique which can generate stylized novel views of a scene.
- We qualitatively and quantitatively compared our method with the previous stylization methods. Our qualitative results and quantitative metrics demonstrate that StyleTRF is consistent across the views, having stylized the underlying 3D representation.
- Concurrently, Zhang et al. [39] presented stylized novel view synthesis using voxel-based grids, partly similar to our method. But they chose PlenOxels [9] to represent the radiance fields. They focussed more on obtaining brush strokes while ours concentrates on geometric preserved fast-style adaptation.
