---
title: "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos"
date: 2022-12-14T07:21:45.000Z
author: "Hao-Wen Dong, Naoya Takahashi, Yuki Mitsufuji, Julian McAuley, Taylor Berg-Kirkpatrick"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07065v1.webp" # image path/url
    alt: "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07065)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07065).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/clipsep-learning-text-queried-sound).

# Abstract
- Recent years have seen progress beyond domain-specific sound separation for speech or music towards universal sound separation for arbitrary sounds.
- Prior work on universal sound separation has investigated separating a target sound out of an audio mixture given a text query. Such text-queried sound separation systems provide a natural and scalable interface for specifying arbitrary target sounds.
- However, supervised text-queried sound separation systems require costly labeled audio-text pairs for training. Moreover, the audio provided in existing datasets is often recorded in a controlled environment, causing a considerable generalization gap to noisy audio in the wild.
- In this work, we aim to approach text-queried universal sound separation by using only unlabeled data. We propose to leverage the visual modality as a bridge to learn the desired audio-textual correspondence. The proposed CLIPSep model first encodes the input query into a query vector using the contrastive language-image pretraining (CLIP) model, and the query vector is then used to condition an audio separation model to separate out the target sound. While the model is trained on image-audio pairs extracted from unlabeled videos, at test time we can instead query the model with text inputs in a zero-shot setting, thanks to the joint language-image embedding learned by the CLIP model.
- Further, videos in the wild often contain off-screen sounds and background noise that may hinder the model from learning the desired audio-textual correspondence. To address this problem, we further propose an approach called noise invariant training for training a query-based sound separation model on noisy data. Experimental results show that the proposed models successfully learn text-queried universal sound separation using only noisy unlabeled videos, even achieving competitive performance against a supervised model in some settings.

# Paper Content

## INTRODUCTION
- Humans can focus on to a specific sound in the environment and describe it using language
- In machine listening, selective listening is often cast as the problem of sound separation, which aims to separate sound sources from an audio mixture
- Prior work in psychophysics also suggests the intertwined cognition of vision and hearing
- Motivated by this observation, we aim to tackle text-queried sound separation using only unlabeled videos in the wild
- We propose a text-queried sound separation model called CLIPSep that leverages abundant unlabeled video data resources by utilizing the contrastive image-language pretraining (CLIP) model

## RELATED WORK
- Much prior work on sound separation focuses on separating sounds for a specific domain such as speech (Wang & Chen, 2018) or music (Takahashi & Mitsufuji, 2021;Mitsufuji et al., 2021).
- Recent advances in domain specific sound separation lead several attempts to generalize to arbitrary sound classes. Kavalerov et al. (2019) reported successful results on separating arbitrary sounds with a fixed number of sources by adopting the permutation invariant training (PIT) (Yu et al., 2017), which was originally proposed for speech separation. While this approach does not require labeled data for training, a post-selection process is required as we cannot not tell what sounds are included in each separated result.
- Follow-up work (Ochiai et al., 2020;Kong et al., 2020) addressed this issue by conditioning the separation model with a class label to specify the target sound in a supervised setting. However, these approaches still require labeled data for training, and the interface for selecting the target class becomes cumbersome when we need a large number of classes to handle open-domain data.
- Wisdom et al. (2020) later proposed an unsupervised method called mixture invariant training (MixIT) for learning sound separation on noisy data. MixIT is designed to separate all sources at a time and also requires a post-selection process such as using a pre-trained sound classifier (Scott et al., 2021), which requires labeled data for training, to identify the target sounds.
- We summarize and compare related work in Table 1.

## Method

## Query type
- Tzinis et al. introduced an unsupervised method called AudioScope for separating on-screen sounds using noisy videos based on the MixIT model.
- While image queries can serve as a natural interface for specifying the target sound in certain use cases, images of target sounds become unavailable in low-light conditions and for sounds from out-of-screen objects.
- Another line of research uses the audio modality to query acoustically similar sounds. Chen et al. showed that such approach can generalize to unseen sounds.
- Later, Gfeller et al. cropped two disjoint segments from single recording and used them as a query-target pair to train a sound separation model, assuming both segments contain the same sound source. However, in many cases, it is impractical to prepare a reference audio sample for the desired sound as the query.
- Most recently, text-queried sound separation has been studied as it provides a natural and scalable interface for specifying arbitrary target sounds as compared to systems that use a fixed set of class labels.
- Liu et al. employed a pretrained language model to encode the text query, and condition the model to separate the corresponding sounds.
- Kilgour et al. proposed a model that accepts audio or text queries in a hybrid manner. These approaches, however, require labeled text-audio paired data for training.
- Different from prior work, our goal is to learn text-queried sound separation for arbitrary sound without labeled data, specifically using unlabeled noisy videos in the wild.

## METHOD
- The CLIPSep model is a model for text-queried sound separation without using labeled data
- The CLIPSep-NIT model is a model for noise invariant text-queried sound separation
- The noise invariant loss is defined as the smallest loss achievable
- The noise regularization level is defined as the sum of the means of all the noise masks divided by the sum of the means of all the query masks
- The training objective of the CLIPSep-NIT model is a weighted sum of the noise invariant loss and regularization term

## EXPERIMENTS
- The paper uses code provided by Zhao et al.
- The implementation details can be found in Appendix C.

## EXPERIMENTS ON CLEAN DATA
- Evaluates the proposed CLIPSep model without the noise invariant training on musical instrument sound separation task
- The MUSIC dataset is a collection of 536 video recordings of people playing a musical instrument out of 11 instrument classes
- The CLIPSep model achieves a mean signal-to-distortion ratio (SDR) (Vincent et al., 2006) of 5.49 dB and a median SDR of 4.97 dB using text queries in a zero-shot modality transfer setting
- When using image queries, the performance of the CLIPSep model is comparable to that of the SOP model.

## EXPERIMENTS ON NOISY DATA
- Next, the proposed method is evaluated on a large-scale dataset
- The dataset is the VGGSound dataset, which contains more than 190,000 10-second videos in the wild
- The method is successful in learning text-queried sound separation even with noisy unlabeled data
- The method achieves 5.22 dB and 3.53 dB SDR improvements over the mixture on the MUSIC + and VGGSound-Clean + datasets, respectively

## EXAMINING THE EFFECTS OF THE NOISE REGULARIZATION LEVEL γ
- The proposed regularizer successfully keeps the total mean noise head activation close to the desired level, γ, for γ ≤ 0.5
- Interestingly, the total mean noise head activation is still around 0.5 when γ = 1.0

## DISCUSSIONS
- The proposed models do not require any labeled data for training
- The proposed model can comb multiple queries to extract multiple target sounds
- The proposed model is robust to different text queries and can extract the desired sounds

## CONCLUSION
- Uses text-queried universal sound separation model that can be trained on noisy unlabeled videos
- Uses contrastive imagelanguage pretraining to bridge the audio and text modalities
- Uses noise invariant training for training a query-based sound separation model on noisy data
- Shows that the proposed models can learn to separate an arbitrary sound specified by a text query out of a mixture, even achieving competitive performance against a fully supervised model in some settings
- Adopts a learning rate schedule with a warm-up-the learning rate starts from 0 and grows to 0.001 after 5,000 steps, and then it linearly drops to 0.0001 at 100,000 steps and keeps this value thereafter
- Validates the model every 10,000 steps using image queries as they don't assume labeled data is available for the validation set
- Uses a sampling rate of 16,000 Hz and works on audio clips of length 65,535 samples (≈ 4 seconds)
