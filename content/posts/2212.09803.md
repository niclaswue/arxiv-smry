---
title: "Training Trajectories of Language Models Across Scales"
date: 2022-12-19T19:16:29.000Z
author: "Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09803v1.webp" # image path/url
    alt: "Training Trajectories of Language Models Across Scales" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09803)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09803).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/training-trajectories-of-language-models).

# Abstract
- Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger.
- At a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior.
- Early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities.
- Perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.

# Paper Content

## Introduction
- Scaling up language models improves language modeling perplexity
- Language models of different sizes learn the same phenomena in the same order
- The overall model perplexity is a composite measure of which language phenomena have been learned

## Experimental Settings
- All experiments use OPT (Zhang et al., 2022), a collection of open-source autoregressive language models
- The models share the same tokenization and are trained on the same training data, covering a total of 300B Validation perplexity
- Validation perplexity (Valid PPL) follows a similar power-law pattern observed in previous scaling work (Kaplan et al., 2020;Hoffmann et al., 2022)

## Next-Token Prediction
- Autoregressive language models are trained to predict the next token given a context.
- Figure 1 shows that validation perplexity, aggregated over all positions, gradually declines as training progresses.
- However, it is not clear if all token instances evolve similarly to the aggregated measurement.
- In this section, we study the trajectory of next-token predictions, dividing them into three categories-stagnated, upward trend, or downward trend-to understand how language models gradually learn new language phenomena.

### Methodology
- To continue our search for such texts, we next devise a decoding approach that combines signals from two models and generates texts based on the interpolation of their distributions
- When λ 1 = 0, λ 2 = 1, it is simply decoding with the large model; when λ 1 = 1, λ 2 = −1, the decoding process favors the small model's prediction and suppresses the large model's prediction
- This is the configuration that decodes sequences that small models have a lower perplexity on than large models

### Analysis
- As shown in Figure 6, we confirm that the perplexity of texts generated with the p s − p t configuration presents an inverse scaling trend-perplexity increases as model size increases (column 1, 5).
- Other configurations either only show a modest upward trend (p s ), or a normal downward trend (p l and p l −p s ).
- Even though models of intermediate sizes (1.3B, 6.7B, 13B) are not involved in decoding, the scaling trend holds systematically across all model sizes.
- To further verify the universality of the phenomenon in other families of language models, we evaluate the generated texts with final GPT Neo checkpoints (Black et al., 2021), which were trained on the Pile dataset (Gao et al., 2020).
- As shown in Figure 7, the perplexity trend aligns with OPT models. This confirms that the texts generated with our approach are not  a result of model or data artifacts, but embody universal properties exhibiting a similar scaling trend in other model families.
- Perplexity trajectory of generated sequences.
- In the second row of Figure 6, we present the perplexity trajectory of texts generated with different configurations. We observe that texts generated based on p s − p l and, to a less extent, p s , largely differ from the other configurations: 125M checkpoints presents a downward trend, while other checkpoints present an upward trend.
- This might suggest that differently-sized models optimize in different directions for phenomena specific to these texts. However, taking a closer look, we observe that the 1.3B model also shows a downward trend at the beginning, which turns upward afterwards. This indicates that all models improve the perplexity of these texts at first but, with more training FLOPs, larger models shift away from this specific distribution where the 125M model stalls.
- In Appendix C.7, we further show that perplexity of the sequences decoded by contrasting the two models (p s −p l and p l −p s ) are less aligned with validation perplexity as other configurations.
- Generated examples.
- Table 1 presents examples generated with different configurations. We find that the generations from p s − p l are grammatically correct and carry actual meanings both for greedy search and nucleus sampling, but manifest other issues: 1) they entail highly-unlikely semantic usages such as Unfortunately, it wasn't all that great-an ending word with a negative sentiment should be more prevalent; 2) the nucleus sampling example, despite being fluent and consistent, hardly grounds to real world scenarios. This suggests that small models are highly capable linguistically, and learning at scale primarily focuses on acquiring other types of knowledge.

### Manual Design
- We hypothesize that injecting noise into human texts might reverse the scaling trend (i.e., perplexity on corrupted texts might increase as model size increases).
- We replace 20%, 40%, 60%, 80%, and 100% of the subwords in each sequence with random subwords.
- We evaluate corrupted datasets on the final model checkpoints and report the perplexity in Figure 5 (left).
- Contrary to our hypothesis, downward trends largely retain across all noise levels, even when the entire sequence consists of random tokens (100%). This can be explained by the copy-and-complete interpretation for in-context learning described in Olsson et al. ( 2022): larger models fare better at making predictions to follow the context distribution than smaller models, even when the context is pure noise.
- We next hypothesize that the perplexity of incorrect options for multiple-choice tasks might present an inverse scaling trend, as they are generally factually wrong.
- We present the perplexity of correct and incorrect options of 74 multiple-choice tasks from the BIG-Bench dataset in Figure 5.
- However, we find that the perplexity of correct and incorrect options decreases as the size of the model increases.

## Downstream Tasks
- Downstream tasks are evaluated on few-shot in-context learning
- context learning examples are for each test instance
- candidate for each evaluation example is the one with the highest probability normalized over its length

### Trajectory of ICL Performance
- From Figure 8 (left), we observe that under the same amount of training FLOPs, a smaller model consistently outperforms a larger counterpart except the 125M model.
- This suggests that OPT's training protocol might not be compute-optimal (Hoffmann et al., 2022) in terms of downstream performance, and larger models have potential for further improvements when more training FLOPs or data are available.
- From Figure 8 (right), we observe that the downstream task performance correlates well with validation perplexity for all model sizes. The curves overlap greatly in various model sizes, showing that if a small model and a large model are trained to the same perplexity, they achieve similar downstream task performance.

### Trajectory of Option Perplexity
- We select 12 tasks that present a linearity scaling pattern and 6 tasks that present a breakthroughness scaling pattern
- The performance of breakthroughness tasks increases tremendously as the validation perplexity drops below 8
- The perplexity gap between the correct and incorrect options also starts to expand at this point for the 30B and 175B models

## Related Work

## Conclusion
- Validation perplexity serves as a strong indicator of OPT models' behavior
- Large models, with more computation and a larger capacity, reproduce small models' behavior
- Some edge cases exist where models behave differently, sometimes even in opposite directions, such as the perplexity of the texts generated by contrasting two models
- With more open-sourced model checkpoints available, 17 studying training trajectories could be an accessible and effective way for interpreting language model behaviors
- The techniques we propose can be extended to analyze language models trained with different resources and procedures, and we also leave open questions for future work, such as understanding the double-descent phenomenon more in-depth
