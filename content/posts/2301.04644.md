---
title: "Does progress on ImageNet transfer to real-world datasets?"
date: 2023-01-11T18:55:53.000Z
author: "Alex Fang, Simon Kornblith, Ludwig Schmidt"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04644v1.webp" # image path/url
    alt: "Does progress on ImageNet transfer to real-world datasets?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04644)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04644).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/does-progress-on-imagenet-transfer-to-real).

# Abstract
- Does progress on ImageNet transfer to real-world datasets?
- We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets.
- In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models.
- On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements.
- For certain tasks, interventions such as data augmentation improve performance even when architectures do not.

# Paper Content

## INTRODUCTION
- ImageNet is a large, well-annotated dataset of images of objects
- The success of AlexNet in 2012 was a key factor in re-popularizing neural networks
- Ten years later, the ImageNet dataset is still a main benchmark for state-of-the-art computer vision models
- As a result of Ima-geNet's prominence, the machine learning community has invested tremendous effort into developing model architectures, training algorithms, and other methodological innovations
- Comparing methods on a common task has important benefits
- But the singular focus on ImageNet also raises the question whether the community is over-optimizing for this specific dataset
- For instance, it is possible that early methodological innovations transferred more broadly to other tasks, but later innovations have become less generalizable
- The goal of our paper is to investigate this possibility specifically for neural network architecture and their transfer to real-world data not commonly found on the Internet.

## RELATED WORK
- Transferability of ImageNet architectures is strongly correlated with downstream accuracy on a wide variety of web-scraped object-centric computer vision benchmark tasks.
- Later studies have investigated the relationship between ImageNet and transfer accuracy for self-supervised networks (Ericsson et al., 2021;Kotar et al., 2021;Nayman et al., 2022), adversarially trained networks (Salman et al., 2020), or networks trained with different loss functions (Kornblith et al., 2021), but still evaluate primarily on web-scraped benchmark tasks.
- Transferability of networks trained on other datasets is investigated by Abnar et al. (2022) who find that downstream accuracy saturates with up-stream accuracy.
- Other work has evaluated transferability of representations of networks trained on datasets beyond ImageNet. Most notably, Abnar et al. (2022) explore the relationship between upstream and downstream accuracy for models pretrained on JFT and ImageNet-21K and find that, on many tasks, downstream accuracy saturates with up-stream accuracy.
- In contrast, our work focuses on the fine-tuning aspect of transfer learning.

## DATASETS
- The dataset selection criteria were: diverse data sources, relevance to an application, and availability of well-tuned baseline models for comparison.
- The suite of target tasks includes: image recognition, text recognition, and natural language processing.
- The target tasks were chosen to be diverse and relevant to an application.

## SELECTION CRITERIA
- Many different datasets have been used to investigate the transfer of ImageNet architectures to other datasets
- The six datasets used in the study are diverse and have different application relevance
- The datasets were chosen to avoid datasets that were harvested from the web
- The datasets are available as well-tuned baselines for other target tasks

## DATASETS STUDIED

## MAIN EXPERIMENTS
- We run our experiments across 19 model architectures, including both CNNs and Vision Transformers (ViT and DeiT).
- They range from 57% to 83% ImageNet top-1 accuracy, allowing us to observe the relationship between ImageNet performance and target dataset performance.
- In order to get the best performance out of each architecture, we do extensive hyperparameter tuning over learning rate, weight decay, optimizer, and learning schedule.
- Details about our experiment setup can be found in Appendix C.
- We now present our results for each of the datasets we investigated.
- Figure 1 summarizes our results across all datasets, with additional statistics in Table 2.
- Appendix A contains complete results for all datasets across the hyperparameter grids.
- We see in Figure 1 (top-left) an overall positive trend between ImageNet performance and CCT-20 performance.
- The overall trend is unsurprising, given the number of animal classes present in ImageNet.
- But despite the drastic reduction in the number of classes when compared to ImageNet, CCT-20 has its own set of challenges.
- Animals are often pictured at difficult angles, and sometimes are not even visible in the image because a sequence of frames triggered by activity all have the same label.
- Despite these challenges, an even higher performing model still does better on this task -we train a CLIP ViT L/14-336px model (85.4% ImageNet top-1) with additional augmentation to achieve 83.4% accuracy on CCT-20.

## CALTECH CAMERA TRAPS

## APTOS 2019 BLINDNESS DETECTION
- Images are taken using fundus photography and vary in terms of clinic source, camera used, and time taken
- Images are labeled by clinicians on a scale of 0 to 4 for the severity of diabetic retinopathy
- Given the scaled nature of the labels, the competition uses quadratic weighted kappa (QWK) as the evaluation metric
- We create a local 80% to 20% random class-balanced train/validation split, as the competition test labels are hidden
- We find that models after VGG do not show significant improvement
- Similar to in CCT-20, DeiT and EfficientNets performs slightly worse, while deeper models from the same architecture slightly help performance
- We also find that accuracy has a similar trend as QWK, despite it being an inferior metric in the context of this dataset
- When performance stagnates, one might ask whether we have reached a performance limit for our class of models on the dataset
- To answer this question, we compare with the Kaggle leaderboard's top submissions. The top Kaggle submission achieves 0.936 QWK on the private leaderboard (85% of the test set) (Xu, 2019)
- They do this by using additional augmentation, using external data, training on L1-loss, replacing the final pooling layer with generalized mean pooling, and ensembling a variety of models trained with different input sizes.
- The external data consists of 88,702 images from the 2015 Diabetic Retinopathy Detection Kaggle competition.
- Even though performance saturates with architecture, we find that additional data augmentation and other interventions still improve accuracy.
- We submitted our ResNet-50 and ResNet-152 models with additional interventions, along with an Inception-ResNet v2 (Szegedy et al., 2017b) model with hyperparameter tuning.
- We find that increasing color and affine augmentation by itself can account for a 0.03 QWK point improvement.
- Once we train on 512 input size, additional augmentation, and additional data, our ResNet-50 and Inception-ResNet v2 both achieve 0.896 QWK on the private leaderboard, while ResNet-152 achieves 0.890 QWK, once again suggesting that better ImageNet architectures by themselves do not lead to increased performance on this task.
- As a comparison, the ensemble from the top leaderboard entry included a single model Inception-ResNet v2 trained with additional interventions that achieves 0.927 QWK.

## HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION
- The Human Protein Atlas runs the Human Protein Atlas Image Classification competition on Kaggle
- Images can contain multiple of the 28 different proteins, so the competition uses the macro F1 score
- Given the multi-label nature of the problem, this requires thresholding for prediction
- We use a 73% / 18% / 9% train / validation / test-validation split created by a previous competitor
- We report results on the validation split, as we find that the thresholds selected for the larger validation split generalize well to the smaller test-validation split
- We find a slightly positive trend between task performance and ImageNet performance, even when ignoring AlexNet and MobileNet
- These results suggest that models with more parameters help with downstream performance, especially for tasks that have a lot of room for improvement

## SIIM-ISIC MELANOMA CLASSIFICATION
- The Society for Imaging Informatics in Medicine (SIIM) and the International Skin Imaging Collaboration (ISIC) jointly ran this Kaggle competition for identifying Melanoma.
- Competitors use images of skin lesions to predict the probability that each observed image is malignant.
- Images come from the ISIC Archive, which is publicly available and contains images from a variety of countries.
- The competition provided 33,126 training images, plus an additional 25,331 images from previous competitions.
- We split the combined data into an 80% to 20% class-balanced and year-balanced train/validation split.
- Given the imbalanced nature of the data (8.8% positive), the competition uses area under ROC curve as the evaluation metric.
- We find only a weak positive correlation (0.44) between ImageNet performance and task performance, with a regression line with a normalized slope of close to zero (0.05).
- But if we instead look at classification accuracy, Appendix H shows that there is a stronger trend for transfer than that of area under ROC curve, as model task accuracy more closely follows the same order as ImageNet performance.
- This difference shows that characterizing the relationship between better ImageNet models and better transfer performance is reliant on the evaluation metric as well.

## CASSAVA LEAF DISEASE CLASSIFICATION
- The Makerere Artificial Intelligence Lab is an academic research group focused on applications that benefit the developing world
- Their goal in creating the Cassava Leaf Disease Classification Kaggle competition was to give farmers access to methods for diagnosing plant diseases
- Images were taken with an inexpensive camera and labeled by agricultural experts
- Each image was classified as healthy or as one of four different diseases
- We report results using a 80%/20% random class-balanced train/validation split of the provided training data
- Once we ignore models below 70% ImageNet accuracy, the relationship between the performance on the two datasets has both a weak positive correlation (0.12) and a near-zero normalized slope (0.02)

## ADDITIONAL STUDIES

## AUGMENTATION ABLATIONS
- In our main experiments, we keep augmentation simple to minimize confounding factors when comparing models.
- However, it is possible pre-training and fine-tuning with different combinations of augmentations may have different results.
- This is an important point because different architectures may have different inductive biases and often use different augmentation strategies at pre-training time.
- To investigate these effects, we run additional experiments on CCT-20 and APTOS to explore the effect of data augmentation on transfer.
- Specifically, we take ResNet-50 models pre-trained with standard crop and flip augmentation, AugMix (Hendrycks et al., 2020), andRandAugment (Cubuk et al., 2020), and then fine-tune on our default augmentation, AugMix, and RandAugment.
- We also study DeiT-tiny and Deit-small models by fine-tuning on the same three augmentations mentioned above.
- We choose to examine DeiT models because they are pre-trained using RandAugment and RandErasing (Zhong et al., 2020).
- Our experimental results are found in Appendix G.

## CLIP MODELS
- CLIP models from Radford et al. (2021) achieve high performance on a variety of downstream datasets
- We fine-tune CLIP models on each of our downstream datasets by linear probing then fine-tuning (LP-FT)
- Our results are visualized by the purple stars in Appendix I Figure 8

## DISCUSSION
- Alternative explanations for saturation are ruled out
- Comparison of datasets statistics suggests that the number of classes and dataset size do not explain the differences from Kornblith et al.
- VGG BN models appear to outperform their ImageNet accuracy on multiple datasets, but model size is not a good indicator of improved transfer performance

## Differences between web-scraped datasets and real-world images
- We find that the average accuracy on other real-world datasets explains significant variance beyond that explained by ImageNet accuracy.
- In Appendix N.2 we compare the Spearman rank correlation (i.e., the Pearson correlation between ranks) between each dataset and the accuracy averaged across the other real-world datasets to the Spearman correlation between each dataset and ImageNet. We find that the correlation with the average over real-world datasets is higher than the correlation with ImageNet and statistically significant for CCT-20, APTOS, HPA, and Cassava.
