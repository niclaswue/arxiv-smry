---
title: "SpatioTemporal Feature Integration and Model Fusion for Full Reference Video Quality Assessment"
date: 2018-04-13T07:42:33.000Z
author: "Christos G. Bampis, Zhi Li, Alan C. Bovik"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1804-04813v1.webp" # image path/url
    alt: "SpatioTemporal Feature Integration and Model Fusion for Full Reference Video Quality Assessment" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1804.04813)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1804.04813).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/spatiotemporal-feature-integration-and-model).

# Abstract
- Perceptual video quality assessment models are either frame-based or video-based
- Video-based approaches are time-consuming and harder to efficiently deploy
- To balance between high performance and computational efficiency, Netflix
- However, this fusion framework does not fully exploit temporal video quality
- Measures which are relevant to temporal video distortions. To this end, we
- Both algorithms exploit efficient temporal video features which are fed into a single or

# Paper Content

## I. INTRODUCTION
- Video traffic from content delivery networks is expected to rise to 71% by 2021
- There have been numerous approaches to the design of FR VQA algorithms
- Image-based approaches, such as exploiting C. G. Bampis and A. C. Bovik's work, exploit spatial information by capturing statistical and structural irregularities between distorted video frames and corresponding reference frames
- Video-based FR VQA models have also been studied in the literature
- In [11], the notion of spatio-temporal slices was derived and the "most apparent distortion principle" was applied to predict video quality
- Optical flow measurements were also used in [13], where video distortions were modeled by deviations between optical flow vectors
- A space-time Gabor filterbank was used in [14] to extract localized spatio-spectral information at multiple scales
- VM-VFD used a neural network trained with a large number of features such as edge features
- Data-driven models hold great promise for the VQA problem
- Netflix recently announced the Video Multimethod Fusion Approach (VMAF), which is an open-source, learning-based FR VQA model
- In [11], the notion of spatio-temporal slices was derived and the "most apparent distortion principle" was applied to predict video quality
- Optical flow measurements were also used in [13], where video distortions were modeled by deviations between optical flow vectors
- A space-time Gabor filterbank was used in [14] to extract localized spatio-spectral information at multiple scales
- VM-VFD used a neural network trained with a large number of features such as edge features
- Data-driven models hold great promise for the VQA problem
- The SpatioTemporal and Ensemble VMAF improvements proposed in this paper rely on statistical models of frame differences and hence avoid computationally expensive spatio-temporal filtering

## II. BACKGROUND ON VMAF

## A. How VMAF works
- VMAF extracts a number of quality metrics as features
- These features are weighted and averaged to produce a single feature value per video frame and feature type
- The six feature values are fed to an SVR model to produce an objective prediction of video quality

## B. VMAF Limitations and Advantages
- VMAF has been developed with a particular application context in mind, specifically compression and scaling artifacts
- For the Netflix use case, there are two main video impairments that are of interest: compression artifacts and scaling artifacts
- VMAF achieves good predictive performance by weighting the elementary video quality features
- Figure 2 illustrates an example of the performance gains afforded by VMAF fusion

## III. SPATIOTEMPORAL VMAF

## A. S-SpEED and T-SpEED features
- Statistical models of frame differences are used to extract temporal quality measurements
- Conditioning is applied to remove local correlations from band-pass filtered coefficients
- Entropy measurements and entropic differencing are correlated with human judgements of video quality
- The block entropies are weighted by a logarithmic factor and differenced to measure the statistical distance between GSM models
- The T-SpEED feature captures the information loss due to temporal video distortions, while the S-SpEED feature captures spatial quality degradations

## B. SpatioTemporal Feature Integration
- SpEED-QA does not account for the effects of film grain on the perceived visual quality
- The human visual system processes visual information in a multiscale fashion
- T-SpEED and T-VIF measure temporal information loss using the GSM statistical model
- To calculate the aggregate quality over an entire video sequence, we applied the hysteresis temporal pooling method

## IV. ENSEMBLE VMAF

## A. Why an Ensemble Model?
- In the previous section, we described a simple way to integrate strong temporal quality measurements into VMAF, by concatenating the spatial VMAF features with the T-VIF and T-SpEED features.
- However, in cases where the available subjective video data is limited, increasing the feature dimensionality (or using deep neural networks) may lead to overfitting.
- Video databases are usually pretty diverse in their design and contents and hence a particular feature subset may work well on one dataset, but not on another.
- For example, we have empirically observed that the ADM feature carries a large weight for spatial degradations, but does not generalize well on unseen data.
- One option is to carefully tune the regression model parameters to effectively regularize the predictions.
- Another alternative, which we have decided to follow here, is to consider fusion approaches.
- Model fusion (or ensemble learning) is a well-studied concept [50] which combines multiple individual learners.
- The main idea is to fuse multiple simple models that do not overfit, are easier to tune, and that complement each other towards reducing the prediction variance.
- Among other fusion possibilities, we experimented with training multiple SVRs on different video databases or training different regressors (e.g. a Random Forest and a SVR) on the same dataset.
- Nevertheless, we found that aligning predictions coming from models that were trained on subjective data collected under different experimental conditions and/or assumptions was a difficult proposition.
- We also found that the SVR predictions always outperformed Random Forest predictions and hence their combination was not beneficial.
- The performance merits of using an SVR for image and video quality assessment have also been demonstrated in [15], [24], [45].

## B. An Ensemble Approach to Video Quality Assessment
- We propose E-VMAF, an ensemble enhancement to VMAF, wherein multiple feature subsets are used to train diverse VQA models that are then aggregated to deliver a single prediction value.
- The VMAF feature set already captures the combined effects of compression and scaling (and the predominance of these distortions in practice), so we use the same features (6 features) for the first individual model, denoted by M 1 .
- To design M 2 , it is desirable to capture both spatial and temporal quality measurements, such that the individual predictions are accurate enough.
- Motivated by the perceptual relevance of the T-SpEED features used in ST-VMAF, we combined the 3 T-SpEED features with the 3 S-SpEED features calculated at the same scales (2, 3 and 4).
- The VIF features of M 1 and the S-SpEED features of M 2 both exploit the GSM model of high-quality video frames, but they also have some differences.
- The S-SpEED features are based on conditional entropies which are weighted by local variances, while VIF uses mutual information between reference and distorted image coefficients.
- Temporal quality measurements are complementary between the two models: T-SpEED of M 2 expresses temporal information loss by conditioning and applying temporal variance weighting, while the TI feature of M 1 measures motion changes as a proxy for temporal masking effects.
- Interestingly, we found that optimizing weighted averages of the individual predictions from M 1 and M 2 did not yield significant performance gains. This suggests that the prediction power of the two learners are at near-parity.

## V. THE VMAF+ SUBJECTIVE DATASET
- Data-driven approaches to VQA deeply depend on the training data that is used to train the regressor engines.
- We believe that a useful training dataset should include a diverse and realistic set of video contents and simulate diverse yet practical distortions of varying degradation levels.
- Collecting consistent subjective data has the potential to significantly increase the performance of data-driven VQA models on unseen data.
- To this end, we conducted a large-scale subjective study targeting multiple viewing devices and video streams afflicted by the most common distortions encountered in large geographicscale video streaming: compression and scaling artifacts.
- We first gathered 29 10-second video clips from Netflix TV shows and movies, from a variety of content categories, including, for example, drama, action, cartoon and anime.
- The source videos were of different resolutions, ranging from 480p up to 1080p, while the frame rates were 24, 25 or 30 frames per second.
- In our content selection, we also included darker scenes, which are particularly challenging for encoding and video quality algorithms.
- It should be noted that some of the source videos contain film grain noise. This allows us to gather valuable subjective data on videos that not only suffer from compression and scaling artifacts, but importantly, where there may be degradations of quality in the original source video.
- To describe content variation and encoding complexity, we employed an approach different from the usual SI-TI plots [51].
- We encoded all video contents using a fixed Constant Rate Factor (CRF) setting of 23, then measured the bitrate of each video file.
- Figure 6 shows that the video contents span a large range of encoding complexities, from less than 1Mbps up to around 19 Mbps.
- In streaming applications, the source video is usually divided into smaller chunks (e.g. of 2 seconds each) and stored in multiple representations, where each representation is defined by a specific pair of an encoding resolution and bitrate level.
- To generate the distorted videos, we downsampled each source video to six different encoding resolutions: 320x240, 384x288, 512x384, 720x480, 1280x720 and 1920x1080, then encoded them using the H.264 codec using three different CRF values: 22, 25 and 28, thereby yielding 18 distorted videos per content.
- For display purposes, all of the videos were upscaled to 1920x1080 display resolution.
- Both the downscaling and upscaling operations were performed using a lanczos filter.
- Due to copyright restrictions, the videos cannot be made publicly available.
- To avoid subjective fatigue, we employed a content selection scheme, where each subject only viewed a subset of all video contents.
- To avoid any memory biases, we ensured that video contents were displayed in a random order such that no video content was consecutively displayed.
- Overall, we gathered more than 6600 scores from 55 subjects on a laptop viewing device.
- When training our models, we applied standard subject rejection protocols [52] on the collected human opinion scores.
- Figure 7 shows the distribution of the raw subjective data. It can be seen that the scores widely cover the subjective range.

## VI. EXPERIMENTAL ANALYSIS
- SROCC measures the monotonic relationship between the objective predictions and the ground truth data
- PLCC measures the degree of linearity between the two
- Before computing PLCC, a non-linear logistic fitting was applied to the objective scores
- We evaluated the proposed approaches against a number of popular FR (and RR) VQA models
- We found this simple parameter selection scheme to work very well for E-VMAF

## A. Video Quality Prediction
- Cross-database results were used to evaluate the performance of video quality prediction models
- VMAF+ subjective dataset was used for training and testing
- SROCC and PLCC were computed and averaged over all tested databases
- ST-VMAF and E-VMAF performed well on most databases, while VMAF+ did not
- The proposed models have excellent generalization capabilities

## B. Monotonicity Analysis
- E-VMAF does not require an increased feature dimensionality as ST-VMAF does, and hence, is less likely to overfit.
- To demonstrate the effects of overfitting, we studied the prediction consistency of ST-VMAF and E-VMAF when the encoding resolution and the compression ratio were varied.
- Given any two compression levels c 1 ≤ c 2 , a consistent objective model should satisfy o c1 ≤ o c2 , where o c1 and o c2 are the respective calculated quality predictions.
- Likewise, for any two encoding resolutions r 1 ≤ r 2 , then o r1 ≤ o r2 should also be satisfied.
- A VQA model should satisfy these two "monotonicity" properties.
- If a trained model is not monotonic with compression and resolution, it may be an indication of overfitting.

## C. JND Prediction
- The USC-JND dataset was designed specifically for JND detection
- Both ST-VMAF and E-VMAF outperformed other powerful VQA models, including ST-RRED and VMAF
- JND prediction performance was evaluated using the USC-JND dataset

## D. QoE Prediction
- The predominant video impairments that occur during video streaming are compression, spatial scaling artifacts, and rebuffering events.
- We studied the behavior of the ST-VMAF and E-VMAF models on the recently released LIVE-NFLX Video QoE Database [60], which simulates realistic buffer and network constraints, and contains rebuffering events, rate adaptations and constant bitrate encodes.
- None of the considered FR-VQA models performed particularly well, since they do not model the effects of rebuffering.
- This suggests that more sophisticated QoE predictors (than just VQA algorithms) are required for the more general problem of QoE assessment.
- However, both of the new models achieved better performance than all the other models, especially in terms of PLCC 4 .
- We also examined the potential of incorporating the ST-VMAF and E-VMAF VQA models into an existing continuous-time QoE predictor.
- We tested the revised QoE predictor using the LIVE-HTTP Video QoE Database [61] which studies the effects of HTTP-based rate adaptation on 5 min. long HD video sequences.
- Table V reports the outcomes of the experiments when using the NARX QoE predictor [7], which has demonstrated promising results on the few available video QoE databases.
- First, we split the database into content independent train-test splits, then determined the best NARX configuration on the training set.
- Next, we tested the selected parameter setting on the test videos using a number of leading VQA models as integral components of the NARX QoE predictor.
- For evaluation purposes, we reported the SROCC and root mean squared error (RMSE) values between the continuous QoE predictions and the continuous ground truth data.
- It can be seen that ST-VMAF outperformed all of the other VQA models when used in this way, suggesting that it is an excellent choice for inclusion in future perceptuallydriven online QoE prediction systems.

## E. Observations and Takeaways
- Both ST-VMAF and E-VMAF performed very well for video quality and JND prediction
- E-VMAF was slightly better in terms of PLCC for JND prediction and ST-VMAF was a bit better in terms of SROCC in the LIVE-NFLX experiment
- The main benefit of using E-VMAF is that it better preserves monotonicity at very high compressions
- Also, it is easier to tune, since using the same SVR parameters as VMAF yielded excellent results
- By contrast, to train ST-VMAF, its larger number of features (compared to the VMAF baseline) had to be regularized using more careful SVR tuning.

## VII. FUTURE WORK
- We developed two high-performing data-driven video quality assessment models
- In the future, we plan to improve those models by combining NR source VQA measurements with the FR system
- To do so, we also plan to develop better data-driven NR video quality models that can be used in lieu of existing NR VQA approaches

## VIII. ACKNOWLEDGEMENTS
- The authors would like to acknowledge Anush K. Moorthy, Ioannis Katsavounidis and Anne Aaron for their support.
- The authors would like to thank the anonymous reviewers for their feedback.
- The authors would like to thank the anonymous reviewers for their feedback.
- The authors would like to thank the anonymous reviewers for their feedback.

## IX. APPENDIX

## A. Cross-database performance for ST-VMAF and E-VMAF
- The proposed models rely on three components: the VMAF+ training subjective data, the spatiotemporal feature integration and the temporal pooling step.
- In this section, we investigate the effects on the predictive performance of ST-VMAF and E-VMAF when each of these components varies.
- First, we investigated the effects on the predictive performance of ST-VMAF when trained on other databases. It was found that the VMAF+ dataset proved to be highly consistent, and served as an excellent training dataset for ST-VMAF.
- Similarly, the aggregate SROCC values (for a fixed training dataset) achieved by ST-VMAF were very close to, or significantly exceeded 0.8.
- Having established that training on VMAF+ is the best option, we studied how the performance of ST-VMAF and E-VMAF compares to that of the individual models M 1 and M 2 . It was found that M 1 and M 2 (Section IV) deliver similar performances, but afford significant performance gains when combined using E-VMAF.
- Similarly, ST-VMAF combines some features that may also belong to either M 1 or M 2 , but their combination performs significantly better.
- Hysteresis pooling further improves the predictive performance of both ST-VMAF and E-VMAF.

## B. Computational Analysis for ST-VMAF and E-VMAF
- To deploy VQA models for video quality prediction at global scale, ensuring low time complexity is a critical requirement
- For our analysis, we selected videos from 6 different resolutions ranging from CIF (352x288) up to Full HD (1920x1080)
- All of the compute time analysis was carried out on a 16.04 Ubuntu LTS Intel i7-4790@3.60GHz system
- ST-MAD required the most compute time, followed by ST-RRED and VQM-VFD. When implementing ST-MAD and VQM-VFD, we encountered out-of-memory issues on long Full HD videos.
