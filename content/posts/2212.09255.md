---
title: "Multi hash embeddings in spaCy"
date: 2022-12-19T06:03:04.000Z
author: "Lester James Miranda, Ákos Kádár, Adriane Boyd, Sofie Van Landeghem, Anders Søgaard, Matthew Honnibal"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09255v1.webp" # image path/url
    alt: "Multi hash embeddings in spaCy" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09255)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09255).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multi-hash-embeddings-in-spacy).

# Abstract
- Distributed representation of symbols is one of the key technologies in machine learning
- Traditional word embeddings associate a separate vector with each word, which requires a lot of
- To reduce the memory footprint, the default embedding layer in spaCy is a hash embeddings layer, which
- Hash embeddings represent each word as a summary of the normalized word form, subword
- In this technical report, we lay out a bit of history and introduce the embedding methods in spaCy in
- Second, we critically evaluate the hash embedding architecture with multi-embeddings on Named Entity

# Paper Content

## INTRODUCTION
- spaCy is a popular suite of Natural Language Processing software
- It provides a selection of well-tuned algorithms and models for common NLP tasks, along with well optimized data structures
- The library also pays careful attention to stability, usability and documentation
- Early versions of spaCy assumed that users would mostly use the default models and architectures, and did not offer a fine-grained API for to customize and control training
- This changed with the release of v3, which introduced a new configuration system and also came with a newly designed and documented machine learning system, Thinc
- Together these give users full control of the modelling details
- Over the years spaCy has developed its own model architectures and default hyperparameters, informed by practical considerations that are not only motivated by scoring well on a single standard benchmark
- In particular, spaCy prioritizes run-time efficiency on CPU, the ability to run efficiently on long documents, robustness to domain-shift, and the ability to fine-tune the model after training, without access to the original training data
- In this technical report, we focus on one of the more unusual features of spaCy's default model architectures: its strategy for embedding lexical items
- We explain how this layer operates, provide experiments that show its sensitivity to key hyperparameters, and compare it to a more standard embedding architecture on a few datasets
- Our experiments focus on how the embedding layer functions in the context of spaCy's default Named Entity Recognition architecture
- We these experiments aim to provide useful background information to users who wish to customize, change or extend spaCy's default architectures in their own experiments

## EMBEDDING LAYERS
- Language processing deals with inputs that are made up of discrete symbols
- A word embedding layer ε is a mapping from entries from a dictionary to vectors
- Task-specific embedding functions are often learned in an end-to-end fashion, in which case ε adapts the representation of words to the downstream task at hand while preserving relevant linguistic regularities
- General-purpose embeddings, on the other hand, are trained in a fully self-supervised fashion, typically to predict words from contexts or with a contrastive loss
- The simplest representation of input tokens is to encode the identity of each symbol, and map each s ∈ Σ in our vocabulary Σ to a binary one-hot vector b ∈ {0, 1} |Σ|
- Such a representation disregards any information about how symbols are used, and its dimensionality grows linearly with the number of input symbols
- It has become the de facto standard in NLP to define a fixed width embedding matrix E ∈ R |Σ|×d where width d is a hyperparameter, and embed each symbol as E T b.

## Algorithm 2 Create embedding table
- The procedure EMBED(s, map) returns an embedding for each word in the input text.
- The embedding for a word is a vector that represents the probability that that word will be found in the text.
- The procedure EMBED(s, map) uses a threshold to determine the minimum frequency of a token that is worth embedding.
- Another popular criterion is to fix the size k of the embedding table E and only learn vectors for the top-k most frequent symbols.

## HASH EMBEDDING LAYER
- Hash embeddings reduce the memory footprint by applying the hashing trick
- This method is inspired by Bloom filters, a simple probabilistic data structure to solve the membership problem
- Bloom filters only have two operations: inserting an element and testing whether an element has already been inserted
- They represent a large set by a compact bit vector, where we choose to be small |S|
- Before an element is inserted, it is hashed by k number of uniform hash functions
- Insertion of s is performed by hashing it k times and setting the corresponding bits in b to 1
- Testing whether s is in the filter is done by performing the hashing and looking up the corresponding values
- If all values are 1 then the element may exist in the filter otherwise it definitely does not exist
- Similarly, hash embeddings are parametrized by the number of rows, the width, and by the number of mutually independent hash functions
- Each incoming symbol is hashed k times and modded into the embedding table
- Given a list of hash functions, the embedding of the symbol s is:
- Algorithm 3

## COLLISIONS

## MULTI-EMBEDDINGS WITH ORTHOGRAPHIC FEATURES
- Tokenizer extracts features from a token's orthographic representation
- These features are: NORM (lowercased token with additional normalizations), currency symbols, punctuation, and alternate spellings
- The Tokenizer is implemented as a component-wise maximum over multiple linear layers
- In spaCy, we use three pieces for the multi-embedding layers: a standard embedding layer, MultiHashEmbed, which combines the multi-embedding process described here with hash embeddings, and MultiEmbed, which is identical to MultiHashEmbed except that it does not use the hashing trick

## EXPERIMENTAL SETUP
- We used a variety of named entity recognition datasets from multiple domains
- We tested on a variety of named entity recognition datasets from multiple domains
- For word embeddings we used the vectors distributed with spaCy 3.4.3 (large) models.
- We include a separate evaluation for unseen test entities.

## MODEL ARCHITECTURE AND TRAINING DETAILS
- Named entity recognizer model is transition-based
- Uses BILUO sequence encoding scheme to determine whether tokens are at the beginning (Begin), in the middle (In) or at the end (Last) of an entity
- Based on the current word and the words from the current entity in the stack if one exists
- Uses a single-layer Maxout network to compute a state vector and another feed-forward network with a Softmax activation to compute the action probabilities

## RESULTS
- MultiHashEmbed outperforms the state-of-the-art in terms of F1-score
- MultiHashEmbed is faster than the state-of-the-art for all benchmarking scenarios
- MultiHashEmbed is more efficient than the state-of-the-art

## COMPARING MU L T IEM B E D AND MU L T IHA S HEM B E D EMBEDDING STRATEGIES
- Pretrained embeddings improve performance
- The benefits are most pronounced for smaller datasets
- Unseen entities are handled similarly by both methods

## NUMBER OF ROWS
- The main motivation behind hash embeddings is to use a small amount of vectors and still achieve good performance.
- We have already seen that on OntoNotes: MultiHashEmbed performs comparably to MultiEmbed even if it only has half of the number of vectors for NORM.
- Here we go one step further and test MultiHashEmbed, when using only 20% or 10% of the number of vectors available to MultiEmbed.
- Table 1 shows the results on the development sets of the Spanish CoNLL and Dutch Archeology datasets without using pretrained vectors.
- On the Spanish CoNLL dataset we find no degradation of performance even when using as little as 10% of the number of rows as MultiHashEmbed.
- On the Dutch Archeology dataset using 20% of the rows available to MultiHashEmbed causes a 1% drop in performance, while using 10% leads to a 3% drop.

## ORTHOGRAPHIC FEATURES
- spaCy's default NORM, PREFIX, SUFFIX and SHAPE features can degrade performance
- ORTH performs the worst overall
- removing any of the features decreases error, but ORTH is the worst when engaging with a more fine-grained analysis

## NUMBER OF HASH FUNCTIONS
- The number of rows in the MultiHashEmbed tables can be used to control the capacity of the embedding layer
- In spaCy, this is currently fixed to four, but to critically evaluate this choice we tried the model with one, two, three and four hash functions
- The results in Figure 5 indicate that the performance does not vary too much on most datasets
- To understand the findings better, Table 4 shows the number of unique NORM, PREFIX, SUFFIX and SHAPE features found for each dataset
- First, note that having 2500 rows for the PREFIX feature is too large, especially on datasets with Latin scripts
- In terms of collisions, we did not see significant improvements from using multiple hash functions even if for example the CoNLL datasets contain five times the number of the default 5000 rows for NORM
- However, we do see a different pattern for the larger OntoNotes dataset. Here, we see a 5% increase in F1-score from using three hash functions as opposed to one, but we do not observe further improvements when using four

## DISCUSSION AND CONCLUSION
