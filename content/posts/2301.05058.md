---
title: "Sparse Coding in a Dual Memory System for Lifelong Learning"
date: 2022-12-28T12:56:15.000Z
author: "Fahad Sarfraz, Elahe Arani, Bahram Zonooz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-05058v1.webp" # image path/url
    alt: "Sparse Coding in a Dual Memory System for Lifelong Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.05058)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.05058).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/sparse-coding-in-a-dual-memory-system-for).

# Abstract
- Efficient continual learning is enabled by a rich set of neurophysiological mechanisms
- The brain encodes information in non-overlapping sparse codes, which facilitates the learning of new associations faster
- To mimic sparse coding in DNNs, we enforce activation sparsity along with a dropout mechanism
- This provides us with an efficient mechanism for balancing the reusability and interference of features, depending on the similarity of classes across tasks.

# Paper Content

## Introduction
- The ability to continually acquire, consolidate, and retain knowledge is a hallmark of intelligence.
- Standard DNNs are not designed for lifelong learning and exhibit catastrophic forgetting of previously learned knowledge (McCloskey and Cohen 1989) when required to learn tasks sequentially from a stream of data.
- The core challenge in continual learning (CL) in DNNs is to maintain an optimal balance between plasticity and the stability of the model.
- One approach to address this is Rehearsal-based methods, which aim to reduce forgetting by continual rehearsal of previously seen tasks.
- However, due to the limited buffer size, it is difficult to approximate the joint distribution with the samples alone.
- There is an inherent imbalance between the samples of previous tasks and the current task.
- This results in the network update being biased towards the current task, leading to forgetting and recency bias in predictions.
- More information from the previous state of the model is needed to better approximate the joint distribution and constrain the update of the model to preserve the learned knowledge.
- However, it is still an open question what the optimal information is for replay and how to extract and preserve it.
- The human brain provides an existence proof for successful CL in complex dynamic environments without intransigence or forgetting.
- Therefore, it can provide insight into the design principles and mechanisms that can enable CL in DNNs.
- The human brain maintains a delicate balance between stability and plasticity through a complex set of neurophysiological mechanisms.
- In particular, evidence suggests that the brain employs Sparse Coding, that the neural code is characterized by strong activations of a relatively small set of neurons.
- The efficient utilization of sparsity for information representation enables new associations to be learned faster with controlled interference with previous associations while maintaining sufficient representation capacity.
- Furthermore, complementary learning systems (CLS) theory posits that effective learning requires two complementary learning systems.
- The hippocampus rapidly encodes episodic information into non-overlapping representations, which are then gradually consolidated into the structural knowledge representation in the neocortex through the replay of neural activities.
- Inspired by these mechanisms in the brain, we hypothesize that employing a mechanism to encourage sparse coding in DNNs and mimic the interplay of multiple memory systems can be effective in maintaining a balance between stability and plasticity.
- The proposed semantic dropout provides us with an efficient mechanism to balance the reusability and interference of features depending on the similarity of classes across tasks.
- Furthermore, we maintain additional long-term semantic memory that aggregates the information encoded in the synaptic weights of the working memory.
- Long-term memory interacts with episodic memory to retrieve structural knowledge from previous tasks and facilitates information consolidation by enforcing consistency in functional space.
- Our empirical evaluation on challenging CL settings and characteristic analysis show that equipping the model with these biologically inspired mechanisms can further mitigate forgetting and effectively consolidate information across the tasks.

## Related Work
- Regularization-based methods regularize the update of the model in the parameter space (Farajtabar et al. 2020;Kirkpatrick et al. 2017;Ritter et al. 2018;Zenke et al. 2017) or the functional space (Rannen et al. 2017;Li and Hoiem 2017);
- Dynamic architecture expands the network to dedicate a distinct set of parameters to each task, and Rehearsal-based methods (Riemer et al. 2018;Aljundi et al. 2019b) mitigate forgetting by maintaining an episodic memory buffer and continual rehearsal of samples from previous tasks.
- Among these, our method focuses on rehearsal-based methods, as it has proven to be an effective approach in challenging continual learning scenarios (Farquhar and Gal 2018).
- The base method, Experience Replay (ER) (Riemer et al. 2018) interleaves the training of the current task with the memory sample to train the model on the approximate joint distribution of tasks.
- Several studies focus on the different aspects of rehearsal: memory sample selection (Lopez-Paz and Ranzato 2017; Isele and Cosgun 2018), sample retrieval from memory (Aljundi et al. 2019a) and what information to extract and replay from the previous model (Li and Hoiem 2017;Ebrahimi et al. 2020;Bhat et al. 2022).
- Dark Experience Replay (DER++) samples the output logits along with the samples in the memory buffer throughout the training trajectory and applies a consistency loss on the update of the model.
- Recently, CLS theory has inspired a number of approaches that utilize multiple memory systems (Wang et al. 2022a,b;Pham et al. 2021) and show the benefits of multiple systems in CL.
- CLS-ER (Arani et al. 2022) mimics the interplay between fast and slow learning systems by maintaining two additional semantic memories that aggregate the weights of the working model at different timescales using an exponential moving average.
- Our method enforces sparse coding for efficient representation and utilization of multiple memories.

## Methodology
- Biological systems are motivated by their environment
- The proposed approach uses a modular architecture
- The modular architecture is composed of different modules that can be combined to create different applications
- The different modules can be used to solve different problems

### Continual Learning in the Biological System
- Effective CL in the brain is facilitated by a complex set of mechanisms and multiple memory systems
- Evidence suggests that the brain employs Sparse Coding, in which sensory events are represented by strong activations of a relatively small set of neurons
- A different subset of neurons is used for each stimulus
- There is a correlation between these sparse codes (Lehky et al. 2021) that could capture the similarity between different stimuli
- Sparse codes provide several advantages: they enable faster learning of new associations with controlled interference with previous associations and allow efficient maintenance of associative memory while retaining sufficient representational capacity
- Another salient feature of the brain is the strong differentiation and specialization of the nervous systems
- There is evidence for modularity in biological systems, which supports functional specialization of brain regions
- The brain is believed to utilize multiple memory systems
- Complementary learning systems (CLS) theory states that efficient learning requires at least two complementary systems

### Sparse coding in DNNs
- The sparse neural codes in the brain are in stark contrast to the highly dense connections and overlapping representations in standard DNNs which are prone to interference.
- In particular, for CL, sparse representations can reduce the interference between different tasks and therefore result in less forgetting, as there will be fewer task-sensitive parameters or fewer effective changes to the parameters (Abbasi et al. 2022;Iyer et al. 2021).
- Activation sparsity can also lead to the natural emergence of modules without explicitly imposing architectural constraints (Hadsell et al. 2020).
- Therefore, to mimic sparse coding in DNNs, we enforce activation sparsity along with a complementary semantic dropout mechanism which encourages the model to activate similar units for semantically similar samples.
- Sparse Activations: To enforce the sparsity in activations, we employ the k-winner-take-all (k-WTA) activation function (Maass 2000).
- k-WTA only retains the top-k largest values of an N × 1 input vector and sets all the others to zero before propagating the vector to the next layer of the network.
- Importantly, we deviate from the common implementation of k-WTA in convolutional neural networks (CNNs) whereby the activation map of a layer (C × H × W tensor where C is the number of channels and H and W are the spatial dimensions) is flattened into a long CHW × 1 vector input and the k-WTA activation is applied similar to the fully connected network (Xiao et al. 2019;Ahmad and Scheinkman 2019).
- We believe that this implementation does not take into account the functional integrity of an individual convolution filter as an independent feature extractor and does not lend itself to the formation of task-specific subnetworks with specialized feature extractors.
- Instead, we assign an activation score to each filter in the layer by taking the absolute sum of the corresponding activation map and select the top-k filters to propagate to the next layer.
- Given the activation map, we flatten the last two dimensions and assign a score to each filter by taking the absolute sum of the activations.
- Based on the sparsity ratio for each layer, the activation maps of the filters with higher scores are propagated to the next layers, and the others are set to zero.
- This enforces global sparsity, whereby each stimulus is processed by only a selected set of convolution filters in each layer, which can be considered as a subnetwork.
- We also consider each layer's role when setting the sparsity ratio. The earlier layers have a lower sparsity ratio as they learn general features, which can enable higher reusability, and forward transfer to subsequent tasks use a higher sparsity for later layers to reduce the interference between task-specific features.
- Semantic Dropout: While the k-WTA activation function enforces the sparsity of activation for each stimulus, it does not encourage semantically similar inputs to have similar activation patterns and reduce overlap with semantically dissimilar inputs.
- To this end, we employ a complementary Semantic Dropout mechanism, which controls the degree of overlap between neural activations between samples belonging to different tasks while also encouraging the samples belonging to the same class to utilize a similar set of units.
- We utilize two sets of activation trackers: global activity counter, A g ∈ R N , counts the number of times each unit has been activated throughout training, whereas classwise activity counter, A s ∈ R C×N , tracks the number of times each unit has been active for samples belonging to a particular class.
- N and C denote the total number of units and classes, respectively.
- For each subsequent task, we first employ Heterogeneous Dropout (...

### Multiple Memory Systems
- Inspired by the interaction of multiple memory systems in the brain, our method builds a long-term memory that aggregates the learned information in the working memory.
- Episodic Memory: Information consolidation in the brain is facilitated by replaying the neural activation patterns that accompanied the learning event. To mimic this mechanism, we employ a fixed-size episodic memory buffer, which can be thought of as a very primitive hippocampus.
- The memory buffer is maintained with Reservoir Sampling, (Vitter 1985) which aims to match the distribution of the data stream by assigning an equal probability to each incoming sample.
- Long-Term Memory: We aim to build a long-term semantic memory that can consolidate and accumulate the structural knowledge learned in the working memory throughout the training trajectory.
- The knowledge acquired in DNNs resides in the learned synaptic weights (Krishnan et al. 2019). Hence, progressively aggregating the weights of the working memory (θ w ) as it sequentially learns tasks allows us to consolidate the information efficiently.
- To this end, we build long-term memory (θ s ) by taking the exponential moving average of the working memory weights in a stochastic manner (which is more biologically plausible (Arani et al. 2021)), similar to (Arani et al. 2022):
- where α is the decay parameter and r is the update rate.
- Long-term memory builds structural representations for generalization and mimics the slow acquisition of structured knowledge in the neocortex, which can generalize well across tasks.
- The long-term memory then interacts with the instance-level episodic memory to retrieve structural relational knowledge (Sarfraz et al. 2021) for the previous tasks encoded in the output logits.
- Consolidated logits are then utilized to enforce consistency in the functional space of the working model. This facilitates the consolidation of information by encouraging the acquisition of new knowledge while maintaining the functional relation of previous knowledge and aligning the decision boundary of working memory with long-term memory.

### Overall Formulation
- The CL task is to learn the joint distribution of all the observed tasks without the availability of task labels at test time.
- Our proposed method, SCoMMER, involves training a working memory θ w , and maintains an additional long-term memory θ s and an episodic memory M.
- The long-term memory is initialized with the same parameters as the working memory and has the same sparsity constraints.
- We initialize heterogeneous dropout probabilities π h randomly to set the probability of retention of a fraction of units to 1 and others to 0 so that the first task is learned using a few, but sufficient units and the remaining can be utilized to learn subsequent tasks.
- Long-Term Memory During each training step, we interleave the batch of samples from the current task x t ∼ D t , with a random batch of exemplars from episodic memory x m ∼ M.
- Working memory is trained with a combination of cross-entropy loss on the interleaved batch x ← (x t , x b ), and knowledge retrieval loss on the exemplars.
- Thus, the overall loss is given by: L = L ce (f (x; θ w ), y) + γL kr (f (x m ; θ w ), f (x m ; θ s ))
- The training step is followed by stochastically updating the longterm memory (Eq. 4).

## Evaluation Protocol
- To gauge the effectiveness of SCoMMER in tackling the different challenges faced by a lifelong learning agent, we consider multiple CL settings that test different aspects of the model.
- Class-IL presents a challenging CL scenario where each task presents a new set of disjoint classes, and the model must learn to distinguish between all the classes seen so far without the availability of task labels at the test time.
- It requires the model to effectively consolidate information across tasks and learn generalizable features that can be reused to acquire new knowledge.
- Generalized Class-IL (GCIL) (Mi et al. 2020) extends the Class-IL setting to more realistic scenarios where the agent has to learn an object over multiple recurrences spread across tasks and tackle the challenges of class imbalance and a varying number of classes in each task.
- GCIL utilizes probabilistic modeling to sample the number of classes, the appearing classes, and their sample sizes.
- Details of the datasets used in each setting are provided in the Appendix.

## Empirical Evaluation
- SCoMMER outperforms state-of-the-art rehearsal-based methods across different CL settings under uniform experimental settings
- SCoMMER provides performance gains in the majority of the cases
- In particular, it provides considerable improvement under low buffer size settings
- Compared to dense activations in CLS-ER, the sparse coding in SCoMMER leads to the emergence of subnetworks that provide modularity and protection to other parts of the network

## Ablation Study
- All components of SCoMMER contribute to the performance gains
- The drop in performance from removing semantic dropout suggests that it is effective in enforcing sparse coding on the representations of the model
- We also observe the benefits of multiple memory systems in CL
- Additional long-term memory provides considerable performance improvement and suggests that the EMA of the learned synaptic weights can effectively consolidate knowledge across tasks
- Sparsity is a critical component for enabling CL in DNNs

## Characteristics Analysis
- We use a different model to understand the performance gains
- We analyze the model trained on S-CIFAR100 with a buffer size of 200

### Stability-Plasticity Dilemma
- The diagonal of the heatmap shows the plasticity of the model as it learns the new task, whereas the difference between the accuracy of the task when it was first learned and at the end of the training indicates the stability of the model.
- SCoMMER is able to maintain a better balance and provides a more uniform performance on tasks compared to baselines.
- While CLS-ER provides better stability than DER++, it comes at the cost of the model's performance on the last task, which could be due to the lower update rate of the stable model.
- SCoMMER, on the other hand, retains performance on the earlier tasks (T1 and T2) and provides good performance on the recent task.

### Emergence of Subnetworks
- The model uses activation sparsity and semantic dropout to enforce sparse coding
- The model tracks the average activity of the units in the penultimate layer and enforces sparse coding using semantic dropout probabilities
- The model has high correlation between the test set activation counts and the semantic dropout probabilities at the end of training

### Task Recency Bias
- SCoMMER outperforms CLS-ER in mitigating recency bias
- CLS-ER reduces the probability of predicting the last task, which explains the low performance

## Conclusion
- SCoMMER is a novel sparse coding approach that employs activation sparsity and semantic dropout to mitigate forgetting in CL scenarios
- SCoMMER was effective in mitigating forgetting in challenging CL scenarios
- SCoMMER enables efficient consolidation of knowledge in the long-term memory
