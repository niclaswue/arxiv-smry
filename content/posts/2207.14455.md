---
title: "Neural Density-Distance Fields"
date: 2022-07-29T03:13:25.000Z
author: "Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hiroaki Aizawa, Hidehiko Shishido, Itaru Kitahara"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2207-14455v1.webp" # image path/url
    alt: "Neural Density-Distance Fields" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2207.14455)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2207.14455).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/neural-density-distance-fields).

# Abstract
- Neural fields for 3D vision tasks are now indisputable
- Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields
- However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects' surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to NeRF on novel view synthesis.

# Paper Content

## Introduction
- Neural fields, also known as neural radiance fields, have recently attracted attention as an alternative to using point clouds, voxels, meshes, and others.
- Neural fields, in particular, have shown impressive quality for tasks such as novel view-synthesis. However, since NeRF has limited regions with smooth spatial density and color, many conventional methods still require good initial values for registration and localization tasks.
- This paper proposes a distance field representation that is reciprocally constrained to the density field, named Neural Density-Distance Field (NeDDF).
- NeDDF achieves robust localization with distance fields while providing object reconstruction quality comparable to NeRF.
- As shown in Fig. 1, there are two main types of 3D shape representation in neural fields: density field used in NeRF [14] and distance field used in NeuS [24].
- Density field has high expressiveness for translucent objects, such as smoke and water, and high-frequency shapes, such as hair. However, in most areas except the boundary, the gradient of the field is zero. This makes it difficult to set up a convex objective function in a problem setting such as registration, as shown in Fig. 2.
- Distance field provides the gradient over a wide range even after the optimization converges. Thus, we can establish objective functions with high convexity in registration.
- The field can be learned from the image via volume rendering by defining a conversion equation from distance to density.
- For example, NeuS assumes that the density follows a logistic distribution close to the object surface.
- On the other hand, since we assume explicit boundaries, the convertible density field is tightly restricted.
- As shown in Fig. 3 (a), we focus on the Unsigned Distance Field (UDF), which ignores surface direction inside objects and can distinguish between the inside and outside objects not only by the sign of distance D but also by the magnitude of its gradient.
- We extend the distance field to be able to recover arbitrary density distributions by interpreting D by the depth derived from the volume rendering equation and fitting the density information of translucent objects to the mid-level gradient magnitude, as shown in Fig. 3 (b).
- This method eliminates the need for constraints on the density, as in NeuS, when learning the distance field from images.
- In other words, when learning the density field, we can simultaneously obtain a consistent distance field where the shape and camera pose have the same optimal values.
- As shown in Fig. 4, the NeDDF has a network that inputs a position and outputs the distance and its gradient, and a converter that explicitly calculates the density.
- NeDDF enables both high expressiveness of the density field and good registration of the associated distance field.
- The present paper provides the following three contributions: (1) Extending the distance field to be definable for arbitrary density distributions. (2) Proposing a method to recover the corresponding density from independent points using the distance field and gradient information. (3) Providing an implementation to alleviate the instability of the distance gradient caused by cusp points and sampling frequency. Furthermore, the effectiveness of the proposed method in terms of both expressiveness and registration performance is evaluated through experiments.

### Neural Fields
- Traditional way of representing volumes is to discretize the density or distance from the surface at each position into voxels
- Since voxels require data complexity that is cubic in resolution, it is difficult to increase the spatial resolution
- In recent years, memory-efficient representations such as octree [2,7] or hash table [17] have been proposed
- However, these grid structures cannot represent the geometry information with higher frequencies than the Nyquist frequency
- With advances in geometric deep learning following AtlasNet [6] and Foldingnet [27], some studies have focused on handling irregular non-grid structures such as point clouds and meshes
- While these methods can handle detailed geometry information efficiently, they are limited to interpolation due to their spatially discrete representations based on the coordinates of each point and vertex
- In recent years, the methods called neural fields [26]   and output dimensions without increasing the model capacity
- It is also possible to model topological changes by considering the density field as a three-dimensional slice of high dimensional space and embed time-series information by adding a multi-dimensional deformation code to the input
- By using a smooth activation function in the neural networks, neural fields can be regarded as a continuously differentiable field
- Modeling using gradient information has been proposed, such as divergence in Non-Rigid-NeRF [23] and elasticity constraints using the Jacobian matrix of the deformation field in Nerfies [20]
- Inspired by the ideas behind these approaches, we have developed a model in which the gradient of the distance field describes the density information
- The proposed model outputs a distance field and a density field that are explicitly consistent with each other
- Since an optimization-based penalty term does not constrain the model, it can be optimized reciprocally from an objective function that is appropriate for each field.

### Density Field
- The density field outputs the volume density for the input of the 3D position.
- Many methods, such as NeRF, use the density field together with the color field, thus enabling volume rendering.
- The density field is characterized by high expressiveness.
- For example, a low value of the density field can describe a semi-transparent object such as glass or smoke to represent proportional light transmission.
- For spatially high-frequency shapes such as fur, for which the boundary surface is complex, the field can describe a scene by considering the light interaction at an arbitrary point as a function of density, ray direction, and color.
- In particular, combining this with the color field makes it possible to model specular reflections, including viewing angle dependency.
- Although NeRF can describe complex scenes, it has a substantial limitation in that the camera pose should be known for the observed image and the scene should be static, making it challenging to capture a usable set of images with unknown camera poses.
- Therefore, many NeRF-based methods for estimation of camera pose and registration of object deformation have been proposed [30,12].
- However, since blank areas with a density value of 0 have uncertain gradient directions, camera pose tracking is only valid with initial values such that most of the object area overlaps.
- NeRF−− [25] optimizes camera parameters directly with backpropagation but is limited to camera configurations close to the line of sight.
- NSFF [11] requires the optical flow to follow the deformation.
- D-NeRF is limited to CG images with no background and low-frequency texture.
- Nerfies [20] adds warmup to positional embedding and delays learning of high-frequency components to ensure registration but is limited to camera configurations with close view directions.
- NeDDF provides a consistent distance field while retaining the expressiveness of the density field.
- By providing gradients where no objects are present, we can improve registration performance from a rough initial camera poses.

### Distance Field
- SDF is widely used in fusion and registration because it can provide stable bounding surfaces and normal vectors.
- SDF also provides residuals and gradient directions, enabling fast-fitting of two shapes by the Gauss-Newton method without corresponding point matching [16,15,10,9,8,5].
- A typical example is KinectFusion [16], which performs localization and shape integration from a depth map of unknown viewpoints for a static scene.
- DynamicFusion [15] constructs a sparse deformation field called WarpField to describe the deformation amount and performs registration for non-rigid scenes.
- In addition, several studies have proposed methods by which to handle distance fields by neural fields.
- DeepSDF [19] proposes a generative model for the continuous SDFs based on the auto-decoder model.
- SAL [1] enables the neural field to learn the shapes of boundary surfaces directly from raw unsigned data such as point clouds.
- UDF [3] makes the unsigned distance field continuous and shows its suitability for unclosed surfaces and complex shapes.
- Since density fields such as NeRF are noisy in surface reconstruction by level sets, several methods have been proposed to handle distance fields in neural fields that can present boundary surfaces and assume a static density distribution for the signed distance.
- IDR [29] introduces differentiable surface rendering to learn the neural field from multi-view images.
- However, unlike volume rendering, calculating a single surface intersection for each ray makes shape reconstruction unstable for complex shapes that cause abrupt depth changes in the image.
- UNISURF [18] enables a combination of surface and volume rendering with a neural field that describes occupancy.
- Several studies attempt to optimize the SDF with differentiable volume rendering by modeling the transform equation from the distance to the density field with the hypothesis on the shape of the density distribution.
- VolSDF [28] interprets the volume density as Laplace's cumulative distribution function for SDF.
- NeuS [24] assumes a density distribution over the signed distance using the learnable variance values.
- However, these distance fields assume the existence of a boundary surface, which limits the kind of shape that can be represented.
- This study extends the distance field to correspond to various density distributions from depth values derived from volume rendering.

## Method
- The distance field can be redefined to interpret arbitrary density fields.
- The conversion formula can be used to obtain the density of independent points from the distance and the gradient of the distance value in the redefined distance field.

### Distance Field from Density Field
- The distance field in boundary surfaces D b (p) describes the distance to the nearest surface for location p ∈ R 3 .
- We can interpret D b (p) as the minimum of the depth value d b (p, v) over the viewing direction v ∈ S 2 .
- We extend the distance field to be defined for arbitrary density distributions by replacing the depth value d b (p, v) with the depth value derived from the volume rendering equation.
- Similarly, the depth d(p, v) is defined to be an integral of the depths at each point, as follows: In practice, calculating the depths over all directions is computationally expensive, so we use a distance field that removes the dependence on the viewing direction v.
- We define the distance field D(p) := min v∈S 2 [d(p, v)], for adopting the shortest depth for each position.
- Assuming continuity in the adopted view direction we can restore this quantity from the tangent plane using the gradient of the distance field ∇D(p) as follows.

### Density from Distance Field
- Extended the distance field to shapes with no explicit boundary surface but with a known density field
- Derived the corresponding density field when the distance field is known
- For the distance field around position p, considered D(r(t)), r(t) = p + tv, which is sliced in the gradient direction v
- Calculated the derivative of the distance field in the direction of the gradient, ∂D(r(t)) ∂t
- Derived an expression for σ as follows: (the derivation is given in supplementary material)
- Can also express ∂D(r(t)) ∂t using the gradient vector of the distance field ∇D(p) as follows: (10)
- From Equations 7 and 10, the density σ can be obtained as follows:
- From Equation 3, t n is the interval in which the transmittance T (t n ) = 1, which is the lower limit of D
- While it is natural for t n to take the value 0, the density is undefined for D ∼ 0 as a numerical problem
- Assuming t n is sufficiently small, by introducing the approximations in Equations 12,13, σ can be calculated as in Equation 14:

### Removing Cusps
- The distance to density conversion by Equation 14 assumed that the distance field is first-order differentiable. However, a distance field is not differentiable at the cusps where the minimum distance direction switches.
- In practice, although the distance field around the cups is smoothly connected due to the continuity of the neural field, small gradient values around the cups are still converted to false densities that should not exist by Equation 14(see Fig. 7).
- To alleviate this problem at the caps, we extend the domain of the distance field from a 3-dimensional space to a 4-dimensional hyperspace [x, y, z, w] with an auxiliary gradient axis (w-axis) and consider the slice at w = 0 of the hyperspace to be the distance field.
- The gradient components ∂D ∂w are distributed so that the gradient ∇D satisfies the Equation 14 in the vicinity of the cusp to suppress spurious densities.
- When the auxiliary gradient also describes the density outside the cusp region, the shape of the global distance field is compromised. Therefore, we constrain the shape of the auxiliary gradient and use a penalty term to induce convergence to the optimal form.

## Reprojection error for volume rendering
- Photometric error is used to calculate correspondence points
- NeDDF provides the object's direction and approximate distance from a sampling point
- The reprojection error is used to calculate the camera pose

## Conclusions
- The study proposed NeDDF to represent reciprocally constrained distance and density fields.
- We extended the distance field to a formulation that can adapt to any density field.
- We also derived the conversion formula between distance and density using the distance and its gradient, enabling learning the these fields while constraining each other.
- We also alleviate the problem of discontinuity points by introducing the auxiliary gradient.
- The visualization experiments demonstrated that NeDDF could acquire the properties of both the conventional density field and the distance field.
- The quantitative evaluation showed that NeDDF could provide competitive quality of novel view synthesis, more stable meshes, and a more comprehensive range of following camera poses than NeRF.
- One limitation of our method is the lack of information about the distance field inside the objects.
- Since NeDDF is based on UDF, it cannot provide helpful gradient directions in the interior object region.
- NeDDF also has the same limitations as the original NeRF [14], such as time-consuming optimization and rendering.
- However, since NeDDF provides a differentiable density field and retains the same formulation as NeRF, many of the latest advances in improving NeRF, such as speedup and stabilization, may apply to NeDDF.
