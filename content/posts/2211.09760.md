---
title: "VeLO: Training Versatile Learned Optimizers by Scaling Up"
date: 2022-11-17T18:39:07.000Z
author: "Luke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, Jascha Sohl-Dickstein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2211-09760v1.webp" # image path/url
    alt: "VeLO: Training Versatile Learned Optimizers by Scaling Up" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2211.09760)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2211.09760).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/velo-training-versatile-learned-optimizers-by).

# Abstract
- Deep learning models are still trained with hand-designed optimizers
- We leverage the same scaling approach behind the success of deep learning to learn versatile optimizers
- Meta-trained with approximately four thousand TPU-months of compute on a wide variety of optimization tasks, our optimizer not only exhibits compelling performance, but optimizes in interesting and unexpected ways

# Paper Content

## Introduction
- Scaling up has been crucial to the success of deep learning across many domains
- Scaling brings with it several challenges: increased compute, larger datasets and considerably more engineering effort
- In meta-learning, a large training dataset corresponds to a large set of tasks, which are representative of the tasks a practitioner might want to optimize.
- While in supervised learning model sizes are often increased to improve performance, simply scaling up the model size of a learned optimizer can be problematic.
- Larger optimizers may require fewer iterations to achieve good performance, but the overhead per step may increase.
- A simpler hand-designed optimizer with less overhead could be run for more training steps or more carefully tuned to achieve competitive performance.
- Thus, a balance between overhead and performance must be struck

### Try VeLO
- VeLO is easy to try on any JAX model that uses Optax
- We provide a Colab notebook that trains one of a variety of test problems
- The core idea behind learned optimizers is to replace the fixed-form update rule U SGD with a more flexible form, parameterized by many more meta-parameters
- In this work, we leverage gradient-based meta-learning, but with gradients computed with Evolution Strategies

## Methods: Large Scale Optimizer Training
- The learned optimizer architecture is a functional form of U (• ; θ)
- The optimizer is meta-trained on a distribution of tasks
- Details of the meta-training are provided in the Appendix

### Learned Optimizer Architecture
- Hierarchical hypernetwork: A two-layer hierarchy of computation is used, with a "per-tensor" LSTM which operates on features derived by aggregating information from each parameter tensor in various ways, and a "per-parameter" MLP which operates on each parameter scalar.
- To increase the capacity of the network, additional computation can be added to the per-tensor network or to the per-parameter MLP.
- Figure 2 visualizes the learned optimizer architecture.
- Next, how to route information through the hierarchy is considered.
- Past work [Wichrowska et al., 2017, Metz et al., 2020a] passes the results of the per-tensor network directly to the per-parameter MLP, which introduces a number of additional input features.
- As a solution, the per-tensor network generates the weight matrices of the per-parameter MLP, which allows for more expressive per-tensor computation without increasing the cost of the per-parameter network.

### Data: A Diverse Distribution of Tasks
- There are no standard, large-scale distributions of tasks for learned optimizer training.
- To provide additional variation during meta-training, and analogous to data-augmentation, we perform a series of "task-augmentations"-programmatic modifications to tasks.
- Because tasks are sampled from a wide distribution, their run times vary greatly, sometimes by more than 3 orders of magnitude.

### Meta-Training
- Meta-objective is the measure of optimization performance we focus on
- Empirically, we found that targeting final loss yields optimizers which train models to lower loss values than would be possible by targeting average loss
- Multi-task training is challenging as each task has a different loss scale and thus gradients can not be directly averaged
- We make extensive use of JAX's vectorization (vmap) to parallelize compute across batches of different tasks
- Data-parallel training on a massive cluster is used to speed up meta-training

## Evaluating Learned Optimizers
- Evaluation of optimizers in machine learning is notoriously difficult
- Evaluating learned optimizers is more difficult still, as one has to additionally consider the degree to which evaluation tasks are "out of distribution" relative to the training task distribution.
- In this section we present several different evaluations of VeLO, with varying degrees and types of distribution shift in the evaluation tasks.
- First, we present VeLOdrome, a diverse evaluation set that can be run relatively efficiently.
- Second, we benchmark VeLO on the MLCommons algorithms test problems.
- Finally, we present evaluations on a broad range of real-world state of the art models including vision, language, and decision Transformers among several others.
- We also present an investigation of problems in which VeLO fails or underperforms baselines.

### VeLOdrome: A Canonical Evaluation Set of 83 Tasks
- For our first set of evaluations, we use a test set of relatively small deep learning models, which we refer to as VeLOdrome.
- These are similar to tasks used for meta-training, though they are hand-designed to be more canonical than the often unusual task specifications sampled during meta-training.
- Our main consideration for size here is training time; each model is designed to be able to be trained on a single accelerator in under an hour.
- This enables both rapid evaluation of our learned optimizer, but also aggressive tuning of baseline optimizers for fair comparison.
- This set contains 83 tasks, including convolutional networks, variational auto-encoders, residual networks, and language models such as recurrent networks and Transformers.
- In addition to evaluating learned optimizers, we hope this distribution of tasks can serve as a starting point for hand-designed optimizer evaluation.
- For full details on VeLOdrome, see Appendix C.
- One area that makes optimizer comparison difficult is hyperparameter tuning.
- By design, our learned optimizers require no per-problem tuning, whereas hand-designed optimizers are typically ineffective unless hyperparameter-tuned on a new task.
- We explore different levels of hyperparameter tuning, ranging from learning rate tuning-evaluating 15 different trials logarithmically spaced with half powers of 10-to searching over a wider search space with 1K trials.
- For our learning rate-tuned optimizers, we evaluate 15 hand-designed optimizers: Adam [Kingma and Ba, 2014], AdaBelief [Zhuang et al., 2020], SGD with momentum, RMSProp [Tieleman and Hinton, 2012], SM3 [Anil et al., 2019], SM3 with momentum, Yogi [Zaheer et al., 2018], RAdam [Liu et al., 2019], LARS [You et al., 2017], LAMB [You et al., 2019], Fromage [Bernstein et al., 2020], AdamW [Loshchilov and Hutter, 2017], AdaFactor [Shazeer and Stern, 2018], Adagrad [Duchi et al., 2011], and Shampoo [Gupta et al., 2018, Anil et al., 2020] with 6 different grafting types [Agarwal et al., 2020].
- As a more aggressively-tuned baseline optimizer, we use Nesterov accelerated AdamW [Dozat, 2016] with tunable learning rate, β 1 , β 2 , , weight decay (both applied separately from momentum as in AdamW, and not), and a cosine learning rate schedule (with optional warm-up).
- We searched over 1000 hyperparameter configurations for this optimizer; see Appendix C.2 of Metz et al. [2019b] for a complete description.
- NAdamW is a superset of many popular optimizers (see discussion of NAdam in Choi et al. [2019]), and so with sufficient tuning will achieve best-case performance across a variety of hand-designed optimizers.
- We also evaluate a meta-learned list of hyperparameter configurations-"OptList", described by Metz et al. [2019b].
- OptList achieves much better performance than the learning rate-tuned optimizers, while only requiring 10 hyperparameter evaluation trials.
- To the best of our knowledge, this is the largest optimizer benchmark to date with respect to both the number of tasks, and the number of optimizers evaluated.
- Learning curves for >1 million trained models are open-sourced.

### MLCommons Tasks
- We investigate a set of six different tasks from the MLCommons algorithms track.
- These tasks are out-of-distribution relative to training tasks, primarily due to their scale-they are significantly larger than those seen during meta-training.
- We present only the training loss in the main text. Other metrics are presented in Appendix G.7.
- For each model family, we compare to an Adam baseline with a learning rate warm up of 5% of total training iterations, and a cosine decay with a learning rate and weight decay searched in log space between [10 −2 , 10 −4 ] and [10 −2 , 1] respectively with 20 random trials.
- This search space itself was chosen by the MLCommons organizers, based on the top performing hyperparameters across the tasks from a larger search space with 100 trials each.

### Generalization to Tasks Unlike Any Used for Meta-Training
- VeLO outperforms a variety of other optimizers on a diverse set of tasks, many of which are outside of the meta-training distribution.
- VeLO is able to make use of larger training batches than other optimizers, and is able to match or outperform published baselines on difficult tasks.

### Limitations and Failure Cases
- VeLO is not comparable in performance to a tuned baseline
- Across several domains, we observed performance decreases relative to baselines with larger model size
- In our ViT experiments, VeLO's performance lagged behind tuned baselines for the largest models
- In particular, the H/14 model (650M) notably underperformed relative to the baseline model
- When applied for 200K steps or longer however, it begins to perform worse not only relative to the baseline, but also in an absolute sense
- It is common to extend the training of a model after an initial run
- For example, in transfer learning, a pretrained model is finetuned on a different dataset and/or objective to perform a specific task or set of tasks
- Underfit models may also have their training continued on additional data or epochs of the same dataset
- Since VeLO is conditioned on the total number of iterations during inner-training and the inner parameters were always initialized to a random state during meta-training, we find that it struggles to extend training beyond its initially specified number of iterations
- The resulting behavior differs depending on how the optimizer state is set for the extended training, but in many cases it is possible to partially remedy the poor performance
- We explored the following options for continuation from a completed VeLO training run:
- Naïve Continue: Continue from the final optimizer state of the previous run, allowing the iteration number to be greater than the total number of steps the optimizer is conditioned on: a state that was never observed during meta-training. We train an image MLP model on a CIFAR10 for 8,000 steps with VeLO ("Complete, 8K") and then extend training for an additional 12,000 steps with VeLO, adjusting the optimizer state in several ways, as described in the text.
- Histogram of ranks of final training loss for each continuation method in large-scale evaluation of 620 tasks. We train each task for 20,000 total steps, splitting the steps across 2 VeLO runs (except for "Complete") at a point sampled from N (10000, 5000 2 ), and applying each of the continuation methods.
- Increase Steps: Continue from the final optimizer state of the previous run but increase the number of steps the optimizer is conditioned on to be the sum of the lengths of the initial run and continuation. We compare these approaches anecdotally (Figure 10, left) and by evaluating them on 620 different tasks (Figure 10, right) with continuation points sampled from a Normal distribution centered at half of the total training steps. When comparing the final training losses, we find that naïve continue performs the worst in 52% of experiments and observe anecdotally that it tends to diverge slowly throughout the run. Full reset is second worst, often resulting in a large initial spike in the loss, followed by a slow partial recovery. Increase steps performs better than all other continuation methods in 49% of the experiments, the next best being reset steps for 25%. However, doing the complete training in a single run performs best overall in 38% of the experiments, beating increase steps in 54% of them.

## Understanding Learned Optimizer Behavior
- Learned optimizers can behave in more diverse ways than hand-designed optimizers.
- In this section we experimentally characterize aspects of VeLO's behavior.

### VeLO Adapts to Training Horizon
- Learning rate decay is a simple yet powerful technique to increase performance near some prespecified end of training
- VeLO intelligently makes use of this feature and drops the loss dramatically just before the end of training
- For both problems, near the end of training the loss decreases rapidly

### VeLO Can Have a Larger Critical Batch Size than Baseline Optimizers
- Training on large batches is important for large scale distributed training
- Prior work has shown that one can increase the batch size while proportionally decreasing the number of weight updates (maintaining a fixed number of total gradient evaluations) up until a point where performance starts to fall
- It has been shown that optimizers that make use of momentum, and/or more sophisticated preconditioners can be used to increase this critical batch size
- We explore whether VeLO can make effective use of batches that are larger than the critical batch size for hand-designed optimizers
- For all models, we train for 2 19 examples. Thus, for a batch size of 2 16 , we only make 8 training steps
- In addition to doing considerably better than the baseline optimizers, VeLO makes use of significantly larger batch sizes

## Related Work
- The idea of meta-learning update rules for optimization dates back to Bengio et al. [1992], Runarsson and Jonsson [2000] which both learn simple update rules on simple neural networks.
- Andrychowicz et al. [2016] revived the topic by meta-training an RNN-parameterized learned optimizer on deep learning tasks by backpropogating through the optimization procedure [Maclaurin et al., 2015].
- Since then, there has been a flurry of new techniques, ranging from learned optimizer architectures to meta-training algorithms.
- Closest to our work is the line of work on hyperparameter-free, neural-network parameterized learned optimizers trained on large distributions of tasks.
- Wichrowska et al. [2017] introduced hierarchical learned optimizers, similar to our work, and meta-trained them on a large distribution of synthetic tasks.
- Metz et al. [2020a] train on a more realistic task distribution [Metz et al., 2020b], with an improved learned optimizer architecture.
- In this work, we leverage ES for meta-training [Rechenberg, 1973, Nesterov and Spokoiny, 2011, Salimans et al., 2017].
- There has been extensive work studying different meta-training techniques ranging from ES improvements [Maheswaranathan et al., 2019, Metz et al., 2019a, Vicol et al., 2021], to reinforcement learning [Li and Malik, 2017a,b], to techniques designed specifically to train learned optimizers [Lv et al., 2017, Chen et al., 2020].
- In contrast to general-purpose learned optimizers, task specific learned optimizers have been proposed in many settings, including chemistry [Merchant et al., 2021], robustness [Metz et al., 2019b], adversarial training [Xiong and Hsieh, 2020], few-shot learning [Ravi and Larochelle, 2016], min-max optimization [Shen et al., 2021], human motion reconstruction [Gärtner et al., 2022], unsupervised learning [Metz et al., 2018], swarm optimization [Cao et al., 2019], black box optimization [Chen et al., 2016], and MCMC sampling [Levy et al., 2017, Wang et al., 2017, Gong et al., 2018].
- Improvements to the LSTM learned optimizer architecture in Andrychowicz et al. [2016] have also been proposed.
- Lv et al. [2017] modify the inputs to improve training; Metz et al. [2019a] swap out the LSTM with an MLP; Premont-Schwarz et al. [2022] introduce the ability to fall back to a hand-designed optimizer to ensure convergence.
- Hyperparameter controllers-neural networks which dynamically set the hyperparameters of existing optimizers-have also been explored [Daniel et al. [2016], Xu et al., 2019, Almeida et al., 2021].
- Finally, work has been done to meta-learn symbolic parameter update rules.
- Bello et al. [2017] use reinforcement learning to learn a policy which produces symbolic optimizers.
- Zheng et al. [2022] first meta-train a neural network, and subsequently distill it to a symbolic form.
- Real et al. [2020] explores learning not just an optimizer, but an entire symbolic learning algorithm.

## Discussion and Outlook
- Improved generality and performance of learned optimizers
- No hyperparameters yet outperforms heavily hyperparameter-tuned baselines

### Open Questions
- There are many open questions and clear directions for improvement in the optimizer.
- These include improving the learned optimizer architecture, for instance to leverage second order or quasi-second order information; using more available information about the target task, for instance by conditioning on an embedding of the target task's computation graph; more control to target both validation and training loss; and improving the computational efficiency of meta-training.

### Meta-Learned Algorithms are the Future
- Machine learning has shown that learned algorithms can outperform even the most well-motivated hand-designed heuristics
- In this paper, we show that this lesson applies to the parameter update function used to train a neural network
- Similar benefits from meta-learning have been demonstrated in neural architecture search [Zoph andLe, 2017, Tan et al., 2019] and data augmentation [Cubuk et al., 2018]

### B Learned Optimizer Architecture
- VeLO is a hierarchical architecture for deep learning
- The hierarchical structure is based on a model-level abstraction
- Each component has input features and connections to other components
- The optimizer is complex with respect to the complexity of the underlying model

### B.1 Extended Architecture Overview
- Most hand-designed optimizers consist of a handful of element-wise operations and are therefore relatively inexpensive to compute.
- Adam [Kingma and Ba, 2014], for example, is typically written as three (coupled) update equations applied independently to each parameter of the underlying model.
- Learned optimizers, on the other hand, can make use of more complex functional forms, possibly parameterized by neural network.
- While this greater expressivity results in greater representational power, it can can also be considerably more costly to compute.
- For learned optimizers to be useful, this extra compute per-step must be balanced with the improvements in per-step optimization efficiency.
- Recently, Metz et al. [2022] showed that per-parameter learned optimizers can be parameterized by a extremely small neural network, and thus efficient to compute, while still outperforming tuned hand-designed optimizers.
- While these small models are capable of strong performance on single tasks, we found that they lack the representational capacity needed to perform well across many tasks (See Figure 17, in particular the MLP LOpt and AdaFac LOpt).
- Past work has introduced hierarchy in learned optimizer parameterizations to address this limitation [Wichrowska et al., 2017, Metz et al., 2020a].
- This hierarchy enables more expressive representations, while allowing for shared computation across either a full tensor, layer, or the entire network.
- Results of the upper level computation are then routed down and treated as conditioning inputs to the lower levels of the hierarchy.
- For example, per-tensor computations are fed into the per-parameter network.
- While this does greatly increase the capacity (and thus performance) of the learned optimizer without requiring excessive additional compute, it does not fix the core expressivity restriction of the per-parameter optimizers, as this per-parameter network would still need to perform a different computation depending on the conditioning thus potentially requiring a more expressive, and thus more expensive function.
- To address the computational bottleneck at the parameter optimizer level, we propose a new class of learned optimizer architecture based on HyperNetworks [Ha et al., 2016].
- Our optimizer consists of a two-layer hierarchy.
- The upper level acts at the tensor level, and is a recurrent network.
- Instead of this tensor-level network passing information to the per-parameter layer as additional inputs, we instead have the network produce the entire weight matrix of the per-parameter network.
- This decouples the computational requirements of the upper and lower layer networks, and allows for increased expressivity without additional compute cost at the lower level.
- To make this prediction easier, we parameterize it as a linear combination of some larger set of per-parameter network weights.
- In the following sections we describe all the details of this model starting with what state it accumulates iteration to iteration, followed by a description of the per-tensor and per-parameter networks.

### B.2 Optimizer State: Non-Learned Accumulators
- VeLO keeps track of the following information from iteration to iteration:
- The number of iterations the learned optimizer has been applied.
- Momentum at different timescales: We keep track of an exponential moving average of gradient values at multiple timescales. We use 3 timescales (0.9, 0.99, 0.999).
- Squared gradients: We track the exponential moving average of the gradient squared, similarly to what is accumulated in Adam [Kingma and Ba, 2014]. We track these squared gradients using a single EMA coefficient of 0.999.
- Adafactor-style accumulators: As in Metz et al. [2022] we also track a factorized variant of the squared gradients [Shazeer and Stern, 2018]. The size of these accumulators is sub-linear in parameter count, so we track 3 different exponential moving averages of these values: (0.9, 0.99, 0.999).
- Loss features: We additionally track details about the loss values. This is parameterized as an exponential moving average of each training loss value at multiple timescales. To be precise, we construct 10 values between 1 and log(num_steps), and then to determine decay we use exp(−1/x) where x is each one of these 10 values. Comparing the results from different timescales will allow our optimizer to react to loss changes over time.
- Running minimums: We also keep track of a running minimum of each of the different exponential timescales. See the following section for how these values are used.

### B.3 The Tensor-Level Recurrent Network
- The tensor-level network is an LSTM [Hochreiter and Schmidhuber, 1997], with 512 hidden units which operates on features which are broadly reflective of overall training status and bulk statistics of per-parameter features.
- These are described below:
- Fraction of training remaining. We use as an input the current training iteration, as well as the number of target steps to produce a set of values representing a soft progress through training. This is implemented by computing tanh(10 * (t/T − s)) where t is the current iteration, T is the total number of steps, and s is the fraction through training. We used s ∈ [0.03, 0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0, 1.1] but did not ablate these values.
- Loss features. These features enable the optimizer to tell if optimization is converging or diverging. We leverage both the exponential moving averages of loss, and the running minimum to construct a loss magnitude-invariant featurization. Values of -1 roughly correspond to a loss going down, values of 1 mean diverging, and values of 0 mean no change in loss. We have 10 loss timescales, and use pairs of these timescales to construct these features.
- First and second moment features. We use the variance of each of the momentum timescales as an input feature, and both the mean and variance of the second moment features. Each feature x is processed as follows: clip(log(10 −8 + abs(10x)), −5, 5). This ensures the inputs to the tensor-level network remain on a consistent scale and appropriate dynamic range.
- Tensor rank. We use a one hot of the rank of the tensor as an additional feature. For each tensor, we compute these features, which are fixed length, and form a rank 2 array with a leading "batch" dimension being the number of tensors. Before feeding these values into the per-tensor LSTM, we seek to "mix" information across the tensors. To do this, we first apply a small neural network which operates on the feature dimension and consists of 2 linear layers with 512 units with ReLU activations. Inspired by Zaheer et al. [2017], we additionally mix this information by taking the max across the number of tensors dimension and adding this value to the original features projected linearly to 512 dimensions. In code this operation looks like: )), axis = 0, keep_dims = T rue), where F i is a linear layer, and σ is a ReLU activation.
- The tensor-level model, given the mixed features described above, acts independently across each tensor. From this recurrent model, we output a scalar c lr which is used to modulate the step size for all tensor elements and a vector, c hyper which is used to linearly interpolate between a meta-learned collection of weights of the same shape as the per-parameter MLP network.

### B.4 The Parameter-Level Network
- The per-parameter optimizer we use is based on the optimizer investigated in Metz et al. [2022]
- The per-parameter weights for a given tensor are computed by taking an un-normalized weighted average (with weights c hyper ) of a bank of differently initialized, and meta-learned per-parameter weights.
- The output of this is then multiplied by 100, a scaling factor to keep the MLP weights in a reasonable range at initialization.
- Because these weights are computed per tensor, each tensor now has a different set of per-parameter weights.
- The per-parameter features are normalized by the second moment across the entire tensor (each feature is normalized independently) and passed into the weights produced by the tensor-level LSTM hypernetwork.
- This produces 2 values: d, m, which are combined (along with the scalar learning rate c lr also produced by the RNN), and 2 scaling hyperparameters, both 0.001, chosen to keep the outputs of this network to a reasonable range.

### B.5 Comparing our Architecture to Hyperparameter Controllers
- Hyperparameter controller-based learned optimizers are another class of learned optimizer architecture
- These optimizers map from training features to the hyperparameters of chosen optimizers
- One can view our work as a more complex version of this parameterization
- Our HyperNetwork LSTM is producing a small number of weights, which are then used throughout the target network much in the same way hyperparameters act

### B.6 Experimental Validation of Our HyperNet Compared to Past Work
- To demonstrate this, we explore training different learned optimizers on a small scale,
- multi-task distribution of problems consisting of training a 1 hidden layer, 32 unit MLP on 4 different 8x8 datasets.
- See the code for more info on these tasks.
- We show the per parameter optimizers from Metz et al. [2019a] (MLPLOpt) and Metz et al. [2022] (AdaFac LOpt), hand-designed optimizers (Learnable Adam-Adam where we meta-learn the hyperparameters), hyperparameter controller learned optimizers (NNAdam [Metz et al., 2022]), the hierarchical optimizer from Metz et al. [2020a] (RNN MLP LOpt), and our HyperNetwork based learned optimizer (HyperNet LOpt).
- We find our HyperNetwork based optimizer has low meta-loss, implying that this architecture has a higher capacity and thus can learn a function which performs optimization better.

### B.7 Understanding Computational Costs of VeLO
- The complexity of predicting exact run times of any deep learning system, especially one that leverages a compiler, is complex.
- Instead, we designed a simple model for performance based on 3 different components: a constant execution overhead, per-tensor scaling cost, and per-parameter scaling cost.
- For further simplicity we assume a constant tensor count combining both the constant overhead and the per-tensor scaling.
- This leaves us with the following simplified model: To test this model, we run a set of different optimization algorithm (learned and hand-designed) on a simple set of parameters-three square matrices, each of the same number of rows and columns.
- We fit the parameters of this model (λ overhead and λ params ) using via gradient descent in log space.
- Despite its simplicity, our model is well-aligned with the data and strongly predictive.
- In Figure 18a we plot our predicted time-per-step for the learned optimizer, SGD, and Adam, as well as the data.
- The learned optimizer has both higher overhead, and significantly larger cost per parameter, as expected.
- SGD takes 3.4e-11 seconds per parameter, whereas our model is 1.58e-9 seconds per parameter (46× longer).
- In terms of the constant overhead time, our optimizer has about 2.6 higher overhead.
- This overhead region dominates below approximately one million parameters.
- We expect a better implementation (instead of naively relying upon XLA) will shift this overhead region.
- In Figure 18b we look at the effects of changing the size of the per-parameter MLP.
- As expected, the cost per parameter grows roughly linearly with the size of the per-parameter MLP.
- Finally, in Figure 18c we look at the size of the per-tensor LSTM.
- As we increase the size, the per-parameter cost remains constant, though the overhead grows.
- The impact of this overhead goes away, however, as the parameter count grows again saturating after around a million parameters.

## C Data: A Large, Diverse Distribution of Meta-Training Tasks
- Large and diverse datasets are necessary for effective deep learning training
- To make matters worse, we cannot possibly cover all possible tasks
- To make up for this, we build a procedural generative process for machine learning tasks
- We also develop a method for sampling only tasks that are not prohibitively expensive to meta-train on

### D Meta-Training
- Our meta-objective is the training loss computed at the end of inner-training.
- In an effort to increase generality, this objective is computed in expectation over several sampled quantities: 1. Task static configuration. A sample from the distribution of tasks defined in C.5. A sample of configurations which can be vectorized over.

### D.2 Meta-Gradient Estimation
- Leverages ES [Salimans et al., 2017]
- Uses antithetic samples [Owen, 2014]
- Shares as much randomness between pairs as possible to further reduce variance
- Uses ES rather than a more sophisticated method such as Truncated ES [Metz et al., 2019a]
- Amenable to target the loss at the end of training

### D.3 Curriculum and Meta-Generalization
- meta-training is prohibitively expensive
- we make use of both curricula over various meta-training parameters
- the fact that our optimizer can generalize from smaller to larger sized inner tasks is crucial
- we show the effect of curriculum over the length of inner problem in the following section
- experiment: curriculum over unroll length
- in this experiment, our goal is to train a learned optimizer to train a distribution of tasks for 2,000 inner iterations
- because computational cost of computing meta-gradients is proportional to the inner problem length, we can compute metagradients faster with shorter inner-problems at the cost of bias
- this bias can be mitigated by eventually meta-training with problems run for the desired number of iterations
- for our learned optimizers, we ultimately target 200K iterations-2 orders of magnitude longer
- as meta-training a learned optimizer is inherently a multi-task optimization problem, we must balance the gradient contributions from each task
- unlike many other multi-task problems, there is both no natural or expected scale for the losses from each task which makes targeting performance across all tasks difficult
- we opt for a simple solution to this problem-simply to normalize the length of each meta-gradient independently per task making each gradient constant norm
- as a result, the meta-training procedure only depends on the ratios of how tasks are sampled, and is invariant to the scale of losses for each task
- using this normalization, however, comes at the cost of losing a well-defined meta-objective, and can even produce non-descent directions when the meta-loss is stochastic
- nevertheless, we found this method works quite well in practice
- we also experimented with normalizers designed by hand to roughly re-scale losses into a standardized range
- for each task family, we manually define a transformation via trial and error to keep the loss between -10 and 10
- examples of such a transformation include normalizing by the number of classes predicted for classification tasks, as well as clipping the max loss
- implementations of these normalizations can be found in the normalizer function for all task families
- despite the fact that meta-training with these resulted in worse performance, we find these aggregated normalized loss values to be useful to monitor the rough performance of our learned optimizer
- we explored normalizers based on the performance of a set of baseline optimizers on a given task
- because our task distribution does not contain a finite number of tasks, we cannot simply run these baselines on all problems
- instead, we attempted to train a neural network model to map from task configuration and baseline hyperparameters to learning curves using an architecture similar to that described in C.4. While initially promising, this method substantially increases complexity, and did not outperform the much simpler normalization of gradient and thus we chose not to explore this direction further even though we believe this is closer to correct solution in the long run

## E Meta-Training Infrastructure
- The paper discusses the need for a distributed meta-training infrastructure and how it was built inside of Google's infrastructure
- The paper also discusses the need for a distributed file system and how Courier was used to make RPC calls

### E.1 One Learner, Many Workers
- The main meta-training set of machines consists of a single learner process which provides an RPC interface fetching the current learned optimizer weight values, and an RPC interface to receive meta-gradients calculated with these weights.
- Each worker process has a collection of different tasks on which VeLO is trained.
- The learner process is always run in a more reliable manner (i.e. dedicated hardware) so that it will not be preempted.
- While meta-training, the learner process saves weights of the learned optimizer to the distributed file system.
- An "evaluation chief" process monitors this file system, and when a new checkpoint is found, it enqueues some set of different evaluation configurations on a distributed task queue.
- These evaluations are defined by a list of configurations which specify some target model to evaluate, the target length to train that model, and other information required to for the evaluation.
- Evaluation workers (each with a single TPU chip) fetch these tasks, load weights of the learned optimizer, train this models, and report back the results to the evaluation chief.

### E.3 Task Selection and Staleness of Meta-Gradients
- The most straightforward implementation of our training infrastructure would consist of each worker sampling a new task (both static and dynamic configurations) i.i.d. from the task distribution after the completion of the previous task.
- This would be extremely wasteful as the computation itself often only takes a few seconds to run.
- As such, we compute multiple gradients for a given static task configuration before sampling a new task.
- Because dynamic configurations leverage the same compilation, we are able to use different settings for each computation of these gradient estimates, along with different dynamic task configurations for each of the vectorized problems we run in parallel.
- While re-sampling dynamic tasks increases variation beyond simply evaluating multiple gradients on the exact same task, our sampling scheme does induce auto-correlation of gradients in time.
- Reusing tasks in this way also results in a different problem: machines which sample a fast task would produce significantly more meta-gradient estimates than machines which sampled a slow task.
- This further breaks i.i.d. assumptions and causes yet more gradient correlation resulting in unstable meta-training.
- To compensate for this issue, each machine samples more than one task (we use 8 for the majority of our experiments), and when computing meta-gradients cycle through each to mitigate gradient correlation.
- As we increase the number of tasks each machine has we will approach a uniform rate of gradients from each machine by the law of large numbers, though this comes at the cost of increased compile times.
- We found 8 tasks balances startup costs (which take approximately 40 minutes) while sufficiently lowering gradient auto-correlation.
- We additionally resample elements of these tasks probabilistically so as to ensure coverage of the entire task distribution.
- These meta-gradients are all sent to a centralized learner which aggregates them in batches, and applies weight updates by passing them through Adam.
- To combat staleness (meta-gradients computed with old learned optimizer parameter values), we throw out any gradients which have been computed too far in the past.
- Globally-Distributed Workers.
- The computational load required for meta-learning workloads is quite different than those of large supervised models and as such we can make use of considerably cheaper hardware than a dedicated supercomputer.
- The bulk of our compute infrastructure consists of TPU chips designed originally for inference scattered across the globe.
- Thus, we are leveraging idle compute resources in Google's fleet, and which are connected via commodity networking (by data center standards) rather than specialized ICI links in the usual TPU deployments [Jouppi et al., 2020].
- As we perform data-parallel training over this cluster (Section E.1), each machine can operate largely independently.
- Leveraging this amount of machines is possible due to extremely low networking requirements when transmitting meta-gradients compared to the costly compute requirements (training some sampled task with a learned optimizer)

### E.4 Interactive Hyperparameter Tuning
- During meta-training, we frequently made modification to the running job. These changes include increasing the batch size, lowering the learning rate, changing the distribution of inner-problems to include larger problems, and modifying the maximum staleness.
- This was inspired by the reported success of online hyperparameter modification in OpenAI Five [Berner et al., 2019].
- The exact meta-training was relatively ad-hoc and not easily replicated. Due to the computational cost, we could only afford to train one model at full scale. Our previous largest model used a fraction of the total compute.
- Our training was divided into 4 phases, each using the previous weights as the starting point. We monitored a variety of losses, as well as qualitatively test our trained learned optimizers to help us determine if any modifications are needed.
- Phase 1: Phase one consisted of the smallest sized problems. We use a total outer-batch size of 131,072 tasks which is made up of 2048 batches of outer-gradients, each of which being the average of 8 different static configurations, and vectorized over 8 dynamic configurations. We use a learning rate of 3e-4. This phase lasted for approximately 17 days.
- Phase 2: Next, we switch to the mid-sized problem distribution. This was in an effort to better align the training distribution with more realistic problems as we started to notice meta-overfitting in evaluations. We additionally switch how gradients are aggregated, resulting in a smaller total batch size of 40,960, computed with 5120 batches of outer-gradients, where each outer-gradients only comes from one (possibly duplicated) static configuration, and again is averaged over 8 dynamic samples.
- As we describe in Section D.4 there is no single meta-loss to plot. Instead, in these figures we measure the performance of our learned optimizer applied to an i.i.d. sample of tasks from the meta-training distribution, applied for 10K steps for the first 3 phases, and 100K steps for the last phase. Losses are first normalized with the hand-designed normalizers described in Section D.4, and then aggregated with a mean. Each dot denotes the average over 20 static configurations, with 2 dynamic configurations per static.
- Phase 3: Upon noticing divergence in the evaluation tasks, we increased the batch size by a factor of 2 to 81,920 and reset training to an earlier checkpoint.
- Phase 4: Finally, we noticed our learned optimizers performed poorly on long horizon problems. To solve this, we shifted the max problem length from 20K to 200K. This slowed down meta-training considerably, but dramatically improved performance in these settings. In this phase we also shifted the distribution of tasks to even larger problems. We visualize these different phases in a variety of ways. First, in Figure 21 we show the closest thing to a meta-loss for each phase of training. In Figure 22 we visualize the evaluation performance of our learned optimizer on a set of hand-designed tasks which are faster to compute. We visualize performance at different percentiles of performance. These figures we often used the 10th percentile to guide much of our development as we found good performance on all tasks was more important than extremely fast training on a small number of tasks and often generalized better to larger problems. Finally, in Figure 23 we show statistics from meta-training including the number of outer-iterations, number of problems trained, and the number of times our learned optimizer has been applied.

### E.5 Areas of Improvement
- Our infrastructure is still in its infancy and there are several directions for improvement
- TPU utilization is low due to the mismatch in hardware design, and GPUs are actually worse as kernel execution overhead dominates
- Compile time overhead is significant and each TPU machine compiles its own computation graph
- Sensitivity to cluster status is exacerbated by the asynchronous training dynamics
- Meta-training requires further iterations to be more synchronous in nature
