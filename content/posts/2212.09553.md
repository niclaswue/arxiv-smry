---
title: "Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models"
date: 2022-12-19T15:45:36.000Z
author: "Yong Cheng, Yu Zhang, Melvin Johnson, Wolfgang Macherey, Ankur Bapna"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09553v1.webp" # image path/url
    alt: "Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09553)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09553).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/mu-2-slam-multitask-multilingual-speech-and).

# Abstract
- Mu$^{2}$SLAM is a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text, and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT)
- By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text
- On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points

# Paper Content

## Introduction
- Pre-training methods in speech have started to show a similar trend towards unified models from the dominant encoder-only models (Baevski et al., 2020;Hsu et al., 2021;Babu et al., 2021;Bapna et al., 2021;2022) to generative models on cross-modal speech and text data, exemplified by a couple of recent trails such as decoder-only models (Borsos et al., 2022) and encoderdecoder models (Ao et al., 2021;Chen et al., 2022;Sainath et al., 2022;Zhou et al., 2022;Zhang et al., 2022b;Tang et al., 2022).
- Except for SLAM and mSLAM (Bapna et al., 2021;2022), most of them merely focus on speech-related tasks by taking text data as auxiliary inputs while ignoring the evaluation on text-related benchmarks, which leaves us unknown to gauge the effect of interference and capacity dilution.
- There are few studies investigating multilingual modeling with both speech and text (Bapna et al., 2022;Chen et al., 2022), which limits them in leveraging cross-lingual transfer to enrich the speech and text joint representations.
- Multitask learning has demonstrated the effectiveness of inductive transfer to improve model generalization, yet it is understudied in speech-text pre-training (Tang et al., 2022;Zhang et al., 2022b;Chen et al., 2022) where they explicitly differentiate the utilization of labeled data in pre-training by introducing customized networks and losses.
- It is essential for prior speech-text models to design modalityspecific blocks and losses to yield high performance (Bapna et al., 2022;Chen et al., 2022;Tang et al., 2022;Zhang et al., 2022b;a) which somewhat violates the principle of the unified models by using one model for all tasks, thus undermining the language and modality transfer to learn general speech and text shared representations.
- In this work, we propose a multi-task multilingual pretraining method for speech and text based on an encoderdecoder model, called Mu 2 SLAM. The speech-text model is jointly pre-trained on a set of different tasks involving unlabeled speech, unlabeled text, labeled speech-text (ASR&AST), and labeled text-text (MT).
- We scale up the language type in both speech and text to more than 100, covering most of mainstream spoken languages.
- For the simplicity of extending our current pre-training to more data, we unify the pre-training losses for unlabeled and labeled data by defining a masked language modeling (MLM) loss on the encoder (Devlin et al., 2019), a similar T5 loss on decoder (Song et al., 2019;Raffel et al., 2020) and an alignment loss only for the labeled data.
- To enforce the sharing and take full advantage of modality capacity for speech and text, we minimize the number of modality-specific layers in our model design with only a conventional CNN block used to extract speech representations, which pushes forward speech-text models towards the unified models.
- As our pre-training method inherits the idea of BERT (Devlin et al., 2019) to reconstruct the masked tokens according to the contextual unmasked tokens, the artificial token [MASK] used in pre-training is absent from labeled data in fine-tuning (Yang et al., 2019).
- The discrepancy between pre-training and fine-tuning hinders the model from being adequately optimized on the downstream applications....

## Approach
- We propose a multi-task multilingual pre-training method, Mu 2 SLAM, for speech and text
- The speech and text data can be cast into two types of data, unlabeled data without supervised labels and labeled data usually accompanied with human-annotated labels
- As Figure 1 shows, we consider four types of data, i.e., speechonly, text-only, speech-text and text-text
- The main idea is to unify these training examples into the sequence-to-sequence format and apply similar optimization objectives on the encoder and decoder

### Model Architecture
- Our multi-task multilingual speech-text pre-training method (Mu 2 SLAM) is based on an encoder-decoder backbone model.
- For speech inputs, we follow SLAM (Bapna et al., 2021) and mSLAM (Bapna et al., 2022) to convert an acoustic feature sequence of 80-dimensional log Mel spectrograms into a sequence of latent speech representations via a CNN block.
- The CNN block consisting of two 2Dconvolutional layers with strides (2, 2) also acts as a subsampling mechanism with a 4x reduction in the sequence length dimension.
- A subsequent linear projection layer is used to map the dimension of the latent speech representations to that of the encoder stack, we denote the speech representations as S.
- To specify the language and modality, we add language and modality embeddings to word embeddings or speech representations S in addition to the conventional positional embeddings.
- The speech and text representations are then feed into a shared multi-modal encoder-decoder model.
- We prefer a deep encoder with 24 Conformer layers (Gulati et al., 2020) (a similar encoder as mSLAM) and a shallow decoder with 6 Transformer layers (Vaswani et al., 2017), which favors faster inference while maintaining competitive quality (Kasai et al., 2020).

### Speech Tokenization
- The proposed speech-text pre-training approach is to treat the speech inputs as an additional language in the multilingual text pre-training
- This requires a speech tokenizer network to quantize the continuous speech representations S = (s 1 , s 2 , ..., s N ) into discrete ids z = (z 1 , z 2 , ..., z N ).
- To this end, each speech representation vector s is independently projected into a discrete id z by finding the nearest neighbour in the speech codebook G: In mSLAM, the parameters of the speech tokenizer are learned from scratch by a contrastive loss (Baevski et al., 2020) over a speech-only encoder.

### Pre-training Objectives
- Uses a sequence-to-sequence model to pre-train on unlabeled data
- Uses a masking vector to replace tokens in the source x with a [MASK] token if m i = 1
- Uses a loss on the labeled data to pre-train the model
- Uses a loss on the unlabeled data and labeled data together

### Fine-tuning
- The fine-tuning method is crucial to unlock the capability of a strong pre-trained model
- In this section, we outline how we exploit the speech-text joint representations in the downstream tasks
- Direct fine-tuning is a common way of adapting a pretrained model to a specific downstream task
- Gradual fine-tuning is a method to mitigate the discrepancy between pre-training and fine-tuning due to the artificial [mask] tokens in pre-training
- To defend these errors from the decoder, we follow (Cheng et al., 2019) to add some noise to decoder inputs

## Experiments
- The paired text-text data comes from WMT and TED translation tasks, which are identical as the MT sets in mSLAM
- Because the language distribution in this combination set is highly skewed, we also apply the similar temperature-based data sampling with temperature

### Models and Hyperparameters
- We use an identical Conformer layer from SLAM (Bapna et al., 2021) and mSLAM (Bapna et al., 2022), in which the model dimension is 1024, feedforward hidden dimension is 4096, convolution kernel size is 5 and the number of attention heads is 8.
- The Transformer layers in the decoder share the same setting as Conformer layer in terms of model dimension, hidden dimension and attention heads but we set dropout to 0.1 for the Transformer layers rather than the default 0 in the Conformer layers.
- We use the same learning schedule as Transformer during pretraining and fine-tuning but warmup steps are set to 40k and 16k respectively.
- The Adam optimizer (Kingma & Ba, 2014) is applied to pre-traing with learning rate as 3 while AdamW (Loshchilov & Hutter, 2017) is used as fine-tuning optimizer with weight decay rate as 0.01.
- The batch sizes per TPU for speechonly, text-only, AST, ASR and MT data are 4, 8, 1, 1 and 1.
- We mask approximately 50% of the speech frames with spans of length up to 10 ( Chung et al., 2021).
- However, for text inputs, we mask a continuous span of around 50% of words except for MT tasks where the mask ratio is 25%.
- The loss coefficients related to speech-only and text-only data are set to 1.
- The loss coefficients for the text to speech and alignment tasks are 0.1 while speech to text tasks need a slightly higher loss coefficient 0.3 for the decoder loss.
- We pre-train two sets of speech-text models in which two different text vocabularies are used, i.e., a character-level model (Mu 2 SLAM-char) of 4096 chars and a spm-level model (Mu 2 SLAM-spm) of 64k word pieces.
- These two models run on 256 TPUv4 chips for 1.5M steps.
- Fine-tuning setup
- We fine-tune our pre-trained models on CoVoST-2 multilingual speech translation (Wang et al., 2021b), VoxPopuli multilingual speech recognition (Wang et al., 2021a), and XTREME multilingual text understanding (Hu et al., 2020) benchmarks.
- We report the detokenized BLEU scores calculated by the SacreBLEU script (Post, 2018).
- For each fine-tuning tasks, we use grid search to tune the hyperparameters including batch sizes per TPU over {2, 4, 8}, learning rates over {0.5, 1, 2, 3, 5}, dropout ratios for encoder inputs and Transformer decoder over {0.1, 0.3}, warm-up steps over {4k, 8k, 16k}.
- Generally, we observe that speech-related and text-related tasks are not very sensitive to the batch size so we use 8.
- Speech-related tasks prefer a larger learning rate of 5 while text-related tasks needs a smaller one of 1 or 0.5.
- The warm-up steps are universally set to 16k.
- The pre-trained spm-level model is in favor of a larger dropout of 0.3.
- In our multi-task multilingual fine-tuning experiments, the training examples of AST, ASR and MT for a batch is set to 4, 2 and 2.
- For AST, ASR and MT, we randomly incorporate synonym noises into decoder inputs, the noise ratio is set to 0.06.
- All of fine-tuning experiments are conducted on 64 TPUv4 chips.
- Except for the multi-task multilingual fine-tuning experiments in which we select a maximum fine-tuning step of 300k and report results from the last checkpoint, we pick the best model based on validation sets.

### Multilingual Speech Translation
- Table 1: BLEU scores on the CoVoST 2 dataset by fine-tuning the pre-trained models on English to non-English (en-xx) and non-English to English (xx-en) language pairs.
- Table 2: AST results on four English to Non-English (en-xx) directions from CoVoST 2 by following the identical setting in baseline methods (Wang et al., 2021c;Bapna et al., 2021;Chen et al., 2022;Zhang et al., 2022a).

### Multilingual Speech Recognition
- We use a recurrent neural network to generate ASR outputs
- The ASR outputs are good at recognizing the target language
- The ASR outputs are better than a baseline model
- The ASR outputs are better than a model trained on the same data
- The ASR outputs are better than a model trained on a different data set
- The ASR outputs are better than a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different ASR task
- The ASR outputs are better than a model trained on a different language and a model trained on a different task
- The ASR outputs are better than a model trained on a different language and a model trained on a different language and a model trained on a different task
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language and a model trained on a different task
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language and a model trained on a different task
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on a different language
- The ASR outputs are better than a model trained on a different language and a model trained on a different task and a model trained on a different task and a model trained on a different task and a model trained on...

### Multilingual Text Understanding
- Our speech-text pretrained models with respect to text understanding on the XTREME multilingual dataset (Hu et al., 2020)
- In the zero-shot setting, similar to classification results in Table 4, Mu 2 SLAMspm maintains better performance than Mu 2 SLAM-char.
- However, our Mu 2 SLAM models can not even surpass mT5base although we achieve significantly better results than mT5-base on XNLI.
- We analyze the breakdown results on each language pair attached in the appendix. We find our models is able to deliver good results on English but relatively worse results on non-English languages. More specifically, our models nearly approach 0 on Bengali language.
- After looking into the model outputs, we find the model can not properly output the correct tokens in non-English languages.

### Analysis
- Effect of paired data is studied by using different combination sets of AST, ASR and MT data during pre-training and fine-tuning stages
- In Row 1-4, only AST data is enabled for fine-tuning pre-trained models
- The Table 6. Effect of paired data when being incorporated into pretraining or finetuning stages. We report AST results from finetuned models with different setups. Notice that we only run all of our experiments for a certain number of steps (60k) for faster comparisons.
- Best model comes from Row 4 which digests all of available speech and text paired data.
- It is interesting that removing ASR data improves the model performance on en-xx directions. We speculate that translation data (AST and MT) are important to learn the alignment between encoder and decoder but ASR with very strong monotonic alignment hurts the translation alignment, particularly for en-xx in which a specific non-English translation data is not as abundant as English.
- When applying multi-task multilingual fine-tuning, we find the model with all data in pre-training does not perform the best.
- Row 6 without MT involved in pre-training takes the lead in the AST results. It is probably because text occupies model capacity in encoder pre-training but the effect of MT data is made up during the fine-tuning stage.
- These comparisons suggest that multi-task pre-training is beneficial to learning general speech-text representations if we do not have any assumption on downstream tasks.

## Related Work
- Speech-text pre-training methods have dominated research and industry fields due to their superior capabilities of exploiting unlabeled data.
- In recent days, the research community has started to move towards speechtext joint training, aiming to learn the shared representation space between speech and text.
- There are a lot of work along this research line which can be roughly categorized into, encoder-only pre-training (Bapna et al., 2021;2022;Zhang et al., 2022a), encoder-decoder pre-training (Ao et al., 2021;Chen et al., 2022;Sainath et al., 2022;Zhou et al., 2022;Zhang et al., 2022b;Tang et al., 2022).
- Our approach Mu 2 SLAM adopts an encoder-decoder backbone model but our model design minimizes the utilization of modality-specific blocks only with a CNN block used to extract speech representations, which dramatically simplify the cross-modal model architecture and enforce the representation sharing from the beginning.
- Among them, the most related Maestro (Chen et al., 2022) also incorporates ASR, AST and MT data into their method, however, their model training has to rely on a pre-trained mSLAM as initialization and applies a duration model to over-sample the text which is not activated during fine-tuning.
- In contrast, Mu 2 SLAM pre-trains the model from scratch which can be applied in the downstream tasks without wasting any parameter.
- We also verify our models on text-related benchmarks while they just focus on speech tasks.
- In addition, a text-to-speech loss is also introduced in pre-training which endows our model with the ability of speech generation.

## Conclusion
- The pre-training models span more than 100 languages in both speech and text and involve unlabeled data and labeled data from speech/text-only data, ASR, AST to MT.
- We not only introduce two kinds of training objectives to unify the unlabeled and labeled data in pre-training, but also propose gradual fine-tuning and noisy fine-tuning to improve the model performance on downstream tasks during fine-tuning.
- Extensive experiments on multilingual AST, ASR and MT benchmarks show that our pre-training models can achieve very strong results with new SOTA on CoVoST and comparable performance against mSLAMon VoxPopuli, and narrow the gap between speech-text models and text-only models on text tasks.
- As a text-to-speech pre-training loss is also applied in our pre-training, we would like to explore speech generation based on our pre-trained models in the future work.
