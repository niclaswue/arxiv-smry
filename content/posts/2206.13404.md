---
title: "Avocodo: Generative Adversarial Network for Artifact-free Vocoder"
date: 2022-06-27T15:54:41.000Z
author: "Taejun Bak, Junmo Lee, Hanbin Bae, Jinhyeok Yang, Jae-Sung Bae, Young-Sun Joo"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2206-13404v3.webp" # image path/url
    alt: "Avocodo: Generative Adversarial Network for Artifact-free Vocoder" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.13404)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.13404).


# Abstract
- Neural vocoders based on the generative adversarial neural network (GAN) are fast and lightweight,
- However, in preliminary experiments, they found that the multi-scale analysis which focuses on the low-frequency bands causes unintended artifacts, e.g., aliasing and imaging artifacts, which degrade the synthesized speech waveform quality.
- To address this issue, they propose a GAN-based vocoder, called Avocodo, that allows the synthesis of high-fidelity speech with reduced artifacts.
- The two kinds of discriminators used are a collaborative multi-band discriminator and a sub-band discriminator.
- Additionally, they use a pseudo quadrature mirror filter bank to obtain downsampled multi-band speech waveforms while avoiding aliasing.
- According to the results of their experiments, Avocodo outperforms baseline GAN-based vocoders, both objectively and subjectively, while reproducing speech with fewer artifacts.

# Paper Content

## Introduction

## Artifacts in GAN-based Vocoders
- Upsampling layers cause tonal artifacts
- Tonal artifacts appear as a horizontal line on spectrogram
- Additionally, mirrored low frequencies are observed in high-frequency bands, which are called imaging artifacts

### Aliasing in downsampling
- GAN-based vocoders use discriminators to evaluate downsampled waveforms to learn the spectral information in lowfrequency bands
- Typical downsampling methods, such as the average pooling used in (Kumar et al. 2019;Kong, Kim, and Bae 2020;Yang et al. 2020) or the equally spaced sampling used in (Kong, Kim, and Bae 2020;Kim et al. 2021;  Jang et al. 2021), are easy to implement and efficient for obtaining band-limited speech waveforms
- In preliminary experiments, however, aliasing was observed in the downsampled waveforms using the aforementioned methods
- Figure 2 illustrates examples of the downsampled waveforms using several approaches; the downsampling factor is set to 8
- Taking into consideration downsampling uses equally spaced sampling (Figure 2c), high-frequency components, which are supposed to be removed, fold back and distort the harmonic frequency components at low-frequency bands
- In the case of the average pooling (Figure 2d), which is a composition of a simple low-pass filtering and a decimation, aliasing is not as apparent in low-frequency bands but harmonic components over 800Hz are distorted
- As the downsampling factor increases, the artifacts increase too
- Using these distorted downsampled waveforms during the training makes it difficult for the model to generate accurate waveforms
- To avoid this aliasing, downsampling using a band-pass filter equipped with a high stopband attenuation is required

## Proposed Method
- Figure 3a is the overall architecture of Avocodo
- It consists of a single generator and the two proposed discriminators
- Taking a mel-spectrogram as input, the generator outputs not only full-resolution waveforms but also intermediate outputs
- Subsequently, the CoMBD discriminates the full-resolution waveform and its downsampled waveforms along with the intermediate outputs; the PQMF is used as a low-pass filter to downsample the full-resolution waveform
- Additionally, the SBD discriminates sub-band signals obtained by the PQMF analysis

### Generator
- The generator has the same structure as the HiFi-GAN generator, but it produces multi-scale outputs
- The generator has four sub-blocks, three of which G k (1 ≤ k ≤ 3) generate waveforms xk with the corresponding resolution of 1 2 3−k of the full resolution
- To elaborate, x3 is a full-resolution waveform; moreover, x1 and x2 denote intermediate outputs.
- Each sub-block comprises multi-receptive field fusion (MRF) blocks (Kong, Kim, and Bae 2020) and transposed convolution layers.
- The MRF blocks contain multiple residual blocks of diverse kernel sizes and dilation rates to capture the spatial features of input.
- Additional projection layers are added, unlike HiFi-GAN, after each subblock to return the intermediate outputs.

### Collaborative Multi-Band Discriminator
- The proposed CoMBD discriminates multi-scale outputs from the generator
- It comprises identical sub-modules, which evaluate waveforms at different resolutions
- Additionally, each sub-module is based on the discriminator module of MSD
- The module comprises fully convolutional layers and a leaky ReLU activation function
- Either a multi-scale structure (Figure 4a) or a hierarchical structure (Figure 4b) is commonly used in conventional GAN-based neural vocoders; however, in this paper, the two structures are combined to take advantage of each structure
- This collaborative structure helps the generator to synthesize high-quality waveforms with reduced artifacts
- The multi-scale structure increases speech quality by discriminating not only the full-resolution waveform but also the downsampled waveform (Kumar et al. 2019;Kong, Kim, and Bae 2020;Yang et al. 2021;Jang et al. 2021)
- In particular, the discrimination of waveforms downsampled into multiple scales helps the generator to focus on the spectral features in low-frequency bands (Kumar et al. 2019)
- Meanwhile, the hierarchical structure uses intermediate output waveforms of each generator sub-block, helping the generator to learn the various levels of acoustic properties in a balanced manner (Yang et al. 2020;Zhang, Xie, and Yang 2018)
- In particular, the generator sub-blocks are trained to learn expansion and filtering in a balanced way by inducing the sub-blocks of the generator to generate a band-limited waveform. Therefore, upsampling artifacts are expected to be suppressed by adopting the hierarchical structure
- For the proposed collaborative structure, the sub-modules at low resolution, i.e., CoMBD 1 and CoMBD 2 , take both the intermediate outputs x and the downsampled waveforms x as their inputs. For each resolution, both inputs share the sub-module. For example, as shown in Figure 3, the intermediate output x2 and the downsampled waveform x 2 share the weights of CoMBD 2 for output p 2 and p 2 , respectively. The intermediate output and downsampled waveform are intended to match each other after collaboration. Note that no additional parameters are necessary for collaborating the two structures because of the weight-sharing process
- To further improve speech quality by reducing artifacts, a differentiable PQMF is adopted to obtain downsampled waveform with restricted aliasing

### Sub-Band Discriminator
- An SBD is introduced that discriminates multiple sub-band signals by PQMF analysis.
- The PQMF enables the n th sub-band signal b n to contain frequency information corresponding to the range from (n − 1)f s /2N to nf s /2N , where f s is the sampling frequency and N is the number of sub-bands.
- Inspired by this characteristic of sub-band signals, the SBD sub-modules learn various discriminative features by using different ranges of the sub-band signals.
- Two types of sub-modules are designed: one captures the changes in spectral feature over the time axis and the other captures the relationship between each sub-band signal. These two sub-modules are referred to as tSBD and fSBD, respectively, in Figure 3a
- Training Objectives GAN Loss
- For training GAN networks, the least square adversarial objective (Mao et al. 2017) is used, which replaces a sigmoid cross-entropy term of the GAN training objective proposed in (Goodfellow et al. 2014) with the least square for stable GAN training.
- The GAN losses, V for multi-scale outputs and W for downsampled waveforms, are defined as follows:
- where x k represents the k th downsampled ground-truth waveform, and s denotes the speech representation.
- In this paper, mel-spectrogram is utilized.
- Feature Matching Loss
- Feature matching loss is a perceptual loss for GAN training (Salimans et al. 2016), which has been used in GAN-based vocoder systems (Kumar et al. 2019;Kong, Kim, and Bae 2020;Yang et al. 2020).
- Moreover, the feature matching loss of a sub-module in the discriminator can be established with L1 differences between the intermediate feature maps of the ground-truth and predicted waveforms.
- The loss can be defined as follows:
- where T denotes the number of layers in a sub-module.
- D t and N t represent the t th feature map and the number of elements in feature map, respectively.
- Reconstruction Loss
- Reconstruction loss based on a melspectrogram increases the stability and efficiency in the training of waveform generation (Yamamoto, Song, and Kim 2020).
- For that, L1 differences are calculated between the mel-spectrograms of the ground-truth x and predicted x speech waveforms.
- The reconstruction loss can be expressed as follows:
- where φ(•) denotes the transform function to melspectrogram.
- Final Loss
- Final loss for the overall system training can be established from the aforementioned loss terms and defined as follows:
- where D C p and D S q denote the p th sub-module of CoMBD and the q th sub-module of SBD, respectively.
- λ f m and λ spec denote the loss scales for feature matching and reconstruction losses, respectively.
- λ f m and λ spec are set as 2 and 45, respectively.

### Training Setup
- HiFi-GAN is a baseline model that uses discriminators based on multi-scale structure downsampling with average pooling and equally spaced sampling
- VocGAN uses a discriminator based on hierarchical structure downsampling with average pooling
- StyleMelGAN utilizes discriminators that discriminate the sub-band signals of random window selected signal obtained by PQMF analysis
- For the single speaker speech synthesis, Avocodo3 and HiFi-GAN were both trained up to 3M steps
- VocGAN and StyleMelGAN were trained up to 2.5M and 1.5M steps, respectively
- Next, for the unseen speaker synthesis, all models were trained up to 1M steps
- The hyper-parameters of Avocodo's generator are the same as that of the HiFi-GAN
- The HiFi-GAN generator has two versions with an identical architecture but different number of parameters: V 1 is larger than V 2
- The number of sub-bands N is 16 for tSBD and M is 64 for fSBD
- Moreover, the parameters of PQMF were selected empirically
- An AdamW optimizer (Loshchilov and Hutter 2019) was used with an initial learning rate of 0.002
- The optimizer parameters (β 1 , β 2 ) were set as (0.8, 0.99), and an exponential learning rate decay of 0.999 was applied (Kong, Kim, and Bae 2020)

## Experimental Results

### Audio Quality & Comparison
- The proposed model for each dataset was assessed using various subjective and objective measurements
- Subjective evaluation tests were conducted for single and unseen speaker syntheses
- For the English dataset, 15 native English speakers and 19 native Korean speakers participated for the Korean dataset
- All participants were requested to assess the sound quality of 20 audio samples randomly selected from each testset
- Table 1 lists subjective evaluation results
- We can see Avocodo V 1 performs the best performance in both single and unseen speaker synthesis tasks
- For synthesizing highquality speech waveform of unseen speakers, learning generalized characteristic of speech signals is crucial
- Because artifacts inhibit the generator from learning the generalized characteristics of speech signals, Avocodo's approaches for suppressing artifacts are much more robust than baseline models
- In particular, Avocodo outperforms baseline models even in Unseen(KR)
- Since the dataset includes various speech styles, the overall differences from the ground truth are larger than other datasets
- Note that StyleMelGAN even failed to train for Unseen(KR)
- Objective evaluation objectives were conducted to quantitatively compare vocoders
- To validate the reproducibility of F 0 , we measured the F 0 RMSE in the voiced frame
- Because the artifacts exist in very short regions (only a few frames), the average value of F 0 RMSE is insufficient to represent the artifacts
- Therefore, we further calculated the standard deviation of F 0 absolute error (F 0 AE-STD); a low F 0 AE-STD value means less distortion exists in harmonics
- For evaluating the accuracy in voiced/unvoiced (VUV) frame, false positive and negative rates of the VUV classification (VUV fpr , VUV fnr ) were measured
- To measure the perceived quality of the synthesized speech, the mel-cepstral distortion (MCD) (Kubichek 1993) and perceptual evaluation of speech quality (PESQ) (Rix et al. 2001) were calculated
- Additionally, we measured the log-spectral distance (Rabiner and Juang 1993;Han and Lee 2022) in low-frequency bands from 0Hz to 5.5kHz (LSD-LF) and in high-frequency bands from 5.5kHz to 11.02kHz Hz (LSD-HF); the low value of LSD-HF means the imaging artifacts are less
- Table 2 shows that Avocodo V 1 also outperformed baseline models in overall results

### Discriminator-wise Comparison
- The MOS test was conducted to compare the performances of the proposed discriminators
- All discriminators were trained with Avocodo's generator V 2. λ f m and λ spec are observed to empirically affect the training, therefore they were adjusted to 2 and 10, respectively.
- However, λ spec was adjusted to 20 for the CoMBD which has a larger loss value owed to weight-sharing.
- Table 3 shows that each Avocodo discriminator contributes to the generator synthesizing higher-quality speech with fewer artifacts.

### Analysis on artifacts
- Audio samples are generated with HiFi-GAN and Avocodo
- The ability of Avocodo to suppress artifacts is explained by observing the upsampling artifacts occurring in intermediate upsampling layers
- Audio samples of the first row of Figure 5 are the projected output of the last transposed convolution layer for upsampling from 1 2 f s to f s while skipping the last MRF block
- In Figure 6d, the harmonic components of the downsampled waveforms are distorted due to the aliasing caused by downsampling
- Downsampled waveforms with anti-aliasing PQMF preserve F 0 as shown in Figure 6e

## Conclusions
- Artifacts, such as upsampling artifacts and aliasing, are observed to originate from the limitation of the upsampling layer and the objective function biased towards the low-frequency bands obtained by naive downsampling methods.
- To solve these problems, two novel discriminators, namely CoMBD and SBD, are designed.
- The CoMBD performs multi-scale analysis with a collaborative structure of multi-scale and hierarchical structures.
- The SBD discriminates the sub-band signals decomposed by PQMF analysis in both time and frequency aspects. Furthermore, PQMF is utilized for downsampling and PQMF analysis.
- Various experimental results proved that these discriminators and the PQMF effectively reduce the artifacts in synthesized speech.
