---
title: "Avocodo: Generative Adversarial Network for Artifact-free Vocoder"
date: 2022-06-27T15:54:41.000Z
author: "Taejun Bak, Junmo Lee, Hanbin Bae, Jinhyeok Yang, Jae-Sung Bae, Young-Sun Joo"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2206-13404v3_yuOa_OLp2q.jpg" # image path/url
    alt: "Avocodo: Generative Adversarial Network for Artifact-free Vocoder" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.13404)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.13404).


# Abstract
- Neural vocoders based on the generative adversarial neural network (GAN) have been widely used
- The perceptually important speech components are primarily concentrated in the low-frequency
- bands, most GAN-based vocoders perform multi-scale analysis which evaluates
- discovered that the multi-scale analysis which focuses on the low-frequency
- bands causes unintended artifacts, e.g., aliasing and imaging artifacts, which
- degrade the synthesized speech waveform quality. Therefore, in this paper, we
- investigate the relationship between these artifacts and GAN-based vocoders and
- propose a GAN-based vocoder, called Avocodo, that allows the synthesis of
- high-fidelity speech with reduced artifacts. We introduce two kinds of
- discriminators to evaluate speech waveforms in various perspectives: a
- collaborative multi-band discriminator and a sub-band discriminator. We also
- utilize a pseudo quadrature mirror filter bank to obtain downsampled multi-band
- speech waveforms while avoiding aliasing. According to experimental results,
- Avocodo outperforms baseline GAN-based vocoders, both objectively and
- subjectively, while reproducing speech with fewer artifacts.

# Paper Content

## Abstract
- Neural vocoders based on the generative adversarial neural network (GAN) have been widely used
- The perceptually important speech components are primarily concentrated in the low-frequency
- bands, most GAN-based vocoders perform multi-scale analysis which evaluates
- discovered that the multi-scale analysis which focuses on the low-frequency
- bands causes unintended artifacts, e.g., aliasing and imaging artifacts, which
- degrade the synthesized speech waveform quality. Therefore, in this paper, we
- investigate the relationship between these artifacts and GAN-based vocoders and
- propose a GAN-based vocoder, called Avocodo, that allows the synthesis of
- high-fidelity speech with reduced artifacts. We introduce two kinds of
- discriminators to evaluate speech waveforms in various perspectives: a
- collaborative multi-band discriminator and a sub-band discriminator. We also
- utilize a pseudo quadrature mirror filter bank to obtain downsampled multi-band
- speech waveforms while avoiding aliasing. According to experimental results,
- Avocodo outperforms baseline GAN-based vocoders, both objectively and
- subjectively, while reproducing speech with fewer artifacts.

## Introduction
- Speech synthesis generates speech waveforms that correspond to the input text.
- An acoustic model initially generates acoustic features corresponding to the input text (Wang et al. 2017;Li et al. 2019;Ren et al. 2019Ren et al. , 2021)).
- A vocoder then converts the acoustic features into a speech waveform (Masanori, Yokomori, and Ozawa 2016;van den Oord et al. 2016).
- Recently, vocoders based on generative adversarial network (GAN) (Goodfellow et al. 2014) with non-autoregressive convolutional architectures have been proposed (Yamamoto, Song, and Kim 2020;Kumar et al. 2019;Yang et al. 2020;Kong, Kim, and Bae 2020;Yang et al. 2021;Mustafa, Pia, and Fuchs 2021;Kim et al. 2021).
- Compared to other neural vocoders (van den Oord et al. 2016;Prenger, Valle, and Catanzaro 2019;Kim et al. 2019;van den Oord et al. 2018), GAN-based vocoders are faster and lighter while still maintaining a high level of synthesized speech quality.
- Specifically, a generator converts input features such as random noise or a mel-spectrogram into speech waveforms.
- A discriminator then evaluates the generated speech waveforms.
- Because the speech spectrum in the low-frequency bands is much more crucial to perceptual quality, most GANbased vocoders perform multi-scale analysis that evaluates the downsampled speech waveforms.
- Multi-scale analysis allows a generator to focus on the speech spectrum in lowfrequency bands; downsampling limits the frequency range of speech by decreasing the sampling rate (Shannon 1949).
- In MelGAN (Kumar et al. 2019), a multi-scale discriminator (MSD) evaluates the downsampled waveforms that used an average pooling technique.
- In HiFi-GAN (Kong, Kim, and Bae 2020), a multi-period discriminator (MPD) specializing in periodic components was proposed.
- It discriminates downsampled waveforms obtained by using an equally spaced sampling technique with various periods.
- Consequently, these GAN-based vocoders have successfully increased the quality of synthesized speech (Yang et al. 2020;Kim et al. 2021;Jang et al. 2021).
- In preliminary experiments, however, we discovered that, GAN-based vocoders suffer from two major issues. The first issue is the artifacts caused by the upsampling layer (Pons et al. 2021).
- For example, artifacts in high-frequency bands degrade the quality of speech by introducing noise.
- The second issue is the degraded reproducibility of the harmonic components.
- The fundamental frequency (F 0 ) of synthesized speech is often inaccurate (Morrison et al. 2022) with the aliasing during simple downsampling, such as an average pooling or an equally spaced sampling, being one of the reasons behind this problem.
- These artifacts significantly reduce the perceptual quality, when synthesizing speech with large pitch variation (Lorenzo-Trueba et al. 2019;Zaïdi et al. 2022).

## Artifacts in GAN-based Vocoders
- The upsampling layer incorporates transposed convolution, which causes tonal artifacts
- Tonal artifacts appear as horizontal lines on spectrogram
- Additionally, mirrored low frequencies are observed in high-frequency bands, which are called imaging artifacts
- In digital signal processing, the signal is upsampled by inserting zeros between neighboring samples, and then applying low-pass filtering
- Without the filtering, low-frequency components appear in high-frequency bands, as shown in Figure 1b
- The upsampling layer should also remove enough of the unintended frequency components, but it is unable to meet that criteria
- As shown in Figure 1c, the unintended frequency components, which are imaging artifacts, eventually degrade the speech quality by distortion in high-frequency bands
- Such artifacts are similar to texture sticking of image generative models

### Aliasing in downsampling
- GAN-based vocoders use discriminators to evaluate downsampled waveforms to learn the spectral information in lowfrequency bands
- typical downsampling methods, such as the average pooling used in (Kumar et al. 2019;Kong, Kim, and Bae 2020;Yang et al. 2020) or the equally spaced sampling used in (Kong, Kim, and Bae 2020;Kim et al. 2021;  Jang et al. 2021), are easy to implement and efficient for obtaining band-limited speech waveforms
- in preliminary experiments, however, aliasing was observed in the downsampled waveforms using the aforementioned methods
- Figure 2 illustrates examples of the downsampled waveforms using several approaches
- the downsampling factor is set to 8
- taking into consideration downsampling uses equally spaced sampling (Figure 2c), high-frequency components, which are supposed to be removed, fold back and distort the harmonic frequency components at low-frequency bands
- in the case of the average pooling (Figure 2d), which is a composition of a simple low-pass filtering and a decimation, aliasing is not as apparent in low-frequency bands but harmonic components over 800Hz are distorted
- as the downsampling factor increases, the artifacts increase too
- using these distorted downsampled waveforms during the training makes it difficult for the model to generate accurate waveforms
- downsampling using a band-pass filter equipped with a high stopband attenuation is required

## Proposed Method
- Figure 3a is the architecture of Avocodo
- The architecture includes a single generator and the two proposed discriminators
- The generator outputs not only full-resolution waveforms but also intermediate outputs
- Subsequently, the CoMBD discriminates the full-resolution waveform and its downsampled waveforms along with the intermediate outputs
- The PQMF is used as a low-pass filter to downsample the full-resolution waveform
- Additionally, the SBD discriminates sub-band signals obtained by the PQMF analysis

### Generator
- The generator has the same structure as the HiFi-GAN generator
- The generator produces multi-scale outputs that is composed of both high-resolution and intermediate waveforms
- The generator has four sub-blocks, three of which G k (1 ≤ k ≤ 3) generate waveforms xk with the corresponding resolution of 1 2 3−k of the full resolution
- To elaborate, x3 is a full-resolution waveform; moreover, x1 and x2 denote intermediate outputs. Each sub-block comprises multi-receptive field fusion (MRF) blocks and transposed convolution layers. The MRF blocks contain multiple residual blocks of diverse kernel sizes and dilation rates to capture the spatial features of input. Additional projection layers are added, unlike HiFi-GAN, after each subblock to return the intermediate outputs.

### Collaborative Multi-Band Discriminator
- The CoMBD discriminates multi-scale outputs from the generator
- It comprises identical sub-modules, which evaluate waveforms at different resolutions
- Additionally, each sub-module is based on the discriminator module of MSD
- The module comprises fully convolutional layers and a leaky ReLU activation function
- Either a multi-scale structure (Figure 4a) or a hierarchical structure (Figure 4b) is commonly used in conventional GAN-based neural vocoders; however, in this paper, the two structures are combined to take advantage of each structure
- This collaborative structure helps the generator to synthesize high-quality waveforms with reduced artifacts
- The multi-scale structure increases speech quality by discriminating not only the full-resolution waveform but also the downsampled waveform (Kumar et al. 2019;Kong, Kim, and Bae 2020;Yang et al. 2021;Jang et al. 2021)
- In particular, the discrimination of waveforms downsampled into multiple scales helps the generator to focus on the spectral features in low-frequency bands (Kumar et al. 2019)
- Meanwhile, the hierarchical structure uses intermediate output waveforms of each generator sub-block, helping the generator to learn the various levels of acoustic properties in a balanced manner (Yang et al. 2020;Zhang, Xie, and Yang 2018)
- In particular, the generator sub-blocks are trained to learn expansion and filtering in a balanced way by inducing the sub-blocks of the generator to generate a band-limited waveform. Therefore, upsampling artifacts are expected to be suppressed by adopting the hierarchical structure
- For the proposed collaborative structure, the sub-modules at low resolution, i.e., CoMBD 1 and CoMBD 2 , take both the intermediate outputs x and the downsampled waveforms x as their inputs. For each resolution, both inputs share the sub-module. For example, as shown in Figure 3, the intermediate output x2 and the downsampled waveform x 2 share the weights of CoMBD 2 for output p 2 and p 2 , respectively. The intermediate output and downsampled waveform are intended to match each other after collaboration.
- Note that no additional parameters are necessary for collaborating the two structures because of the weight-sharing process

### Sub-Band Discriminator
- An SBD is introduced that discriminates multiple sub-band signals by PQMF analysis.
- The PQMF enables the n th sub-band signal b n to contain frequency information corresponding to the range from (n − 1)f s /2N to nf s /2N , where f s is the sampling frequency and N is the number of sub-bands.
- Inspired by this characteristic of sub-band signals, the SBD sub-modules learn various discriminative features by using different ranges of the sub-band signals.
- Two types of sub-modules are designed: one captures the changes in spectral feature over the time axis and the other captures the relationship between each sub-band signal.
- These two sub-modules are referred to as tSBD and fSBD, respectively, in Figure 3a.
- Training Objectives GAN Loss
- For training GAN networks, the least square adversarial objective (Mao et al. 2017) is used, which replaces a sigmoid cross-entropy term of the GAN training objective proposed in (Goodfellow et al. 2014) with the least square for stable GAN training.
- The GAN losses, V for multi-scale outputs and W for downsampled waveforms, are defined as follows:
- In this paper, mel-spectrogram is utilized.
- Feature Matching Loss
- Feature matching loss is a perceptual loss for GAN training (Salimans et al. 2016), which has been used in GAN-based vocoder systems (Kumar et al. 2019;Kong, Kim, and Bae 2020;Yang et al. 2020).
- Moreover, the feature matching loss of a sub-module in the discriminator can be established with L1 differences between the intermediate feature maps of the ground-truth and predicted waveforms.
- The loss can be defined as follows:
- D t and N t represent the t th feature map and the number of elements in feature map, respectively.
- Reconstruction Loss
- Reconstruction loss based on a melspectrogram increases the stability and efficiency in the training of waveform generation (Yamamoto, Song, and Kim 2020).
- For that, L1 differences are calculated between the mel-spectrograms of the ground-truth x and predicted x speech waveforms.
- The reconstruction loss can be expressed as follows:
- Final Loss
- Final loss for the overall system training can be established from the aforementioned loss terms and defined as follows:
- λ f m and λ spec denote the loss scales for feature matching and reconstruction losses, respectively.
- λ f m and λ spec are set as 2 and 45, respectively.

### Training Setup
- HiFi-GAN1 uses discriminators based on multi-scale structure downsampling with average pooling and equally spaced sampling
- VocGAN uses a discriminator based on hierarchical structure downsampling with average pooling
- StyleMelGAN2 uses discriminators that discriminate the sub-band signals of random window selected signal obtained by PQMF analysis
- For the single speaker speech synthesis, Avocodo3 and HiFi-GAN were both trained up to 3M steps
- VocGAN and StyleMelGAN were both trained up to 2.5M and 1.5M steps, respectively
- Next, for the unseen speaker synthesis, all models were trained up to 1M steps
- The hyper-parameters of Avocodo's generator are the same as that of the HiFi-GAN
- The HiFi-GAN generator has two versions with an identical architecture but different number of parameters: V 1 is larger than V 2, and Avocodo also follows this rule.

## Experimental Results

### Audio Quality & Comparison
- The performance of the proposed model for each dataset was assessed using various subjective and objective measurements
- Subjective evaluation 5-scale mean opinion score (MOS) tests were conducted for single and unseen speaker syntheses.
- For the English dataset, 15 native English speakers and 19 native Korean speakers participated for the Korean dataset.
- All participants were requested to assess the sound quality of 20 audio samples randomly selected from each testset.
- Table 1 lists subjective evaluation results.
- We can see Avocodo V 1 performs the best performance in both single and unseen speaker synthesis tasks.
- For synthesizing highquality speech waveform of unseen speakers, learning generalized characteristic of speech signals is crucial.
- Because artifacts inhibit the generator from learning the generalized characteristics of speech signals, Avocodo's approaches for suppressing artifacts are much more robust than baseline models.
- In particular, Avocodo outperforms baseline models even in Unseen(KR).
- Since the dataset includes various speech styles, the overall differences from the ground truth are larger than other datasets.
- Note that StyleMelGAN even failed to train for Unseen(KR).
- Objective evaluation Objective evaluations were conducted to quantitatively compare vocoders.
- To validate the reproducibility of F 0 , we measured the F 0 root mean square error (F 0 RMSE) in the voiced frame.
- Because the artifacts exist in very short regions (only a few frames), the average value of F 0 RMSE is insufficient to represent the artifacts.
- Therefore, we further calculated the standard deviation of F 0 absolute error (F 0 AE-STD); a low F 0 AE-STD value means less distortion exists in harmonics.
- For evaluating the accuracy in voiced/unvoiced (VUV) frame, false positive and negative rates of the VUV classification (VUV fpr , VUV fnr ) were measured.
- To measure the perceived quality of the synthesized speech, the mel-cepstral distortion (MCD) (Kubichek 1993) and perceptual evaluation of speech quality (PESQ) (Rix et al. 2001) were calculated.
- Additionally, we measured the log-spectral distance (Rabiner and Juang 1993;Han and Lee 2022) in low-frequency bands from 0Hz to 5.5kHz (LSD-LF) and in high-frequency bands from 5.5kHz to 11.02kHz Hz (LSD-HF); the low value of LSD-HF means the imaging artifacts are less.
- Table 2 shows that Avocodo V 1 also outperformed baseline models in overall results.
- In particular, due to the methods for suppressing upsampling artifacts, LSD-HF results show that Avocodo improves reproducibility in highfrequency bands.
- Avocodo also takes advantage of reduced aliasing in training. Training with aliased waveform makes it easy to distort harmonic components.
- Because of antialiasing methods of Avocodo, F 0 AE-STD and VUV errors are improved.
- Despite the smaller number of parameters in discriminators, Avocodo also performs better than HiFi-GAN.

### Discriminator-wise Comparison
- The MOS test was conducted to compare the performances of the proposed discriminators
- All discriminators were trained with a generator V2.λf m and λ spec . λ spec was adjusted to 2 for the CoMBD.
- Table 3 shows that each Avocodo discriminator contributes to the generator synthesizing higher-quality speech with fewer artifacts.

### Analysis on artifacts
- The ability of Avocodo to suppress artifacts is explained by observing the upsampling artifacts occurring in intermediate upsampling layers of the generator.
- Audio samples are generated with HiFi-GAN and Avocodo, and their linear-scale spectrograms are depicted in Figure 5.
- Audio samples of the first row of Figure 5 are the projected output of the last transposed convolution layer for upsampling from 1 2 f s to f s while skipping the last MRF block; Figure 5a
- To observe the distortion in F 0 caused by aliasing, we trained GAN-based vocoders with singing voice datasets with a large range of F 0 .
- Large-scale downsampling for adequately modeling low-frequency components causes incomplete F 0 reconstruction; for example, HiFi-GAN and VocGAN downsample by a factor of up to 11 and 16, respectively.
- In Figure 6d, the harmonic components of the downsampled waveforms are distorted due to the aliasing caused by downsampling, while downsampled waveforms with anti-aliasing PQMF preserve F 0 as shown in Figure 6e.
- Therefore, HiFi-GAN (Figure 6b), trained using distorted downsampled waveforms, fails to reconstruct F 0 higher than 750Hz, while Avocodo (Figure 6c) preserves F 0 contour.

## Conclusions
- Artifacts, such as upsampling artifacts and aliasing, originate from the limitation of the upsampling layer and the objective function biased towards the low-frequency bands obtained by naive downsampling methods.
- To solve these problems, two novel discriminators, namely CoMBD and SBD, are designed.
- The CoMBD performs multi-scale analysis with a collaborative structure of multi-scale and hierarchical structures.
- The SBD discriminates the sub-band signals decomposed by PQMF analysis in both time and frequency aspects. Furthermore, PQMF is utilized for downsampling and PQMF analysis.
- Various experimental results proved that these discriminators and the PQMF effectively reduce the artifacts in synthesized speech.
