---
title: "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models"
date: 2022-12-15T18:45:29.000Z
author: "Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, Kellie Webster"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08037v1.webp" # image path/url
    alt: "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08037)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08037).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/attributed-question-answering-evaluation-and).

# Abstract

# Paper Content

## Introduction
- Large language models (LLMs) have shown impressive results across a variety of natural language understanding and generation tasks
- In attributed question answering the input to the model is a question, and the output from the model is an answer string together with a pointer to a short segment of text that supports that answer.
- There is increasing evidence that LLMs may have potential in information-seeking scenarios, producing compelling output in scenarios ranging from "simple" question answering (e.g., Kwiatkowski et al. (2019); Rajpurkar et al. (2016); Joshi et al. (2017)), to long-form question answering (Amplayo et al., 2022;Stelmakh et al., 2022), and information-seeking dialog (Thoppilan et al. (2022);Glaese et al., 2022;Shuster et al., 2022;Nakano et al., 2021).
- This lack of direct supervision is particularly appealing given the difficulties of constructing labeled datasets for even simple question answering,
- we propose Attributed Question Answering (QA). In our formulation, the input to the model/system is a question, and the output is an (answer, attribution) pair where answer is an answer string, and attribution is a pointer into a corpus of attribution units (e.g. paragraphs).
- The returned attribution should give supporting evidence for the answer; for example, it should satisfy the conditions in Rashkin et al. (2021) (see Section 3.1).

## Related Work
- A study of how to improve the efficiency of algorithms
- A study of how to improve the accuracy of algorithms
- A study of how to improve the speed of algorithms
- The study of how to improve the efficiency of algorithms focuses on ways to make the algorithms run faster.
- The study of how to improve the accuracy of algorithms focuses on ways to make the algorithms more accurate.
- The study of how to improve the speed of algorithms focuses on ways to make the algorithms run faster.

### Question Answering Tasks
- Question answering has emerged as a key way to discover and demonstrate advances in large language models
- Reading comprehension is a QA task in which a model takes as input a question and a passage which possibly contains the answer to the question, and is asked to extract that answer
- Since, there has been a proliferation of reading comprehension datasets developed to benchmark different machine capabilities that are important for QA
- The Natural Questions (Kwiatkowski et al., 2019) effort provided a large reading comprehension dataset based on real information-seeking queries to the Google search engine, and has served more recently via Open-NQ (Lee et al., 2019) as a benchmark for open-domain question answering
- In open-domain QA in which a system receives an input query and must return its answer based on a provided corpus of documents
- Open-domain question answering was first approached using retrievethen-read pipelines, which use a trained retrieval engine to find relevant passages, before performing reading comprehension over these to deduce an answer
- In response, answer selection (Yang et al., 2015) was proposed as a sub-task of question answering, wherein a model needed to produce a sentence answering each question (e.g. Garg et al., 2019)
- Both retrieval and reading comprehension have since been developed further, e.g. using neural indexing (Tay et al., 2022;Wang et al., 2022), dual encoders (Ni et al., 2021) and few-shot prompting (Chowdhery et al., 2022)
- Retrievethen-read architectures are proposed as one class suitable for Attributed QA in Section 4.
- Roberts et al. (2020) further strips the degree of direct supervision required for a question answering task, by showing that T5 (Raffel et al., 2020) can perform closed-book question answering
- Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to "remember" information digested in pretraining. This result is tantalizing because it opens the possibility of more powerful question answering than so far realized in proposed task datasets
- Unfortunately, it is difficult to know how accurate the resulting provenance metric is: it is likely that it is still only a subset of potential provenance paragraphs that have been identified as gold labels (see Remark 1 in Section 3.1), and no measurements of coverage are given in the paper

### LLMs with Attribution
- This work is an important reference in attributed question answering
- The evaluation is limited
- Of 307 questions presented to raters, only 115 questions were retained for evaluation, with the remaining 192 (62.5% of questions) being discarded due to raters skipping some items
- Little is said about the basis on which raters skipped items, but this makes the 80% accuracy of the system hard to interpret

## Attributed Question Answering
- Defines the attributed questionanswering task
- Discussion

### Task Definition
- to be the fraction of test examples for which the NLI classifier returns a 1 for g(x i ).
- to be a better measure of attribution than human ratings, as it is not influenced by human bias.
- to be more accurate than human ratings in development settings.
- to be more accurate than human ratings in testing settings.
- to be more accurate than human ratings in general.
- to be more accurate than human ratings in specific domains.
- to be more accurate than human ratings in specific questions.
- to be more accurate than human ratings in specific sources.
- to be more accurate than human ratings in specific times.
- to be more accurate than human ratings in specific contexts.
- to be more accurate than human ratings in specific languages.
- to be more accurate than human ratings in specific documents.
- to be more accurate than human ratings in specific paragraphs.
- to be more accurate than human ratings in specific sentences.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific sentences of a paragraph.
- to be more accurate than human ratings in specific sentences of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific sentences of a paragraph.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific sentences of a paragraph.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be more accurate than human ratings in specific paragraphs of a document.
- to be...

### Discussion
- The size of the label space for many questions is likely to be large
- This is due to two causes, first there may be multiple paragraphs that support a given answer, and second, for many questions there may be a diverse set of answers that have some supporting paragraph in the literature.
- Attribution allows either a system developer or user to see the underlying source supporting an answer, and to assess trustworthiness and other aspects of the underlying source.
- Attribution deliberately avoids the need for judgments requiring "factuality" of answers or utterances, something that is challenging for all but the most simple questions.
- Closed-book QA evaluations are a highly effective and convenient measure of LLM performance, but they have two significant drawbacks- 1. Closed-book QA evaluations do not require a system to provide attribution for its answers, and 2. Closed-book QA evaluations depend on goldcurated labels, which can be difficult to obtain for questions with diverse answers.
- Human ratings are a primary motivation for attributing QA because of the size of the space of (a, c) pairs.

## Approaches to Attributed QA
- Class 1: No supervision
- Class 2: Limited supervision
- Class 3: Full supervision
- At a high level, systems in Class 1 are no supervision, while systems in Class 3 are full supervision.
- In Class 2, there is limited supervision, which means that the computer system is not always aware of what the user is doing.
- In Class 1, the computer system is always aware of what the user is doing.
- Class 1 systems are more vulnerable to security breaches.
- At a high level, systems in Class 1 are no supervision, while systems in Class 3 are full supervision.
- In Class 2, there is limited supervision, which means that the computer system is not always aware of what the user is doing.
- In Class 1, the computer system is always aware of what the user is doing, but it is more vulnerable to security breaches.

### Architectures
- Retrieve-then-read (RTR) models first perform retrieval of k relevant passages based on the input question alone, where k is a relatively small number.
- A second-stage model then takes P ⊂ k retrieved passages, possibly reranked, as input to generate a short answer, and chooses one of A ⊂ k retrieved passages as support for that answer.
- In our experiments, we used BM25 (Robertson and Zaragoza, 2009) for sparse retrieval, GTR (Ni et al., 2021) for dense retrieval, and Fusion-in-Decoding (FID, Izacard et al., 2022) for answer generation.
- FID may be trained with T ⊂ k retrieved passages as input to answer generation, to reduce memory requirements.
- GTR may be used in the open-source version (PT-GTR in following tables), or further tuned for NQ (GTR).
- In these systems, an LLM is first used to generate an answer to the input question, typically using few-shot prompting, without any use of retrieval.
- The question and answer are then concatenated to form a query to sparse or dense retrieval, again giving k relevant passages.
- For k > 1, a final step selects the highest scoring passage containing the answer generated by the LLM as the attribution.
- An LLM is used to generate both an answer and a pointer into the attribution corpus through some combination of prompting and fine-tuning but without any use of either sparse or dense retrieval.

### Supervision
- Systems can be differentiated as follows: NQ-64 systems have very limited supervision, in the form of 64 randomly chosen training examples from the Natural Questions (consisting of question/answer pairs)
- When NQ-full is used to select prompt exemplars in post-hoc pipelines, the 64 most similar examples based on the BM25 score are used.
- Fine-tuning or prompting can be used to provide supervision for NQ-64 or NQ-full data.

### Best Systems
- The next section of the paper describes experiments with a number of systems.
- We briefly highlight four particularly important ones (and two variants) that achieve the highest AIS score for their architecture class.
- In addition to these, we will explore ablations of various components that they use in the next section.
- Best RTR system. GTR is first used for retrieval of top k = 50 passages from an input query, and the NQ-reranker is then used to rerank these passages.
- FID is trained with T = 50 passages but generates an answer based on the top P = 1 passage, which is returned as the attribution.
- (Note that this approach is very close to the approach of Izacard et al. (2022), with the added final passage selection step that allows evaluation for attribution.)
- Best post-hoc retrieval system. A prompted version of a 540B parameter PaLM produces an answer to the question.
- The prompts are 64 question/answer pairs from the Natural Questions training set, chosen at random.
- GTR is then used for retrieval of an attribution paragraph, using the question concatenated with answer as the input query, by selecting the passage in the top k = 50 which contains the PaLM-predicted answer string.
- Best low-resource system (very close to unsupervised). A prompted version of a 540B parameter PaLM produces an answer to the question.
- Again, prompts are 64 question/answer pairs from the Natural Questions training set, chosen at random (hence both are NQ-64 systems).
- BM25 is used for post-hoc retrieval, again with the question/answer pair concatenated to form the query.
- We refer to this system as "very close to unsupervised" as it only requires 64 NQ examples, it does not require fine-tuning, and the underlying retrieval method does not require supervision or fine-tuning.
- Best LLM-as-retriever system. We explore the possibility of more end-to-end approaches to Attributed QA by fine-tuning a 540B parameter PaLM to generate an answer and Wikipedia URL, given a question input.
- The fine-tuning data used for this was questions generated by a question-generation model trained on SQuAD (Rajpurkar et al., 2016) over decontextualized (Choi et al., 2021) Wikipedia sentences, as well as the Natural Questions training set.
- The paragraph in the generated Wikipedia page with the highest BM25 score is used as the attribution.
- AutoAIS reranked variants. To fairly assess how good RTR and PaLM post-hoc pipelines are at producing answers which could be attributed, we additionally experiment with system variants where AutoAIS is used as a reranker.
- These are identical to the above RTR and post-hoc systems, but instead use AutoAIS scores to select the attribution passages: the retrieved passage in the top k = 50 with highest AutoAIS score is selected as the attribution (and so is used to generate the answer in RTR).
- Since these variants use AutoAIS as a system component, we evaluate performance only on AIS (not AutoAIS).

## Experiments
- Technical details for the datasets and evaluation metrics used
- System results
- Analysis of the available evaluation metrics

### Datasets

### Evaluation Metrics
- The AIS measure assessed by human raters, as described in Section 3.1.
- Raters are prompted with the AIS guidelines from Rashkin et al. ( 2021) and trained using repeated annotations with feedback, until reaching high performance on the task (Fleiss' κ > 0.97).
- We take the majority vote from 5 raters to make the final decision of attribution.
- Our human evaluation pipeline performs continuous assessment of new system predictions and our AIS (only) results are preliminary and reflect the assessment of only a subset of the question set (between 300-350 questions per system).
- Since we make no assumption that batches are processed in an order that coheres to strict random sampling, we are mindful that it is possible that the results may shift to some degree as more batches of human Low resource 39.5 41.9 53.9 ± 2.7 LLM-as-retriever 50.1 41.5 51.5 ± 2.7
- The highest-AIS systems in each architecture and reranked variants, as outlined in Section 4.
- Auto-AIS reranking, Auto-AIS is used to select attribution passages, to assess how good the system is at producing answers which could be attributed.
- Auto-AIS is not reported for reranked variants given its use as a system component.

### System Results
- Table 1: Best AIS score for systems in each architecture class with Auto-AIS variants.
- Table 2: EM correlates modestly with human judgment of AIS.
- Table 3: Ablation studies show that dense retrieval outperforms sparse retrieval.
