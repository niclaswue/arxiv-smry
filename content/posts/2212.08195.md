---
title: "Improving Chess Commentaries by Combining Language Models with Symbolic Reasoning Engines"
date: 2022-12-15T23:38:31.000Z
author: "Andrew Lee, David Wu, Emily Dinan, Mike Lewis"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08195v1.webp" # image path/url
    alt: "Improving Chess Commentaries by Combining Language Models with Symbolic Reasoning Engines" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08195)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08195).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/improving-chess-commentaries-by-combining).

# Abstract
- State-of-the-art language models lack grounding in the real world
- advances in the symbolic reasoning capabilities of AI have led to systems that outperform humans in games like chess and Go
- Chess commentary provides an interesting domain for bridging these two fields of research
- In this work we demonstrate how to combine symbolic reasoning engines with controllable language models to generate chess commentaries
- We conduct experiments to demonstrate that our approach generates commentaries that are preferred by human judges over previous baselines.

# Paper Content

## Introduction
- Large-scale language models can generate plausible responses that share no relationship with the true state of the world
- Symbolic reasoning systems that play games such as chess or Go at a higher level than humans or purely neural models have been demonstrated
- In this work, we combine a controllable language model with a symbolic reasoning engine to generate chess commentaries.

## Related work
- Chess commentary generation
- Language models
- Related work
- Two streams
- Commentary
- Generation
- Models
- Related work

### Chess commentary generation
- Chess commentary generation was first proposed by Jhamtani et al.
- The authors craft features to represent chess game-states, which is attended to by a decoder model.
- Zang et al. (2019) extend this work by using AlphaZero (Silver et al., 2018) to encode the game-state.
- Similar tasks include Shogi commentary generation (Kameko et al., 2015).

### Controlled text generation
- Text generation is often controlled by conditioning a language model with specific signals
- This can be done by providing special tokens, often special tokens, to the language model
- This allows researchers to control various aspects of text generation, such as style, content, or task-specific behavior.

### Reasoning systems with language models
- Researchers have successfully combined symbolic reasoning agents with language generation models for different tasks.
- FAIR et al. (2022) builds a language model that interfaces with a strategic reasoning model to play Diplomacy, a dialogue-based board game.
- He et al. (2018) and Yarats and Lewis (2018) build competitive negotiating agents by decoupling their language generation model from a reasoning agent, in which the language generation model is conditioned on the decisions or actions made by the underlying strategic agents.
- Lastly, researchers have used pragmatic reasoning to improve text generation of image descriptions or instructions (Andreas and Klein, 2016;Fried et al., 2018).

## Task description and data
- The task is to find all pairs of elements in a given set that satisfy a given condition
- The data is a list of pairs of integers
- The condition is that the sum of the two integers in the pair is greater than or equal to a given number
- The list of pairs of integers is provided
- A pair is a two element list
- The sum of the two integers in a pair is the sum of the two integers in the list
- The given number is 3

### Task: Chess Commentary Generation
- The goal of chess commentary generation is to provide a description or analysis of moves made in a chess game.
- Analyses include suggesting alternative or future moves, or explaining the quality of a move.
- The strategic nature of chess makes our task interesting, in that it requires robust language modeling as well as strong symbolic reasoning.
- Borrowing Jhamtani et al. (2018)'s notations, given a game-state G, move M , and corresponding commentary text C, we want to model P (C|G, M ).
- Jhamtani et al. (2018) introduces 6 categories of commentary types: move descriptions, move quality, move comparisons, rationales, contextual, and general.
- In this work we focus on the first three categories.

### Data
- We use commentary data from an online forum
- We collect a set of 373,919 triplets in the form of (G, M, C)

### Additional pre-training data
- We use data from chess forums to pre-train "tag-extraction" models
- We build question and response pairs using each of the following forums: Chess.com, Chess.com has a dedicated forum for community members to post their games and ask for feedback. Feedback is often provided both at a strategic high-level and for specific moves. We use all question and answer pairs in which the question contains a chess game, resulting in 11,657 pairs.4, StackExchange, StackExchange contains question and answer pairs pertaining to chess. Questions contain up to 5 categorical tags, while responses have arbitrarily many up or down votes. We filter out irrelevant tags and non-positive upvote responses, resulting in 11,549 pairs. r/chess, r/chess contains multiple threads regarding chess. We use [f] "While rook to d4 is not a bad move, Ne3 is better." "Qd4 is questionable, as Bd4 is much stronger." "Black plays Bc5, a popular response to the Italian Game. Nf6 was also a good choice" a pre-existing Reddit dataset (Baumgartner et al., 2020) extracted by a third party and made available on PushShift. Of that data, we use threads in which the response post contains chess move notation or other game-specific text (see appendix). We use a total of 32,826 dialogues. Across all three datasets, any personally identifiable information (PII) was scrubbed. Given such question and response pairs, our pretraining task (ยง4.2) is to generate the response given the question as input.

## Methodology
- We provide an overview of our approach
- We explain each step
- We first provide an overview of our approach
- We explain each step
- We provide an overview of our approach
- We explain each step
- We provide an overview of our approach
- We explain each step
- We provide an overview of our approach
- We explain each step

### Overview
- Figure 1: A visual representation of our approach
- Given training data in the form of (G, M, C) (Figure 1 [b]), we first build a token representation of game-state 5 https://files.pushshift.io/reddit/ G ([a]).
- We then extract various "tags" T from commentary C that describe the game-state using tag-extraction models ([c]).
- Tags are factual information regarding the game-state that is mentioned in the commentary, such as quality of a move or alternative lines of moves.
- The goal is for the extracted tags T to provide signals that are consistent with the semantics of the commentary text, such that they can be used as control codes by the commentary generation model.
- The commentary generation model is then trained to be conditioned on the extracted tags: P (C|G, M, T ) ([d]).
- An example of our task can be found in [e.1], which depicts [e.2].
- Once our controllable commentary generation model is trained, during inference, a new game-state G is passed into a chess engine, which provides new tags T to control our commentary generation ([f]).
- Details regrading each step are provided below.

### Training
- We identify tags T to extract from our commentary training data.
- For each tag type, we build a tag-extraction model P (T |C), then annotate our commentary generation data P (C|G, M ) as P (C|G, M, T ), giving us finer control over the text generation task.
- It is critical that the extracted tags are consistent with the semantics of the commentary text. These tags provide the commentary generation model explicit signals to learn meaningful patterns between game-state G and commentary C during training.
- For the first 3 tags, we build tagextraction models using BART (Lewis et al., 2020) that is further pre-trained using the data described in ยง3.3.
- We then fine-tune each BART model using manually annotated samples as described below.
- Commentary Type tags indicate which of the six categories of commentaries from Jhamtani et al. (2018) that a training example belongs to.
- To train a commentary type extraction model, we manually label 1,800 random samples of commentaries with one of the six categories.
- We then train a classifier by adding a linear classification layer to the last decoder layer of our pre-trained BART model. The pre-trained weights for BART are frozen, and only the linear layer is learned.
- Move Quality tags indicate how good or bad a move is. Chess communities use a standard set, consisting of excellent, good, inaccurate, mistake, and blunder.
- We manually annotate 610 commentaries, with an additional class "None" if the commentary does not refer to move quality.
- We train a similar classifier as described above.
- We demonstrate our approach on 3 of the 6 categories (move description, move quality, move comparison), but our approach can be applied to the remaining categories using additional tags.
- Because we extract all 6 categories, our model technically can commentate regarding the remaining categories, but will likely be incorrect without additional tags.
- Details regarding training is provided in the Appendix.
- Suggested Moves tags indicate alternative lines of moves that are mentioned in the commentary.
- We manually label 830 commentaries, annotating the suggested moves S using standard algebraic notation (SAN) to create (C, S) pairs.
- S is given the value "None" if there is no move being suggested.
- Unlike the previous two classification models, here we use a generative model. Namely, given (C, S), we fine-tune our pre-trained BART model to generate S given C.
- In practice, we observe two additional tags that improve our commentaries.
- Because our commentary data is from a community forum, the text includes multiple perspectives, including personal pronouns and proper nouns (grandmasters).
- To generate a consistent perspective that contain neither, we tag our commentary data with pronouns and proper nouns that appear in each commentary.
- We use SpaCy's named entity recognizer9 to extract these tags.
- Length is correlated with the length of the generated commentary and its quality. Short ones are often uninteresting ("White captures."), while longer ones are more prone to including an error.
- We enumerate the pieces on the board for each player.
- For each player, we represent each piece by the piece-type, followed by their current square, separated by an underscore ("White R_c5").
- Attacks are represented by listing the attacking piece, followed by the attacked piece, separated by a special token ("$").
- Moves are represented using SAN. Tags are preceded by special tokens to indicate the type of tag ("[Move Quality] Good").

### Inference
- After training our commentary generation model, during inference, we use a chess engine to derive new tags to control our commentary generation.
- We use Leela11, an AlphaZero (Silver et al., 2018) variant agent that plays chess at a super-human level.
- Concretely, given a new game-state G and move M , we use Leela to derive new tags T , which in turn controls the generation of commentaries.
- We explain how we derive each tag below.
- Commentary Type tags are simply selected by the user depending on what kind of analysis they wish to generate.
- Move Quality Given game-state G and move M , we derive three values using Leela: P M : the probability of winning after playing move M , M * : the optimal move at game-state G, and P M* : the probability of winning had M * been played.
- We use the difference between P M* and P M to classify the quality of move M using a pre-defined set of ranges.
- Given a move quality classification, we format it the same way we did for training.
- (Figure 1 [f]).
- To commentate on move suggestions, we simply use Leela's suggested moves M * ("[Suggested Move] Ne4").
- Pronouns, Proper Nouns Recall that any commentary training data that contain pronouns or proper nouns are tagged.
- In order to generate commentary that contain neither, during inference, we always omit any pronoun or proper noun tags.
- Length For inference, we use "medium" as our length tag to control the length of our outputs.

## Results
- The accuracy of the text extraction models
- Experiments to assess how well the models are able to control the commentaries
- Use of human judges to compare the quality of the commentaries

### Tag-extraction models
- 80% of the time, the models are able to correctly identify the game-state and resulting commentary
- 85% of the time, the models are able to generate the correct commentary
- Test accuracy is high enough to provide control over the commentary generation model

### Perplexity
- Figure 2: Drop in perplexity compared to different baselines, each of which are BART models trained on different input representations and tags

### Human evaluations
- Our model generates more accurate commentaries than baseline models
- Crowdworkers often mention that our system understands good or bad plays and provides correct move suggestions

## Discussion: limitations and challenges
- Logical reasoning errors are still occasionally generated by the model
- A more comprehensive interface could help relieve the language model of the burden of reasoning tasks
- Crowdworkers sometimes disagree with the model's assessment, even when the commentaries are accurate
- Grounding error analysis is used to better understand why the model makes mistakes

## Conclusion
- Uses symbolic reasoning system and controllable language model to generate chess commentaries
- Uses Reddit data to find question-answer pairs that are likely referring to a specific chess game
- Uses patterns to match comment posts against a set of patterns
- Minimizes the validation perplexity during training
- Uses classification models to determine the type of commentary that was written
- Uses up to 12 prompts per game-state (2 players * 6 chess pieces), excluding cases in which the prompted piece is not on the board
