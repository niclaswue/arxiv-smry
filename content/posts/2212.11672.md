---
title: "Trustworthy Social Bias Measurement"
date: 2022-12-20T18:45:12.000Z
author: "Rishi Bommasani, Percy Liang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11672v1.webp" # image path/url
    alt: "Trustworthy Social Bias Measurement" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11672)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11672).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/trustworthy-social-bias-measurement).

# Abstract
- Measures of social bias should be trustable based on cross-disciplinary theory of measurement
- To combat the frequently fuzzy treatment of social bias in NLP, we explicitly define social bias
- We operationalize our definition by proposing a general bias measurement framework DivDist
- To validate our measures, we propose a rigorous testing protocol with 8 testing criteria

# Paper Content

## Introduction
- Language technologies are increasingly critical to our lives and to broader societal function
- Social bias is a central consideration (Hovy and Spruit, 2016;Bender et al., 2021;Weidinger et al., 2022, inter alia): how we represent people and what we associate them with has material consequences
- Biased language technology can cause several types of harm (Dev et al., 2022;Bommasani et al., 2021, §5.1): allocational (e.g. lower hiring rates for marginalized groups due to algorithmic resume screening), representational (e.g. associating Muslims with violence), and psychological (e.g. stereotype threat)
- Measurement functions as the primary lens for understanding social bias in NLP
- And measurement is seen as an essential to successfully reducing bias: to determine if an intervention mitigates bias, the measured bias should decrease due to the intervention
- If all paths forward for making progress on bias in NLP pass through measurement, then what is the current state of bias measurement?
- Many works have proposed bias measures, spanning different settings like text, vector representations, language models, and task-specific models (see Blodgett et al., 2020;Dev et al., 2022). Most measure bias between two social groups. However, no standard exists for what evidence is required to trust these measures: works provide a mixture of intuitive, empirical, and theoretical justifications.
- Perhaps as a consequence, many works are subject to scrutiny: measures have been shown to be brittle (Ethayarajh et al., 2019;Nissim et al., 2020;Antoniak and Mimno, 2021;Delobelle et al., 2022), contradictory (Bommasani et al., 2020), unreliable (Aribandi et al., 2021;Seshadri et al., 2022), invalid (Blodgett et al., 2021), and the space overall is unclear on what bias means and what metrics purport to measure (Blodgett et al., 2020).
- Trust is necessary: for metrics to productively guide progress and inform decision-making, we must trust them.
- Consequently, we focus on trustworthy bias measurement.
- We apply measurement modeling to address this challenge: measurement modeling is an expansive theory used across the social sciences to design and validate measures of (complex) social constructs.
- Therefore, measurement modeling is well-suited to social bias measurement: the theory has a longstanding tradition for similar social constructs, including even for social bias in humans (e.g. the Implicit Association Test; Chequer, 2014).
- Under measurement modeling, we must first define the theoretical construct of social bias. In contrast, Blodgett et al. (2020) showed many works in NLP failed to (adequately) define social bias.
- To define social bias, we draw upon principles in so-arXiv:2212.11672v1 [cs.CL] 20 Dec 2022 cial science research; these principles dictate how we operationalize our definition into a general measurement framework.
- Our measurement framework DivDist, based on divergences between probability distributions, improves over prior work in two key ways: (i) compatibility, meaning bias measures can be instantiated for several settings (e.g. text, vector representations) that yield comparable measurements and (ii) multi-group, meaning we are not restricted to binary bias measurement.
- These properties are valuable for NLP: for example, we may want to understand how different processes change biases (e.g. the potential bias amplification between training data and a learned model, or between a generative model and its samples).
- Beyond offering generality, our framework also makes explicit that bias is fundamentally a relative phenomenon, which has been neglected in all prior work.
- To meaningfully measure bias, one must state the normative reference frame: what would constitute (no...

## Principles for Social Bias
- Social bias is defined in terms of social groups and a target concept
- Social bias is reduced to associations between the target concept and social groups
- Social bias must be systematic

### Bias is Relative
- It depends on the bias measure.
- Most bias measures in NLP ignore this fundamental property of bias.
- Bias is an inherently relative construct, which requires a reference to be specified.

### Defining Social Bias
- Social bias is the divergence in the observed associations between a target concept and a set of social groups from corresponding reference associations.
- Definition 2.1 (Social Bias) specifies that social bias is the asymmetry in the observed associations between a target concept and a set of social groups.
- To measure social bias, we propose our twostage measurement framework DivDist.
- First, given parameters, which specify the associations of interest, DivDist yields a bias measure bias.
- Second, given inputs (i.e. the target concept, social groups, and reference mentioned in our distribution), bias yields a bias measurement bias(T, G 1 , . . . , G k ; p 0 ).
- Parameters. To map from the abstract framework DivDist to a concrete bias measure bias, we specify three functions (SoA, normalize, D).
- First, SoA quantifies the strength of association between the target concept and a social group as a numerical value in R ≥0 .
- Second, we normalize s to a categorical distribution p using normalize.
- Third, we quantify the divergence using D between the (normalized) observed associations p and the reference associations p 0 , which we also specify as a categorical distribution distributed over the groups.
- Observe the clear correspondence between our framework DivDist and our definition: Step 1 extracts the observed associations, Step 2 prepares these associations, and Step 3 measures the divergence from reference associations. This correspondence indicates our measures demonstrate structural fidelity (Loevinger, 1957), one of 8 desiderata we consider in measurement modeling.
- Inputs. To further map from the bias measure bias to the bias measurement bias(T, G 1 , . . . , G k ; p 0 ), we specify three inputs (T, G 1 , . . . , G k ; p 0 ).
- In many cases, we will represent social groups G 1 , . . . , G k and the target concept T using word lists, i.e. representative words that embody the associated concept.
- Further, for the reference p 0 , we will specify it as a categorical probability distribution distributed over the k social groups, which encodes the association between each group and the target concept when there is no bias.

### Text
- Systematic bias in text manifests in distributional statistics
- To implement SoA text , we quantify associations in text based on these statistics
- The association between concept T and group G j in a corpus L is quantified as follows: Contexts, mentions, and associations.

### Word Embeddings
- We quantify the strength of association between words using cosine similarity
- This similarity metric has been found to be closely resembling how Caliskan et al. (2017) and Garg et al. (2018) quantify association

### Contextualized Representations
- Most prior measures compute a single bias value for contextualized representations
- We argue this is a type error and present two context-sensitive approaches that quantify strength of association
- Reduction to SoA WE produces high-quality static embeddings from contextualized representations
- Probing is the key downside to the reduction approach

### Normalization and Divergence Parameters
- DivDist requires normalization to ensure the bias measures are fully instantiated.
- For conceptual simplicity, the default settings for normalize and D are dividing a vector by its sum.
- These settings correspond to our proof, where we show our measure generalizes several prior measures.
- We show that our measurements are quite robust to these choices empirically in our sensitivity analysis.

## Testing Protocol
- We use measurement modeling to build trust in our measure of bias
- This measure has been used in many social science disciplines
- Following Messick and Jackman, we believe that our measure of bias is reliable

## Testing Protocol for Validity
- Face validity: the measure passes the "sniff test"
- Measure faithfully reflects theoretical understanding of the construct
- Measure correlates with other credible measures of the same construct
- Measure enables scientific inquiry related to the construct
- Measure's eventual usage amounts to desirable social impact
- Inter-annotator agreement: the measure measurements are stable up to difference in annotators
- Measurements are stable up to difference in (hyper)parameters
- Content validity: the measure reflects theoretical understanding of the underlying construct; the measure's structure should match the construct's structure
- Convergent validity: prior measures are (known to be) credible
- Predictive validity: the measure is predictive of measures of related constructs
- Hypothesis validity: the measure is useful for addressing scientific hypotheses

## Testing Protocol for Reliability
- measures must be agreed upon by multiple annotators in order to be reliable
- measures involving human judgments are generally reliable
- measures are stable to variations in input data

## Related Work
- In the social sciences, work across many disciplines has qualitatively characterized social bias in specific corpora of interest (e.g. Blumberg, 2007;Atir and Ferguson, 2018).
- While several quantitative measures have recently been proposed (Rudinger et al., 2017;Bordia and Bowman, 2019;Field and Tsvetkov, 2020;Falenska and Çetinoglu, 2021;Sun and Peng, 2021;Mitchell et al., 2022), to our knowledge, these methods have neither been significantly adopted to facilitate social science research nor to measure bias in NLP datasets.
- We find this surprising given especially how large text corpora have been instrumental to the rise of language models in the field (Peters et al., 2018;Devlin et al., 2019;Brown et al., 2020, inter alia), alongside growing broader interest in dataset documentation and governance (Caswell et al., 2021;Bandy and Vincent, 2021;Dodge et al., 2021;Bender and Friedman, 2018;Gebru et al., 2021;Jernite et al., 2022).
- For this reason, we apply our measures to bias measurement on both sides of language modeling: the initial human-authored training corpora as well as the final machine-generated samples, and our measures have been similarly applied in the HELM benchmark for many language models and use cases (Liang et al., 2022).
- Mechanically, our bias measures for text, as well as other bias measures for text (e.g. Bordia and Bowman, 2019), bear strong resemblance to the estimates of mutual information introduced by Church and Hanks (1989).
- Representations. Bolukbasi et al. (2016) initiated the study of bias measurement for word embeddings, with a growing collection of such measures (e.g. More recently, these measures have been adapted to measure bias in contextualized representations, generally by reducing measurement to the word embedding setting (Bommasani et al., 2020), either by specifying a singular canonical context (May et al., 2019;Tan and Celis, 2019;Ross et al., 2021) or averaging representations across many contexts (Bommasani et al., 2020;Guo and Caliskan, 2020;Steed and Caliskan, 2021).
- In comparison to prior measures for representations, we delineate the following differences. First, our measures are the only existing measures that are directly constructed under a unified framework for text and representation bias measurement. As we show in Table 6, this enable us to study the effects of training (a transformation from text to representations) and generation (a transformation from representations to text).
- All of our measures permit multiclass bias measurement, which is necessary given the underlying social categories are generally nonbinary.
- To our knowledge, the measure of Manzini et al. (2019) (and measures that directly extend it) is the only prior measure that also extends to the multiclass setting.
- Given this, we further examined this measure to understand the difference between it and our measures. Empirically, in Table 5, we found our measure was highly correlated with both diachronic and contemporary trends in employment, whereas the measure of Manzini et al. (2019) was either uncorrelated or anti-correlated, indicating it lacks predictive validity.
- Further, in Table 7, we found that mitigation methods that...

## Discussion of Measurement Modeling
- Measurement modeling is an interdisciplinary theory with a long history
- Our work joins a growing collection of recent works that embrace measurement modeling in computational and AI contexts
- For social bias in NLP, recent works use measurement modeling to identify failures in the validity (Blodgett et al., 2021) and reliability (Zhang et al., 2020a;Du et al., 2021) of existing bias measures
- In contrast, our work is the first to argue for the trustworthiness of social bias measures based on testing via measurement modeling
- With that said, we emphasize that this does not unequivocally cement the trustworthiness of our measures, especially in contexts they have not been tested in: we have shown our measures pass the tests we introduce, but there certainly may be (and likely are) others that would demonstrate their weaknesses

## Conclusion
- Accumulate evidence to warrant trusting bias measures
- Our work contributes a general measurement framework DivDist to measure bias, based on principles in social science
- Together, this makes the case for our social bias measures being trustworthy
- However, as Messick (1987Messick ( , 1988)) explains, the task of validating a measure is an ongoing process
- All code is made available at https://github.com/rishibommasani/BiasMeasures
- We aim to release further tooling to facilitate adoption of our measures in future work along with documentation of the impact of our measures over time
