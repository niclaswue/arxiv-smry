---
title: "Traditional and Heavy-Tailed Self Regularization in Neural Network Models"
date: 2019-01-24T08:20:42.000Z
author: "Charles H. Martin, Michael W. Mahoney"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/1901-08276v1_5FGCnlgrK.jpg" # image path/url
    alt: "Traditional and Heavy-Tailed Self Regularization in Neural Network Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1901.08276)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1901.08276).


# Abstract
- Random Matrix Theory is used to analyze weight matrices of Deep Neural Networks
- The empirical spectral density (ESD) of DNN layer matrices displays signatures of
- For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed

# Paper Content

## Introduction
- From the earliest days of DNNs, it was suspected that VC theory did not apply to these systems
- It was originally assumed that local minima in the energy/loss surface were responsible for the inability of VC theory to describe NNs
- However, it was very soon realized that the presence of local minima in the energy function was not a problem in practice
- At the time, there did exist other theories of generalization based on statistical mechanics
- Instead, VC theory and related techniques continued to remain popular, in spite of their obvious problems
- More recently, theoretical results of Choromanska et al. (which are related to [6,7,8,9]) suggested that the Energy/optimization Landscape of modern DNNs resembles the Energy Landscape of a zero-temperature Gaussian Spin Glass; and empirical results of Zhang et al. (which are related to [11]) have again pointed out that VC theory does not describe the properties of DNNs
- Martin and Mahoney then suggested that the Spin Glass analogy may be useful to understand severe overtraining versus the inability to overtrain in modern DNNs
- We should note that it is not even clear how to define DNN regularization. The challenge in applying these well-known ideas to DNNs is that DNNs have many adjustable "knobs and switches," independent of the Energy Landscape itself, most of which can affect training accuracy, in addition to many model parameters.
- Evaluating and comparing these methods is challenging, in part since there are so many, and in part since they are often constrained by systems or other nottraditionally-ML considerations
- Motivated by this situation, we are interested here in two related questions.
- Theoretical Question. Why is regularization in deep learning seemingly quite different than regularization in other areas on ML; and what is the right theoretical framework with which to investigate regularization for DNNs?
- Practical Question. How can one control and adjust, in a theoretically-principled way, the many knobs and switches that exist in modern DNN systems, e.g., to train these models efficiently and effectively, to monitor their effects on the global Energy Landscape, etc.? That is, we seek a Practical Theory of Deep Learning, one that is prescriptive and not just descriptive. This theory would provide useful tools for practitioners wanting to know How to characterize and control the Energy Landscape to engineer larger and betters DNNs; and it would also provide theoretical answers to broad open questions as Why Deep Learning even works.

### Marchenko-Pastur (MP) theory for rectangular matrices
- MP theory considers the density of singular values ρ(ν i ) of random rectangular matrices W.
- This is equivalent to considering the density of eigenvalues ρ(λ i ), i.e., the ESD, of matrices of the form X = W T W.
- MP theory then makes strong statements about such quantities as the shape of the distribution in the infinite limit, it's bounds, expected finite-size effects, such as fluctuations near the edge, and rates of convergence.
- To apply RMT, we need only specify the number of rows and columns of W and assume that the elements W i,j are drawn from a distribution that is a member of a certain Universality class (there are different results for different Universality classes).
- RMT then describes properties of the ESD, even at finite size; and one can compare perdictions of RMT with empirical results.
- Most well-known is the Universality class of Gaussian distributions. This leads to the basic or vanilla MP theory, which we describe in this section.
- More esoteric-but ultimately more useful for us-are Universality classes of Heavy-Tailed distributions.
- In Section 2.2, we describe this important variant.
- No edge.
- Frechet Table 1: Basic MP theory, and the spiked and Heavy-Tailed extensions we use, including known, empirically-observed, and conjectured relations between them.
- Boxes marked " * " are best described as following "TW with large finite size corrections" that are likely Heavy-Tailed [25], leading to bulk edge statistics and far tail statistics that are indistinguishable.
- Boxes marked " * * " are phenomenological fits, describing large (2 < µ < 4) or small (0 < µ < 2) finite-size corrections on N → ∞ behavior. See [26,25,27,28,29,30,31,32,21,33] for additional details.
- Gaussian Universality class.
- We start by modeling W as an N × M random matrix, with elements from a Gaussian distribution, such that:
- Then, MP theory states that the ESD of the correlation matrix, X = W T W, has the limiting density given by the MP distribution ρ(λ):
- Here, σ 2 mp is the element-wise variance of the original matrix, Q = N/M ≥ 1 is the aspect ratio of the matrix, and the minimum and maximum eigenvalues, λ ± , are given by Finite-size Fluctuations at the MP Edge.
- In the infinite limit, all fluctuations in ρ N (λ) concentrate very sharply at the MP edge, λ ± , and the distribution of the maximum eigenvalues ρ ∞ (λ max ) is governed by the TW Law.
- Even for a single finite-sized matrix, however, MP theory states the upper edge of ρ(λ) is very sharp; and even when the MP Law is violated, the TW Law, with finite-size corrections, works very well at describing the edge statistics.
- When these laws are violated, this is very strong evidence for the onset of more regular non-random structure in the DNN weight matrices, which we will interpret as evidence of Self-Regularization.

### Heavy-Tailed extensions of MP theory
- MP-based RMT is applicable to a wide range of matrices
- In statistical physics, it is common to model strongly-correlated systems by Heavy-Tailed distributions
- The reason is that these models exhibit, more or less, the same large-scale statistical behavior as natural phenomena in which strong correlations exist
- recent results from MP/RMT have shown that new Universality classes exist for matrices with elements drawn from certain Heavy-Tailed distributions
- We use these Heavy-Tailed extensions of basic MP/RMT to build an operational and phenomenological theory of Regularization in Deep Learning
- Briefly, our theory for simple Self-Regularization is inspired by the Spiked-Covariance model of Johnstone
- Our theory for more sophisticated Heavy-Tailed Self-Regularization is inspired by the application of MP/RMT tools in quantitative finance by Bouchuad, Potters, and coworkers
- In this paper, we highlight basic results for this generalized MP theory; see [26,25,27,28,29,30,31,32,21,33] in the physics and mathematics literature for additional details
- Universality classes for modeling strongly correlated matrices
- Consider modeling W as an N × M random matrix, with elements drawn from a Heavy-Tailed-e.g., a Pareto or Power Law (PL)-distribution:
- (In these cases, if W is element-wise Heavy-Tailed, then the ESD ρ N (λ) likewise exhibits Heavy-Tailed properties, either globally for the entire ESD and/or locally at the bulk edge.
- Table 1 summarizes these recent results, comparing basic MP theory, the Spiked-Covariance model, and Heavy-Tailed extensions of MP theory, including associated Universality classes.
- To apply the MP theory, at finite sizes, to matrices with elements drawn from a Heavy-Tailed distribution of the form given in Eqn. (2), we have one of the following three Universality classes.
- (Weakly) Heavy-Tailed, 4 < µ: Here, the ESD ρ N (λ) exhibits "vanilla" MP behavior in the infinite limit, and the expected mean value of the bulk edge is λ + ∼ M −2/3 . Unlike standard MP theory, which exhibits TW statistics at the bulk edge, here the edge exhibits PL / Heavy-Tailed fluctuations at finite N . These finite-size effects appear in the edge / tail of the ESD, and they make it hard or impossible to distinguish the edge versus the tail at finite N .
- (Moderately) Heavy-Tailed, 2 < µ < 4: Here, the ESD ρ N (λ) is Heavy-Tailed / PL in the infinite limit, approaching ρ(λ) ∼ λ −1−µ/2 . In this regime, there is no bulk edge. At finite size, the global ESD can be modeled by ρ N (λ) ∼ λ −(aµ+b) , for all λ > λ min , but the slope a and intercept b must be fit, as they display large finite-size effects. The maximum eigenvalues follow Frechet (not TW) statistics, with , and they have large finite-size effects. Thus...

## Empirical Results: ESDs for Existing, Pretrained DNNs
- Small DNNs have well-understood deviations from the theoretical ρ mp (λ), while large DNNs have a different form of ESD
- We retrain a modern variant of one of the very early and well-known Convolutional Nets-LeNet5 to see if the ESD changes
- For the larger, modern models, we examine selected layers from AlexNet, InceptionV3, and many other models (as distributed with pyTorch)
- The ESD of AlexNet FC2 (full in Figures 1(c), and zoomed-in in 1(d)) differs even more profoundly from standard MP theory
- PL is the best fit by comparing the distribution to a Truncated Power Law (TPL), as well as an exponential, stretch-exponential, and log normal distributions

## 5+1 Phases of Regularized Training
- MP Soft Rank is a ratio of the largest eigenvalue of the matrix to the noise part of the matrix
- The MP Soft Rank is a measure of how well the matrix fits the ESD
- The MP Soft Rank decreases as the ESD gets more strongly correlated
- The noise term in the weight matrix is modeled as small in the Bleeding-out, Bulk+Spikes, and Bulk-decay phases, and as Heavy-Tailed in the Rank-collapse phase

## Empirical Results: Detailed Analysis on Smaller Models
- The basic architecture consists of two 2D Convolutional layers, each with Max Pooling and Batch Normalization, giving 6 initial layers;
- it then has two Fully Connected (FC), or Dense, layers with ReLU activations; and it then has a final FC layer added, with 10 nodes and softmax activation.
- W F C1 is a 4096 × 384 matrix (Q ≈ 10.67);
- W F C2 is a 384 × 192 matrix (Q = 2); and
- W F C3 is a 192 × 10 matrix.
- All models are trained using Keras 2.x, with TensorFlow as a backend.
- We use SGD with momentum, with a learning rate of 0.01, a momentum parameter of 0.9, and a baseline batch size of 32; and we train up to 100 epochs.
- We save the weight matrices at the end of every epoch, and we analyze the empirical properties of the W F C1 and W F C2 matrices.

## Explaining the Generalization Gap by Exhibiting the Phases
- The Generalization Gap refers to the peculiar phenomena that DNNs generalize significantly less well when trained with larger mini-batches (on the order of 10 3 - 10 4 )
- In this section, we demonstrate that smaller batch sizes lead to more well-regularized models
- This regularization can be either the more traditional Tikhonov-like regularization or the Heavy-Tailed Self-Regularization corresponding to strongly-correlated models

## Discussion and Conclusion
- Our theory can be applied to other types of layers
- Initial results suggest yes, but the situation is more complex than the relatively simple picture we have described here
