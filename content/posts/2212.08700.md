---
title: "'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers"
date: 2022-12-16T20:01:22.000Z
author: "James A. Michaelov, Benjamin K. Bergen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08700v1.webp" # image path/url
    alt: "'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08700)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08700).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/rarely-a-problem-language-models-exhibit).

# Abstract
- Language models appear to perform poorly on quantification
- We ask how badly
- 'Few'-type quantifiers, as in 'few children like vegetables' might pose a particular challenge for Language Models, since the sentence components without the quantifier are likely to co-occur, and because 'few'-type quantifiers are rare
- Not only do the models perform poorly on 'few'-type quantifiers, but overall the larger the model, the worse its performance
- We interpret this inverse scaling as suggesting that larger models increasingly reflect online rather than offline human processing, and argue that decreasing performance of larger models may challenge uses of Language Models as the basis for Natural Language Systems

# Paper Content

## Introduction
- Quantifiers can dramatically alter the meaning of an utterance.
- Language models struggle to predict which quantifier is used in a given context.
- As model size increases, performance decreases rather than increases.
- Inverse scaling is an issue of serious concern from both the perspectives of developing and training new language models; and from the perspective of considering their use.

### Language Models
- The GPT-2, GPT-3, and OPT language models were used in the paper
- The GPT-Neo language model was used in a previous paper
- The InstructGPT language model was used in a previous paper

### Evaluation
- For each stimulus sentence, we calculate the surprisal of the critical word, that is, the word for which the N400 response was measured in the original study.
- Because humans only encounter the context preceding the critical word when processing the word, and because the language models we analyze are all autoregressive, we only consider the surprisal of the critical word given its preceding context.
- To do this we truncated the sentence before the critical word, and then used the relevant language model to calculate the probability p of the target word given the preceding context, which was then converted to surprisal S following Equation 1.
- In previous work of this type (e.g. Ettinger, 2020), only words that were single tokens in the models' vocabularies were used. In this study, all models are autoregressive, so for multi-token words, consecutive sub-word tokens can be predicted, the product of which is a well-defined probability for the whole word.
- The surprisal of such words, then, is the sum of the surprisals of the subword tokens.
- Calculating surprisal this way allows us to compare the predictions of all the models for all the stimuli in the original experiment.
- In order to evaluate how well each model takes into account the quantifier in its predictions, we compared which of the two possible critical words (typical or atypical) had a lower surprisal, i.e., was more strongly predicted by the model.
- Following human plausibility judgements, following a mosttype quantifier, the typical continuation was judged to be the correct one, and following a few-type quantifier, the atypical continuation was judged to be the correct one.
- Accuracy was calculated as the fraction of the stimulus pairs for which the model predicted the appropriate critical word-that is, predicted the correct continuation more strongly than the incorrect one.
- For example, the set of stimuli presented in (2) is made up of 4 pairs of stimuli, and for a model to achieve 100% accuracy (4/4), it would need to predict (a) over (b), (d) over (c), (e) over (f), and (h) over (g). This design intrinsically controls for any differences in unconditioned probability among the final words themselves.

## Results
- All models show the same general tendency in accuracy: as model size increases, word prediction following most-type quantifiers improves, but it degrades following few-type quantifiers.
- From GPT-2 762M to 1542M and from InstructGPT 13B to 175B, while mostperformance increases, few-performance does not decrease. Furthermore, from OPT 125M to 350M, and from OPT 2.7B to 6.7B, there is actually a slight improvement.
- However, that these differences are small compared to the overall decreases in performance, and the general trends are still clear-for example, no model performs better than a model two or more steps below it in size.

## Discussion

### Inverse scaling with quantifiers
- As models increase in size, they tend to improve at predicting words following most-type quantifiers
- In this study, models that were better at predicting the more typical word after a most-type quantifiers were also found to be worse at predicting the less typical word following a least-type quantifier
- Since the models were evaluated on which of the two options they predicted to be more likely, this suggests that in general, the larger models made predictions increasingly in accordance with typicality, overwhelming any sensitivity to quantifier type

### Further implications
