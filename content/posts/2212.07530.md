---
title: "Causes and Cures for Interference in Multilingual Translation"
date: 2022-12-14T22:30:55.000Z
author: "Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, Shruti Bhosale"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07530v1.webp" # image path/url
    alt: "Causes and Cures for Interference in Multilingual Translation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07530)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07530).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/causes-and-cures-for-interference-in).

# Abstract
- Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference.
- While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited.
- This work identifies the main factors that contribute to interference in multilingual machine translation.
- Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancing the amount of interference between low and high resource language pairs effectively, and can lead to superior performance overall.

# Paper Content

## Introduction
- Multilingual machine translation models can benefit from transfer between different language pairs (synergy), but may also suffer from interference (Ha et al., 2016;Firat et al., 2016;Aharoni et al., 2019;Arivazhagan et al., 2019).
- While there are methods to reduce interference and achieve better performance (Wang et al., 2020a;Kreutzer et al., 2021;Wang et al., 2021), such approaches are often more expensive, and do not always work (Xin et al., 2022).
- In this work, we demonstrate that interference in multilingual translation largely occurs when the model is very small compared to the abundance of training data, and that the simple principled approach of enlarging the model and tuning the data sampling temperature provides a consistent solution to the interference problem that can even promote synergy.
- This work methodically deduces the most simple ways of reducing interference in multilingual translation.
- We begin by inquiring what are the dominant factors that may interfere with learning to translate a particular language pair of focus s → t, in the context of learning a multilingual translation model with many different language pairs.
- Controlled experiments show that besides model size and number of s → t training examples, the main factor that correlates with the level of interference is the proportion of focus pair examples (s → t) observed out of the total number of examples (all language pairs) seen at each training step on average.
- Surprisingly, aspects like language similarity or number of translation directions have a much smaller effect.
- In model and data scaling experiments, we observe that interference mainly occurs in extreme parameter poverty, when the language pair of focus is data-rich, but has to "share" a crowded parameter space with large quantities of other data.
- Enlarging the model to standard model sizes in machine translation literature alleviates interference and even facilitates synergy.
- For context, given a language pair of 15M sentence pairs that accounts for 20% of the total training data (75M), we observe severe levels of interference with 11M-and 44M-parameter transformers, but no interference when scaling the model to 176M parameters (the "big" model of Vaswani et al. (2017)) and significant synergy with 705M parameters.
- Interestingly, when the model is large enough, we find that increasing the amount of non-focus data to a certain point can further increase synergy.
- Finally, given the evidence that data sizes and ratios strongly correlate with interference, we experiment with a natural lever that controls the proportion of each dataset in the overall mix in the simplest way: sampling temperature. Indeed, we find that calibrating the distribution of language pairs via temperature can substantially reduce the amount of interference in both high-and lowresource language pairs.

## Measuring Interference
- We assume a common multilingual translation setup that involves L language pairs s → t, where the source is always the same language s (English), and the target language t varies (English-to-many), or vice versa (many-to-English).
- The overall training data is a union of these training subsets, we note their sizes by D s→t . Sampling a training example x follows the distribution:
- T = 1 maintains the original data proportions, 0 < T < 1 starves low resource language pairs, and T > 1 increases their representation in the training distribution.
- We mostly focus on the English-to-many setting in which interference is more apparent.
- We define interference as a negative interaction between different translation directions in a multilingual translation model. It is measured for a specific translation direction s → t by the relative difference in performance (test-set cross-entropy loss) between a bilingual model trained to translate only from s to t (L bi s→t ) and a multilingual counterpart that is trained to translate other additional directions (L multi s→t ):
- We use the original2 transformer-base and transformer-big variants, as well as a smaller and a larger versions by adjusting the width of the architecture (Table 1).
- Data We use the multilingual benchmark introduced by Siddhant et al. (2020)
- Tokenization We build a shared vocabulary of 64K BPE tokens with sentencepiece (Kudo and Richardson, 2018) using a sampling temperature of 5 to increase the lower resource languages' representation.
- We use this vocabulary for all our experiments.
- We also add language ID tokens to our vocabulary, which are prepended to each source and target sequence to indicate the target language (Johnson et al., 2017).
- Training We use Fairseq (Ott et al., 2019) to train transformer models with the Adam optimizer (Kingma and Ba, 2015) for up to 100K steps, with a dropout rate of 0.1, inverse square root learning rate schedule up to a maximum of 0.004, 8K warmup steps, and a batch size of 256K tokens.
- We choose the best checkpoint according to the average validation loss of all language pairs.

### Does Language Similarity Matter?
- Intuitively, data from languages that humans perceive as similar should have a more positive effect on translation quality comparing to data from distinct languages.
- To test this, we fix a focus language, and train trilingual models to translate from English to two languages, the focus language and an additional interfering language.
- We then look at interference trends as we vary the  interfering language while controlling the amount of training data for each language pair.
- Setup We run two sets of experiments, one with Spanish (es, 15.2M parallel sentences) as the focus language, and another with Estonian (et, 2.2M examples).
- For each focus language, we select one of four interfering languages; Spanish is paired with French,5 Czech, Russian, and Chinese, while Estonian is paired with Finnish,6 French, Russian, and Chinese.
- To control the effects of data size in the English-Spanish experiments, we randomly sample 15.2M examples from each interfering language pair, making the ratio between focus and interfering languages 1:1. Similarly, in the English-Estonian experiments, we sample 6.6M examples from each interfering language to create a data ratio of 1:3.
- We also conduct similar experiments when we use only 118K focus language examples, to see the trends when the focus language pair is extremely low resource.

### Does the Number of Languages Matter?
- The number of interfering languages has a mild positive effect on the accuracy of a multilingual model
- The positive effect diminishes as the amount of focus language data and/or model parameters scales up

### The Impact of Model and Data Size
- The number of interfering languages has a limited affect on interference
- We design a controlled setup to measure interference as a function of the remaining three factors: model size, focus language data size, and its proportion in the total amount of data seen during training
- Setup: We train models using all the available 15.2M English-Spanish examples, with an increasing example budget for interfering language pairs, ranging from 1/8 (1.9M) to 8 times (122M) the English-Spanish data, divided as evenly as possible between French, Czech, Russian, and Chinese
- Results: Figures 3a and 3b show the interference and synergy for English-Spanish using a varying number of interfering examples
- For smaller models (XS and S), increasing the amount of interfering data (i.e. decreasing the proportion of focus data) exacerbates interference. However, larger models appear to benefit from significant quantities of interfering examples; for instance, when training with D s→t = 3.8M, a large model (L) can gain over 10% relative loss improvement when there is 32 times more interfering data than focus data (P (x ∈ s → t) ≈ 3%).
- Interestingly, we also observe that interference is sensitive to the ratio between model parameters and focus data, as the M model trained on 15.2M focus examples produces a similar curve to that of the 4-times smaller S model trained on 3.8M examples, both intersecting the synergy/interference line at the same point.
- Finally, Figures 3c and 3d show that when translating into English, we observe that interference is much less of an issue, occurring only in the XS model when the total amount of training data significantly exceeds the model's capacity.

### Tuning Interference with Temperature
- The dominant factors impacting interference are the model size, the amount of focus language pair data D s→t , and the proportion of focus pair examples observed during training P (x ∈ s → t).
- In a practical situation where both model size and multilingual data are fixed, how can one control the level of interference?
- Recalling Equation 1, we observe that the proportion of focus pair examples P (x ∈ s → t) is controlled via the temperature hyperparameter T .
- Although previous literature has largely used a value of T = 5 following Arivazhagan et al. ( 2019), our systematic experiments with different temperatures across three different data distributions and four model sizes suggest that this value can be sub-optimal and induce a substantial amount of interference, especially for model sizes that alleviate significant amounts of interference (M and L).
- Conversely, tuning the temperature shows that lower values (T = 1, 2) are typically able to reduce high-resource interference without harming low-resource synergy in our standard multilingual translation setting.
- Setup
- We train models of four sizes with temperature ranging from 1 to 5 on three training distributions: (1) all available training data, (2) discarding 3 high resource languages (Czech, French and Russian), (3) discarding 4 low resource languages (Latvian, Lithuanian, Romanian and Hindi).
- When illustrating the results, we assign languages to high and low resource according to whether their relative data proportion decreases or increases when going from T = 1 to T = 2.
- Results
- Figure 4 shows the trade-offs between the lower and higher resource languages, as defined above.
- First, we can see a clear trade-off for the smaller models (XS and S) from T = 1 to T = 4 in most cases, when increasing T helps promote synergy for low resource languages at the cost of increasing interference for the high resource languages.
- However, the larger ones (M and L) clearly suffer from degradation when using T ≥ 3; in fact, values of T = 1 and T = 2 are often better for high-and low-resource language pairs than the commonly-used T = 5.
- These results align with recent work Xin et al. (2022) showing that tuned scalarization is key to achieving strong bilingual baselines that often outperform more complicated Average interference in high resource: cs,fr,ru,zh Scaling Laws in Machine Translation
- Previous work also looked at scaling trends of data and models sizes for machine translation. Gordon et al. (2021) proposed scaling laws in the data and model parameters and demonstrated their ability to predict the validation loss of bilingual translation models from Russian, Chinese, and German to English. Concurrently, Ghorbani et al. (2022) found scaling laws for different configurations for the encoder and decoder, independently varying the number of layers in each of them. Bansal et al. (2022) examined different architectures and described data size scaling laws for machine translation in a very large scale for English to German and English to Chinese.
- While all of these works focused on the bilingual setting, we unveil trends for multilingual translation, which has increased complexity.
- Concurrently and most closely to our work, Anonymous (2023) proposed scaling laws for multilingual machine translation. The main difference from our setting is that Anonymous (2023) focused only on trilingual models trained to translate from English to Chinese and German, or English German and French, while in this work we examine a more true multilingual setting experimenting with up to 15 language pairs.
- Additionally, as Anonymous (2023) shared insights from experiments with a large private dataset of about 600M parallel sentences per language pair, we experiment with a publicly available benchmark.

## Conclusion
- The dominant factors that influence interference in multilingual machine translation are model size, amount of parallel data for the focus language pair, and the proportion of examples from the focus language pair with respect to the total data seen during training.
- While specialized multitask techniques are sometimes demonstrated on a small transformer models, we find that a standard baseline model of 176M parameters reduces the interference problem significantly, and further scaling up results in synergy among the different language pairs.
- We further demonstrate the importance of tuning the temperature at which different language pairs are sampled during training; while existing literature largely relies on high temperatures, which indeed improve low-resource performance in parameter-poor settings, larger models benefit from a more natural distribution that reflects the raw training data.
- These simple strategies for addressing interference call into question the necessity and perhaps even the validity of recentlyproposed complex anti-interference methods and reaffirm the tried-and-true method of increasing model capacity to accommodate for higher data diversity.
