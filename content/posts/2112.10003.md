---
title: "Image Segmentation Using Text and Image Prompts"
date: 2021-12-18T21:27:19.000Z
author: "Timo Lüddecke, Alexander S. Ecker"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2112-10003v2.webp" # image path/url
    alt: "Image Segmentation Using Text and Image Prompts" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2112.10003)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2112.10003).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/prompt-based-multi-modal-image-segmentation).

# Abstract
- Image segmentation is usually addressed by training a model for a fixed set of object classes
- Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions
- Here we propose a system that can generate image segmentations based on arbitrary prompts at test time
- A prompt can be either a text or an image
- This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation
- We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction
- After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query
- We analyze different variants of the latter image-based prompts in detail
- This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated.
- Finally, we find our system to adapt well to generalized queries involving affordances or properties.

# Paper Content

## Introduction
- The ability to generalize to unseen data is a fundamental problem relevant for a broad range of applications in artificial intelligence.
- For instance, it is crucial that a household robot understands the prompt of its user, which might involve an unseen object type or an uncommon expression for an object.
- Image segmentation requires a model to output a prediction for each pixel. Compared to whole-image classification, segmentation requires not only predicting what can be seen but also where it can be found.
- Classical semantic segmentation models are limited to segment the categories they have been trained on.
- Different approaches have emerged that extend this fairly constrained setting (see Tab. 1):
- In generalized zero-shot segmentation, seen as well as unseen categories needs to be segmented by putting unseen categories in relation to seen ones, e.g. through word embeddings [1] or WordNet [2].
- In one-shot segmentation, the desired class is provided in form of an image (and often an associated mask) in addition to the query image to be segmented.
- In referring expression segmentation, a model is trained on complex text queries but sees all classes during training (i.e. no generalization to unseen classes).
- To this work, we introduce the CLIPSeg model (Fig. 1), which is capable of segmenting based on an arbitrary text query or an example image. CLIPSeg can address all three tasks named above.
- This multi-modal input format goes beyond existing multi-task benchmarks such as Visual Decathlon [3] where input is always provided in form of images.
- To realize this system, we employ the pre-trained CLIP model as a backbone and train a thin conditional segmentation layer (decoder) on top. We use the joint textvisual embedding space of CLIP for conditioning our model, which enables us to process prompts in text form as well as images.
- Our idea is to teach the decoder to relate activations inside CLIP with an output segmentation, while permitting as little dataset bias as possible and maintaining the excellent and broad predictive capabilities of CLIP.
- We employ a generic binary prediction setting, where a foreground that matches the prompt has to be differentiated from background. This binary setting can be adapted to multi-label predictions which is needed by Pascal zero-shot segmentation.

## Related Work
- Foundation models are very large pre-training models that are applicable to multiple downstream tasks
- One of these models is CLIP which has demonstrated excellent performance on several image classification tasks
- In contrast to previous models which rely on ResNet backbones, the best-performing CLIP model uses a novel visual transformer
- Analogously to image classification, there have been efforts to make use of transformers for segmentation: TransUNet [11] and SETR [12] employ a hybrid architecture which combine a visual transformer for encoding with a CNN-based decoder. Segformer [13] combines a transformer encoder with an MLPbased decoder. The Segmentor model [14] pursues a purely transformer-based approach.

### Referring Expression Segmentation
- We evaluate referring expression segmentation performance (Tab. 3) on the original PhraseCut dataset and compare to scores reported by Wu et al. [20] as well as the concurrently developed transformer-based MDETR method [21].
- For this experiment we trained a version of CLIPSeg on the original PhraseCut dataset (CLIPSeg [PC]) using only text labels in addition to the universal variant which also includes visual samples (CLIPSeg [PC+]). Our approaches outperform the two-stage HULANet approach by Wu et al. [20]. Especially, a high capacity decoder ( D = 128 ) seems to be beneficial for PhraseCut.
- However, the performance worse than MDETR [21], which operates at full image resolution and received two rounds of fine-tuning on PhraseCut.
- Notably, the ViTSeg baseline performs generally worse than CLIPSeg, which shows that CLIP pre-training is helpful.

## CLIPSeg Method
- We use the visual transformer-based (ViT-B/16) CLIP model as a backbone
- We extend it with a small, parameter-efficient transformer decoder
- The decoder is trained on custom datasets to carry out segmentation, while the CLIP encoder remains frozen
- A key challenge is to avoid imposing strong biases on predictions during segmentation training and maintaining the versatility of CLIP
- We do not use the larger ViT-L/14@336px CLIP variant as its weights were not publicly released as of writing this work
- Decoder Architecture
- Considering these demands, we propose CLIPSeg: A simple, purely-transformer based decoder, which has U-Net-inspired skip connections to the CLIP encoder that allow the decoder to be compact (in terms of parameters)
- While the query image (R W × H × 3) is passed through the CLIP visual transformer, activations at certain layers S are read out and projected to the token embedding size D of our decoder
- Then, these extracted activations (including CLS token) are added to the internal activations of our decoder before each transformer block
- The decoder has as many transformer blocks as extracted CLIP activations (in our case 3)
- The decoder generates the binary segmentation by applying a linear projection on the tokens of its transformer (last layer) , where P is the token patch size of CLIP
- In order to inform the decoder about the segmentation target, we modulate the decoder's input activation by a conditional vector using FiLM
- This conditional vector can be obtained in two ways: (1) Using the CLIP text-transformer embedding of a text query and (2) using the CLIP visual transformer on a feature engineered prompt image
- CLIP itself is not trained, but only used as a frozen feature extractor
- Due to the compact decoder, CLIPSeg has only 1,122,305 trainable parameters for D = 64
- The original CLIP is constrained to a fixed image size due to the learned positional embedding
- We enable different image sizes (including larger ones) by interpolating the positional embeddings
- To validate the viability of this approach, we compare prediction quality for different image sizes and find that for ViT-B/16 performance only decreases for images larger than 350 pixels (see supplementary for details)
- In our experiments we use CLIP ViT-B/16 with a patch size P of 16 and use a projection dimension of D = 64 if not indicated otherwise

### PhraseCut + Visual prompts (PC+)
- We use the PhraseCut dataset which encompasses over 340,000 phrases with corresponding image segmentations.
- Originally, this dataset does not contain visual support but only phrases and for every phrase a corresponding object exists.
- We extend this dataset in two ways: visual support samples and negative samples.
- To add visual support images for a prompt, we randomly draw from the set of all samples, which share the prompt.
- In case the prompt is unique (|S| = 1), we rely only on the text prompt.
- Additionally, we introduce negative samples to the dataset, i.e. samples in which no object matches the prompt.
- To this end, the sample's phrase is replaced by a different phrase with a probability.
- Phrases are augmented randomly using a set of fixed prefixes (as suggested by the CLIP authors).
- On the images we apply random cropping under consideration of object locations, making sure the object remains at least partially visible.
- In the remainder of this paper, we call this extended dataset PhraseCut+ (abbreviated by PC+).

## Visual Prompt Engineering
- masked pooling [31] is a standard technique to compute a prototype vector for conditioning
- this method can't be applied directly to transformer-based architectures
- circumventing the CLS token and deriving the conditional vector directly from masked pooling of the feature maps is not possible either
- to learn more about how target information can be incorporated into CLIP, we compare several variants
- the cosine distance (alignment) between visual and text-based embedding is used to score the variants
- by softmax-normalizing the vector of alignments [ s h t 0 , s h t 1 , . . . ] for different highlighting techniques and images, we obtain the distributions shown in Fig. 3
- for quantitative scores, we consider only the target object name embedding t 0 , which we expect to have a stronger alignment with the highlighted image embedding s h than with the original image embedding s 0

## Experiments
- We first evaluate our model on three established segmentation benchmarks before demonstrating the main contribution of our work: flexible few-shot segmentation that can be based on either text or image prompts.
- Metrics Compared to approaches in zero-shot and oneshot segmentation (e.g. [25,26]), the vocabulary we use is open, i.e. the set of classes or expressions is not fixed. Therefore, throughout the experiments, our models are trained to generate binary predictions that indicate where objects matching the query are located.
- If necessary, this binary setting can be transformed into a multi-label setting (as we do in Section 5.2).
- In segmentation, intersection over union (IoU, also Jaccard score) is a common metric to compare predictions with ground truth. Due to the diversity of the tasks, we employ different forms of IoU: Foreground IoU (IoU FG ) which computes IoU on foreground pixels only, mean IoU, which computes the average over foreground IoUs of different classes and binary IoU (IoU BIN ) which averages over foreground IoU and background IoU.
- In binary segmentation, IoU requires a threshold t to be specified. While most of the time the natural choice of 0.5 is used, the optimal values can strongly deviate from 0.5 if the probability that an object matching the query differs between training and inference (the a-priori probability of a query matching one or more objects in the scene depends highly on context and dataset). Therefore, we report performance of one-shot segmentation using thresholds t optimized per task and model. Additionally, we adopt the average precision metric (AP) in all our experiments.
- Average precision measures the area under the recall-precision curve. It measures how well the system can discriminate matches from non-matches, independent of the choice of threshold.
- In our experiments we differentiate two variants of CLIPSeg: One trained on the original PhraseCut dataset (PC) and one trained on the extended version of PhraseCut which uses 20% negative samples, contains visual samples (PC+) and uses image-text interpolation (Sec. The robust latter version we call the universal model. To put the performance of our models into perspective, we provide two baselines: • CLIP-Deconv encompasses CLIP but uses a very basic decoder, consisting only of the basic parts: FiLM conditioning [48], a linear projection and a deconvolution. This helps us to estimate to which degree CLIP-alone is responsible for the results. [20] 39.4 47.4 -RMI [20] 21.1 42.5 -Table 3: Referring Expression Segmentation performance on PhraseCut ( t refers to the binary threshold). • ViTSeg shares the architecture of CLIPSeg, but uses an ImageNet-trained visual transformer as a backbone [51]. For encoding text, we use the same text transformer of CLIP. This way we learn to which degree the specific CLIP weights are crucial for good performance. We rely on PyTorch [52] for training and use an image size of 352 × 352 pixels throughout our experiments (for details see appendix).

### Generalized Zero-Shot Segmentation
- In generalized zero-shot segmentation, test images contain categories that have never been seen before in addition to known categories.
- We evaluate the model's zero-shot segmentation performance using the established Pascal-VOC benchmark (Tab. 4).
- The results (Tab. 4) indicate a major gap between seen and unseen classes in models trained on Pascal-VOC, while our models tend to be more balanced. This is due to other models being trained exclusively on the 10 or 16 seen Pascal classes in contrast to CLIPSeg which can differentiate many more classes (or phrases).
- In fact, our model performs better on unseen classes than on seen ones.

### One-Shot Semantic Segmentation
- A single example image is presented to the network
- Regions that pertain to the class highlighted in the example image must be found in a query image
- Compared to previous tasks, we cannot rely on a text label but must understand the provided support image
- Above (Sec. 4) we identified the best method for visual prompt design, which we use here: crop

### Ablation Study
- CLIPSeg is important for the performance of CLIPSeg
- CLIPSeg is important for the performance of PhraseCut
- CLIPSeg is important for the performance of visual prompts
- CLIPSeg is important for the performance of text prompts
- CLIPSeg is important for the performance of visual prompts when the number of parameters is reduced

## Conclusion
- We presented the CLIPSeg image segmentation approach that can be adapted to new tasks by text or image prompts at inference time instead of expensive training on new data.
- Specifically, we investigated the novel visual prompt engineering in detail and demonstrated competitive performance on referring expression, zero-shot and one-shot image segmentation tasks.
- Beyond that, we showed -both qualitatively and quantitatively -that our model generalizes to novel prompts involving affordances and properties.
- We believe that tackling multiple tasks is a promising direction for future research toward more generic and real-world compatible vision systems.
- In a wider con-text, our experiments, in particular the comparison to the ImageNet-based ViTSeg baseline, highlight the power of foundation models like CLIP for solving several tasks at once.
- Limitations Our experiments are limited to only a small number of benchmarks, in future work more modalities such as sound and touch could be incorporated.
- We depend on a large-scale dataset (CLIP) for pre-training.
- Note, we do not use the best-performing CLIP model ViT-L/14@336px due to weight availability. Furthermore, our model focuses on images, an application to video might suffer from missing temporal consistency.
- Image size may vary but only within certain limits (for details see supplementary).
- To develop a better understanding of when our model performs well, we compare different text prompts (Fig. 7), object sizes (Fig. 8, left) and object classes (Fig. 8, right). This evaluation is conducted on a pre-trained CLIPSeg (PC+).
- In all cases we randomly sample different prompt forms during training. Here we assess the performance on 5,000 samples of the PhraseCut test set. We see a small effect on performance for alternative prompt forms.
- In terms of object size there is a clear trend towards better performance on larger objects.
- Performance over different classes is fairly balanced.
- Figure 2: Architecture of CLIPSeg: We extend a frozen CLIP model (red and blue) with a transformer that segments the query image based on either a support image or a support prompt.
- N CLIP activations are extracted after blocks defined by S . The segmentation transformer and the projections (both green) are trained on PhraseCut or PhraseCut+.
- Figure 3: Different forms of combining an image with the associated object mask to build a visual prompt have a strong effect on CLIP predictions (bar charts).
- We use the difference in the probability of the target object (orange) in the original image (left column) and the masking methods for our systematic analysis.
- Figure 4: Qualitative predictions of CLIPSeg (PC+) for various prompts, darkness indicates prediction strength. The generalized prompts (left) deviate from the PhraseCut prompts as they involve action-related properties or new object names.
- Figure 6: Qualitative predictions of CLIPSeg (PC+) (top, same as Fig. 4 of main paper for reference) and ViTSeg (PC) (bottom).
- Figure 8: Effect of object size and class on performance. Comparison of different segmentation tasks.Negative means samples that do not contain the target (or one of the targets in multi-label segmentation).
- All approaches except classic segmentation adapt to new targets dynamically at inference time.
- a versatile model, we find that CLIPSeg achieves competitive performance across three low-shot segmentation tasks. Moreover, it is able to generalize to classes and expressions for which it has never seen a segmentation.
- Visual prompt engineering: Average improvement of object probability for different forms of combining image and mask over 1,600 samples.
- Cropping means cutting the image according to the regions specified by the mask, "BG" means background.
