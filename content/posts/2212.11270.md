---
title: "Generalized Decoding for Pixel, Image, and Language"
date: 2022-12-21T18:58:41.000Z
author: "Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, Jianfeng Gao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11270v1.webp" # image path/url
    alt: "Generalized Decoding for Pixel, Image, and Language" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11270)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11270).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/generalized-decoding-for-pixel-image-and).

# Abstract
- X-Decoder is a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly
- With such a novel design, X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks
- After pretraining on a mixed set of a limited amount of segmentation data and millions of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings

# Paper Content

## Introduction
- The work is developed during an internship at Microsoft.
- Until recently, most of these tasks have been separately tackled with specialized model designs, preventing the synergy of tasks across different granularities from being exploited.
- In light of the versatility of transformers [72], we are now witnessing a growing interest in building general-purpose models that can learn from and be applied to a diverse set of vision and vision-language tasks, through multi-task learning [27,32], sequential decoding [7,54,76,85], or unified learning strategy [84,91,94,95].
- While these works have shown encouraging cross-task generalization capabilities, most target the unification of image-level and region-level tasks, leaving the important pixel-level understanding underexplored.
- In [7,54], the authors attempt to unify segmentation into a decoding of a coordinate sequence or a color map, which, however, produces suboptimal performance and limited support for open-world generalization.
- Arguably, understanding images down to the pixel level is one of the most important yet challenging problems in that: (1) pixel-level annotations are costly and undoubtedly much more scarce compared to other types of annotations; (2) grouping every pixel and recognizing them in an open-vocabulary manner is less studied; and (3) more importantly, it is non-trivial to learn from data at two substantially different granularities while also obtaining mutual benefits.
- Some recent efforts have attempted to bridge this gap from different aspects. In [12], Chen et al. propose a unified architecture Mask2Former that tackles all three types of segmentation tasks but in a closed set.
- To support open vocabulary recognition, a number of works study how to transfer or distill rich semantic knowledge from imagelevel vision-language foundation models such as CLIP [64] and ALIGN [34] to specialist models [17,25,65].
- However, all these initial explorations focus on specific segmentation tasks of interest and do not show generalization to tasks at different granularities.
- In this work, we take one step further to build a generalized decoder called X-Decoder1 towards the unification of pixel-level and image-level visionlanguage understanding, as shown in Figure 1.
- A generalized decoding framework. Specifically, X-Decoder is built on top of a vision backbone and a transformer encoder for extracting multi-scale image features, following the framework of Mask2Former [12].
- The key novelty lies in the decoder design. First, it takes two sets of queries as input: (i) generic non-semantic queries that aim to decode segmentation masks for universal segmentation, similar to Mask2Former [12], and (ii) newly introduced textual queries to make the decoder language-aware for a diverse set of language-related vision tasks.
- Second, it predicts two types of outputs: pixel-level masks and token-level semantics, and their different combinations can seamlessly support all tasks of interest.
- Third, we use a single text encoder to encode the textual corpus involved in all tasks, including concepts in segmentation, phrases in referring segmentation, tokens in image captioning and questions in VQA, etc.
- As a result, our X-Decoder can naturally facilitate the synergy across tasks and advocate the learning of a shared visual-semantic space, while respecting the heterogeneous nature of different tasks.
- With our generalized decoder design, we propose an end-to-end pretraining method to learn from all granularities of supervision.
- We unite three types of data: panoptic segmentation, referring segmentation, and image-text pairs.
- Unlike previous works that use pseudo-labeling techniques to extract fine-grained supervision from image-text pairs [25,95], X-Decoder directly groups and proposes a few meaningful segmentation candidates, so that it can map the regions easily to the contents described in the captions on the fly.
- Meanwhile, the referring segmentation task bridges generic segmentation and image captioning by sharing the pixel-level decoding with the former and semantic queries with the latter.

## From Specialist to Generalist Models

### Pixel-Level Understanding
- There are mainly three welldefined tasks for pixel-level understanding, including semantic, instance, and panoptic segmentation.
- Semantic segmentation cares about the per-pixel semantic within an image, whereas instance segmentation groups pixels of the same semantic meaning into object instances.
- Models for both tasks have evolved from CNNbased architectures to transformer-based ones to one-stage models and to the recent query-based approaches.
- With the capability of per-pixel and instance-level understanding, a natural step was taken to formulate panoptic segmentation.
- Most recently, Mask2Former proposed to address all three tasks with a unified encoderdecoder architecture. However, all these works cope with a limited number of categories, i.e., models can hardly recognize concepts absent in the training set.
- In MSeg [40], the authors manually merge different datasets and train a more generalized model on the composite set, which is still limited to being a closed set.
- Open-Vocabulary Segmentation. Recently, a number of works opt to transfer or distill the rich visual-semantic knowledge from foundation models like CLIP [64] and ALIGN [34] to specific segmentation tasks.
- Prominent examples include LSeg [41], OpenSeg [25], and [33].
- Instead of using existing models, GroupViT [81] performed language-image pretraining from scratch with a bottom-up grouping ViT [19], while DenseCLIP [65] demonstrated the superiority of foundation models in finetuning settings compared with supervised models.
- Recently, MaskCLIP [17] proposed to tackle open-vocabulary panoptic and semantic segmentation by leveraging CLIP, and achieved SoTA performance on ADE20K [98] and PASCAL [22,59].
- Referring Segmentation by nature is open-vocabulary in that it does not presume a fixed number of phrases in the training and inference times.
- Models are usually designed specifically to learn from target datasets using various multimodal fusion strategies.
- Since the emergence of vision transformers, works like LAVT [86] enhance the cross-modal interactions from the very beginning, which led to SoTA on RefCOCO [92], RefCOCO+ [92] and G-Ref [56,60].
- CLIPSeg [55] extended the textual query to a visual query and showed superior performance not only on referring segmentation but also on semantic segmentation.
- In this work, we propose X-Decoder, which is the first model to tackle generic and referring segmentation tasks all in one model. Furthermore, the generalized decoder jointly learns from segmentation data and image-text pairs end-to-end, and thus can augment the synergy across tasks for rich pixel-level and image-level understanding.

### Vision-Language Understanding
- Vision-language (VL) pretraining has been found to be effective for various VL tasks
- The field has evolved from a transformer fusion model with pre-extracted object features to end-to-end transformers that directly learn from raw image pixels
- Recently, researchers have found that image-text data at scale can be helpful for visual representation learning (e.g., enabling zero-shot image classification and action recognition)
- VL pretrained models can be further extended to region-level tasks, such as phrase grounding and open-vocabulary object detection
- A comprehensive review on this topic is provided in [24]

## X-Decoder

### Formulation
- The model follows the generic design of encoderdecoder architecture as shown in Fig. 2
- The image encoder extracts features Z
- The text encoder encodes a textual query T into The visual features, textual queries and the m non-semantic or latent queries
- The X-Decoder predicts the outputs: where O p and O s are the pixel-level masks and token-level semantics, respectively
- The text encoder is shared for all textual corpus
- The image and text encoder are fully decoupled

### Unification of Tasks
- Based on the above designs, X-Decoder can be used to seamlessly unify different vision and vision-language tasks, simply with different combinations of queries as inputs.
- For this task, there are no textual queries as inputs. Hence, Eq. (1) becomes: where O p , O s have the same size of Q h .
- Eq. (2) reduces to Mask2former [12], but with open-vocabulary capacity since we use mask-text matching for mask classification.
- Referring Segmentation. It requires both latent and text queries as inputs, thus shares the same formula as Eq.
- Similar to generic segmentation, we only use the first m decoded outputs corresponding to the latent queries.
- Compared with Eq. ( 2), referring segmentation can be regarded as language-conditioned generic segmentation.
- Image-Text Retrieval. The decoupled image and text encoder in our X-Decoder makes it straightforward for interimage retrieval tasks.
- Specifically, we only feed the latent queries to the decoder and obtain the semantic representation of an image: where O s has the same length as Q h , and the last (m-th) token in O s is then used to compute the similarities between images and texts.
- Image Captioning and VQA. For both tasks, X-Decoder takes both latent and text queries and decodes the outputs: where O s correspondingly has equal size to Q t , and no masks are predicted.
- There are two slight differences between the two tasks. First, the caption prediction follows a causal masking strategy while VQA does not. Second, we use all the outputs in O s for captioning, but only the last one to predict the answer for VQA.
- The adaptation of our X-Decoder to each task is further depicted in Fig. 3.

### Unified Architecture
- We follow Mask2Former [12] to build our decoder architecture.
- Given an image I ∈ R H×W ×3 , we extract hierarchical visual features from L layers: (5)
- These hierarchical feature maps are important for pixel-level understanding at different scales.
- One Decoder XDec for All Tasks.
- Given the visual features Z, X-Decoder uses a stack of transformer layers to refine the queries and render the outputs.
- At layer l, it first cross-attends the visual features and then performs selfattention among latent and text queries: In Eq. ( 6), we let all queries cross-attend the visual features.
- For latent queries, we use a masked cross-attention mechanism as in [12], and full attention for the textual queries.
- In Eq. ( 7), we specifically design the self-attention mechanism to prompt the synergy of tasks: (i) we use the last latent query to extract the global image representation and the remaining for generic segmentation; (ii) for image captioning, each textual query can attend itself, its predecessors and all latent queries; (iii) for referring segmentation, latent queries will attend all text queries to use it as the language condition.
- Based on these rules, the resulting self-attention in our X-Decoder is shown in Fig. 4.
- The output of our X-Decoder is also categorized into two types: 1) pixel-wise mask and 2) semantic outputs.
- X-Decoder always produces the masks only for the m latent queries, i.e., O p = {o p 1 , • • • , o p m } ∈ {0, 1} m×H×W for all the latent queries.
- As for the semantic outputs, X-Decoder predicts the outputs for both latent and text queries, i.e., , to cover both mask recognition and caption generation.

### End-to-End Pre-training
- There are three losses on the semantic outputs corresponding to three tasks.
- For image-text retrieval, we compute the language-image contrastive loss as [64].
- We take the last valid token feature of Q t from the text encoder to represent a text as qt and take the last entry in O s derived from X-Decoder as ôs .
- As a result, we obtain B pairs of features qt i , ôs i B i=1 for a minibatch of B imagetext pairs.
- Afterwards, we compute the dot-product between these B × B feature pairs to obtain an affinity matrix S it ∈ R B×B , and compute the bidirectional cross-entropy loss: where y it are the class labels corresponding to diagonal entries in S it , and S T it is the transpose of S it .
- For mask classification, we encode all C class names including "background" into C text queries and take the last valid token feature from each to represent the concept.
- Afterward, we take the decoder outputs corresponding to the first (m − 1) latent queries and compute the dot-product between these outputs and concept embeddings to obtain an affinity matrix S cls ∈ R (m−1)×C and compute the loss L cls = CE(S cls , y cls ), with the ground-truth class y cls .
- We compute the language-image contrastive loss as [64].
- We take the last valid token feature of Q t from the text encoder to represent a text as qt and take the last entry in O s derived from X-Decoder as ôs .
- As a result, we obtain B pairs of features qt i , ôs i B i=1 for a minibatch of B imagetext pairs.
- Afterwards, we compute the dot-product between these B × B feature pairs to obtain an affinity matrix S it ∈ R B×B .
- Then we compute the bidirectional cross-entropy loss: where y it are the class labels corresponding to diagonal entries in S it , and S T it is the transpose of S it .
- For mask classification, we encode all C class names including "background" into C text queries and take the last valid token feature from each to represent the concept.
- Afterward, we take the decoder outputs corresponding to the first (m − 1) latent queries and compute the dot-product between these outputs and concept embeddings to obtain an affinity matrix S cls ∈ R (m−1)×C and compute the loss L cls = CE(S cls , y cls ), with the ground-truth class y cls .
- Given the predictions O p , O s derived from m latent queries, we use Hungarian matching [5,12] to find the matched entries of first (m − 1) outputs to ground-truth annotations.
- Afterward, we follow [12] to use binary crossentropy loss L bce and dice loss L dice to compute the loss for masks.
- We combine the above four losses to pretrain our X-Decoder.

## Experiments

### Experimental Setup

### Task-Specific Transfer
- X-Decoder is a model that is specifically designed to finetune for task transfer.
- X-Decoder outperforms other models on a variety of tasks.
- X-Decoder can be finetuned to achieve better performance.

### Zero-Shot Transfer
- X-Decoder can be applied to various segmentation tasks and datasets after pretraining
- In Table 2, X-Decoder is compared to state-of-the-art methods on seven datasets in 10 different settings
- X-Decoder-Seg shows clear advantages over MSeg on open-vocabulary segmentation
- The extra supervision from COCO captions improves model performance on 9 out of 15 metrics
- When pretraining with the full X-Decoder, the performance is significantly boosted
- Scaling X-Decoder to large size further improves mIoU by 2.4 and 1.4

### Model Inspection
- Image-text retrieval can help open-vocabulary segmentation.
- Image captioning helps referring segmentation and vice versa.
- Image captioning and retrieval can mutually benefit each other.
- Image captioning and retrieval can mutually hurt each other.
- Query interactions are highly dependent on the interaction between latent and text queries.
- The default batch size of VL task is 1024, here we explore the gradual decreasing of VL batch size.
- Each VL dataset is removed individually to investigate the pre-trained performance on different tasks.

### Task Composition
- X-Decoder has the unique benefit of task interaction, thanks to the sophisticated architecture design on latent and text queries as well as the decoder architecture.
- It enables joint task inference and iterative task inference with a sin-gle set of weights.
- In Fig. 6, we show our model can perform region-based retrieval and referring based captioning without any architecture/weight change.
- For example, given a set of animal images (row 1, Fig. 6) and text query, our model first retrieves the correct image (flamingo and giraffe) and then grounds the query with pixel-level predictions.
- Further, our model can easily adapted to referring captioning by first localizing a given word and then modulating the predicted mask in the cross-attention layers.
- Lastly, we also integrate X-Deocder with diffusion model to do referring image editing demonstrated in the latter half of the second row in Fig. 6.

## Conclusion
- X-Decoder is a model that seamlessly supports pixel-level and image-level vision-language understanding
- With a simple and generalized design, X-Decoder can unite and support generic segmentation, referring segmentation and VL tasks effortlessly, achieving strong generalizability and competitive or even SoTA performance
- We hope this work can shed a light on the design of the next-generation general-purpose vision system
- In the main paper, all the pre-trained models are trained with 50 epochs of COCO data and roughly 45 epochs of 10 million image-text pairs.
- The batch size of COCO images and image text pairs are 32 and 1024 respectively. And 32 GPUs are used for pretraining.
- AdamW optimizer is used in pretraining with the initial learning rate 1e-4.
- A step-wise scheduler is used to decay the learning rate by 0.1 on the fraction [0.88889, 0.96296] of training steps.
- Image-Text Retrieval. For both COCO and Flickr30k image-text retrieval, we finetune the models for 10 epochs using AdamW as the optimizer.
- We set the image resolution to 384 and the batch size to 2048. The learning rates are 3e-5 for the X-Decoder part and 3e-6 for the vision and language backbones.
- Image Captioning. Similar to image-text retrieval, we finetune the captioning models for 10 epochs using AdamW as the optimizer. We set the image resolution to 480 and the batch size to 256. The learning rates are 2e-5 for the X-Decoder part and 2e-6 for the vision and language backbones.
- We use beam search during caption generation with the beam size set to 5. We do not use CIDEr optimization for our captioning models.
- VQA. For VQA, we add a new classification layer on the top of the model and finetune the models for 10 epochs using AdamW as the optimizer. We set the image resolution to 640 and the batch size to 256. The learning rates are 1e-4 for the X-Decoder part, 1e-5 for the vision and language backbones, and 1e-3 for the VQA classification layer.
- Generic Segmentation. For generic segmentation, we finetune the pretrained checkpoint with 24 epochs with start learning rate 1e-4. We decay the learning rate by factor 10 at epoch 21 and 23, respectively.
- Referring Segmentation. For referring segmentation, we also finetune the pretrained checkpoint with 24 epochs. However, as RefCOCO has been used in pretraining, thus the initial learning rate is 1e-5. It also decays twice at 21 and 23 epochs.
- We use a batch size of 64 during training.
- Further, in addition to the normal setting that multiple backbone and language encoder learning rates with 0.1, here we also multiply the transformer encoder learning rate by 0.1.
- We propose an open vocabulary segmentation benchmark on 9 datasets with different evaluation metrics. The goal of this benchmark is to provide a comprehensive and standard evaluation protocol for open-vocabulary segmentation on different vocabulary sizes and image domains.
- Table 8 shows the dataset statistics in the benchmark. It supports all generic segmentation tasks including semantic/instance/panoptic segmentation. It covers a variety of scopes ranging from 20 to 847 classes. In addition, the evaluation scene includes common objects, in-door scenes as well as autonomous driving scenarios.
- To enable a better understanding of the open-vocabulary ability on the training...
