---
title: "Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution"
date: 2022-12-03T09:32:14.000Z
author: "Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Risheng Yu, Xiansheng Hua, Lei Zhang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-01579v1.webp" # image path/url
    alt: "Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.01579)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.01579).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/box2mask-box-supervised-instance-segmentation).

# Abstract
- In contrast to fully supervised methods, box-supervised instance segmentation takes advantage of simple box annotations
- Box2Mask integrates the classical level-set evolution model into deep neural network learning
- The Box2Mask approach achieves accurate mask prediction with only bounding box supervision

# Paper Content

## Introduction
- Box2Mask is a box-supervised instance segmentation method that uses the classical level-set model to iteratively learn a series of level-set functions for implicit curve evolution.
- Box2Mask is more robust than existing methods and preserves finer details with accurate boundaries.
- Box2Mask is faster and more accessible than existing methods due to its label-efficient nature.
- Box2Mask is able to exploit rich context information and is promising for image segmentation.

## Related Work
- Our work is related to fully-supervised instance segmentation methods
- Box-supervised instance segmentation methods
- Level-set based segmentation methods

### Fully-supervised Instance Segmentation
- Instance segmentation is a fundamental task in computer vision that aims to generate a pixel-level mask with a category label for each individual instance of interest in an image.
- Most existing works are fully supervised approaches with pixel-wise mask supervision, which can be roughly divided into three categories.
- The first category is the Mask R-CNN family [20,8,31,13,89], which performs segmentation on the regions (e.g., ROIs) extracted from the detection results.
- The second category includes ROI-free approaches [80,65,73,92], which directly segments each instance in a fully convolutional manner without resorting to the detection regions.
- The above mentioned instance segmentation approaches prevail with the convolutional neural network (CNN)-based framework, until recently transformer-based methods [5,17,12,10,38] have achieved remarkable progress.
- These transformer based methods employ a set matching mechanism [5], and learn object queries to achieve the goal of instance segmentation.
- Though being able to segment objects with accurate boundaries, these methods rely on the expensive and laborious pixel-wise mask annotations, which limits their deployments for new categories or scene types in real-world applications.
- The weakly supervised methods with label-efficient annotations have received increasing attention [47,56,88].
- In our work, we show that by using the simple bounding box obtained by some object detectors as the weak supervision, accurate instance segmentation results can still be achieved.

### Box-supervised Instance Segmentation
- Box-supervised instance segmentation employs the simple bounding box annotations to obtain an accurate pixel-level mask prediction.
- Khoreva et al. [29] proposed to predict the mask with the box annotations under the deep learning framework, which heavily depends on the region proposals generated by the unsupervised segmentation methods like GrabCut [59] and MCG [58].
- Based on the Mask R-CNN [20], Hsu et al. [24] formulated the box-supervised instance segmentation problem as a multiple instance learning (MIL) problem by making use of the neighbouring pixel-pairwise affinity regularization.
- BoxInst [66] is proposed to use the color-pairwise affinity with box constraint under the efficient RoI-free CondInst framework [65].
- DiscoBox [34] further introduces the intra-image and cross-image pairwise potential to build a self-ensembled teacher network, where the pseudo-labels generated from the teacher are utilized by the student task network to reduce the uncertainties.
- Despite of the promising performance, the pairwise affinity relationship is built upon either partial or all neighbouring pixel pairs with the oversimplified assumption that similar pixel or color pairs share the same label.
- This inevitably introduces much label noises, especially for complicated background or nearby similar objects.
- Some recently developed methods [35,71] focus on the generation of proxy mask as the pseudo-label supervision by using detachable networks.
- BBAM [35] and BoxCaseg [71] employ multiple training stages or extra saliency data to achieve this goal, where the first stage is designed for pseudo mask generation.
- In addition to box annotation, Cheng et al. [11] and Tang et al. [64] proposed to employ extra points as supervision within the bounding box to obtain more accurate results.
- Unlike the above methods, our proposed level-set based approach learns to evolve the object boundary curves implicitly in an end-to-end manner, which is able to iteratively segment the instances' boundaries by optimizing the energy function within the given bounding box region.

### Level-set based Image Segmentation
- Classical variational approach
- Level-set methods
- Image segmentation
- Region-based approaches
- Edge-based approaches
- Deep learning
- Fully supervised
- Unsupervised
- Chan-Vese functional
- Global multi-phase Mumford-Shah function
- Low-level features
- Initialization of level-set

## Proposed Method

### Overview
- Fig. 2 provides the overview of our proposed Box2Mask method for box-supervised instance segmentation
- The backbone is adopted as the feature encoder to extract basic features
- The IAD learns to embed the characteristics of each instance by kernel learning to generate the instance-aware mask maps dynamically conditioned on the input instances
- The box-level matching assignment is introduced to assign the high-quality mask map samples as the positives
- Finally, we introduce a novel level-set evolution module to generate the accurate supervisions with only bounding box annotations
- The designed level-set energy function enables the whole network to learn a series of level-set functions evolving to the instance boundaries implicitly
- We develop two types of frameworks, a CNN-based one and a transformer-based one, to ensure efficient level-set evolution in a single-stage manner
- In each framework, high-quality mask prediction of an instance can be obtained via the iteratively optimization within its corresponding bounding box region

### Instance-aware Decoder
- The IAD learns to embed the unique characteristics (e.g., intensity, appearance, shape and location, etc.) of each instance to generate the instance-aware mask map.
- The IAD mainly consists of a pixel-wise decoder and a kernel learning network, which decouples the instance-aware mask map output into the generation of unified mask features and the corresponding unique kernels.
- CNN-based IAD. For CNN-based IAD, we adopt the dynamic convolution method as in SOLOv2 [73,74].
- Based on the basic features F extracted from the backbone, the kernel learning network first employs several convolution layers to embed instance-wise characteristics and generate the instance-unique kernels K i,j .
- As the parallel branch, the pixel-wise decoder employs the Feature Pyramid Network (FPN) [43] to extract multi-scale features, based on which the feature aggregation is performed at the same resolution to get the unified feature representation F mask .
- The learning kernel K i,j performs the convolution dynamically on the unified mask features F mask to generate the instance-aware mask map M , i.e., M i,j = K i,j * F mask , where M i,j is the full-image mask map containing only one instance centered at location (i, j).
- This CNN-based module produces the dynamic mask maps and distinguishes individual instances.
- Transformer-based IAD. Inspired by MaskFormer [12], the segmentation map of each instance can be represented as a D-dimensional feature vector, which can be computed by a transformer decoder [69] with N learnable instance-wise queries using the set prediction mechanism [5].
- As shown in Fig. 3, to achieve dynamic IAD, we adopt a transformer decoder to compute N instance-aware kernel vectors K ∈ R N ×C , which encode the object characteristics conditioned on input instances.
- At the same time, a pixel decoder is used to gradually upsample the low-resolution features from the output of backbone to generate the high-resolution mask features F mask ∈ R C×H×W .
- Specially, instead of the convolutional network, we adopt the multi-scale deformable attention transformer [93] layers to extract stronger feature representation with long-range context.
- The final instance-aware mask maps are decoded from the embeddings using object queries via a dot product operation with learned instance-aware kernels, i.e., .
- With this transformer-based IAD, the set of instance-aware mask maps can be obtained in our transformer-based framework.

### Box-level Matching Assignment
- Label assignment plays a critical role in training instance segmentation networks.
- It assigns the potential mask samples as the positives, and the reminder as negatives.
- For CNN-based framework, dense instance-aware map candidates will be generated at any location (i, j) of an image.
- We employ the box-based center sampling scheme to ensure that each potential instance-aware map contains only one target instance, whose center is at location (i, j).
- The mask map candidate is considered as a positive sample if the corresponding location (i, j) falls into the pre-defined center region of any ground truth bounding box; otherwise, it is treated as a negative sample.
- For transformer-based framework, we adopt the Hungarian algorithm based bipartite matching assignment scheme.
- The matching cost consists of both the category and spatial location differences between the class-wise mask predictions and ground truth bounding boxes.
- For the matching cost C inst on instance segmentation, we employ the coordinate projection P on x-axis and y-axis to calculate the spatial differences between the predicted instance-aware maps m p and box-based ground truth masks m b measured by 1-D dice coefficient.
- Such a simple box-based projection cost function can reflect the spatial relationships accurately.
- Besides, the commonly used cross entropy loss is adopted as the category cost C cate to measure the similarity of predicted category and ground-truth class label.
- Therefore, the total matching cost C can be formulated as follows: The instance-aware mask maps with no match will yield a "no object" (∅) target class label.

### Level-set Evolution

### Training Loss and Inference
- We employ the level-set energy as the training objective for network optimization in an end-to-end fashion.
- Once the network is trained, the inference stage is efficient, which could directly output the mask prediction without the iterative level-set evolution process.
- Loss Function. The loss function to train our Box2Mask network consists of two items, the category classification loss L cate and instance segmentation loss L inst : where w is the balance parameter.
- Here L cate can be instantiated as the commonly used cross-entropy loss or Focal loss [44].
- For L inst , we employ the differentiable level-set energy as the objective: where N pos indicates the number of positive samples and α is the balance weight.
- 1 {cond} represents the indicator function with the corresponding conditions in CNN-based and transformer-based frameworks, which ensures that only the positive instance mask samples will be involved in the level-set evolution.
- In the CNN-based framework, p * i,j is the category probability of target located at (i, j), and hence the conditional function is p * i,j > 0 to filter the low-quality candidates.
- In the transformer-based framework, the target class c i should not be empty. Therefore, the conditional function is c gt j = ∅.
- With the above loss function, our Box2Mask model can be trained in an end-to-end fashion.
- Inference. It is worth noting that the level-set evolution is only employed during training to generate implicit supervisions for the network optimization. The inference process is direct and efficient without the need of level-set evolution.
- Given the input image, the instance-wise mask predictions are directly generated.
- For the model trained under the CNN-based framework, the efficient matrix non-maximum suppression (NMS) [74] is needed for post-processing.
- For the model trained under the transformerbased framework, the inference process is NMS-free, which directly outputs the instance masks.

## Experiments

### Datasets
- The Box2Mask approach is evaluated on five challenging datasets
- Pascal VOC [16] is used as a training dataset and COCO [45] is used as a testing dataset
- iSAID [77] is used as a training dataset and LiTS [3] is used as a testing dataset
- ICDAR2019 ReCTS [91] is used as a training dataset

### Implementation Details
- Our models are trained on 8 NVIDIA V100 GPUs
- The Box2Mask-C models are pre-trained on ImageNet-1K and have an initial learning rate of 10−4 and weight decay of 0.1
- The Box2Mask-T models are pre-trained on ImageNet-1K and have an initial learning rate of 5 × 10−5 and weight decay of 0.05
- We train the models for 50 epochs
- We use the mmdetection toolbox and follow the commonly used training settings on each dataset
- We employ the ResNet and Swin-Transformer backbones
- The non-negative weight γ in Eq. 4 is set to 10−4
- We keep the level-set energy value at the same level for input data terms (including original image and high-level deep features)
- We set λ 1 = 0.05 and λ 2 = 5.0 in Eq. 9

### Main Results
- Box2Mask outperforms state-of-the-art box-supervised instance segmentation approaches on COCO val2017 and test-dev split.
- Box2Mask-C achieves 32.2% mask AP on COCO val2017 split, which outperforms DiscoBox [34] by 0.8% AP (32.2% vs. 31.4%).
- Box2Mask-T further achieves 36.1% mask AP with +3.9% improvement over Box2Mask-C.
- With ResNet-50 backbone, Box2Mask-C achieves 32.6% mask AP on COCO test2017 split, which outperforms the state-of-the-art method BoxInst [66] by 0.5% AP (32.6% vs. 32.1%).
- Box2Mask-C achieves 16.0% AP S on small objects, which is slightly lower than BoxInst [66] by 0.2%.
- By employing the longer 3× training scheme, our method achieves 26.6% mask AP.

### Deep Variational-based Instance Segmentation
- DeepSnake [57] is based on the classical snake method [28]
- Levelset R-CNN [22] and DVIS-700 [87] are built upon level-set evolution
- Box2Mask-C achieves comparable results to the fully supervised variationalbased methods, even outperforming DeepSnake [57] by 0.7% AP on COCO val dataset using ResNet-50 as backbone.
- Box2Mask-T surpasses Levelset R-CNN [22] and DVIS-700 [87] by 1.8% AP (36.1% vs. 34.3) and 3.5% AP (36.1% vs. 32.6) with ResNet-50 backbone, respectively.
- Using ResNet-101, Box2Mask-T achieves the best result of 37.5% AP, outperforms all these fully supervised deep variational-based methods.

### Inference Speed
- The inference speed and accuracy of typical mask-supervised and box-supervised methods are reported in Table 8.
- The Box2Mask-C with ResNet-50 backbone achieves 11.5 FPS and 32.6% mask AP.
- The speed of our method is slower than DiscoBox [34] and BoxInst [66] but with better accuracy.
- Compared with Box2Mask-C, Box2Mask-T needs more computational cost.
- Box2Mask-T with ResNet-50 backbone runs at a speed of 8.7 FPS but brings a significant accuracy improvement to 36.7% mask AP.
- By using a stronger backbone ResNet-101, Box2Mask-T can run at a similar FPS of 7.9 but achieves the best box-supervised performance of 38.3% mask AP.

### Ablation Experiments
- We conduct ablation studies on Pascal VOC dataset to examine the effectiveness of each module in our proposed Box2Mask framework.
- Level-set Energy. We firstly investigate the impact of level-set energy functional with different settings in Box2Mask-C.
- Table 9 shows the evaluation results. Our method achieves 27.1% AP by using the box projection function as F φ 0 to initialize the boundary during training. This demonstrates that the initialization for level-set function is effective to generate the initial boundary.
- When the original image I u is employed as the input data term in Eq. 9, our method achieves 30.6% AP. When the deep features I f are employed as the extra input data, our method achieves a better performance of 33.8% AP. This demonstrates that both original image and high-level features can provide useful information for robust level-set evolution.
- Note that the above results are obtained by constraining curve evolution within the bounding box B region. When the global region with the full-image size is regarded as the Ω, there is a noticeable performance drop (30.6% vs. 28.3% and 33.8% vs. 28.8%) with the same input data terms.
- Table 9: The impact of level-set energy with different settings. I u and I f denote that the low-level image feature and high-level deep feature are used as the input data term, respectively. B and I represent the Ω space of bounding box or the full-image region for level-set evolution.
- Deep Structural Feature with Tree Filters. We study the impact of tree filters [62], which can capture long-range feature dependency, on generating structural features as the input to drive level-set evolution.
- Table 10 shows the results. Without using the tree filters, our method achieves 30.8% mask AP. By using only the low-level image feature as the guidance of tree filters, the performance will drop from 30.8% to 28.2%. This is because the low-level features can be noisy for the tree filters to build structural features. By using the low-level and high-level features together as the guidance, the tree filters can bring 3.0% AP improvement, achieving 33.8% mask AP.
- Local Consistency Module. We study the effectiveness of the proposed local consistency module (LCM), which aims to maintain the local affinity consistency and avoid the intensity inhomogeneity in level-set evolution.
- Table 11 shows the results. Without using LCM for level-set prediction, our approach can obtain 33.8% AP. In LCM, different dilation rates can be used to control the scope of local region. One can see that by applying LCM with dilation rate 3, 36.3% AP can be achieved, with +2.5% AP improvement. Actually, dilation rates 1, 2 and 3 lead to similar performance. When the local region is enlarged too much by using dilation rate 4 or 5, the performance drops. This demonstrates that LCM benefits local affinity consistency, instead of global dependency.
- Training Schedule. We evaluate the proposed Box2Mask-C and Box2Mask-T models using different training schedules.
- Table 12 reports the results of Box2Mask-C using 12 epochs (1×) with single-scale input size, as well as 36 epochs (3×) with multi-scale input size.
- For Box2Mask-T, we also evaluate its performance using 30 epochs and 50 epochs training schedules. It can be observed that a longer training schedule can benefit our proposed models, bringing visible AP improvement (36.3% vs. 38.0%, 39.4% vs. 41.4%).
- Balance Weights of Matching Cost. We explore the setting of balance weights in Eq. 1 for box-level matching assignment in transformer-based framework.
- The hyper-parameters β 1 and β 2 denote the importance of category and segmentation in matching cost, respectively.
- Table 13 reports the experimental results. It can be seen that the best performance (39.4% AP) is obtained when we set β 1 = 2.0 and β 1 = 6.0.
- The Number of MSDeformAtten Layers. We further study the selection of the number of multiscale deformable attention (MSDeformAtten) layers in pixel decoder to encode the robust mask features.
- The results are reported in Table 14. When the number of MSDeformAttn is 1, we achieve 36.8% mask AP. When two MSDeformAtten layers are used, a visible improvement of +2.1%...

## Conclusion
- The Box2Mask-C with CNN-based framework outperforms BoxInst [66] by 3.7% and 3.1% mask AP with ResNet-50 and ResNet-101 backbones, respectively.
- Box2Mask-T with ResNet-101 backbone achieves 83.2% AP 25 , 70.8% AP 50 , 50.8% AP 70 , 44.4% AP 75 and 75.2% ABO, outperforming Box2Mask-C by +5.9%, +4.2%, +2.9%, +3.5% and +3.3% accordingly.
- Box2Mask-C with 3× training schedule achieves 38.3% AP 50 .
