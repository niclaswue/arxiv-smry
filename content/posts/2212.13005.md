---
title: "TextBox 2.0: A Text Generation Library with Pre-trained Language Models"
date: 2022-12-26T03:50:36.000Z
author: "Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Zican Dong, Xiaoxue Cheng, Yuhao Wang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13005v1.webp" # image path/url
    alt: "TextBox 2.0: A Text Generation Library with Pre-trained Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13005)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13005).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/textbox-2-0-a-text-generation-library-with).

# Abstract
- TextBox 2.0 is a comprehensive and unified library for text generation that covers a wide range of tasks and incorporates a variety of pre-trained language models.
- The library is easy to use, with interfaces in both Python and command line, and is validated through extensive experiments.

# Paper Content

## Introduction
- Text generation, aiming to generate human-like texts on demand, has been a fundamental technique in many text applications, such as machine translation (Dabre et al., 2020), text summarization (El-Kassas et al., 2021), and dialogue system (Chen et al., 2017).
- Recently, pre-trained language models (PLMs) such as BART (Lewis et al., 2020) have been the mainstream approach to developing effective text generation models.
- With the great advances in text generation, it has become increasingly important to reproduce, develop, and compare various text generation models in a reliable, flexible, and unified way.
- Considering the rapid progress of PLMs on text generation, in this paper, we present a significant extension of a previously released text generation library, TextBox 1.0 (Li et al., 2021), called TextBox 2.0.
- Different from TextBox 1.0 and other text generation libraries (Miller et al., 2017;Klein et al., 2018;Zhu et al., 2018) (mostly including classical models based on recurrent neural networks or generative adversarial networks), this extension mainly focuses on building a comprehensive and unified framework for better supporting PLM-based text generation models.
- Although some libraries (e.g., Fairseq (Ott et al., 2019) and Hugging Face (Wolf et al., 2020)) also include PLMs, they are designed for performing myriad NLP tasks (only considering a few text generation tasks). Moreover, they don't maintain a complete evaluation pipeline (e.g., data loading, training, inference, and evaluation) specially designed for text generation. Thus, it is not fully suited for developing and evaluating text generation models in a unified way.
- In order to better facilitate research on text generation, TextBox 2.0 introduces a series of new features for supporting the use of PLMs, which can be summarized into three major aspects: Generation Tasks: Our library supports 13 commonly studied text generation tasks (e.g., translation and story generation) and their corresponding 83 datasets, including most of the existing mainstream tasks and datasets for research. We reorganize these datasets so that they are framed in a unified text-to-text format. Users can simply set the dataset via the command line or configuration file without additional preprocessing efforts. Generation Models: As a key contribution, our library incorporates 45 pre-trained language models, covering the categories of general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight models (modules). We unify the interface to use existing PLMs and incorporate new PLMs, and it is convenient to run different PLMs for a specified task in our library. We also provide a standard way to compare these models and analyze the generated results. Training Strategies: To support the optimization of PLMs, we provide four efficient and robust training strategies (e.g., efficient decoding) and four pre-training objectives (e.g., denoising auto-encoding) for text generation. These strategies make optimizing text generation models more efficient and reliable. Users can either pre-train a new model from scratch or fine-tune a pre-trained model for research purposes. As another merit, TextBox 2.0 has been largely aligned with our previous survey on PLM-based text generation (Li et al., 2022b) in terms of task, model, and training. It will be meaningful for both research beginners and experts to learn and explore text generation models with the survey and accordingly supporting libraries.

## Library Design
- TextBox 2.0 introduces various new features, mainly from three aspects: generation tasks, generation models, and training strategies.
- New features include the ability to generate text from images, the ability to generate text with multiple languages, and the ability to generate text with different styles.
- The new features make it easier to generate text for research purposes.

### Generation Tasks
- Tasks: 13 tasks, including text summarization, machine translation, open-ended dialogue system, data-to-text generation, question generation, question answering, story generation, task-oriented dialogue system, commonsense generation, paraphrase generation, text style transfer, and text simplification.
- Datasets: 83 datasets, including English-centric datasets and Chinese datasets.
- Metrics: 4 general metrics and 5 task-specific metrics.
- Visualization tools: Figure 2 offers new insights to improve summarization tasks.

### Generation Models
- TextBox 2.0 incorporates 45 PLMs
- PLMs make it possible to deal with different text generation tasks
- For specific tasks such as dialogue system, users can adopt task-specific PLMs

### Training Strategies
- TextBox 2.0 provides four pre-training objectives to help users pre-train a model from scratch
- These pre-training tasks can also be utilized for domain-adaptive pre-training and task-adaptive pre-training
- TextBox 2.0 also provides four useful training methods for improving the optimization of PLMs
- It supports distributed data parallel to implement models on multiple GPUs and machines to improve the efficiency of training
- We incorporate Accelerate (Sylvain Gugger, 2022) to support distributed training with a simple API
- To further accelerate the decoding efficiency, we integrate FastSeq (Yan et al., 2021) to optimize the decoding process by attention cache optimization, repeated n-gram detection, and asynchronous parallel I/O.

## Library Usage
- PLMs can be quickly run and reproduced results are provided
- It is convenient to specify the dataset and model by setting the configurations
- Pre-training a new model is possible using the provided pre-training objectives
- Visualization analysis is available to perform deep analysis on the generated results

## Experiments
- The TextBox 2.0 generation abilities have been verified.
- The TextBox 2.0 generation abilities are reliable.

### Result Reproduction
- TextBox 2.0 is an open-source library that can reproduce the results of existing work effectively
- To verify this, we select a number of widely-used datasets for each task (introduced in Section 2.1) and compare the results conducted by TextBox 2.0 with those in the original papers
- We evaluate 13 tasks using 14 datasets, including CNN/DailyMail (See et al., 2017), Wiki-Auto + Turk (Liu et al., 2021a), LCSTS (Hu et al., 2015), CSL4 , ADGEN (Shao et al., 2019), WMT 16 English-Romanian (En↔Ro) (Bojar et al., 2016), WebNLG 2.1 (Gardent et al., 2017), Com-monGen (Lin et al., 2020a), SQuAD (Rajpurkar et al., 2016), PersonaChat (Zhang et al., 2018), MultiWOZ 2.0 (Budzianowski et al., 2018b), ROCStories (Mostafazadeh et al., 2016), GYAFC (E&M and F&R) (Rao and Tetreault, 2018), and Quora (Kumar et al., 2020)
- Since BART is the prevalent PLM for text generation, we endeavor to reproduce existing works with BART LARGE 5
- For all experiments, we em-ploy the sequence-to-sequence cross-entropy loss with a label smoothing factor of 0.1 as the objective function
- We optimize the model using AdamW (Loshchilov and Hutter, 2019) with a constant learning rate of 3 × 10 −5
- The accumulated batch size is set to 192

### Efficiency Comparison
- Accurately reproduces results
- Streamlines the training process and 2020)
- In the generation process, our library is significantly faster than the other two libraries due to the incorporation of efficient decoding strategies introduced in Section 2.3.

### Visualization Analysis
- It is important to reproduce a model
- Existing methods should be analyzed
- Directions for improvement can be explored
- A leaderboard is set for each dataset
- Figures 2 (a) and 2 (b) show the results of the leaderboard
- Figures 2 (c) and 3 (a) show the results of the TextBox 2.0 analysis

## Conclusion
- TextBox 2.0 is a comprehensive and unified library for conducting research on PLM-based text generation
- Our library makes significant extensions in three major aspects, namely generation tasks (13 tasks and 83 datasets), generation models (45 PLMs), and training strategies (e.g., distributed data parallel and efficient decoding)
- Results from extensive test experiments demonstrate that our library can accurately reproduce existing models
- Besides, we also provide a series of utility tools to better analyze and explore the generated results
- To summarize, our library can be very useful to facilitate text generation research
- Our team will improve this library with regular updates
