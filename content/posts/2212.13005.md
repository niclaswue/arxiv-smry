---
title: "TextBox 2.0: A Text Generation Library with Pre-trained Language Models"
date: 2022-12-26T03:50:36.000Z
author: "Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Zican Dong, Xiaoxue Cheng, Yuhao Wang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13005v1.webp" # image path/url
    alt: "TextBox 2.0: A Text Generation Library with Pre-trained Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13005)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13005).


# Abstract
- TextBox 2.0 is a library that provides comprehensive and unified functionality for text generation.
- The library covers tasks such as general text generation, translation, Chinese dialogue, controllable text, distilled text, prompting text, and lightweight text generation.
- The library has a Python API and a command line interface.
- The library is released at the link: https://github.com/RUCAIBox/TextBox.

# Paper Content

## Introduction
- Text generation, aiming to generate human-like texts on demand, has been a fundamental technique in many text applications, such as machine translation (Dabre et al., 2020), text summarization (El-Kassas et al., 2021), and dialogue system (Chen et al., 2017).
- Recently, pre-trained language models (PLMs) such as BART (Lewis et al., 2020) have been the mainstream approach to developing effective text generation models.
- With the great advances in text generation, it has become increasingly important to reproduce, develop, and compare various text generation models in a reliable, flexible, and unified way.
- Considering the rapid progress of PLMs on text generation, in this paper, we present a significant extension of a previously released text generation library, TextBox 1.0 (Li et al., 2021), called TextBox 2.0.
- Different from TextBox 1.0 and other text generation libraries (Miller et al., 2017;Klein et al., 2018;Zhu et al., 2018) (mostly including classical models based on recurrent neural networks or generative adversarial networks), this extension mainly focuses on building a comprehensive and unified framework for better supporting PLM-based text generation models.
- Although some libraries (e.g., Fairseq (Ott et al., 2019) and Hugging Face (Wolf et al., 2020)) also include PLMs, they are designed for performing myriad NLP tasks (only considering a few text generation tasks). Moreover, they don't maintain a complete evaluation pipeline (e.g., data loading, training, inference, and evaluation) specially designed for text generation. Thus, it is not fully suited for developing and evaluating text generation models in a unified way.
- In order to better facilitate research on text generation, TextBox 2.0 introduces a series of new features for supporting the use of PLMs, which can be summarized into three major aspects:
- Generation Tasks: Our library supports 13 commonly studied text generation tasks (e.g., translation and story generation) and their corresponding 83 datasets, including most of the existing mainstream tasks and datasets for research. We reorganize these datasets so that they are framed in a unified text-to-text format. Users can simply set the dataset via the command line or configuration file without additional preprocessing efforts.
- Generation Models: As a key contribution, our library incorporates 45 pre-trained language models, covering the categories of general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight models (modules). We unify the interface to use existing PLMs and incorporate new PLMs, and it is convenient to run different PLMs for a specified task in our library. We also provide a standard way to compare these models and analyze the generated results.
- Training Strategies: To support the optimization of PLMs, we provide four efficient and robust training strategies (e.g., efficient decoding) and four pre-training objectives (e.g., denoising auto-encoding) for text generation. These strategies make optimizing text generation models more efficient and reliable. Users can either pre-train a new model from scratch or fine-tune a pre-trained model for research purposes.
- As another merit, TextBox 2.0 has been largely aligned with our previous survey on PLM-based text generation (Li et al., 2022b) in terms of task, model, and training. It will be meaningful for both research beginners and experts to learn and explore text generation models with the survey and accordingly supporting libraries.

## Library Design
- TextBox 2.0 introduces various new features, mainly from three aspects: generation tasks, generation models, and training strategies.
- These new features make it easier to generate text.
- The new features make it easier to generate text that is accurate and consistent.

### Generation Tasks
- 13 tasks in TextBox 2.0
- including text summarization, machine translation, open-ended dialogue system, data-to-text generation, question generation, question answering, story generation, task-oriented dialogue system, commonsense generation, paraphrase generation, text style transfer, and text simplification
- 83 datasets
- including the dataset description, basic statistics, and training/validation/testing samples
- 4 general metrics and 5 task-specific metrics

### Generation Models
- TextBox 2.0 incorporates 45 PLMs
- PLMs make it possible to deal with different text generation tasks
- For specific tasks, such as dialogue system, users can adopt task-specific PLMs

### Training Strategies
- TextBox 2.0 provides four pre-training objectives to help users pre-train a model from scratch
- These pre-training tasks can also be utilized for domain-adaptive pre-training and task-adaptive pre-training
- TextBox 2.0 also provides four useful training methods for improving the optimization of PLMs
- It supports distributed data parallel to implement models on multiple GPUs and machines to improve the efficiency of training
- We incorporate Accelerate (Sylvain Gugger, 2022) to support distributed training with a simple API
- To further accelerate the decoding efficiency, we integrate FastSeq (Yan et al., 2021) to optimize the decoding process by attention cache optimization, repeated n-gram detection, and asynchronous parallel I/O
- Moreover, TextBox 2.0 enables users to adjust and select hyper-parameters automatically
- Based on the library Hyperopt (Bergstra et al., 2013), users just need to set the parameter range and search methods, and then the optimal hyperparameters and corresponding results will return
- It is useful for PLMs to search for hyper-parameters such as batch size and learning rate. Our library also supports performing repeat experiments using different random seeds in one command line

## Library Usage
- PLMs can be reproduced with just a dataset and model
- Pre-training can be done with a pre-training objective
- Visualization can be done with statistical charts and other methods

## Experiments
- The TextBox 2.0 generation abilities have been verified.
- The TextBox 2.0 generation abilities are reliable.
- The TextBox 2.0 generation abilities are accurate.

### Result Reproduction
- TextBox 2.0 is an open-source library that should be able to reproduce the results of existing work effectively
- To verify this, we select a number of widely-used datasets for each task (introduced in Section 2.1) and compare the results conducted by TextBox 2.0 with those in the original papers
- We totally evaluate 13 tasks using 14 datasets, including CNN/DailyMail (See et al., 2017), Wiki-Auto + Turk (Liu et al., 2021a), LCSTS (Hu et al., 2015), CSL4 , ADGEN (Shao et al., 2019), WMT 16 English-Romanian (En↔Ro) (Bojar et al., 2016), WebNLG 2.1 (Gardent et al., 2017), Com-monGen (Lin et al., 2020a), SQuAD (Rajpurkar et al., 2016), PersonaChat (Zhang et al., 2018), MultiWOZ 2.0 (Budzianowski et al., 2018b), ROCStories (Mostafazadeh et al., 2016), GYAFC (E&M and F&R) (Rao and Tetreault, 2018), and Quora (Kumar et al., 2020)
- Since BART is the prevalent PLM for text generation, we endeavor to reproduce existing works with BART LARGE 5
- For all experiments, we em-ploy the sequence-to-sequence cross-entropy loss with a label smoothing factor of 0.1 as the objective function
- We optimize the model using AdamW (Loshchilov and Hutter, 2019) with a constant learning rate of 3 × 10 −5
- The accumulated batch size is set to 192

### Efficiency Comparison
- Accurately reproduces results
- Streamlines the training process and 2020)
- Incorporates efficient decoding strategies
- More efficient than Fairseq and Hugging Face

### Visualization Analysis
- It is important to reproduce a model to compare existing methods
- Our library sets a specific leaderboard for each dataset, including basic metric results, author repositories, and generated texts
- Figure 2 (a) showcases the leaderboard for the CNN/DailyMail dataset
- Users can also utilize TextBox 2.0 to conduct visualization analysis for specified models
- For example, our library can automatically plot the boxplot of the ROUGE-L score for different input lengths and the n-gram overlap of target and generated texts with the input document
- From the results in Figure 2 (b), we can find that T5 excels at short document summarization while BART excels at long document summarization
- It is useful to analyze and improve the deficiencies of text generation models or obtain better performance by combining their results
- As another example, Figure 2 (c) illustrates that BART and T5 have a significantly higher n-gram overlap ratio than golden sentences, indicating that they tend to "copy" the input document rather than "summarize" it.

## Conclusion
- TextBox 2.0 is a comprehensive and unified library for conducting research on PLM-based text generation
- Our library makes significant extensions in three major aspects, namely generation tasks (13 tasks and 83 datasets), generation models (45 PLMs), and training strategies (e.g., distributed data parallel and efficient decoding)
- Results from extensive test experiments demonstrate that our library can accurately reproduce existing models
- Besides, we also provide a series of utility tools to better analyze and explore the generated results
- To summarize, our library can be very useful to facilitate text generation research
- Our team will improve this library with regular updates
