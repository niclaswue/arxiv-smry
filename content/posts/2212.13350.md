---
title: "A Generalization of ViT/MLP-Mixer to Graphs"
date: 2022-12-27T03:27:46.000Z
author: "Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, Xavier Bresson"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13350v1.webp" # image path/url
    alt: "A Generalization of ViT/MLP-Mixer to Graphs" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13350)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13350).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-generalization-of-vit-mlp-mixer-to-graphs).

# Abstract
- Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning
- Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers
- This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity
- In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision
- We introduce a new class of GNNs, called Graph MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on the Long Range Graph Benchmark (LRGB) and the TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them.

# Paper Content

## Generalizing ViT/ML-Mixer to Graphs Overcome MP-GNN Limitations
- The importance of generalizing the ViT/MLP-Mixer architectures from computer vision to graphs
- Precisely, this generalization is required to achieve three key properties for GNNs simultaneously which are 1) capturing long-range interaction, 2) keeping low linear speed/memory complexity as MP-GNNs and 3) offering high expressivity to distinguish non-isomorphic graphs
- To the best of our knowledge, the proposed work is one of the first GNN architectures that can provide a substantial trade-off between capturing significant long-range dependency and low computational complexity, hence going beyond standard MP-GNNs
- ViT and MLP-Mixer in Computer Vision

## Generalization Challenges

### Overview
- The basic architecture of Graph MLP-Mixer is illustrated in Figure 1
- The goal of this section is to detail the choices we made to implement each component of the architecture
- On the whole, these choices lead to a simple framework that provides speed and quality results
- Notation: Let G = (V, E) be a graph with V being the set of nodes and E the set of edges. The graph has N = |V| nodes and E = |E| edges. The connectivity of the graph is represented by the adjacency matrix A ∈ R N ×N .
- The node features of node i are denoted by h i , while the features for an edge between nodes i and j are indicated by e ij .
- Let {V 1 , ..., V P } be the nodes partition, P be the pre-defined number of patches, and G i = (V i , E i ) be the induced subgraph of G with all the nodes in V i and all the edges whose endpoints belong to V i .
- Let h G be the graph-level vectorial representation and y G be the graph-level target, which can be a discrete variable for graph classification problem, or a scalar for graph regression task.
- The patch extraction module partitions graphs into overlapping patches. The patch embedding module transforms these graph patches into corresponding token representations, which are fed into a sequence of mixer layers to generate the output tokens.
- A global average pooling layer followed by a fully-connected layer is finally used for prediction. Each Mixer Layer is a residual network that alternates between a Token Mixer applied to all patches, and a Channel Mixer applied to each patch independently.

### Patch Extraction
- All image data are defined on a regular grid with the same fixed resolution (H, W ).
- To extract graph patches, the same extraction algorithm can be applied to any arbitrary graph.
- The nodes in the sub-graph patch must be more closely connected than for those outside the patch.
- The extraction complexity must be fast, that is at most linear w.r.t. the number of edges, i.e. O(E).

### Patch Encoder
- For images, patch encoding can be done with a simple linear transformation given the fixed resolution of all image patches.
- For graphs, the patch encoder network must be able to handle complex data structure such as invariance to index permutation, heterogeneous neighborhood, variable patch sizes, convolution on graphs, and expressive to differentiate graph isomorphisms.
- As a result, the graph patch encoder is a GNN, whose architecture is designed to best transform a graph token G p into a fixed-size representation x Gp ∈ R d into 3 steps.
- Step 1. Raw node and edge linear embedding. The input node features α i ∈ R dn×1 and edge features β ij ∈ R de×1 are linearly projected into d-dimensional hidden features: where
- Step 2. Graph convolutional layers with MP-GNN. We apply a series of L convolution layers, where the node and edge representations are updated with a MP-GNN applied to each graph patch G p = (V p , E p ) separately and independently as follows: where
- Step 3. Pooling and readout. The final step produces a fixed-size vector representation by mean pooling all node vectors in G p such that h p = 1 |Vp| i∈Vp h =L i,p ∈ R d , and applying a small MLP to get the patch embedding x Gp ∈ R d .
- Observe that the patch encoder is a MP-GNN, and thus has the inherent limitation of poor long-range dependency. Does it affect the Graph MLP-Mixer to capture long-range interactions? The answer is negative because this problem is limited to large graphs. But for small patch graphs, this issue does not really exist (or is negligible).

### Positional Information
- Regular grids offer a natural implicit arrangement for the sequence of image patches and for the pixels inside the image patches.
- However, such ordering of nodes and patches do not exist for general graphs.
- This lack of positional information reduces the expressivity of the network.
- Hence, we use two explicit positional encodings (PE); one absolute PE for the patch nodes and one relative PE for the graph patches.
- Node PE. Input node features in Eq 1 are augmented with p i ∈ R K : where T 0 ∈ R d×K is a learnable matrix.
- The benefits of different PEs are dataset dependent.
- We follow the strategy in [57] that uses random-walk structural encoding (RWSE) [41] for molecular data and Laplacian eigenvectors encodings [36] for image superpixels.
- Since Laplacian eigenvectors are defined up to sign flips, the sign of the eigenvectors is randomly flipped during training.
- Patch PE. Relative positional information between the graph patches can be computed from the original graph adjacency matrix A ∈ R N ×N and the clusters {V 1 , ..., V P } extracted by METIS in Section 4.2.
- Specifically, we capture relative positional information via the coarsened adjacency matrix A P ∈ R P ×P over the patch graphs: where Cut(V i , V j ) = k∈Vi l∈Vj A kl is the standard graph cut operator which counts the number of connecting edges between cluster V i and cluster V j .
- We observe that matrix A P is sparse as it only connects patches that are neighbors on the original graph.
- This can cause poor long-distance interactions.
- To avoid this situation, we can simply smooth out the adjacency matrix A P with any graph diffusion process.

### Mixer Layer
- The original mixer layer [60] is a simple network that alternates channel and token mixing steps.
- The token mixing step is performed over the token dimension, while the channel mixing step is carried out over the channel dimension.
- These two interleaved steps enable information fusion among tokens and channels.
- The simplicity of the mixer layer has been of great importance to understand that the self-attention mechanism in ViT is not the only critical component to get good performance on visual classification tasks.
- This has also led to a significant reduction in computational cost with little or no sacrifice in performance. Indeed, the self-attention mechanism in ViT requires O(P 2 ) memory and O(P 2 ) computation, while the mixer layer in MLP-Mixer needs O(P ) memory and O(P ) computation.
- We modify the original mixer layer to introduce positional information between graph tokens.
- Let X ∈ R P ×d be the patch embedding {x G1 , ..., x G P }.
- The graph mixer layer can be expressed as Token mixer, where A P D ∈ R P ×P is the patch PE from Eq.7, σ is a GELU nonlinearity [75], LayerNorm(•) is layer normalization [76], and matrices W 1 ∈ R ds×P , W 2 ∈ R P ×ds , W 3 ∈ R dc×d , W 4 ∈ R d×dc , where d s and d c are the tunable hidden widths in the token-mixing and channel-mixing MLPs.
- Then we generate the final graph-level representation by mean pooling all the non-empty patches: where m p is a binary variable with value 1 for non-empty patches and value 0 for empty patches (since graphs have variable sizes, small graphs can produce empty patches).
- Finally, we apply a small MLP to get the graph-level target.

### Data augmentation
- MLP-Mixer architectures are known to be strong over-fitters
- In order to reduce this effect, we propose to introduce some perturbations in METIS
- At each epoch, we apply METIS graph partition algorithm on G to get slightly different node partitions
- Then, we extract the graph patches {G 1 , ..., G P } where G i = (V i , E i ) is the induced subgraph of the original graph G, and not the modified G.

## Experiments
- Evaluates Graph MLP-Mixer on a wide range of graph benchmarks
- 1) Simulated datasets: CSL, EXP, SR25 and TreeNeighbourMatch dataset, 2) Small real-world datasets: ZINC, MNIST and CIFAR10 from Benchmarking GNNs, and MolTOX21 and MolHIV from Open Graph Benchmark, 3) Large real-world datasets: Peptides-func and Peptides-struct from Long Range Graph Benchmark

### Comparison with MP-GNNs
- Graph MLP-Mixer outperforms all base MP-GNNs across various datasets
- We augmented all the base models with the same type of PE as Graph MLP-Mixer to ensure a fair comparison
- These promising results demonstrate the generic nature of our proposed architecture which can be applied to any MP-GNNs in practice

### Comparison with SOTAs
- Next, we compare Graph MLP-Mixer against popular GNN models with SOTA results
- For small molecular graphs, our model achieved 0.075 on ZINC and 0.8073 on MolHIV
- For larger molecular graphs, our model sets new SOTA performance with the best scores of 0.6921 ± 0.0054 on Peptides-fun and 0.2475 ± 0.0015 on Peptides-struct
- Besides, Graph MLP-Mixer offers better space-time complexity and scalability

### Graph MLP-Mixer can achieve high expressivity
- Graph PEs, s.a. Laplacian eigenvectors or k-step Random Walk PEs, cannot guarantee two graphs are generally isomorphic, but it was shown in [41] that they can distinguish non-isomorphic graphs for which the 1-WL test fails.
- Graph MLP-Mixer is strictly more powerful than 1-WL. We experimentally validate on three simulation datasets that our model is strictly more powerful than 1&2-WL, and is not less powerful than 3-WL, see Table 4.
- CSL [34] contains 150 4-regular graphs (1-WL failed) divided into 10 isomorphism classes.
- EXP [79] containing 600 pairs of non-isomorphic graphs (1-WL/2-WL failed).
- SR25 [67] has 15 strongly regular graphs (3-WL failed) with 25 nodes each.
- Our model achieves perfect accuracy on all 3 simulation datasets while MP-GNNs fail.
- We evaluate various choices we made to implement each components of the architecture in Appendix C.
- Table 8 shows how many benefits the METIS can provide against random graph partitioning.
- Figure 4 evaluates the effect of number of graph patches.
- Figure 5 studies the effect of patch overlapping with k-hop extension.
- Figure 6 shows the effects of two kinds of positional encoding (i.e., node PE and patch PE).
- Table 9 studies the effect of data augmentation and the trade off between performance and efficiency.
- Table 10 replaces the Mixer layer in Graph MLP-Mixer with the standard Transformer layer as in ViT [59], keeping the rest the same.
