---
title: "Trace Encoding in Process Mining: a survey and benchmarking"
date: 2023-01-05T17:25:30.000Z
author: "Sylvio Barbon Jr., Paolo Ceravolo, Rafael S. Oyamada, Gabriel M. Tavares"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02167v1.webp" # image path/url
    alt: "Trace Encoding in Process Mining: a survey and benchmarking" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02167)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02167).


# Abstract
- Encoding methods are employed across several process mining tasks
- Most papers choose existing encoding methods arbitrarily or employ a strategy based on a specific expert knowledge domain
- Existing methods are employed by using their default hyperparameters without evaluating other options, which can lead to suboptimal performance and unfair comparisons with the state-of-the-art

# Paper Content

## Introduction
- Encoding methods are responsible for transforming complex information into a representative feature space
- In process mining (PM), several tasks (e.g., predictive process monitoring, trace clustering, anomaly detection, etc.) must encode data before feeding specific algorithms
- This step is crucial to account for the goals of a user correctly
- For instance, if a problem demands a solution where interpretability and explainability are needed, the data should be encoded by methods that tend to accomplish those objectives
- However, if the most essential requirements are space or time complexity, the user should agree to lose part of the previous benefits to match these ones
- In the PM literature, most of the efforts have been dedicated to designing new algorithms and analytical methods but little attention has been given to the impact of encoding methods across the existing tasks
- For instance, in predictive process monitoring [1] used the word embedding method to map the cases of an event log into real-valued vectors, whereas [2] used the one-hot
- A custom function is adopted by [3], whereas the count2vec (occurrence frequencies of activities) is employed by [4]
- Thus, a researcher interested in comparing the results of these works is in front of a factor she cannot control, as the impact of encoding is not documented and the methods used are different
- Moreover, in this work, we emphasize that very few alternative encoding methods have been employed by the community and demonstrate that arbitrarily encoding data might bring suboptimal results and misalignment with the user's goals
- We believe that a better understanding of the effect of encoding methods, according to the datasets' characteristics, is decisive in developing more interpretable, explainable, robust, and accurate PM solutions
- Through an in-depth analysis, we consider the criteria expressivity, which aims at capturing patterns across different characteristics of the employed datasets; scalability, which measures the elapsed time and the memory usage of encoding methods; correlation power, which maps the data characteristics to the algorithm performances; and the domain agnosticism, which considers if the encoding method depends or not on the problem domain
- We demonstrate through our extensive experimental evaluation how difficult it might be to choose a suitable encoding method since each of the evaluated metrics has a different best performing method
- Thus, the main contributions of this work include: a systematic review of encoding methods in PM and a new taxonomy developed according to such review; the proposal of new evaluation metrics to measure the quality of encoding methods in PM tasks; a deep experimental evaluation of several encoding methods never employed before in PM; a discussion of insights into future research on encoding for PM.

## Problem Definition
- encoding methods for pm tasks are difficult
- encoding methods for pm tasks are often adapted from other domains
- simple techniques are often considered
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often adapted from other domains
- encoding methods for pm tasks are often...

## Background Notions
- PM is a set of techniques to extract knowledge from event logs
- The goal is to provide analysis that uses event data to extract process-related insights
- Î£ is the universe of events, i.e. the set of all possible event identifiers
- With abuse of notation, we refer to the name of the activity of an event # activity (e) as the event itself
- An event can also be denoted by its position in the sequence as e i with e n the last event of this sequence
- A sequence of events composes a trace t â Î£ * and it can be defined as follows: t(i) = t(j).
- A trace can also be denoted as a function generating the corresponding event for each position of its sequence: t(i â n) â e i , ..., e n
- A subtrace is a sequence t(i â j) where 0 < i â¤ j < n
- Now let C be the case universe, that is, the set of all possible identifiers of a business case execution
- C is the domain of an attribute case â AN
- Definition 3.3 (Case, Event Log)
- We denote a case c i â C as a, b, d ci , meaning that all events share the same case
- For example, for c i we have # case (e 1 ) = # case (e 2 ) = # case (e 3 ).
- An event log L is a set of cases L â Î£ * where each event appears only once in the log, i.e. for any two different cases the intersection of their events is empty
- In PM, encoding is a crucial step for several tasks in order to project the information contained in an event log to another feature space before combining it with posterior algorithms such as clustering
- In the context of this work, we approach the anomaly detection problem to benchmark encoding methods.

## Encoding Methods
- Proposed a taxonomy of encoding methods
- To the best of our knowledge, this work is the first in the PM literature to propose a systematic review of encoding methods for PM tasks
- There are surveys and benchmarks for specific groups of algorithms, for example regarding graph embedding [24] or text embedding [25], but they fall outside the scope of PM applications
- In PM, different tasks need to employ an encoding method; we focus our review on trace clustering, predictive monitoring, and anomaly detection tasks

### Systematic Review
- systematic review of methods used in the literature
- searched for papers from the last 10 years
- included papers that met the following criteria: "process mining" AND "encoding" OR "encode"
- abstracts were read to eliminate irrelevant papers
- 616 papers were included
- 208 papers were about clustering (CLUS), 165 about predictive process monitoring (PPM), 144 about anomaly detection (AD), 51 about online process mining (OPM), and 48 about security (SEC)

### Taxonomy

### PM-based Encoding
- Retrieve the process model for the event log
- Evaluate each trace in the event log
- Produce the encoded representation of the trace
- Consider trace-replay, trace alignment, and log skeleton

### Text-inspired Encoding
- Many solutions used for trace encoding in PM are adapted from methods used in NLP.
- Exploiting the fact that words in sentences are ordered in sequence and are constrained by dependencies, encoding methods applied to text capture that information.
- Because traces are composed of sequences of activities the same information appears relevant to characterize them.
- In particular, in our survey, we consider the following methods.
- One-hot: given a variable containing n different values, the variable is transformed into an array where each unique value is represented as a binary vector with the i-th position set to one and the rest set to zero.
- CountVectorizer (count2vec): given a collection of categorical documents, this method produces a matrix of token occurrences, where each line in the matrix represents a document and each column a token.
- HashVectorizer (hash2vec): it does the same as count2vec. However, instead of storing tokens, it directly maps each token to a column position in the matrix of occurrences.
- TF-IDF is obtained by multiplying both TF- Word2vec and IDF.
- GloVe: this is an unsupervised learning algorithm for obtaining vector representations for words. The main intuition behind this model is the capturing ratios of word-word co-occurrence probabilities in order to capture both local and global dependencies.

### Graph-based Encoding
- The intuition behind graph embedding methods is to represent nodes of a graph as low dimensional vectors, where such vectors are representative enough to keep its original relations (edges) intact.
- A graph can be described as G = (V, E), where V = {v 1 , ..., v n } is a set of vertices (nodes) and E is a set of edges e = (u, v) that connect a pair of vertices u, v â V .
- Given a graph G, a graph embedding is a mapping function f : v i â y i â R d , such that d << |v| and f preserves the original structure of their local neighborhood and minimizes the information loss.
- In this section, we describe graph embedding methods for event log encoding.
- DeepWalk [39]: it can be seen as a two-stage algorithm. First, a discovery of the local structure is performed through random walks. There are two parameters here, the number of random walks Î± and the number of vertices to visit t for each random walk. Second, similar to the word2vec, the skip-gram is performed to learn the embeddings.
- Node2vec [40]: this algorithm is similar to DeepWalk, where the difference is a biased-random walk that aims at employing a trade-off between breadth-first and depth-first searches. In practice, such balance is capable of providing more informative embeddings than DeepWalk.
- Walklets [41]: while DeepWalk and node2vec implicitly capture a certain level of dependencies by generating multiple random walks through A k ij , this algorithm does explicitly by combining factorization approaches with random walks. It preserves dependencies by sub-sampling short random walks on the vertices and by skipping over steps in each random walk. This results in paths of fixed lengths composing sets of pairs of vertices. Thus, these sets are used to learn the latent representations.
- role2vec [42]: this is a framework that uses random walks to approximate the pointwise mutual information matrix, which is obtained by multiplying a matrix of structural features with the pooled adjacency power matrix.
- Laplacian Eigenmaps [43]: this algorithm intuitively keeps the embedding of two nodes close when the weight W ij is high. Given a graph G, this algorithm computes eigenvalues and eigenvectors Ly = Î»Dy, where D is a diagonal weight matrix D ii = j W ji , and W is the weight matrix. Thus, L = D â W is the Laplacian matrix that can be used to minimize the function Ï GraRep [44]: this algorithms learns the latent representation W â R |V |Ãd of the vertices of the weighted graphs. It leverages global structural information to capture long-distance connections. The overall idea is first to calculate the transition probability matrix A = D â1 S for each k, where 1 <= k <= K is the maximum transition step. Subsequently, obtain each k-step representation by factorizing the log probability matrix using singular value decomposition. Finally, the k-step representations for each vertex on the graph are concatenated and used as latent representations.
- Hope [45]: this embedding algorithm is similar to GraRep, but instead of using the transition probability matrix, it employs a similarity matrix S. Thus, S can be obtained by using different similarity measures and consequently preserves higher-order dependencies.
- BoostNE [46]: this algorithm performs a non-negative matrix factorization to calculate the residuals generated by previous embedding models. It assumes the same idea as the gradient boosting method in ensemble learning, where multiple weak learners lead to a better one when aggregated.
- Given a connectivity matrix obtained through the adjacency matrix of the graph, the algorithm calculates k residual matrices and uses each one as input to the next one using the following equation: where intuitively act like the embedding representation of the center node and the context node in the i â th level, respectively. Assuming the defined residual matrix, the embedding representation at the i â th level is obtained by minimizing the loss function [47]:

### Related Works
- Several tasks in PM, such as predictive monitoring, trace clustering, and anomaly detection, need to encode data to feed algorithms that are applied down the pipeline.
- Although transforming event data into a reasonable feature space is a sensible task, i.e., it might drastically impact algorithms' performances, very little attention has been given to encoding methods.
- Regarding the literature of other problem domains, there are surveys and benchmarks trying to standardize and better investigate the behaviors of encoding methods according to different problems' characteristics.
- For instance, [24] surveyed several graph-based embedding methods on different datasets and discussed the main challenges for future research in the field.
- Regardless of the approached task (e.g. link prediction, node classification, etc.), the authors demonstrate the difficulty of choosing not only the right algorithm but also the right set of parameters (mainly the dimensionality).
- Several trade-offs must always be taken into consideration, for instance increasing the memory usage to achieve more precision or decreasing the dimensionality to decrease the computation time.
- On the other hand, [53] covered a wide range of methods to encode textual information. The work focuses on methods based on encoding methods to feed neural network architectures regarding different tasks and also provides historical notes for each category of task.
- The aforementioned works usually focus on representational learning, which employs neural networks to learn a high-quality representation (encoding) of data.
- In the natural language literature, the word2vec [35,36] can be seen as one of the most important methods for this purpose, which has two architectures variants, one using the CBOW algorithm and another one using the skip-gram model.
- From this perspective, several methods derived from it, for instance, [37,10].
- The resulting feature vectors representing the original data are also called embeddings.
- Recently, representational learning has been applied in PM as well. [10] proposes the act2vec, trace2vec, log2vec, and model2vec. Each approach derives from existing encoding methods in the literature and leverages the previous level information to enrich the learning. That is, the first level is act2vec, which extends the word2vec architecture to learn the representation of activities. Subsequently, the trace2vec adopts the doc2vec concept and jointly learns the representation of activities and traces. The log2vec architecture derives from the same idea as trace2vec where the log representation is included in the architecture to be jointly learned. Finally, for model2vec, the authors extend graph representation learning techniques to represent a process model discovered from the event log. The final architecture also includes all the previous representations to be learned jointly.
- In the literature, we also find "hand-crafted" methods, which are usually developed by following some expertise domain knowledge. [12] proposes using graph convolutional networks for predictive monitoring. In their approach, the authors first perform a feature engineering step to handle time features and then transforms each activity in an event log into a matrix num_unique_activities Ã num_time_f eatures. [16] employs a PM algorithm to encode resources in event logs. In a nutshell, the applied algorithm is able to automatically discover resource pools and, hence, reduce the dimensionality of categorical values by grouping them.

## Methodology
- The experimental analysis was carried out to evaluate encoding methods
- The software and materials used in the experiment were described
- The metrics used to assess the quality of the surveyed encoding methods were provided

### Implementation Overview
- Generated synthetic logs were used
- Encoding was performed using open-source libraries
- Organized according to taxonomy
- Baselines were set to count2vec, one-hot, n-grams, and position profile

### Evaluation Metrics
- Expressivity: Baseline Sklearn n-grams, Baseline NLTK position profile, 2017 Baseline GitHub one-hot, Baseline Sklearn GraphWave, 2001 Graph Karate Club NMF-ADMM, 2014 Graph Karate Club DeepWalk, 2014 Graph Karate Club GraRep, 2015 Graph Karate Club node2vec, 2016 Graph Karate Club Walklets, 2017 Graph Karate Club role2vec, 2018 Graph Karate Club NetMF, 2018 Graph Karate Club NodeSketch, 2019 Graph Karate Club BoostNE, 2019 Graph Karate Club GLEE, 2020 Graph Karate Club Hope, 2016 Graph Karate Club diff2vec, 2018 Graph Karate Club Log skeleton, 2018 PM PM4PY token-replay, 2016 PM PM4PY alignment, 2016 PM4PY word2vec (skip-gram), 2013 Text Gensim hash2vec, Text Sklearn GloVe, 2014 Text GloVe doc2vec, 2014 Text Gensim word2vec (CBOW), 2013 Text Gensim TF-IDF, 1958 Text Sklearn Table 2: Contributions of measures to the study.

### Experimental Design
- Experimental design relies on labeled data
- Synthetic event logs were generated based on standard PM research practices
- Anomalies were injected into the generated traces, representing an anomaly detection PM task
- Afterward, traces were labeled as anomalous or normal, making our data set suitable for supervised learning
- Our dataset was made more realistic by adding heterogeneous behaviors to the event logs
- PLG2 [57] was used to create five different process models by performing a random generation of a process capable of capturing several behaviors, such as sequential, parallel, and iterative control-flow
- The rationale of PLG2 is based on the combination of traditional control-flow patterns [61], e.g., sequence, parallel split, and synchronization
- In order to simulate real-world scenarios, the patterns are progressively combined according to predetermined rules
- Each of the five generated process models defines five different base scenarios based on the activities and gateways included in the scenario
- Creating the log required simulating the process model
- For that, we applied the ProM plug-in 14 for the simulation of a stochastic Petri net
- We went through 10 thousand simulated cases, with a case arrival rate of about 30 minutes, and kept the default values of the others hyperparameters
- We injected anomalies, following [62], by perturbing regular traces as proposed by [63], as in Table 3
- Anomaly Description skip A sequence of 3 or fewer necessary events are skipped insert 3 or fewer random activities are inserted in the case rework A sequence of 3 or fewer necessary events is executed twice early A sequence of 2 or fewer events executed too early, which is then skipped later in the case late A sequence of 2 or fewer events executed too late all Scenario where the event log is affected by all anomalies listed above

## Benchmarking Process Mining Encoding
- Baseline encoding methods achieved the best results
- Graph encoding methods achieved the best results for short texts
- Text encoding methods achieved the best results for long texts
- Process Mining encoding methods achieved the best results for process descriptions

### Expressivity
- PCA was used to identify how complex the mapped space is
- The higher the expressivity, the more similar the mapped space is
- Baseline methods (count2vec, n-grams, position profile, and one-hot) have an average expressivity
- PM-based encoding methods (alignment, token-replay, and Log skeleton) have a very high expressivity
- Text encoding family (GloVe and hash2vec) has an average expressivity
- BoostNE has a very low expressivity

### Scalability
- The time and memory consumed during the encoding task was considered
- Three different versions of event log size were considered
- Baseline family analyses were supported by Figure 10
- In terms of memory cost, the Baseline encoding family showed that n-grams was the most expensive method with a high space complexity
- On the other hand, position profile was the worst method in terms of time complexity
- One-hot demonstrated that memory costs increase more than time as the problem is scaled
- In terms of scalability, count2vec presented the best scalability results of the Baseline family
- PM encoding family presented high memory costs and average time costs, but with good scalability in both measures, as visible when the dataset increased and the performances slightly grew, as in Figure 11
- Alignment method was the cheapest one in terms of memory, token-replay presented a good balance in terms of time, and Log skeleton the most costly in both, memory and time
- Text encoding family presented the fastest methods with low memory consumption
- However, presented time scalability issues by the majority of methods (GloVe, hash2vec, CBOW, skip-gram and doc2vec), as represented by Figure 12
- The least scalable was doc2vec
- A notable exception was TF-IDF, which presented reduced memory usage even with increasing problem size
- Note that the scalability was evaluated considering the ability to not suffer from data growth, and the average usage of time and memory in the Text encoding family is the lowest
- The Graph encoding family presented a heterogeneous usage of memory and an average time cost, as Figure 13 shows
- GLEE, Hope and NetMF were the fastest methods of this family. These methods presented average scalability
- The best scalability was demonstrated by NodeSketch

### Correlation power
- The F1-score is a measure of correlation between encoding and performance
- In our benchmark, we evaluated the correlation based on an anomalous trace detection task
- The N2 value is a measure of distance between examples and is low when there is a greater distance between examples of different classes than between examples from the same class
- Log skeleton, position profile, NodeSketch, NMF-ADMM, GraphWave, BoostNE, NetMF, Walklets, Hope, GLEE, Laplacian Eigenmaps, node2vec, all of them support high predictive performance
- There is no statistical difference among encoding methods that provided lesser predictive models for one-hot, doc2vec, n-grams, count2vec, token-replay, alignment, hash2vec, GloVe, skip-gram, CBOW and TF-IDF

### Domain agnosticism
- The baseline family of encoding methods is domain agnostic
- The Text and Graph families have been used for a wide range of purposes
- Considering our criteria, PM-based solutions are not domain agnostic

## Issues, Concerns and Future Directions
- Research comparisons about encoding methods focused on PM are still embryonic.
- After classifying the papers in Section 4, we observed that the proposed solutions do not investigate the strong and weak points of each method.
- Thus, we have proposed a study involving a large set of methods from different families.
- Investigating the pros and cons of encoding methods based on expressivity, scalability, correlation power, and domain agnosticism over different encoding families and hundreds of event logs with various complexities, we were able to gain insights and share some assumptions for future directions and possible novel PM encoding methods.
- We believe the criteria employed in this work to assess the effectiveness are the most important aspects to take into consideration in order to achieve significant accomplishments for any PM task that needs to encode event logs.
- Moreover, the proposed metrics can also serve as user requirements when deciding which encoding method should be employed for the specific problem.
- The expressivity of an encoding method can measure how straightforward the representation of an event log is according to its complexity.
- The scalability is also an important concern since real-life event logs consist of a large volume of data which can lead to high computational costs regarding elapsed time and memory usage.
- Understanding the correlation power between the nature of an encoding method and the performance of the executed task is also a relevant analysis that allows practitioners to estimate the complexity of analyzing a specific event log.
- Lastly, the domain agnosticism is a novel and important discussion introduced in this work to consider if encoding methods can be adapted for different problem domains.

## Conclusion
- A systematic review of process mining tasks using encoding methods
- A new taxonomy to categorize each type of method into families
- An extensive experimental evaluation and benchmark assessing relevant evaluation metrics to measure the effectiveness of an encoding method
- We believe this work can support researchers and practitioners to achieve significant accomplishments in different application areas in PM
- We highlighted the need for a better understanding of how each method behaves according to different scenarios of event logs and different PM tasks
- Thus, to fill this gap we simulated such scenarios by employing the PLG2 tool to generate synthetic processes with distinct properties and presented the results as a benchmark
- In total, 27 encoding methods were evaluated throughout 420 different event logs containing different anomalies
- We considered four different evaluation criteria to measure the effectiveness of encoding methods for process mining tasks
- We limited our evaluation to only one task, anomaly detection, but the analysis and insights presented in this work can be leveraged for other applications, such as predictive monitoring and clustering
- We conclude this work by stressing the difficulty of choosing suitable algorithms and their parameters according to the user's preferences since each pipeline setting performs differently according to the event log characteristics
