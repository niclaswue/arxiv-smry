---
title: "The Infinite Index: Information Retrieval on Generative Text-To-Image Models"
date: 2022-12-14T19:50:35.000Z
author: "Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07476v1.webp" # image path/url
    alt: "The Infinite Index: Information Retrieval on Generative Text-To-Image Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07476)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07476).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/the-infinite-index-information-retrieval-on).

# Abstract
- The text-to-image model Stable Diffusion has recently become very popular.
- Only weeks after its open source release, millions are experimenting with image
- This is due to its ease of use, since all it takes is a brief description of the desired image to "prompt" the generative model. Rarely do
- Usually, an iterative refinement of the prompt ("prompt engineering") is
- As a new perspective, we recast image prompt engineering as interactive image retrieval - on an "infinite index". Thereby, a
- Selected image-prompt pairs allow direct relevance feedback, as the model can
- This is a form of one-sided interactive retrieval, where the initiative is on the user side, whereas the server side

# Paper Content

## INTRODUCTION
- Conditional generative models allow the generation of a desired output based on a user-specified condition.
- For generative text-to-image models such as DALL-E [57] or Stable Diffusion [59], this means that the model generates images that match a given text description, the "prompt."
- For a user, the prompt is the primary means of controlling 01 / 2021 DALL•E 2 [56] 3.5 B N/A Custom web crawl, licensed sources 2 07 / 2022 Imagen [61] 4.6 B 400 M [66] Common Crawl ★ ★ 05 / 2022 Midjourney [64] N/A N/A N/A 07 / 2022 Stable Diffusion [59] 890 M 400 M [66]  the generated image.
- If an ad hoc prompt does not produce a satisfactory result, the user usually interacts with the model by changing the prompt until they get a satisfactory result (unless they give up after a few tries).
- Since such trial and error is often necessary to achieve satisfactory results, users have begun to exchange ideas and techniques commonly referred to as "prompt engineering."
- But aside from referring to and adapting examples from others, it's often not obvious how to change the prompt to steer image generation in a particular direction.
- As a new perspective on the use of conditional generative models, we conceptualize them as a novel type of index where a prompt takes the role of a query representing a user's information need as in text-based image retrieval.
- Prompt engineering can then be considered a form of interactive text-based image retrieval, in which a user interacts with the model by modifying their prompt as if refining their query in an image search engine to "find" an image that satisfies their information needs.
- Besides the outlined parallels, retrieval on the basis of generative models poses a number of new challenges: At present, the initiative lies solely with the user, without support from the model as a "retrieval system".
- In this regard, the "system side" lacks a retrieval model as an intermediary between the user and the model as an index, tailored to help users obtain satisfactory images fast(er), if not ad hoc.
- For example, the user's manual refinement of prompts as queries is not supported by system-side log analysis and query expansion, and there is also no operationalization of the concept of relevance, especially when many (different) images are generated.
- Perhaps the most striking difference from traditional image retrieval is that when generative text-image models are used as an index, new images are generated rather than existing images retrieved.
- Instead of retrieving relevant images for a query from a finite set of indexed images, a result is returned for every conceivable prompt as a query. This includes prompts for which a traditional image search engine would return no results. Moreover, the number of different images that can be generated per prompt as a query is not conceptually limited, but only by the available computational capacity for model inference.
- A generative model is therefore effectively an "infinite index.

## BACKGROUND AND SURVEY
- Related work on generative models and IR is presented.
- IR is presented as a way to understand how generative models work.
- Related work on generative models is presented to put current developments in context.

## Image Generation
- GANs are a promising method for image synthesis
- Autoregressive transformer models are successful in high-resolution image synthesis
- Recent developments in diffusion models have shown their ability to outperform the traditional models like GANs in image synthesis
- Conditioning the generated images on text forms the foundation of text-to-image models

## Image Retrieval
- Text-to-image generation models aim to bridge the semantic gap between the high-level meaning of images and their low-level visual features through sophisticated image representations.
- Having represented and indexed images, the representation of the query image is then used for similarity-based searching and ranking.
- Text-based image retrieval in the past often focused on retrieval based on image metadata and tags, which is why it is sometimes also referred to as annotation-based, concept-based, or keyword-based image retrieval.
- However, some approaches also generate textual representations for un-annotated images, for example using character recognition.
- Some studies investigated search interactions of users with a text-based image retrieval system.
- Choi [11] analyzed search logs collected from 29 college students, finding that the participants more often modified their textual queries than that they started a completely new one.
- Hollink et al. [27] studied the image search behavior of news professionals, showing that these often modified their query by following semantic relationships of query terms, like first looking for images on a person and then for images on their spouse.
- Cho et al. [10] took a closer look at why people search for images. In their survey of 69 papers they identified seven information need categories: (1) searching for entertainment, (2) searching for an illustration (explanation or clarification of details, e.g., creating presentation slides or preparing study material), (3) searching for images for aesthetic appreciation (e.g., for desktop backgrounds), (4) searching for knowledge construction (four sub-categories: information processing, information dissemination, learning, and ideation), (5) searching for something to engage others (e.g., to grab audiences' attention), (6) searching for images to inspire themselves, and (7) searching for images for social interactions (e.g., images to trigger emotions).
- Moreover, they found seven categories of obstacles that could interfere with a user's ability to find the images they seek: (1) semantic problems, i.e., related to employed terminology, (2) content-based issues, i.e., related to describing content of images, (3) technical limitations of systems, (4) issues of aboutness or lacking relevance of retrieved images, (5) problems of inclusivity with regard to cultural or linguistic aspects of the user, (6) problems of lacking skills in handling search technology, and (7) problems of cognitive overload.

## Retrieval for Creative Tasks
- Text-to-image generation models have sparked discussions especially with regards to its artistic or creative uses.
- Interestingly, text-to-image generation models have been able to quickly build communities around their use, not only with regard to tools, but also around sharing prompts for generating images via community feeds and channels.
- This is in line with artists forming creative communities also in other types of art.
- On the other hand, such a strong community-building is somewhat surprising as arts and crafts hobbyists are usually less reliant on human sources.
- Several studies already analyzed user behavior and goals in creative tasks specifically.
- Chavula et al. [9] investigated the information behavior for 15 graduate students on creative web search tasks using questionnaires and the think-aloud method. They identified four creative thinking processes between which participants switched back-and-forth: planning for creative search tasks (i.e., deciding on a vague idea), searching for new ideas, synthesizing search results, and organizing ideas.
- Palani et al. [54] use log analysis and self-reports in a study with 34 design students. They observed three main goals: To get an overview of the information space, to discover design patterns and criteria, and to seek inspiration and generate ideas.
- In the study they specifically took note of participants first struggling to find adequate terms to describe their information need, using rapid querying and query reformulating to converge on suitable terms. Furthermore, they observe that participants usually go through a divergent exploration phase before a convergent synthesis phase.
- Based on a previous online survey and study [80,81], Li et al. [45]  information in an overview. They notice that image search engines were mostly used to look for a broad range of images, whereas image sites like Pinterest and Instagram were often used to browse for higher-quality images created by specific artists or professionals.

## Interactive Retrieval
- Interactive information retrieval focuses on users' search or information-seeking behavior and, subsequently, developing new interaction methods that assist users in this process
- Query Log Analysis. The analysis of query logs was already a valuable resource in the early days of web search, e.g., to improve the retrieval effectiveness [36], or to study the behavior of users [7,[32][33][34].
- For instance, Broder [7] established the taxonomy of query intents into informational, navigational, and transactional information needs, and this taxonomy is still used today [1].
- Query reformulations are either precision-oriented (when a term is substituted by a more specific one) or recall-oriented (if the query is expanded).
- Searchers do not start with perfect queries and instead reformulate: more than 50% of searchers reformulate at least one query during a search session [34].
- Automatic query expansion approaches, like RM3 [31], may leverage (pseudo) relevance feedback to add new (weighted) terms to the initial query and thereby address the vocabulary mismatch problem prominent in textual retrieval.
- However, it is not yet clear what reformulations are helpful in which situations during the creative work with text-to-image generation models (i.e., precision-oriented or recall-oriented reformulations).
- Query suggestions are important, as 30% of the queries among a commercial query log were previously suggested to its users [17].
- Spelling corrections are particularly important query suggestions, as 10-15% of search queries have spelling errors [14].
- Furthermore, query suggestions often aim at supporting users by showing related terms [28], and query log analysis shows that those suggested related terms are heavily used [33].
- However, it is important not to overwhelm users, as showing fewer alternatives for the suggestions is better than many [75].

## CASTING TEXT-TO-IMAGE GENERATION AS SEARCH
- Users do not know which prompts to use to steer resulting images in a particular direction, putting them in a situation resembling image search
- Prompt engineering resembles query refinement in image search to "find" relevant images
- Due to the possibility of generating random variations for images, a generative model is effectively an "infinite index. "

## Interaction Methods
- When using text-to-image generation models, the interaction method is prompting the model.
- Prompting serves as the first contact point with the model in the image generation process, much like querying for regular web search.
- Some interfaces allow to include images in the prompt to further push the generated images into a certain direction.
- However, unlike in pure content-based image retrieval, the model interfaces usually require the prompt to contain some text.
- Another aspect of prompting in some interfaces is to specify model parameters along with the prompt, for example the size of the image to-be-generated or whether to generate tiling images.
- Like in web search, a negation operator can be used to exclude certain concepts from the to-be-generated image.
- The widespread open-source model Stable Diffusion only provides a command line interface, but its community has implemented several graphical interfaces for it, for example one maintained by AUTOMATIC11112 (cf. Figure 1 (a)).
- Moreover, several services appeared in the larger text-to-image generation ecosystem to aid the user in prompt engineering.
- Specialized search engines allow to search for images that users generated with text-to-image generation models.
- The engines then reveal the prompts to generate the found images, thereby allowing for prompt-reuse.
- Images are indexed either by their prompt or by image content (e.g., using CLIP).
- Examples of such search engines include the "community feed" of the Midjourney web app or the independent search engine Lexica that indexes images from the Stable Diffusion Discord channel (cf. Figure 1 (b)).
- For the latter, according to its developer it served 1.4 million queries in a week and its index contains 12 million images in September 2022, and has raised USD 5 million, clearly indicating the need for such systems.
- Other services allow for (social) prompt engineering in a click interface 3 or even to buy prompts that are said to produce consistent results.
- Yet other projects attempt careful analyses of how the prompt affects the result, providing huge lists of examples.

## Employing Information Retrieval Technologies in Text-to-Image Generation
- The text-to-image generation community created support for a multitude of interaction methods.
- Different retrieval technologies could be employed to support users in the generation of images.
- A central aspect of information retrieval research is the concept of a result's relevance.
- Generating images instead of finding some can, at least in theory, fulfil most of these needs, and may be particularly useful for needs of entertainment, illustrations, aesthetic appreciation, engaging others, inspiration, and social interactions.
- For social interactions, for example, it is very useful that generative models account for general moods mentioned in the prompt, providing a clear path towards generating images that evoke certain emotions.
- The remaining information need, for which generating images is in some cases not the right choice, is the need for knowledge construction, as generated images often do not account for real-world knowledge.
- In this regard, it is necessary to distinguish two different intents when generating images.
- First, the user may already have a clear vision of the target image, for example for an illustration.
- A user with this intent iteratively refines their prompt until the system generates an image close to their vision, which we refer to as descriptive approach.
- Second, they may not have a clear vision or not a clear goal, but only a set of constraints.
- Also, with this intent the user iteratively refines their prompt as feedback to random elements introduced by the system, but directing the system loosely towards an image of their liking, which we refer to as creative approach.
- Though very different from the user's point of view, both approaches seem identical for the system: A generic prompt is expanded with details to become more specific. Moreover, research of interactive retrieval (cf. Section 2) is also applicable for text-to-image retrieval.
- Query log analysis can be essential to identify keywords in prompts that yield satisfactory results in general, and to identify search missions and early abortions that can point to problems in the model.
- In these cases, we expect that methods of query suggestion can be very helpful to assist especially novice users.
- At the moment, reformulating prompts is challenging as users are not able to predict the effect of prompt changes on the generated images.
- In our case study (Section 4), the creative professional thus abstained from optimizing the prompt, rather trying completely new ones instead.

## IR Evaluation
- The "infinite index" has severe implications for the design and evaluation of experiments because the full recall base is unknown.
- Evaluations with Active Judgment Rounds are difficult because the unjudged images are non-relevant.
- Traditionally, unjudged documents are either removed, assumed to be non-relevant, or highly relevant.

## CASE STUDY OF TEXT-TO-IMAGE GENERATION FOR CREATIVE TASKS: GAME ARTWORK SEARCH
- To illustrate information retrieval on generative text-to-image models, we report on a case study in which such a model is employed in a creative task.
- Section 4.1 describes the study setup and exemplary creative task, the generation of artwork for an online card game.
- Section 4.2 then summarizes the main observations from the study. A full report of the study is available as supplementary material.

## Setup of the Case Study
- The creative professional was generalist game designer and developer with an experience of five major game releases.
- He was very intrigued by text-to-image generation models and had encountered them in his Twitter feed.
- He had already generated about 50 images in DALL•E 2, about 20 in Midjourney, and less than 10 using Stable Diffusion on his own hardware, but none of these as part of a project.
- But he assumed that text-to-image generation models would be very useful for the video game industry.
- Based on his own tests, the professional decided to employ Midjourney for the task of creating artwork for a few cards in the online card game he was exploring.
- The study was conducted using the think-aloud method and extensive note-taking was employed.
- A report on the study including all generated images is available as supplementary material.

## Main Observations from the Case Study
- The mood board is a central tool for the professional
- The professional uses the mood board to learn domain knowledge and procedural knowledge
- The user feels a lack of control when using the mood board
- An interface that supports the user in organizing generated images is necessary

## DISCUSSION
- Generative text-to-image models have limitations
- Active learning could help to guide the generation
- There are ethical concerns with active learning

## Limitations
- The text-to-image models are of sufficient quality to be used in real-world applications
- Prompt engineering is not a good interface for text-to-image scenarios
- The influence of the training data is a fundamental limitation of the current models

## Active Learning to Guide Text-to-Image Generation via Relevance Feedback
- Active learning is an iterative procedure between a user and a machine learning model which is used in order to train a model from user input when no training data is available.
- The goal is to maximize the model quality while minimizing the required user input.
- A full active learning setup consist of a model that is trained one some task, a query strategy that selects data from an existing resource or generates new data to be labeled, and a stopping criterion that indicates when the process is likely to stop improving the result.
- During each iteration the query strategy provides examples to the user who annotates them according to the respective task.
- After each iteration, a new model is trained on all data labeled so-far and the loop is repeated until the user is satisfied with one of the generated images.
- In text-to-image generation, the overall active learning setup is shown in Figure 3.
- The process starts with the user and an initial prompt.
- The active learning model learns to reformulate prompts which are in turn passed to the text-to-image model.
- The model is trained with the user feedback as target values so that the resulting images should increasingly become more appealing to the user.
- Next, the query strategy decides which images are shown to the user. It balances exploration versus exploitation, a tradeoff that is well-known in information retrieval.
- Finally, the stopping criterion is the user who stops the process once his information need is satisfied.

## Ethical Concerns
- Artifacts like images, texts, and other media types that are sometimes difficult to distinguish from human illustrations raise ethical concerns.
- It is currently difficult to guide text-to-image models towards a desired outcome.
- Moreover, the decision whether the generated images depicts the desired scene and is of sufficient quality still needs to be made by the user.
- Therefore, we think this new paradigm will be a powerful tool but not replace the human illustrator for the foreseeable future-even assuming the image quality might eventually reach human levels, which is clearly not the case yet.
- This is corroborated by others such as Liu et al. [48], who developed and evaluated a system that supports the user at generating illustrations for news articles, finding that artistic knowledge is still beneficial to the generated result and explicitly stating that "generative AI deployment should [...] augment rather than [...] replace human creative expertise".
- We support this view: Instead of an autonomous AI that acts of its own accord, a "supportive AI" that asks and incorporates the decisions made their users.
- In this regard, a retrieval system by definition, is a supportive AI-based system, since they traditionally act only on behalf of users who have an information need.
- Who is the author of a generated image? And who owns the rights? This is a currently unresolved situation causing uncertainty regarding the use of AI-generated images, which is why large platforms such as the well-known stock image provider Getty Images have recently banned all AI-generated content.
- Can text-to-image models be abused for automated or non-automated misinformation?
- Generated misinformation is already a pervasive problem and widely discussed in the context of so-called "deep fakes" and AI-generated text generation [39,67,78].
- To mitigate this problem in text-to-image models like Stable Diffusion, image watermarking is employed, marking an image as being artificially generated.
- Though watermarks are not easily removed, this may not be enough, unless they are checked on virtually all devices.
- This, however, will require policy makers to legally require device makers to check an warn users.
- Moreover, watermarking images raises privacy concerns of its own.
- Do these models express or even amplify bias?
- Bias in language models is a known problem [38], too.
- Similarly, text-to-image models will be biased, since their training data distribution has not been systematically checked for social and other kinds of biases.
- In this regard, retrieval process on top of generative models may also be used to mitigate their inherent biases.
- For example, in information retrieval research on web search, fair ranking is meanwhile a widely studied problem.
- Image search engines based on generative models will have to postprocess and re-rank their results to control for biases, just like their traditional counterparts.
- However, the technologies applied for traditional search engines can be directly transferred to search engines based on generative models.

## CONCLUSION
- generative text-to-image models will require facilitating systems and services
- their integration into existing systems is already ongoing, as can be observed for years already with writing assistance and translation systems
- an integration into end user software to create slide presentation or artwork, however, will not fully serve the needs for everyone who searches for inspirational imagery
- in this respect, dedicated search engines based on generative text-to-image models as indexes, with a dedicated user interface to formulate information needs, and tailored retrieval models, are likely already under development
- the development of a search engine is not trivial, and the information retrieval community is facing a renewed challenge to develop an understanding and the technological basis for such search engines
- this includes the development of new retrieval models and relevance
