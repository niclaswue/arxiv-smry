---
title: "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model"
date: 2022-11-15T17:44:05.000Z
author: "Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, Humphrey Shi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2211-08332v2.webp" # image path/url
    alt: "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2211.08332)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2211.08332).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/versatile-diffusion-text-images-and).

# Abstract
- The recent advances in diffusion models have set an impressive milestone in many generation tasks.
- Trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest in academia and industry.
- Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks.
- In this work, we expand the existing single-flow diffusion pipeline into a multi-flow network, dubbed Versatile Diffusion (VD), that handles text-to-image, image-to-text, image-variation, and text-variation in one unified model.
- Moreover, we generalize VD to a unified multi-flow multimodal diffusion framework with grouped layers, swappable streams, and other propositions that can process modalities beyond images and text. Through our experiments, we demonstrate that VD and its underlying framework have the following merits:
- a) VD handles all subtasks with competitive quality;
- b) VD initiates novel extensions and applications such as disentanglement of style and semantic, image-text dual-guided generation, etc.;
- c) Through these experiments and applications, VD provides more semantic insights of the generated outputs.
- Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.

# Paper Content

## Introduction
- We introduce Versatile Diffusion (VD), a multi-flow multimodal diffusion network that solves text, images, and variations in one unified model.
- Based on VD, we further introduce a generalized multi-flow multimodal framework in which new tasks and domains can be involved.
- Through experiments, we show that VD generates high-quality outputs on all supported tasks, in which VD's text-to-image and image-variation results can better capture semantics in...

## Related Works
- Multi-modalities are unions of information with different forms, including but not limited to vision, text, audio, etc.
- Early deep learning work led by Ngiam et al. [51] learned a fused representation for audio and video.
- The similar idea was also adopted across vision and text label [51], and across vision and language [38].
- A part of multimodal approaches focused on zero-shot learning, for instance, DiViSE [17] targeted mapping images on semantic space from which unseen category labels can be predicted.
- Socher et al. [71] trained a recognition model with similar ideas in which images were projected on the space of text corpus.
- [42] shared the same design as Di-ViSE but was upgraded for a large and noisy dataset.
- Another set of works [3,33,34,55], focused on increasing classification accuracy via multimodal training: in which [55] and [33] did a simple concatenation on multimodal embeddings; [3] proposed a gated unit to control the multimodal information flow in the network; [34] surveyed FastText [29] with multiple fusion methods on text classification.
- Meanwhile, multimodal training was also wideadopted in detection and segmentation.
- Started from R-CNN [19], series of works structured with shared backbone and multiple heads in order to train and predict class ids, detection boxes [19,47] and object masks [23,28] in one shot.
- Another topic, VQA [2,18], conducted cross-modal reasoning that transferred visual concepts into linguistic answers.
- Methods such as [49,96] extracted visual concepts into neural symbolics, and [90,97] learned additional concept structures and hierarchies.
- Multimodal generative tasks can be formalized as representation learning plus generation [80], in which representation networks [20,37,54,84,85,88] with contrastive loss [1,13,56,82,83] played an essential role.
- Specifically, our model VD adopts VAEs [37] and CLIP [56] as the latent and context encoders, which are two critical modules for the network.
- VD also shares the common cross-modal concepts such as domain transfer [27,99] and joint representation learning [77,81,91].
- Diffusion models (DM) [25,72] consolidate large family of methods including VAEs [37,61,85], Markov chains [6,67,72,74], and score matching models [75,76], etc.
- Differ from GAN-based [8,20,32] and flow-based models [36,62], DM minimizes the lower-bounded likelihoods [25,75] in backward diffusion passes, rather than exact inverse in flow [62] or conduct adversarial training [20].
- Among the recent works, DDPM [25] prompted -prediction that established a connection between diffusion and score matching models via annealed Langevin dynamics sampling [75,87].
- DDPM also shows promising results on par with GANs in unconditional generation tasks.
- Another work, DDIM [73], proposed an implicit generative model that yields deterministic samples from latent variables.
- Compared with DDPM, DDIM reduces the cost of sampling without losing quality.
- Regarding efficiency, FastDPM [39] investigated continuous diffusion steps and generalized DDPM and DDIM with faster sampling schedules.
- Another work, [68], replaced the original fixed sampling scheme with a learnable noise estimation that boosted both speed and quality.
- [15] compared GANs with DMs with exhaustive experiments and concluded that DMs outperformed GANs on many image generation tasks.
- Another work, VDM [35], introduced a family of DM models that reaches state-of-the-art performance on density estimation benchmarks.
- Text-to-image generation, nowadays a joint effort of multimodal and diffusion research, has drawn lots of attention.
- Among these recent works, GLIDE [52] adopted pretrained language models and the cascaded diffusion structure for text-to-image generation.
- DALL...

## Method
- The fundamentals of diffusion models will be revisited
- A multi-flow multimodal framework will be presented and how to expand it with new tasks
- Detailed designs for Versatile Diffusion

### Diffusion basics
- The forward diffusion process is a Markov Chain with T steps that gradually degrade x 0 to x T with random Gaussian noises
- Given the forward diffusion process as prior, diffusion models are trained to reverse the process and recover signal x 0 back from x T by removing the added Gaussian noises. This is known as the backward diffusion process, and each step p θ (x t−1 |x t ) is sampled from the Gaussian distribution with network predicted mean µ θ (x t , t) and variance Σ θ (x t , t), shown as Equation 2.
- The objective function to train a diffusion model is to minimize the variational bound for negative loglikelihood shown in Equation 3.

### Multi-flow multimodal diffusion framework
- The diffusion framework proposed is a multi-flow network with various types of data as input and context
- A single flow in the framework requires a VAE to project data on latent space, a diffuser to diffuse latent vectors, and a context encoder to extract context semantics
- Notice that our settings for single flows closely follow the recent Latent Diffusion Model (LDM) and Stable Diffusion (SD) [63], in which they use au-toKL [37], UNet [64], and optional CLIP/Bert [14,56] correspondingly
- Our multi-flow multimodal diffusion framework inherits the merits of LDM/SD with its interpretable latent space, modulized structure, and lower computation cost
- While LDM and SD are primarily designed as highperforming single-task models, our VD and the proposed framework investigate much broader extensions in which we jointly train multiple flows, each representing a crossmodal task
- Our core design is the grouping, sharing, and swapping protocols inside the diffuser network that adapt the framework to all supported tasks and beyond
- When we update the weights, we set a customizable gradient multiplier for each group and stream based on the number of gradient accumulations and the model initializations

### Versatile Diffusion
- Versatile Diffusion (VD) is a unified diffusion model for text-to-image, image-variation, image-to-text, and textvariation
- We use the well-adopted UNet [64] with cross attentions [86] as the main structure of our diffuser network
- As mentioned in Session 3.2, we group layers into global, data, and context layers, in which both data and context layers have two streams to support image and text
- Regarding the image data streams, we follow LDM [63] and use the residual blocks (ResBlock) [24] with progressively decreased spatial dimension and increased channel number
- For the text data stream, we propose the novel fully connected residual blocks (FCResBlock) that expand 768dimensional text latent vectors into a 320-by-4 hidden feature and follow a similar channel-increasing paradigm
- We then apply GroupNorms [92], SiLU, and skip connections like a regular ResBlock (see Figure 4)
- For context groups, both image and context streams adopt cross-attention layers in which content embeddings manipulate data features via projection layers, dot products, and sigmoids
- Figure 3 shows a graphic explanation of the overall structure of VD

## Experiments
- Describe VD's training data and settings
- Show the performance of VD on all supported tasks
- Introduce several novel downstream applications empowered by VD

### Dataset
- We use Laion2B-en as VD's training dataset.
- Laion2B-en is a collection of nearly two billion images with English captions.
- All images in Laion2B-en come from online sources, and their corresponding captions are autogenerated through web crawling.
- To have a cleaner dataset, we further filtered the two billion samples with the following criteria:
- a) image-text CLIP similarity scores above 0.3;
- b) safety scores (i.e. NSWF) below 0.3;
- c) the probability containing watermark below 0.3;
- d) image aspect ratios within 0.6 to 1.6667;
- e) image area above 256 2 × 0.75.
- The remaining samples served as training samples for our VD.

### Training
- To demonstrate the proposed multi-flow multimodal framework, we trained VD with three settings: basic, dualcontext (DC), and official.
- VD-basic is an image-variation model with a single-flow.
- VD-DC is a two-flow model that supports text-to-image and image-variation.
- Its UNet dif-
- Two children sitting near each other kids, pointing their fingers at the sun.
- Two young kids looking up at the sky.
- There are stars that a child is watching about.
- Two young girls and a boy standing near a star.
- Two young girls are watching a star.
- Kids standing for their starts.
- An old house on the water.
- An island with houses in it.
- A house in the water.
- An island in the sky.
- A pictures of a rocky houses on the shore of a lake with a Mountains silhouette on the horizon.
- Skyport, a houseboat on a rocky cliff, very near the shore, pictured and the shore.
- House on a lake, sky overlooking, and on the shore of the mountain.
- A house on a lake by the sky.
- Fireworks and some people watching.
- Fireworks with lots of colored bursts.
- Fireworks and some clouds.
- Fireworks over the skyline and mountains.
- A fireworks at night.
- Fireworks night.
- A fireworks after huge fireworks.
- Fireworks bright lights at a party in downtown.
- The night sky, with some milkys.
- A person walking across a wooden bridge at night.
- A boy in front of the milky.
- Someone looking at a milky.
- A man standing at a tree outdoors.
- Man standing on a tree.
- Man standing on tree near city on night.
- A man taking a camping hike outdoors.
- An image of a person in an art gallery.
- An abstract painting that looks like it's on the wall.
- A painting that looks like an image of the scream.
- That is why the man when soul steal happens, is alive, if he see the devil?
- A terribly oil painting of his life's pain, on a st. man painting `` warbl.
- A woman standing on a crying city sidewalk painting.
- The man is dead by fate afraid.
- A house and small body of water.
- The mountains behind the water.
- A boat docked next to a building.
- The shore and the water.
- Houses on the lake with boats and trees beside there with the mountains on the background.
- House, mountain, boat, somewhere near lake.
- House on the cliff near the lake.
- Houses on the lake with the trees.

### Performance
- To the best of our knowledge, this is a first effort to introduce multi-flow multimodal diffusion models across different tasks and modalities
- We set corresponding single-task prior works as our baseline models and compared VD's results with these baselines
- Explicitly speaking: we chose SDv1.4 [63] as the baseline model for text-toimage, SD-variation [30] for image-variation, and BLIP for image-to-text
- Meanwhile, we also conducted qualitative comparisons between different VD models
- SD-variation [30] is an alternative version of SD that was specifically finetuned for the imagevariation task
- We acknowledge that DALLE2 [59] and Imagen [65] also achieved SOTA on these tasks
- However, no official code or training detail was publicly released, so Style Focused Variation Input we skipped comparing them
- For image-to-text (i.e. image captioning), we only compare BLIP [45] and VD-official since other settings of VD do not support this task
- Regarding text-variation, to the best of our knowledge, we don't know any prior works that support the task, thus we only showed VD's performance in Appendix B
- All other results are shown in Figure 5, in which image samples from SD and VD are generated with controlled random seeds for better quality checking.

### Disentanglement of style and semantic
- Our VD can enhance or reduce image styles from semantics without further supervision
- This phenomenon inspires us to explore a novel area where disentanglement between styles and semantics can happen on images with arbitrary contents in arbitrary styles
- Recall that prior works such as [5,21] explored similar properties in GAN latent spaces, but their domain of study was restricted to well-aligned data such as faces or churches

### Dual-guided generation
- Dual-guided generation is a downstream application that VD naturally supports
- Explicitly speaking, VD can generate outputs conditioned on both image and text at the same time
- Since diffusion models are iterative refinement approaches that well fit model ensemble techniques, such dual-guided generation may also be conducted by mixing up diffusion steps with single-task models [48]
- Although such model ensembling made a simple baseline, we notice that their results are unsatisfactory, and their costs are doubled
- Figure 7 shows the dual-guided results using SDv1.4 [63] (text-to-image model) ensembled with SDvariation [30] (image-variation model), from which we notice substantial object distortions and missing semantics
- We believe that these problems are due to their modellevel mixing, by which intermediate results in the diffusion process are still separately guided
- Our VD, however, can guide cross-modal conditionings on a much deeper level: layer-level or attention-level
- Such a unique way of mixing attributes to VD's inter-changeable structure, in which data layers can adapt to all streams of context layers
- Through experiments, we found that attention-level mixing on VD could maintain correct object structures and harmonize prompt and image contexts, thus yielding the best performance (see Figure 7)
- On the other hand, layer-level mixing provided decreased performance but still outperformed model-level mixing by a noticeable margin.

### Editable I2T2I
- Convert image to text
- Edit text
- Convert text back to image

## Conclusion
- Proposes a novel diffusion model, Versatile Diffusion, that handles text, image, and variations all in one.
- Generalized from VD, we also proposed a multiflow multimodal framework that can be further extended to new tasks and domains.
- Through experiments and applications, we demonstrate that VD performs well on all supported tasks.
- Meanwhile, our new multi-flow multimodal framework may prompt future research in a broader area, solving multiple cross-modal tasks with one model and one framework.
- The disentanglement application conducts controllable image-variation, supported by the image-variation flow of VD.
- Such flow consists of AutoKL, CLIP image encoder, and VD's diffuser with image data layers and image context layers.
- The core strategy of the disentanglement is to manipulate the 257×768 CLIP image context embedding, which guided the diffusion process via cross-attention.
- Recall that these embeddings are generated by the visual transformer [16], which begins with one global feature vector followed by 256 local feature vectors corresponding to image local patches.
- We first split the vector into the single global vector and the following 256 local vectors.
- We keep the global vector untouched and compute the principal components from the rest of the feature vectors.
- When manipulating the context embeddings, we notice that the first couple of principal components (i.e. the major principal components of the matrix) hold the style information (i.e. color, art, stroke styles), and the remaining principal components hold the semantic information (i.e., objects,object locations, identity).
- Thus, in practice, we generated image variations with style focuses from the guidance of the low-rank context embedding that hosts only major principal components.
- And we generated image variations with semantic focuses when we removed these major principal components from context embeddings.
- In Figure 9, we show additional qualitative results, in which we standardize the disentanglement with five levels: a) 0 represents normal image-variation; b) -1 and -2 are semantic-focuses by removing one and two major principal components; c) 1 and 2 are style focuses results corresponding to keeping only 10 and 2 major principal components.
- In practice, we also notice that principal components after order 50 have little effect on results. Therefore, we can speed up the disentanglement PCA by just computing the first 50 principal components and then conducting the manipulation.
- For the global feature vector, we notice that it mainly serves as a semantic feature that controls object location information. Hence, removing it may negatively impact image structure (see Figure 10) but may be useful in some art generation cases.
- We encourage researchers to explore further the low-rank subspace of the CLIP Image embedding for more exciting applications.
- The dual-guided generation for VD is to generate images or sentences through the guidance of both image context and prompt context.
- As mentioned in the main article, dual-guided tasks, or generally speaking, multi-guided tasks, can be achieved via model-level mixing in which we conduct diffusion steps using one model after another.
- However, we notice that such an approach may cause issues such as structure distortions and wrong semantics despite doubling the model usage.
- Unlike ensembling, VD can conduct the dual-guided task via a much deeper level of mixing.
- As mentioned in the main article Session 3.2, VD's diffuser has three layer groups: global, data, and context.
- When VD faces a designated data type (i.e. images), we only need to forward propagate the shared data layer stream (i.e. ResBlocks).
- On the other hand, when handling dual context, we can select from two options that correspond to...
