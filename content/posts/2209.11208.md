---
title: "A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases"
date: 2022-09-22T17:47:21.000Z
author: "James Harrison, Luke Metz, Jascha Sohl-Dickstein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2209-11208v1.webp" # image path/url
    alt: "A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.11208)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.11208).


# Abstract
- Learned optimizers have the potential to dramatically accelerate training of machine learning models
- However, even when meta-trained across thousands of tasks at huge computational expense, blackbox learned optimizers often struggle with stability and generalization when applied to tasks unlike those in their meta-training set
- In this paper, we use tools from dynamical systems to investigate the inductive biases and stability properties of optimization algorithms, and apply the resulting insights to designing inductive biases for blackbox optimizers.
- Our investigation begins with a noisy quadratic model, where we characterize conditions in which optimization is stable, in terms of eigenvalues of the training dynamics.
- We then introduce simple modifications to a learned optimizer's architecture and meta-training procedure which lead to improved stability, and improve the optimizer's inductive bias.
- We apply the resulting learned optimizer to a variety of neural network training tasks, where it outperforms the current state of the art learned optimizer -- at matched optimizer computational overhead -- with regard to optimization performance and meta-training speed, and is capable of generalization to tasks far different from those it was meta-trained on.

# Paper Content

## Introduction
- algorithms for stochastic non-convex optimization are a foundational element of modern neural network training
- choice of optimization algorithm (and the associated hyperparameters) is critical for achieving good performance of the underlying model, as well as training stability
- practically, there are few formal rules for choosing optimizers and hyperparameters, with researchers and practitioners typically defaulting to a small number of optimizers and associated hyperparameters with which they are familiar
- the algorithms chosen between are usually derived based on analysis in the convex setting or informal heuristics, and algorithm performance varies substantially across different real world tasks
- learned optimizers have been proposed, but they suffer from several fundamental flaws preventing their broad uptake
- first is reduced performance and stability when they are applied in circumstances unlike those in which they are meta-trained-for instance, learned optimizers often diverge when used to train models for more steps than the learned optimizer was applied for during meta-training
- second is instability and inconsistency in the meta-training of the learned optimizers themselves-learned optimizer performance is often highly dependent on random seed, and meta-training trajectories can get stuck for many steps and make inconsistent progress
- in this paper, we propose a series of changes to the architecture and training of learned optimizers that improve their stability and inductive bias
- the resulting stabilized through ample regularization (STAR) learned optimizer is more stable and faster to meta-train, is more stable and performant than baseline learned optimizers even when applied for many more training steps than used during meta-training, and generalizes well to new tasks which are dissimilar from tasks it was meta-trained on

## Related Work
- Learned optimization has seen a recent surge of interest motivated by the success of deep learning methods in a wide variety of problems
- Meta-learning algorithms have been particularly successful and widely investigated in the domain of few-shot learning, in which an agent must learn to make predictions based on a small amount of data
- Early approaches to this problem focused on blackbox models such as recurrent networks
- However, works that integrate algorithmic inductive biases-for example by exploiting gradient descent
- Metacognitive approaches have been proposed to address the instability of learning algorithms
- Truncated zeroth order optimization avoids extremely noisy reparameterization gradients combined with truncated computation graphs
- Reinforcement learning, in which the truncated computational graph is augmented with a value function capturing dependency of future losses on the policy, is used in [7,9]

## Problem Statement

### Nominal Terms Shift the Region of Stability
- Asymptotic stability is guaranteed in linear system (6) when ρ(A) = max i |λ i (A)| < 1 for all eigenvalues λ i (A) of A.
- Asymptotic stability implies lim T →∞ L(θ; T ) is finite, whereas instability implies lim T →∞ L(θ; T ) = ∞ for both the deterministic and stochastic system.
- Thus, stability is a necessary condition for achieving optimizers which, in expectation, reduce the loss over long training horizons.
- However, we emphasize that stability here is a proxy for convergence that yields simplified discussion and analysis in our setting.
- While stability is essential for guaranteeing favorable optimizer performance, the noisy quadratic setting also gives us insight into the relationship between the stability of underlying dynamical system and the gradient with respect to meta-trained optimizer P .
- This gradient is essential for meta-training the optimizer.
- Under the dynamics (6), the parameters at time t > 0 are φ where
- Thus, our expected loss is This loss at time t is polynomial in A with degree 2t (and note that A is constant across time). Thus, the gradient of this loss term with respect to P is polynomial in the parameters of A with degree 2t − 1.
- As a result, instability of the dynamics of φ generally implies instability of the gradient of the loss with respect to P , which in turn implies the expected gradient magnitude and gradient variance both diverge for long horizons.

### Preconditioners can Stabilize and Simplify the Design of Update Dynamics

### Adaptive Nominal Terms Improve Robust Stability
- We have so far motivated choosing a nominal α > 0 to bias the optimizer dynamics toward descent/stability.
- In inner training, decreasing the learning rate over the course of training has been shown to improve empirical performance (and is often necessary for guaranteeing convergence [57]), and is almost always done when training neural networks.
- For simple nominal gradient estimators and comparably complex blackbox terms, the blackbox model should be able to cancel out the nominal term, and induce the effects of reducing α.
- Setting P = P * − αI with nominal gradient term g t = ∇ t yields closed loop dynamics which is optimal with respect to meta-loss for optimal P * .
- In this subsection, we argue from the point of view of robust stability that this strategy is suboptimal relative to direct control of the magnitude of the nominal and blackbox term.
- We introduce a multiplicative error model 4 which captures the sub-optimality of the learned P during meta-training.
- Let P = ∆ P for diagonal disturbance ∆ ∈ D.
- We define this uncertainty set as Within this error model, we can establish conditions for stability in line with the previous subsections.
- We wish to establish robust stability conditions, which guarantee the stability of the dynamical system for all realizations of the disturbance.
- Lemma 3. Let A = I − αH − P H with P = ∆ P .
- We will assume P and ∆ are simultaneously diagonalizable, H symmetric positive definite, and 0 < α < 2/λ max (H).
- Then, ρ(A) ≤ 1 for all These results generally result in the tightening of the margin of stability.
- If we choose P = P * − αI, corresponding to our previously discussed cancellation of the nominal term, our dynamics become which harms the stability margins. This is well understood by taking ∆ = (1− )I, for any adversarial ≤ 1.
- Then, we have update dynamics yielding dynamics matrix Here, α H is the excess term in the dynamics compared to if the nominal term had instead been set to 0.
- In this expression, adversarially perturbs the system toward instability, resulting in a potentially substantial performance drop.
- Instead, we can select α over the course of both inner and outer training via directly controlling the magnitude of α, P . This approach corresponds to a hyperparameter controller, which has both been shown to enable automatic control of step sizes [45] over the course of inner training.

### Non-Markovian Optimizers Require Joint Stability
- Non-Markovian dynamics play a role in learned optimizers
- The role of momentum and stable hidden states is investigated
- Key points summarized are:
- The stability of the hidden state dynamics must be considered together with the parameter dynamics

## Designing a Better Learned Optimizer
- STAR learned optimizer
- Regularization strategies
- In-meta-distribution and out-of-meta-distribution performance
- Appendix B and Appendix C

### New Design Features in the STAR Optimizer
- We add a nominal term, as described in Section 4.1.
- We focus on nominal terms based on Adam [37] and AggMo [63], in part due to the input features to our blackbox optimizer containing momentum at different timescale (as in AggMo) and running exponential moving average (EMA) gradient magnitude estimates (as in Adam).
- We experimentally compare different nominal terms in the appendix.
- We add a magnitude controller on the nominal descent term, following the discussion in Section 4.3.
- This magnitude controller consists of one additional output head with an exponential nonlinearity on the small MLP (requiring five additional weights).
- The combination of magnitude control applied to a nominal optimizer (such as Adam) makes our full nominal term equivalent to a hyperparametercontroller learned optimizer, albeit with a controller that leverages substantial computation reuse with the blackbox term.
- Magnitude control via weight decay.
- In order to discourage violations of the upper bound on stable eigenvalues in Section 4.1, we apply relatively heavy weight decay (L 2 regularization) in outer training to the parameters of the blackbox term.
- By directly controlling weight magnitudes (as opposed to controlling magnitudes via regularizing network outputs) we achieve magnitude regularization for arbitrary inputs (for reasonably-sized inputs) and thus achieve better generalization.
- Preconditioner-style normalization.
- As discussed, we use an adaptive inverse EMA preconditioner in our nominal term to better bias our model toward descent.
- Because these EMA terms are maintained as inputs to the network already, the expense of this transformation is only the cost of dividing the blackbox output by the preconditioner.
- Stable hidden states.
- We discussed the importance of considering the stability of the combined parameter/hidden state dynamics in Section 4.4.
- It is critical to design the hidden state update dynamics to bias toward stability; indeed, this is a well-known fact in the study of recurrent networks in general [64][65][66] and has been discussed in [7,61].
- In this paper, as in previous works [7,9], we use exponential moving average, momentum-style hidden states which are stable by design.

### Overview of the STAR Optimizer
- We apply these modifications to the small_fc_lopt optimizer presented and open sourced in [9].
- This optimizer is a small (197 weight) MLP which is applied elementwise-it takes inputs such as the parameter value, the gradient, and features such as gradient momentum at different timescales, and outputs an update to the parameter.
- This optimizer is applied in parallel to all parameters in the model being trained, and the only interaction between parameter updates is through tensor-level input features.
- We refer to Appendix A of [9] for a complete explanation of the optimizer, but we review the basic details here.
- The optimizer is parameterized as In this expression, β 1 , β 2 are small constants which in [9] and here are set to 0.001.
- The terms d(z t ) and m(z t ), corresponding to direction and magnitude terms respectively (so named because the magnitude term only positively scales the direction output), are heads of the neural network.
- The architecture of the optimizer is an MLP with two hidden layers, each with a width of four.
- This limits the number of total parameters to 197, yielding high computational efficiency versus most learned optimizers.
- The input features of the network are the parameter value, various parameterwise momentum terms, EMA gradient norms estimates, several adafactor-based [67] features which aggregate tensor-level information, and a parameterization of the training step.
- Our modified update takes the form f where m g (•) is a magnitude term for the nominal term and g(z t ) is the nominal term; in our experiments we use a combined AggMo [63] and Adam [37] for this term.
- Critically, this term corresponds directly to a descent direction without being passed through a network, biasing the update toward descent.
- The blackbox term is structured as where v(z t ) is our specified preconditioner term and m b (•) is the blackbox magnitude term.
- The difference between this parameterization and ( 20) is the normalization term.
- In these terms, β 1 , β 2 , β 3 , β 4 are hand-specified constants, and the neural network has three output heads.
- The addition of the extra head as well as the multiplicative factor on that term adds an extra five parameters to the original 197-parameter optimizer, as well as one more for β 1 scaling the entire nominal term.
- Note that the addition of the v(•) term in the denominator of the blackbox term has magnitude comparable to the mean gradient magnitude, which requires choosing β 3 to account for this change in magnitude.

### The STAR Optimizer Improves Performance
- The STAR optimizer is more stable and faster than baselines
- The STAR optimizer generalizes well to different tasks and datasets
- The STAR optimizer is more stable than a hyperparameter-tuned Adam model

## Discussion
- Stability and inductive biases are important for learned optimizers
- Incorporating stabilizing inductive biases results in strong performance
- Further work on injecting stability into learned optimizers remains to be done
- Designing inductive biases for learned optimizers is potentially an exciting new line of work

## C Further Experimental Results
- Preconditioning generally improves meta-losses
- It is less clear what the benefit for generalization is, with performance of with/without the preconditioner being mixed

### C.1 Alternative Views of Main Results
- Without the nominal term, meta-training is slower and converges to worse solutions.
- Interestingly, for the final trained optimizer, performance on the Fashion MNIST MLP seems comparable between all models; this is very much not the case for the CIFAR model.
- Generally, we have found performance at all points in meta-training and generalization to be uniformly improved by the addition on nominal terms, and strongly endorse the inclusion of these terms.

### C.3 Full Generalization Experiments
- The learned optimizers trained on Fashion MNIST are unstable as compared to Hyperparameter, or either of the STAR optimizers.
- In almost all cases, the blackbox optimizer is unstable as compared to Hyperparameter, or either of the STAR optimizers.
- The performance of the hyperparameter controller versus the STAR models is more mixed; although we note that this degree of stabilization of blackbox models represents a significant advance and there are cases in which the STAR models substantially outperform the hyperparameter controller.
- Through a weighted combination of the three interventions of (a-c), all eigenvalues are mapped into the unit circle, and the learned optimizer becomes stable.
