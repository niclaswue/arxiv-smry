---
title: "A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases"
date: 2022-09-22T17:47:21.000Z
author: "James Harrison, Luke Metz, Jascha Sohl-Dickstein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-11208v1.webp" # image path/url
    alt: "A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.11208)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.11208).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-closer-look-at-learned-optimization).

# Abstract
- Learned optimizers have the potential to dramatically accelerate training of machine learning models
- However, even when meta-trained across thousands of tasks at huge computational expense, blackbox learned optimizers often struggle with stability and generalization when applied to tasks unlike those in their meta-training set.
- In this paper, we use tools from dynamical systems to investigate the inductive biases and stability properties of optimization algorithms, and apply the resulting insights to designing inductive biases for blackbox optimizers.
- Our investigation begins with a noisy quadratic model, where we characterize conditions in which optimization is stable, in terms of eigenvalues of the training dynamics.
- We then introduce simple modifications to a learned optimizer's architecture and meta-training procedure which lead to improved stability, and improve the optimizer's inductive bias.
- We apply the resulting learned optimizer to a variety of neural network training tasks, where it outperforms the current state of the art learned optimizer -- at matched optimizer computational overhead -- with regard to optimization performance and meta-training speed, and is capable of generalization to tasks far different from those it was meta-trained on.

# Paper Content

## Introduction
- algorithms for stochastic non-convex optimization are a foundational element of modern neural network training
- choice of optimization algorithm (and the associated hyperparameters) is critical for achieving good performance of the underlying model, as well as training stability
- practically, there are few formal rules for choosing optimizers and hyperparameters, with researchers and practitioners typically defaulting to a small number of optimizers and associated hyperparameters with which they are familiar
- the algorithms chosen between are usually derived based on analysis in the convex setting or informal heuristics, and algorithm performance varies substantially across different real world tasks
- learned optimizers have been proposed, but they suffer from several fundamental flaws preventing their broad uptake
- first is reduced performance and stability when they are applied in circumstances unlike those in which they are meta-trained-for instance, learned optimizers often diverge when used to train models for more steps than the learned optimizer was applied for during meta-training
- second is instability and inconsistency in the meta-training of the learned optimizers themselves-learned optimizer performance is often highly dependent on random seed, and meta-training trajectories can get stuck for many steps and make inconsistent progress
- in this paper, we propose a series of changes to the architecture and training of learned optimizers that improve their stability and inductive bias
- the resulting stabilized through ample regularization (STAR) learned optimizer is more stable and faster to meta-train, is more stable and performant than baseline learned optimizers even when applied for many more training steps than used during meta-training, and generalizes well to new tasks which are dissimilar from tasks it was meta-trained on

## Related Work
- Learned optimization has seen a recent surge of interest motivated by the success of deep learning methods in a wide variety of problems
- These methods fit into a larger class of metalearning methods, which aim to leverage the success of expressive learning algorithms (such as neural networks) to learn learning algorithms
- Meta-learning algorithms have been particularly successful and widely investigated in the domain of few-shot learning, in which an agent must learn to make predictions based on a small amount of data
- Early approaches to this problem focused on blackbox models such as recurrent networks
- However, works that integrate algorithmic inductive biases-for example by exploiting gradient descent
- have been highly successful when applied to few-shot learning.
- In practice, however, tuning of these hyperparameters is still necessary both over the course of training and across optimization problems
- While further methods have been developed for automatic online hyperparameter tuning
- a line of work has focused on meta-learning a neural network that chooses hyperparameters as a function of training history
- This approach results in inherently better stability of learning algorithms as the output is typically restricted to (in expectation) descent directions
- In this work, we find such inherent stability is crucial for guiding training, especially early in meta-training.

## Problem Statement

### Nominal Terms Shift the Region of Stability
- Asymptotic stability is guaranteed in linear system (6) when ρ(A) = max i |λ i (A)| < 1 for all eigenvalues λ i (A) of A.
- Asymptotic stability implies lim T →∞ L(θ; T ) is finite, whereas instability implies lim T →∞ L(θ; T ) = ∞ for both the deterministic and stochastic system.
- Thus, stability is a necessary condition for achieving optimizers which, in expectation, reduce the loss over long training horizons.
- However, we emphasize that stability here is a proxy for convergence that yields simplified discussion and analysis in our setting.
- While stability is essential for guaranteeing favorable optimizer performance, the noisy quadratic setting also gives us insight into the relationship between the stability of underlying dynamical system and the gradient with respect to meta-trained optimizer P .
- This gradient is essential for meta-training the optimizer.
- Under the dynamics (6), the parameters at time t > 0 are φ where
- Thus, our expected loss is This loss at time t is polynomial in A with degree 2t (and note that A is constant across time). Thus, the gradient of this loss term with respect to P is polynomial in the parameters of A with degree 2t − 1.
- As a result, instability of the dynamics of φ generally implies instability of the gradient of the loss with respect to P , which in turn implies the expected gradient magnitude and gradient variance both diverge for long horizons.

### Preconditioners can Stabilize and Simplify the Design of Update Dynamics

### Adaptive Nominal Terms Improve Robust Stability
- We have so far motivated choosing a nominal α > 0 to bias the optimizer dynamics toward descent/stability.
- In inner training, decreasing the learning rate over the course of training has been shown to improve empirical performance (and is often necessary for guaranteeing convergence [57]), and is almost always done when training neural networks.
- For simple nominal gradient estimators and comparably complex blackbox terms, the blackbox model should be able to cancel out the nominal term, and induce the effects of reducing α.
- Setting P = P * − αI with nominal gradient term g t = ∇ t yields closed loop dynamics which is optimal with respect to meta-loss for optimal P * .
- In this subsection, we argue from the point of view of robust stability that this strategy is suboptimal relative to direct control of the magnitude of the nominal and blackbox term.
- We introduce a multiplicative error model 4 which captures the sub-optimality of the learned P during meta-training.
- Let P = ∆ P for diagonal disturbance ∆ ∈ D. We define this uncertainty set as Within this error model, we can establish conditions for stability in line with the previous subsections.
- We wish to establish robust stability conditions, which guarantee the stability of the dynamical system for all realizations of the disturbance.
- Lemma 3. Let A = I − αH − P H with P = ∆ P . We will assume P and ∆ are simultaneously diagonalizable, H symmetric positive definite, and 0 < α < 2/λ max (H). Then, ρ(A) ≤ 1 for all
- These results generally result in the tightening of the margin of stability.
- If we choose P = P * − αI, corresponding to our previously discussed cancellation of the nominal term, our dynamics become which harms the stability margins. This is well understood by taking ∆ = (1− )I, for any adversarial ≤ 1. Then, we have update dynamics yielding dynamics matrix Here, α H is the excess term in the dynamics compared to if the nominal term had instead been set to 0.
- In this expression, adversarially perturbs the system toward instability, resulting in a potentially substantial performance drop. Instead, we can select α over the course of both inner and outer training via directly controlling the magnitude of α, P . This approach corresponds to a hyperparameter controller, which has both been shown to enable automatic control of step sizes [45] over the course of inner training.

### Non-Markovian Optimizers Require Joint Stability
- We briefly discuss the role of non-Markovian (or hidden state) dynamics in learned optimizers.
- An extended discussion is in Appendix A.4.
- The role of momentum and stable hidden states has been investigated in detail in both works on optimization [53,56] and learned optimization [61].
- We summarize the key points below.
- The core analysis change required is that we must consider the joint stability of the hidden state dynamics and the parameter dynamics together.
- Such analysis has been done in the case of Polyak momentum [62], and has been found to yield less restrictive upper stability margins (thus allowing a larger step size).
- In the general case of blackbox optimizer dynamics, the stability properties of the hidden state update have been investigated in [61], and we expand on this in Section 5.
- Momentum often accelerates convergence in the full-batch optimization setting. However, it has the additional benefit of filtering stochastic gradients, yielding better expected descent.
- This filtering behavior is an important consideration in any learned optimizer operating in the stochastic setting.

## Designing a Better Learned Optimizer
- STAR learned optimizer
- Regularization strategies
- In-meta-distribution and out-of-meta-distribution performance
- Appendix B and Appendix C

### New Design Features in the STAR Optimizer
- We add a nominal term, as described in Section 4.1.
- We focus on nominal terms based on Adam [37] and AggMo [63], in part due to the input features to our blackbox optimizer containing momentum at different timescale (as in AggMo) and running exponential moving average (EMA) gradient magnitude estimates (as in Adam).
- We experimentally compare different nominal terms in the appendix.
- We add a magnitude controller on the nominal descent term, following the discussion in Section 4.3.
- This magnitude controller consists of one additional output head with an exponential nonlinearity on the small MLP (requiring five additional weights).
- The combination of magnitude control applied to a nominal optimizer (such as Adam) makes our full nominal term equivalent to a hyperparametercontroller learned optimizer, albeit with a controller that leverages substantial computation reuse with the blackbox term.
- Magnitude control via weight decay.
- In order to discourage violations of the upper bound on stable eigenvalues in Section 4.1, we apply relatively heavy weight decay (L 2 regularization) in outer training to the parameters of the blackbox term.
- By directly controlling weight magnitudes (as opposed to controlling magnitudes via regularizing network outputs) we achieve magnitude regularization for arbitrary inputs (for reasonably-sized inputs) and thus achieve better generalization.
- Preconditioner-style normalization.
- As discussed, we use an adaptive inverse EMA preconditioner in our nominal term to better bias our model toward descent.
- Because these EMA terms are maintained as inputs to the network already, the expense of this transformation is only the cost of dividing the blackbox output by the preconditioner.
- Stable hidden states.
- We discussed the importance of considering the stability of the combined parameter/hidden state dynamics in Section 4.4.
- It is critical to design the hidden state update dynamics to bias toward stability; indeed, this is a well-known fact in the study of recurrent networks in general [64][65][66] and has been discussed in [7,61].
- In this paper, as in previous works [7,9], we use exponential moving average, momentum-style hidden states which are stable by design.

### Overview of the STAR Optimizer
- We apply a 197-parameter small-fc-lopt optimizer to a small MLP which is parameterized with direction, magnitude, and preconditioner terms.
- The optimizer is parameterized with a combined AggMo and Adam term which corresponds to a descent direction without being passed through a network.
- The blackbox term is structured with a multiplicative factor and an extra head.

### The STAR Optimizer Improves Performance
- The STAR optimizer is more stable and faster than baselines
- The STAR optimizer generalizes well to different tasks and datasets
- The STAR optimizer is more stable than a hyperparameter-tuned Adam model

## Discussion
- Stability and inductive biases are important in learned optimizers
- Including stabilizing inductive biases results in strong performance both in-distribution and out-of-distribution
- There is a deep literature on the stability of general computation graphs that can be exploited to allow more fine-grained control of stability properties of optimizers
- Inducing stability into learned optimizers is potentially an exciting new line of work
