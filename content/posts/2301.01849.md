---
title: "NODAGS-Flow: Nonlinear Cyclic Causal Structure Learning"
date: 2023-01-04T23:28:18.000Z
author: "Muralikrishnna G. Sethuraman, Romain Lopez, Rahul Mohan, Faramarz Fekri, Tommaso Biancalani, Jan-Christian Hütter"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-01849v1_PKC8qHzyd.jpg" # image path/url
    alt: "NODAGS-Flow: Nonlinear Cyclic Causal Structure Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01849)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01849).


# Abstract
- Learning causal relationships between variables is a well-studied problem in statistics
- However, modeling real-world systems remain challenging, as most existing algorithms assume that the underlying causal graph is acyclic
- In this work, we propose a novel framework for learning nonlinear cyclic causal graphical models from interventional data, called NODAGS-Flow
- We perform inference via direct likelihood optimization, employing techniques from residual normalizing flows for likelihood estimation
- Through synthetic experiments and an application to single-cell high-content perturbation screening data, we show significant performance improvements with our approach compared to state-of-the-art methods

# Paper Content

## INTRODUCTION
- Understanding the causal relationships between interacting variables is a fundamental problem in science
- Most work on causal structure learning relies on the assumption that the underlying graph connecting the variables is a directed acyclic graph (DAG)
- This assumption facilitates the definition of a probability distribution over the observed variables for very general functional relationships
- It also provides additional regularization to the estimation problem by narrowing down the class of graphs that are compatible with the observed probability distribution (the Markov Equivalence Class)
- However, there is compelling evidence that feedback loops are common in many real-world systems, such as those arising in gene-regulatory networks (Sachs et al., 2005;Freimer et al., 2022), violating the acyclicity assumption
- These networks can however be probed with a large number of interventions through recent technological advances in biological assays building upon CRISPR/Cas9 and single-cell RNA-Sequencing (Dixit et al., 2016;Frangieh et al., 2021), alleviating the need for the additional regularization provided by the DAG constraint
- Moreover, enforcing acyclicity necessitates searching for candidate solutions over large combinatorial search spaces, complicating algorithm design
- Combined, this suggests that Cyclic Causal Graphs (CCG) should be better suited to model causal semantics in this regime
- In this work, we present a novel framework for causal discovery that does not rely on the DAG assumption, but instead allows for the presence of cycles in the underlying graph, while also modeling flexible, nonlinear relationships between the observed nodes.
- It is based on formulating the observation model as the steady state of a discrete dynamical system (Hyttinen et al., 2012)
- Across the benchmarks, NODAGS-Flow beats state-of-the-art algorithms on nonlinear problems, even in the case when the underlying graph is acyclic, highlighting the practical benefits of NODAGS-Flow.

## RELATED WORK
- The primary goal of causal discovery is to recover the underlying causal graph and the associated conditional probability distributions
- Previous approaches to causal discovery focus on either acyclic or cyclic graphs
- Our approach focuses on recovering the underlying causal graph and the associated conditional probability distributions from observational and potentially interventional data

### Acyclic causal discovery
- Most algorithms for causal discovery deal with acyclic graphs
- Constraint-based methods such as the PC algorithm (Spirtes et al., 2000;Triantafillou and Tsamardinos, 2015;Heinze-Deml et al., 2018) aim to recover the underlying graph through constraints given by conditional independence relations encoded by causal graphs
- Score-based methods such as GES (Meek, 1997;Hauser and Bühlmann, 2012) learn the graph structure by optimizing a score function over candidate models
- Hybrid methods combine both previous approaches (Tsamardinos et al., 2006;Solus et al., 2017;Wang et al., 2017)
- NODAGS-Flow is a score-based method and closest in spirit to the NOTEARS family of algorithms because it starts from a simple score function and is entirely based on continuous optimization
- In the presence of interventional data, we show in Sections 4.3 and 5.1 how it can beat the performance of NOTEARS and similar algorithms

### Cyclic causal discovery

### Contributions
- Flow endows the graph with semantics similar to those in Mooij and Heskes (2013) and Hyttinen et al. (2012), modeling the data as generated from the steady state of a dynamical system with an explicit noise model.
- However, instead of linear functional relationships, we allow for a rich class of nonlinear structural functions.
- Contrary to methods like NOTEARS (Zheng et al., 2018) and its nonlinear extensions which necessitate solving a series of optimization problems to deal with the acyclicity constraint, NODAGS-Flow consists of only a single optimization, thus significantly simplifying algorithm design.
- In particular, our model naturally extends the classical notion of a Structural Equation Model (SEM) (Bollen, 1989;Pearl, 2009) and subsumes DAG estimation in these models as a special case.

## PROBLEM SETUP

### Cyclic Causal Models via Structural Equations
- Let G = (V, E) represent a causal graph, where V, E denote the set of vertices and edges, respectively.
- Each vertex v i ∈ V has an associated random variable x i corresponding to its observation and x = (x 1 , . . . , x d ) denotes the complete vector of observations.
- Following the framework proposed by Bollen (1989) and Pearl (2009), we use a Structural Equation Model (SEM), also known as Structural Causal Model (SCM), to represent the system. That is, where pa(i) ⊆ {1, . . . , d} \ {i} is the parent set of x i , f i encodes the functional dependence of x i on its parents, also referred to as the causal mechanism of x i .
- The parent-child relationships defined by the SEM encode the edges in G, i.e., the edge x j → x i exists if and only if j ∈ pa(i).
- The variables (ε 1 , . . . , ε d ) are known as the disturbance variables.
- By combining equation ( 1) over i = 1, . . . , d and writing f = (f 1 , . . . , f d ), we have the following vector-ized form:
- Additionally, the SEM also specifies a probability density p E (ε) over the disturbance variables. We assume that the system is free of confounders, that is, the disturbance variables are independent of each other.
- Finally, we define x as the solution to the system (1) for a random draw of ε.

### Modeling Interventions
- The assumption that the intervened-upon nodes are fixed and the equations for the passively observed variables remain unchanged
- The density function where p E [(id−U k f )(x)] U k denotes subsetting the likelihood to only the variables in U k
- To that end, we employ normalizing flows to model the map x → ε

## NODAGS-FLOW: RESIDUAL FLOW FOR CAUSAL LEARNING
- The NODAGS-Flow framework includes contractive residual flows to calculate the log-det term.
- Neural network architectures are used to model f .
- Diagonal preconditioning is used to enable DAG learning.

### Contractive Residual Flows for Causal Learning
- The residual flows are a class of invertible functions of the form
- The name comes from the resemblance to the structure of residual networks (He et al., 2016)
- We note that solving (2) for ε, the relationship governing our causal model is ε = x − f (x), which is of the same form as (6) with g(z) = −f (z).
- To ensure that our model is well-defined, we need the invertibility of this transformation, along with invertibility for every possible intervention in (4).
- The Contractivity of f is one constraint that guarantees this invertibility.
- In the following, we outline the machinery introduced by Behrmann et al. (2019); Chen et al. (2019) to exploit this constraint for tractable generative modeling, which we adapt for structure learning.
- A function f : R d → R d is said to be contractive if there exists a constant L < 1 such that for any two points It then follows from the Banach fixed point theorem (Rudin, 1953) that if f is contractive, then the residual transformation id − U k f is invertible for any masking matrix U k .
- Although Banach's fixed point theorem guarantees invertibility, we have no analytical form for the inverse. However, the inverse can be obtained via fixed-point iterations. That is, starting with an arbitrary x 0 , repeatedly compute
- The Banach fixed point theorem guarantees that this procedure converges. Moreover, the rate of convergence is exponential in the number of iterations k and bounded by O(L k ).
- This fixed-point iteration also provides an explicit interpretation of the causal semantics of the system in terms of a discrete dynamical system with fixed disturbances.
- To efficiently approximate contractive functions and evaluate (5), two technical challenges remain: enforcing contractivity and evaluating the log-determinant of the Jacobian.
- To address the first challenge, we employ neural networks to approximate f and note that a fixed Lipschitz constant can be enforced on a neural network layer by rescaling its weights by its spectral norm as shown by Behrmann et al. (2019) and Miyato et al. (2018).
- The composition of multiple such Lipschitz layers is still a Lipschitz function.
- To address the second challenge, we employ the unbiased estimator of the log-det term introduced in Chen et al. ( 2019).
- Since f is contractive, by extending the power series expansion of log(1 + x) to matrices, we have
- where I denotes the identity matrix.
- The contractivity of f guarantees the convergence of the above series.
- The trace of J k f (x) can be efficiently computed using the Hutchinson trace estimator (Hutchinson, 1989):
- where w is a random vector with zero mean and unit covariance.
- Behrmann et al. (2019) evaluate the above power series by truncating it to a finite number of terms. However, this approach has the drawback of being biased.
- Chen et al. (2019) improve on this by adding additional randomization to this evaluation, truncating the power series at a random cut-off n ∼ p(N ), where p is a probability distribution over natural numbers N, and re-weighting the terms in the power series to obtain an unbiased estimator.
- Hence the final estimator we use in NODAGS-Flow is now given by,
- (9)

### Parametrization and Sparsity Penalization
- A first naïve implementation of the causal mechanism f through a Multi-Layer Perceptron (MLP) did not produce promising results due to the presence of self-cycles (dependencies from a node v to itself).
- To address this, and to simultaneously add sparsity penalization on the dependency structure of f , we add a dependency mask M ∈ {0, 1} d×d with zero-diagonal that we apply via masking entries of x.
- That is, we introduce an MLP g θ and set where denotes the Hadamard product.
- Similar to Brouillard et al. ( 2020) and Lopez et al. (2022), to enable efficient learning of M during training, we model its entries as draws from a Gumbel-Softmax distribution M ∼ M θ (Jang et al., 2016) with straight-through gradient estimation.
- Sparsity penalization is then achieved by adding λ E M ∼M θ [ M 1 ] to the loss function for a regularization parameter λ > 0, where the expectation can be calculated explicitly from the parameters of M θ .
- The Gumbel-Softmax parametrization M θ also offers access to an estimator for the underlying graph.

### Extension to Non-Contractive DAGs via Preconditioning
- The invertibility of id − f is sufficient, but not necessary, for contractivity
- For the case of DAGs, the causal mechanism f need not be contractive for id − f to be invertible, as the fixed point iterations will always converge after d steps
- However, f being contractive is still convenient to efficiently estimate the absolute Jacobian-determinant
- Via a diagonal rescaling (or preconditioning) of the model parameters, we can significantly increase the space of models that can be represented by contractive functions
- In particular, this includes all models whose underlying graph is a DAG

### Score Function for Differentiable Causal Learning
- Given a set of experiments {E k } and corresponding observations {O},
- We would like to learn the graph structure and the underlying functions governing the parent-child relations.
- To that end, we use the loglikelihood of the not-intervened-on nodes as a score function.
- That is, approximating the log-det term by (9), we consider where x(i,k) denotes the i-th sample in the k-th experiment, and p E,θ is parametrized as independent Gaussian distributions with learnable means and standard deviations.
- Together with 1 penalization introduced in Section 4.2 with parameter λ > 0 and the preconditioning in Section 4.3, inference is performed by solving the following optimization problem with stochastic optimization methods.

## EXPERIMENTS
- NODAGS-Flow is compared with some of the existing state-of-the-art causal discovery algorithms
- Only DCDI and LLC are capable of handling interventional data out of the box
- The other two algorithms were modified to allow for learning from interventions

### Experiments on synthetic data
- The synthetic datasets were generated from graphs with d = 20 nodes
- For each intervention, 5000 observations were sampled
- The observational data consists of 20,000 samples sampled from the graph
- For the function f , we considered three different cases, namely (1) a linear function, f (x) = W x, (2) a nonlinear function, f = ReLU(W x), a single-layer MLP with ReLU (rectified linear unit) activation, ensuring contractivity by rescaling by the operator norm, (3) a non-contractive nonlinear function, f = SELU(W x), a single-layer MLP with SELU (Scaled Exponential Linear Unit) activation, with the underlying graph being a DAG
- In total we obtain 6 different settings for the synthetic datasets
- On linear interventional data (Figure 1, int-dag-lin and int-cyc-lin), NODAGS-Flow attains comparable performance to that of LLC (which was specifically designed for the interventional linear case), both in terms of graph structure recovery and the prediction of unseen interventions
- On non-linear interventional data (Figure 1), NODAGS-Flow performs the best when the graph contains cycles (int-cyc-nonlin) followed by LLC
- When the causal graph is a DAG and the causal mechanism non-contractive (Figure 1, int-dag-nonlin), the added learnable parameter Λ allows NODAGS-Flow to learn a non-contractive function by rescaling a contractive function f by Λ
- In this case, NODAGS-Flow and DCDI are the best performing models

### Experiment on Real-World Transcriptomics Data
- Perturb-seq dataset contains gene expression data from 218,331 melanoma cells
- Dixit et al. 2016 has made it possible to perform genetic interventions at large scales
- Perturb-CITE-seq dataset investigated drivers of resistance to Immune Checkpoint Inhibitors (ICI)
- Each measurement contains the identity of the target genes and the expression profiles of each gene in the genome
- From Figure 4 we can see that NODAGS-Flow outperforms all the baselines with respect to both metrics
- Of all the baselines LLC seemed to attain the worst performance, we, therefore, discuss the performance comparison with LLC in more detail in the appendix
- This shows that learning cycles in the graph allow for better learning of the underlying distribution and thereby improve the predictive power of the model.

## DISCUSSION
- Proposed NODAGS-Flow is a novel causal discovery approach that is capable of learning nonlinear and cyclic relations between variables
- Experiments on synthetic interventional data showed matching performance with state-of-the-art methods (LLC) on linear data and superior performance when recovering nonlinear relationships, both in the case of cyclic and acyclic causal graphical models
- NODAGS-Flow was able to achieve better predictive performance on unseen interventions through an interpretable, mechanistic model that allows for feedback loops
- We hope that applications on more biological datasets could enhance understanding of transcriptomic regulation and aid in the design of novel perturbations
