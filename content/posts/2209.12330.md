---
title: "Personalizing Text-to-Image Generation via Aesthetic Gradients"
date: 2022-09-25T22:03:39.000Z
author: "Victor Gallego"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-12330v1.webp" # image path/url
    alt: "Personalizing Text-to-Image Generation via Aesthetic Gradients" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.12330)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.12330).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/personalizing-text-to-image-generation-via).

# Abstract
- Aesthetic gradients is a method to personalize a CLIP-conditioned diffusion model by guiding the generative process towards custom aesthetics defined by the user from a set of images.
- The approach is validated with qualitative and quantitative experiments, using the recent stable diffusion model and several aesthetically-filtered datasets.

# Paper Content

## arXiv:2209.12330v1 [cs.CV] 25 Sep 2022
- The user can choose a textual prompt y to guide the generation.
- The textual embedding c = CLIP θ,txt (y) is passed through the CLIP text encoder [7] to obtain a textual embedding c = CLIP θ,txt (y).
- Conditioned on this representation c, the diffusion process generates a image that matches the prompt.
- We propose to modify the previous representation c by taking into account another embedding, representing the aesthetic preferences of the user.
- Let {x i } K i=1 be a set of K images representing the aesthetic preference of an user.
- We define its corresponding aesthetic embedding e as the average of the visual embeddings of the previous images, that is, e = 1 K K i=1 CLIP θ,vis (x i ).
- Then we normalize the resulting vector to be of unitary norm.
- The similarity between the two embeddings, computed as the dot product ce , can be used to measure the agreement between CLIP representation of the textual prompt and the preferences of the user.
- Thus, the previous expression can be used as a loss and we can perform gradient descent with respect to CLIP text encoder weights to drive the prompt representation towards the aesthetics of the user: θ = θ + ∇ θ CLIP θ,txt (y)e , with being a user-defined step size.
- After a few iterations, we compute the new, personalized prompt representation, c = CLIP θ ,txt (y), and the generation continues using the underlying diffusion process.
- The resulting representation c is more aligned to the user preference, while preserving the original semantics, as we will see in the experiments from the next section.
- Note that only the weights of the CLIP text encoder are modified, nor the visual encoder nor any other component of the diffusion model.
- The benefits of our approach to personalization are several: (i) it works agnostically of the diffusion model, that is, it only requires a diffusion model which conditions on a textual prompt processed by CLIP.
- In the experiments we use the recent stable diffusion (SD) model.
- It is computationally cheap, as it only requires a few gradient steps (less than 20 in the experiments) of the CLIP text encoder.
- The user only needs to store one aesthetic embedding per set of images, thus saving storage space and being amenable to sharing.
- In the case of CLIP-L/14, the variant used by SD, e is a vector of 768 dimensions.

## Conclusion
