---
title: "Personalizing Text-to-Image Generation via Aesthetic Gradients"
date: 2022-09-25T22:03:39.000Z
author: "Victor Gallego"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2209-12330v1.webp" # image path/url
    alt: "Personalizing Text-to-Image Generation via Aesthetic Gradients" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2209.12330)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2209.12330).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/personalizing-text-to-image-generation-via).

# Abstract
- Proposes a method to personalize a CLIP-conditioned diffusion model
- Guides the generative process towards custom aesthetics
- Validated with qualitative and quantitative experiments
- Uses recent stable diffusion model and aesthetically-filtered datasets
- Code released on GitHub

# Paper Content

## Arxiv:2209.12330v1 [cs.cv] 25 sep 2022
- Aim to provide user personalization to diffusion models
- Focus on learning custom objects from few images
- Alternative approach for personalization of text-to-image diffusion models
- Goal is to guide generative process towards custom aesthetics defined by user
- User chooses textual prompt to guide generation
- Represent aesthetic preferences of user with average of visual embeddings of images
- Measure agreement between CLIP representation of prompt and user preferences
- Perform gradient descent with respect to CLIP text encoder weights
- Only modify weights of CLIP text encoder
- Benefits include agnostic to diffusion model, computationally cheap, and user only needs to store one aesthetic embedding per set of images

## Conclusion
