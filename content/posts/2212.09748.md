---
title: "Scalable Diffusion Models with Transformers"
date: 2022-12-19T18:59:58.000Z
author: "William Peebles, Saining Xie"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09748v1.webp" # image path/url
    alt: "Scalable Diffusion Models with Transformers" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09748)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09748).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers).

# Abstract
- We explore a new class of diffusion models based on the transformer architecture
- We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches
- We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops
- We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID

# Paper Content

## Introduction
- Machine learning is experiencing a renaissance powered by transformers
- Many classes of image-level generative models remain holdouts to the trend, though-while transformers see widespread use in autoregressive models [3,6,40,44], they have seen less adoption in other generative modeling frameworks
- For example, diffusion models have been at the forefront of recent advances in image-level generative models [9,43]; yet, they all adopt a convolutional U-Net architecture as the de-facto choice of backbone
- The seminal work of Ho et al. [19] first introduced the U-Net backbone for diffusion models. The design choice was inherited from PixelCNN++ [49,55], an autoregressive generative model, with a few architectural changes
- In contrast to the standard U-Net [46], additional spatial self-attention blocks, which are essential components in transformers, are interspersed at lower resolutions
- Dhariwal and Nichol [9] ablated several architecture choices for the U-Net, such as the use of adaptive normalization layers [37] to inject conditional information and channel counts for convolutional layers
- However, the highlevel design of the U-Net from Ho et al. has largely remained intact
- With this work, we aim to demystify the significance of architectural choices in diffusion models and offer empirical baselines for future generative modeling research
- We show that the U-Net inductive bias is not crucial to the performance of diffusion models, and they can be readily replaced with standard designs such as transformers
- As a result, diffusion models are well-poised to benefit from the recent trend of architecture unification-e.g., by inheriting best practices and training recipes from other domains, as well as retaining favorable properties like scalability, robustness and efficiency

## Related Work
- Transformers have replaced domainspecific architectures across language, vision, reinforcement learning, and meta-learning
- They have shown remarkable scaling properties under increasing model size, training compute and data in the language domain
- Beyond language, transformers have been trained to autoregressively predict pixels
- They have also been trained on discrete codebooks as both autoregressive models and masked generative models
- The former has shown excellent scaling behavior up to 20B parameters
- Finally, transformers have been explored in DDPMs to synthesize non-spatial data; e.g., to generate CLIP image embeddings in DALL•E 2

## Diffusion Transformers

### Preliminaries
- Gaussian diffusion models assume a forward noising process which gradually applies noise to real data
- By applying the reparameterization trick, we can sample x t = √ ᾱt x 0 + √ 1 − ᾱt t , where t ∼ N (0, I)
- Diffusion models are trained to learn the reverse process that inverts forward process corruptions: p θ (x t−1 |x t ) = N (µ θ (x t ), Σ θ (x t )), where neural networks are used to predict the statistics of p θ
- The reverse process model is trained with the variational lower bound [27] of the loglikelihood of x 0 , which reduces to L(θ) = −p(x 0 |x 1 ) + , excluding an additional term irrelevant for training.
- Since both q * and p θ are Gaussian, D KL can be evaluated with the mean and covariance of the two distributions.
- By reparameterizing µ θ as a noise prediction network θ , the model can be trained using simple mean-squared error between the predicted noise θ (x t ) and the ground truth sampled Gaussian noise t : But, in order to train diffusion models with a learned reverse process covariance Σ θ , the full D KL term needs to be optimized.
- Classifier-free guidance is widely-known to yield significantly improved samples over generic sampling techniques [21,32,43], and the trend holds for our DiT models.
- Training diffusion models directly in high-resolution pixel space can be computationally prohibitive.
- Latent diffusion models (LDMs) [45] tackle this issue with a two-stage approach: (1) learn an autoencoder that compresses images into smaller spatial representations with a learned encoder E; (2) train a diffusion model of representations z = E(x) instead of a diffusion model of images x (E is frozen).
- New images can then be generated by sampling a representation z from the diffusion model and subsequently decoding it to an image with the learned decoder x = D(z).
- As shown in Figure 2, LDMs achieve good performance while using a fraction of the Gflops of pixel space diffusion models like ADM.
- Since we are concerned with compute efficiency, this makes them an appealing starting point for architecture exploration.

### Diffusion Transformer Design Space
- Noise prediction. We use a standard ViT decoder that predicts the mean of the input image. -Diagonal covariance prediction. We use a standard ViT decoder that predicts the covariance between the input image and the diagonal entries in the input noise matrix.

## Experimental Setup
- We explore the DiT design space and study the scaling properties of our model class.
- Our models are named according to their configs and latent patch sizes p; for example, DiT-XL/2 refers to the XLarge config and p = 2.
- We train class-conditional latent DiT models at 256 × 256 and 512 × 512 image resolution on the Ima-geNet dataset [28], a highly-competitive generative modeling benchmark.
- We initialize the final linear layer with zeros and otherwise use standard weight initialization techniques from ViT.
- We use a constant learning rate of 1 × 10 −4 , no weight decay and a batch size of 256.
- The only data augmentation we use is horizontal flips.
- Unlike much prior work with ViTs [54,58], we did not find learning rate warmup nor regularization necessary to train DiTs to high performance.
- Even without these techniques, training was highly stable across all model configs and we did not observe any loss spikes commonly seen when training transformers.
- Following common practice in the generative modeling literature, we maintain an exponential moving average (EMA) of DiT weights over training with a decay of 0.9999.
- All results reported use the EMA model.
- We use identical training hyperparameters across all DiT model sizes and patch sizes.
- Our training hyperparameters are almost entirely retained from ADM.
- We did not tune learning rates, decay/warm-up schedules, Adam β 1 /β 2 or weight decays.
- We use an off-the-shelf pre-trained variational autoencoder (VAE) model [27] from Stable Diffusion [45].
- The VAE encoder has a downsample factor of 8-given an RGB image x with shape 256 × 256 × 3, z = E(x) has shape 32 × 32 × 4.
- Across all experiments in this section, our diffusion models operate in this Z-space.
- After sampling a new latent from our diffusion model, we decode it to pixels using the VAE decoder x = D(z).
- We retain diffusion hyperparameters from ADM [9]; specifically, we use a t max = 1000 linear variance schedule ranging from 1×10 −4 to 2 × 10 −2 , ADM's parameterization of the covariance Σ θ and their method for embedding input timesteps and labels.
- We measure scaling performance with Fréchet Inception Distance (FID) [18], the standard metric for evaluating generative models of images.
- We follow convention when comparing against prior works and report FID-50K using 250 DDPM sampling steps.
- FID is known to be sensitive to small implementation details [34]; to ensure accurate comparisons, all values reported in this paper are obtained by exporting samples and using ADM's TensorFlow evaluation suite [9].
- FID numbers reported in this section do not use classifier-free guidance except where otherwise stated.
- We additionally report Inception Score [48], sFID [31] and Precision/Recall [29] as secondary metrics.

## Experiments
- Four of our highest Gflop DiT-XL/2 models are trained using different block designs in-context (119.4 Gflops), cross-attention (137.6 Gflops), adaptive layer norm (adaLN, 118.6 Gflops) or adaLN-zero (118.6 Gflops).
- At 400K training iterations, the FID achieved with the adaLN-Zero model is nearly half that of the in-context model, demonstrating that the conditioning mechanism critically affects model quality.
- Initialization is also important-adaLN-Zero, which initializes each DiT block as the identity function, significantly outperforms vanilla adaLN.
- For the rest of the paper, all models will use adaLN-Zero DiT blocks.
- Scaling model size and patch size yields considerable improved diffusion models.
- Figure 6 (top) demonstrates how FID changes as model size is increased and patch size is held constant.
- Across all four configs, significant improvements in FID are obtained over all stages of training by making the transformer deeper and wider.
- Similarly, Figure 6 (bottom) shows FID as patch size is decreased and model size is held constant.
- We again observe considerable FID improvements throughout training by simply scaling the number of tokens processed by DiT, holding parameters approximately fixed.
- DiT Gflops are critical to improving performance. The results of Figure 6 suggest that parameter counts are ultimately not important in determining the quality of a DiT model.
- As model size is held constant and patch size is decreased, the transformer's total parameters are effectively unchanged, and only Gflops are increased. These results indicate that scaling model Gflops is actually the key to improved performance.
- To investigate this further, we plot the FID-50K at 400K training steps against model Gflops in Figure 8. The results demonstrate that DiT models that have different sizes and tokens ultimately obtain similar FID values when their total Gflops are similar (e.g., DiT-S/2 and DiT-B/4).
- Indeed, we find a strong negative correlation between model Gflops and FID-50K, suggesting that additional model compute is the critical ingredient for improved DiT models.

### State-of-the-Art Diffusion Models
- 256×256 ImageNet: Following our scaling analysis, we continue training our highest Gflop model, DiT-XL/2, for 7M steps.
- We show samples from the model in Figures 1, and we compare against state-of-the-art class-conditional generative models.
- We report results in Table 2.
- When using classifier-free guidance, DiT-XL/2 outperforms all prior diffusion models, decreasing the previous best FID-50K of 3.60 achieved by LDM to 2.27.
- Figure 2 (right) shows that DiT-XL/2 (118.6 Gflops) is compute-efficient relative to latent space U-Net models like LDM-4 (103.6 Gflops) and substantially more efficient than pixel space U-Net models such as ADM (1120 Gflops) or ADM-U (742 Gflops).
- Our method achieves the lowest FID of all prior generative models, including the previous state-of-the-art StyleGAN-XL [50].

### Model Compute vs. Sampling Compute
- diffusion models are unique in that they can use additional compute after training by increasing the number of sampling steps when generating an image
- smaller-model compute DiTs can outperform larger ones by using more sampling compute
- in this section we study if smaller-model compute DiTs can outperform larger ones by using more sampling compute
- we compute FID for all 12 of our DiT models after 400K training steps, using [16,32,64,128,256, 1000] sampling steps per-image
- the main results are in Figure 10
- sampling compute cannot compensate for a lack of model compute

## Conclusion
- We introduce Diffusion Transformers (DiTs), a simple transformer-based backbone for diffusion models that outperforms prior U-Net models and inherits the excellent scaling properties of the transformer model class.
- Given the promising scaling results in this paper, future work should continue to scale DiTs to larger models and token counts.
- DiT could also be explored as a drop-in backbone for textto-image models like DALL•E 2 and Stable Diffusion.
- We include information about all of our DiT models in Table 4, including both 256 × 256 and 512 × 512 models.
- We include Gflop counts, parameters, training details, FIDs and more.
- We also include Gflop counts for DDPM U-Net models from ADM and LDM in Table 6.
- In this section, we ablate three different choices of the VAE decoder; the original one used by LDM and the two fine-tuned decoders used by Stable Diffusion.
- Because the encoders are identical across models, the decoders can be swapped-in without retraining the diffusion model.
- Table 5 shows results; XL/2 outperforms all prior diffusion models when using the LDM decoder.
- We show samples from our two DiT-XL/2 models at 512 × 512 and 256 × 256 resolution trained for 3M and 7M steps, respectively.
- Figures 1 and 11 show selected samples from both models.
- Figures 13 through 32 show uncurated samples from the two models across a range of classifierfree guidance scales and input class labels (generated with 250 DDPM sampling steps and the ft-EMA VAE decoder).
- As with prior work using guidance, we observe that larger scales increase visual fidelity and decrease sample diversity.
- DiT-XL/2 512 × 512 samples, classifier-free guidance scale = 6.0
- Figure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the flops of the diffusion model. Left: FID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model flops increase. Right: Our best model, DiT-XL/2, is compute-efficient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.
- Figure 4. Input specifications for DiT. Given patch size p × p, a spatial representation (the noised latent from the VAE) of shape I × I × C is "patchified" into a sequence of length T = (I/p) 2 with hidden dimension d. A smaller patch size p results in a longer sequence length and thus more Gflops.
- Figure 5. Comparing different conditioning strategies. adaLN-Zero outperforms cross-attention and in-context conditioning at all stages of training.
- Figure 6. Scaling the DiT model improves FID at all stages of training. We show FID-50K over training iterations for 12 of our DiT models. Top row: We compare FID holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the transformer backbone yields better generative models across all model sizes and patch sizes.
- Increasing transformer forward pass Gflops increases sample quality. Best viewed zoomed-in.
- We sample from all 12 of our DiT models after 400K training steps using the same input latent noise and class label.
- Increasing the Gflops in the model-either by increasing transformer depth/width or increasing the number of input tokens-yields significant...
