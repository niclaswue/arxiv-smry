---
title: "Robust Speech Recognition via Large-Scale Weak Supervision"
date: 2022-12-06T18:46:04.000Z
author: "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2212-04356v1_Whd4fMjKd.jpg" # image path/url
    alt: "Robust Speech Recognition via Large-Scale Weak Supervision" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.04356)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.04356).


# Abstract
- The paper studies speech processing models that are trained to predict large amounts of transcripts of audio on the internet
- The models generalize well to standard benchmarks and are often competitive with prior fully supervised results
- However, the models are also able to predict transcripts accurately and robustly without any prior training
- The paper is releasing models and inference code to serve as a foundation for further work on robust speech processing

# Paper Content

## Introduction
- Pre-trained audio encoders learn high-quality representations of speech
- Unsupervised pre-training methods lack an equivalently performant decoder mapping those representations to usable outputs
- Requires fine-tuning for every deployment distribution
- Recent efforts have created larger datasets for speech recognition
- Relaxing the requirement of goldstandard human-validated transcripts allows for sophisticated automated arXiv:2212.04356v1 [eess.AS] 6 Dec 2022 pipelines to scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data
- There is no drawback and even benefits to joint multilingual and multitask training

## Approach

### Data Processing
- Uses web-scale text from the internet for training machine learning systems
- Uses sequence-to-sequence models to learn to map between utterances and their transcribed form
- Uses a minimalist approach to data pre-processing
- Constructs the dataset from audio that is paired with transcripts on the internet
- Diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneficial
- Many transcripts on the internet are not actually humangenerated but the output of existing ASR systems
- Recent research has shown that training on datasets of mixed human and machine-generated data can significantly impair the performance of translation systems
- In order to avoid learning "transcript-ese", we developed many heuristics to detect and remove machine-generated transcripts from the training dataset
- Many existing ASR systems output only a limited subset of written language which removes or normalizes away aspects that are difficult to predict from only audio signals such as complex punctuation (exclamation points, commas, and question marks), formatting whitespace such as paragraphs, or stylistic aspects such as capitalization
- An all-uppercase or all-lowercase transcript is very unlikely to be human generated
- We use fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset
- We break audio files into 30-second segments paired with the subset of the transcript that occurs within that time segment
- We train on all audio, including segments where there is no speech (though with sub-sampled probability) and use these segments as training data for voice activity detection
- For an additional filtering pass, after training an initial model we aggregated information about its error rate on training data sources and performed manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efficiently
- We perform de-duplication at a transcript level between the training dataset and the evaluation datasets we thought were at higher risk of overlap, namely TED-LIUM 3

### Model
- Uses an off-the-shelf architecture
- Uses 16,000 Hz sampling
- Uses a Mel spectrogram representation
- Uses a small stem with two convolution layers and a GELU activation function
- Adds sinusoidal position embeddings
- Uses pre-activation residual blocks
- Uses a final layer normalization

### Multitask Format
- Predicts language being spoken
- Predicts transcription or translation
- Predicts timestamps or not
- Predicts relative to current audio
- Predicts with caption tokens

### Training Details
- We train a suite of models of various sizes
- We train with data parallelism across accelerators
- Models were trained with AdamW and gradient norm clipping
- A batch size of 256 segments was used
- The models are trained for 2 20 updates which is between two and three passes over the dataset
- Due to only training for a few epochs, over-fitting is not a large concern
- We do not use any data augmentation or regularization and instead rely on the diversity contained within such a large dataset to encourage generalization and robustness

## Experiments

### Zero-shot Evaluation
- Whisper is a speech processing system that is able to generalize well across domains, tasks, and languages
- To study this capability, Whisper is reused a wide set of existing speech processing datasets
- Instead of using the standard evaluation protocol for these datasets, which include both a train and test split, Whisper is evaluated in a zero-shot setting

### Evaluation Metrics
- Speech recognition research typically evaluates and compares systems based on the word error rate (WER) metric.
- However, WER, which is based on string edit distance, penalizes all differences between the model's output and the reference transcript including innocuous differences in transcript style.
- As a result, systems that output transcripts that would be judged as correct by humans can still have a large WER due to minor formatting differences.
- While this poses a problem for all transcribers, it is particularly acute for zero-shot models like Whisper, which do not observe any examples of specific datasets transcript formats.
- This is not a novel observation; the development of evaluation metrics that better correlate with human judgement is an active area of research, and while there are some promising methods, none have seen widespread adoption for speech recognition yet.
- We opt to address this problem with extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences.
- Our text normalizer was developed through iterative manual inspection to identify common patterns where naive WER penalized Whisper models for an innocuous difference.
- Appendix C includes full details.
- For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset's reference transcripts seperating contractions from words with whitespace.
- We caution this development procedure comes at a risk of overfitting to the transcription style of Whisper models which we investigate in Section 4.4.

### English Speech Recognition
- Deep Speech 2 reported a speech recognition system matched human-level performance when transcribing the LibriSpeech test-clean split
- However, seven years later the SOTA WER on LibriSpeech test-clean has dropped another 73% from their 5.3% to 1.4% (Zhang et al., 2021), far below their reported human-level error rate of 5.8%.
- Despite this massive and unanticipated further improvement in performance on held-out but in-distribution data, speech recognition models trained on LibriSpeech remain far above human error rates when used in other settings.
- What explains this gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution?
- We suspect a large part of this gap between human and machine behavior is due to conflating different capabilities being measured by human and machine performance on a test set.
- This claim may seem confusing at first; if both humans and machines are taking the same test, how can it be that different skills are being tested?
- The difference arises not in the testing but in how they trained for it. Humans are often asked to perform a task given little to no supervision on the specific data distribution being studied. Thus human performance is a measure of out-of-distribution generalization. But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization.
- While both humans and machines are being evaluated on the same test data, two quite different abilities are being measured due to a difference in train data.
- Whisper models, which are trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, could potentially match human behavior much better than existing systems.
- To study whether this is the case (or whether the difference between machine and human performance is due to yet-to-be-understood factors) we can compare Whisper models with both human performance and standard fine-tuned machine learning models and check which they more closely match.
- Despite matching or outperforming a human on Lib-riSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness.
- The estimated robustness frontier of zero-shot Whisper models, however, includes the 95% confidence interval for this particular human. To quantify this difference, we examine both overall robustness, that is average performance across many distributions/datasets, and effective robustness, introduced by Taori et al. (2020), which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets.
- A model with high effective robustness does better than expected on out-of-distribution datasets as a function of its performance on the reference dataset and approaches the ideal of equal performance on all datasets.
- For our analysis, we use LibriSpeech as the reference dataset due to its central role in modern speech recognition research and the availability of many released models trained on it, which allows for characterizing robustness behaviors.
- We use a suite of 12 other academic speech recognition datasets to study out-of-distribution behaviors. Full details about these datasets can be found in Appendix A.
- Our main findings are summarized in Figure 2 2, the best zero-shot Whisper models roughly match their accuracy and robustness.
- For a detailed breakdown of this large improvement in robustness, Table 2 compares the performance of the best zero-shot Whisper model with a supervised LibriSpeech model that has the closest performance to it on LibriSpeech test-clean.

### Multi-lingual Speech Recognition
- Whisper outperforms prior work on two low-data benchmarks: Multilingual LibriSpeech (MLS) and VoxPopuli
- Whisper is competitive with prior supervised results on Fleurs when the amount of training data is increased
- There is a strong correlation between the amount of training data and downstream zero-shot performance

### Translation
- Measure translation capabilities of Whisper models
- Compare with Maestro, mSLAM, and XLS-R
- Achieve new state of the art
- Re-purpose Fleurs as a translation dataset
- Correlation between amount of translation training data and resulting BLEU score is much lower than for speech recognition

### Language Identification
- The Fleurs dataset has 102 languages, but 20 of them are not in the training data
- Whisper performs worse on the Fleurs dataset than on other datasets
- The best Whisper model achieves 80.3% accuracy on the 82 languages that are in the training data

### Robustness to Additive Noise
- Whisper models are noise robust
- 14 LibriSpeech-trained models are also noise robust
- Additive noise degrades ASR performance for all models
- Whisper is more robust to noise

### Long-form Transcription
- Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once
- We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model
- We evaluated the long-form transcription performance on seven datasets consisting of speech recordings of various lengths and recording conditions, to cover as diverse a data distribution as possible
- All commercial ASR services are queried using their default English transcription settings as of September 1st, 2022, and for the NVIDIA STT model we used their buffered inference implementation in the FrameBatchASR class

### Comparison with Human Performance
- There are different levels of irreducible error in each dataset
- To quantify how close Whisper's performance is to the human performance, we selected 25 recordings from the Kincaid46 dataset and used 5 services to obtain transcripts produced by professional transcribers
- The audio selection covers various recording conditions such as scripted and unscripted broadcast, telephone and VoIP calls, and meetings
- Figure 7 shows the distribution of per-example WERs and aggregate WER across the 25 recordings, where the computer-assisted service has the lowest aggregate WER that is 1.15% point better than Whisper's, and the pure-human performance is only a fraction of a percentage point better than Whisper's

## Analysis and Ablations

### Model Scaling
- Large amount of promise in weakly supervised training approaches
- However, this comes with the cost of using data that is possibly much noisier and lower quality than gold-standard supervision
- A concern with this approach is that although it may look promising to begin with, the performance of models trained on this kind of data may saturate at the inherent quality level of the dataset
- A related concern is that as capacity and compute spent training on the dataset increases, models may learn to exploit the idiosyncrasies of the dataset
- To check whether this is the case, we study the zero-shot generalization of Whisper models as a function of the model size
- Our analysis is summarized in Figure 8 that shows that performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification.

### Dataset Scaling
- The Whisper dataset is one of the largest ever created in supervised speech recognition
- The raw dataset size is not as important as the amount of data that has been labeled
- The dataset size has a significant impact on the performance of the models
- There is a diminishing returns trend when it comes to increasing the dataset size for speech recognition
- Further analysis is needed to determine the "scaling laws" for speech recognition.

### Multitask and Multilingual Transfer
- There is negative transfer between tasks and languages when a model is trained jointly
- For small models, performance on English speech recognition degrades when trained jointly
- However, multilingual and multitask models benefit more from scale and eventually outperform models trained on English data only
- For our largest experiments, joint models also slightly outperform English-only models

### Text Normalization
- Since we developed our text normalization jointly with Whisper, there is a risk that our normalizer is overfitted to fixing Whisper's peculiarities rather than addressing general variation in transcription.
- To check this, we compared the performance of Whisper using our normalizer versus an independently developed one from the FairSpeech project.
- In Figure 10, we visualize the differences.
- On most datasets the two normalizers perform similarly, without significant differences in WER reduction between Whisper and compared open-source models, while on some datasets, namely WSJ, CallHome, and Switchboard, our normalizer reduces the WER of Whisper models' significantly more.
- The differences in reduction can be traced down to different formats used by the ground truth and how the two normalizers are penalizing them.
- For example, in CallHome and Switchboard, our standardizer did not penalize differences in common English contractions such as "you're" versus "you are", and in WSJ, our normalizer standardized the written and spo- For each dataset, the boxplot shows the distribution of relative WER reduction across different models in our eval suite, showing that using our text normalizer generally results in lower WERs than FairSpeech's.

### Strategies for Reliable Long-form Transcription
- Transcribing long-form audio using Whisper relies on accurate prediction of the timestamp tokens to determine the amount to shift the model's 30-second audio context window by
- Inaccurate transcription in one window may negatively impact transcription in the subsequent windows
- We have developed a set of heuristics that help avoid failure cases of long-form transcription
- First, we use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding
- We start with temperature 0, i.e. always selecting the tokens with the highest probability, and increase the temperature by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than −1 or the generated text has a gzip compression rate higher than 2.4
- Providing the transcribed text from the preceding window as previous-text conditioning when the applied temperature is below 0.5 further improves the performance
- We found that the probability of the <|nospeech|> token alone is not sufficient to distinguish a segment with no speech, but combining the no-speech probability threshold of 0.6 and the average log-probability threshold of −1 makes the voice activity detection of Whisper more reliable
- Finally, to avoid a failure mode where the model ignores the first few words in the input, we constrained the initial timestamp token to be between 0.0 and 1.0 second.

## Related Work
- Scaling compute, models, and datasets leads to improved performance
- Deep learning approaches are more effective when using larger datasets
- Joint training between text and speech language tasks leads to increased robustness

## Limitations and Future Work
- Whisper has a decoder-less conditional language model
- Whisper's robustness is partially due to its strong decoder
- It's currently unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both

## Conclusion
- Whisper suggests that scaling weakly supervised pretraining has been underappreciated so far in speech recognition research.
- We achieve our results without the need for the self-supervision and self-training techniques that have been a mainstay of recent large-scale speech recognition work and demonstrate how simply training on a large and diverse supervised dataset and focusing on zero-shot transfer can significantly improve the robustness of a speech recognition system.
- A. Evaluation Datasets.
- A.1. Short-form English-only datasets
- • LibriSpeech (Panayotov et al., 2015): We used the test-clean and test-other splits from the LibriSpeech ASR corpus.
- • TED-LIUM 3 (Hernandez et al., 2018): We used the test split of TED-LIUM Release 3, using the segmented manual transcripts included in the release.
- • Common Voice 5.1 (Ardila et al., 2019): We downloaded the English subset of Common Voice Corpus 5.1 from the official website.
- • Artie bias corpus (Meyer et al., 2020): We used the Artie bias corpus. This is a subset of the Common Voice dataset.
- • CallHome and Switchboard: We used the two corpora from LDC2002S09 and LDC2002T43.
- • WSJ: We used LDC93S6B and LDC94S13B and followed the s5 recipe to preprocess the dataset.
- • CORAAL: We used the 231 interviews from CORAAL (Kendall & Farrington, 2021) and used the preprocessing script from the FairSpeech project.
- • CHiME-6: For CHiME-6 (Watanabe et al., 2020), we downloaded the CHiME-5 dataset and followed the stage 0 of the s5 track1 recipe to create the CHiME-6 dataset which fixes synchronization. We then used the binaural recordings ( * P??.wav) and the corresponding transcripts.
- • AMI-IHM and AMI-SDM1: We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe.
- A.2. Long-form English-only datasets
- • TED-LIUM 3 (Hernandez et al., 2018): We used the 11 full-length TED talks from the test split of TED-LIUM Release 3, slicing the source audio files between the beginning of the first labeled segment and the end of the last labeled segment of each talk, and we used the concatenated text as the label.
- • Meanwhile: This dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID and the corresponding start and end timestamps are available as part of the code release. The labels are collected from the closed-caption data for each video and corrected with manual inspection.
- • Rev16: We use a subset of 16 files from the 30 podcast episodes in Rev.AI's Podcast Transcription Benchmark, after finding that there are multiple cases where a significant portion of the audio and the labels did not match, mostly on the parts introducing the sponsors. We selected 16 episodes that do not have this error, whose "file number"s are:
- • CORAAL: We used the 231 full-length interviews and transcripts from (Kendall & Farrington, 2021).
- • Multilingual LibriSpeech (Pratap et al., 2020b): We used the test splits from each language in the Multilingual LibriSpeech (MLS) corpus.
- • Fleurs (Conneau et al., 2022): We collected audio files and transcripts using the implementation available as Hug-gingFace datasets. To use as a translation dataset, we matched the numerical utterance IDs to find the corresponding transcript in English.
- • VoxPopuli (Wang et al., 2021): We used the get asr data.py script from the official repository to collect the ASR data in 16 languages, including English.
- • Common Voice 9 (Ardila et al., 2019): We downloaded the Common Voice Corpus 9 from the official website.
- • CoVOST 2 (Wang et al., 2020b): We collected the X into English data collected using the official repository.

## B. Compared Models
- Facebook/wav2vec2-large-960h-lv60-self (Xu et al., 2021)
- Facebook/wav2vec2-large-robust-ft-libri-960h (Hsu et al., 2021b)
- Facebook/wav2vec2-base-100h (Baevski et al., 2020)
- Facebook/wav2vec2-base-960h (Baevski et al., 2020)
- Facebook/hubert-large-ls960-ft (Hsu et al., 2021a)
- Facebook/hubert-xlarge-ls960-ft (Hsu et al., 2021a)
- Facebook/s2t-medium-librispeech-asr (Wang et al., 2020a)
- Facebook/s2t-large-librispeech-asr (Wang et al., 2020a)
- microsoft/unispeech-sat-base-100h-libri-ft (Chen et al., 2022a)
- nvidia/stt en conformer ctc large (Kuchaiev et al., 2019)
- nvidia/stt en conformer transducer xlarge (Kuchaiev et al., 2019)
- speechbrain/asr-crdnn-rnnlm-librispeech (Ravanelli et al., 2021)
- speechbrain/asr-transformer-transformerlm-librispeech (Ravanelli et al., 2021)

## C. Text Standardization
- Whisper outputs any UTF-8 string rather than a restricted set of graphemes
- This means that the rules for text standardization need to be more intricate and comprehensive than those defined on e.g. ASCII characters
- We perform the following steps to normalize English texts in different styles into a standardized form, which is a best-effort attempt to penalize only when a word error is caused by actually mistranscribing a word, and not by formatting or punctuation differences

## D. Raw Performance Tables
