---
title: "Robust Speech Recognition via Large-Scale Weak Supervision"
date: 2022-12-06T18:46:04.000Z
author: "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-04356v1.webp" # image path/url
    alt: "Robust Speech Recognition via Large-Scale Weak Supervision" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.04356)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.04356).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1).

# Abstract
- The models generalize well to standard benchmarks
- The models are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning

# Paper Content

## Introduction
- Progress in speech recognition has been energized by the development of unsupervised pre-training techniques exemplified by Wav2Vec 2.0 (Baevski et al., 2020).
- Since these methods learn directly from raw audio without the need for human labels, they can productively use large datasets of unlabeled speech and have been quickly scaled up to 1,000,000 hours of training data (Zhang et al., 2021), far more than the 1,000 or so hours typical of an academic supervised dataset.
- When fine-tuned on standard benchmarks, this approach has improved the state of the art, especially in a low-data setting.
- These pre-trained audio encoders learn high-quality representations of speech, but because they are purely unsupervised they lack an equivalently performant decoder mapping those representations to usable outputs, necessitating a finetuning stage in order to actually perform a task such as speech recognition.
- There is an additional risk with requiring fine-tuning. Machine learning methods are exceedingly adept at finding patterns within a training dataset which boost performance on held-out data from the same dataset. However, some of these patterns are brittle and spurious and don't generalize to other datasets and distributions.
- In a particularly disturbing example, Radford et al. ( 2021) documented a 9.2% increase in object classification accuracy when fine-tuning a computer vision model on the ImageNet dataset (Russakovsky et al., 2015) without observing any improvement in average accuracy when classifying the same objects on seven other natural image datasets.
- A model that achieves "superhuman" performance when trained on a dataset can still make many basic errors when evaluated on another, possibly precisely because it is exploiting those dataset-specific quirks that humans are oblivious to (Geirhos et al., 2020).
- This suggests that while unsupervised pre-training has improved the quality of audio encoders dramatically, the lack of an equivalently high-quality pre-trained decoder, combined with a recommended protocol of dataset-specific finetuning, is a crucial weakness which limits their usefulness and robustness.
- The goal of a speech recognition system should be to work reliably "out of the box" in a broad range of environments without requiring supervised fine-tuning of a decoder for every deployment distribution.
- As demonstrated by Narayanan et al. (2018), Likhomanenko et al. (2020), and Chan et al. (2021) speech recognition systems that are pre-trained in a supervised fashion across many datasets/domains exhibit higher robustness and generalize much more effectively to held-out datasets than models trained on a single source.
- These works achieve this by combining as many existing high-quality speech recognition datasets as possible.
- However, there is still only a moderate amount of this data easily available.
- SpeechStew (Chan et al., 2021) mixes together 7 pre-existing datasets totalling 5,140 hours of supervision. While not insignificant, this is still tiny compared to the previously mentioned 1,000,000 hours of unlabeled speech data utilized in Zhang et al. (2021).
- Recognizing the limiting size of existing high-quality supervised datasets, recent efforts have created larger datasets for speech recognition. By relaxing the requirement of goldstandard human-validated transcripts, Chen et al. (2021) and Galvez et al. (2021) make use of sophisticated automated arXiv:2212.04356v1 [eess.AS] 6 Dec 2022 pipelines to scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data.
- This trade-off between quality and quantity is often the right call.
- Although understudied so far for speech recognition, recent work in computer vision has demonstrated that moving beyond gold-standard crowdsourced datasets such as ImageNet (Russakovsky et al., 2015) to much larger but weakly supervised datasets significantly improves the robustness and generalization of models (Mahajan et al., 2018;K...

## Approach

### Data Processing
- Uses web-scale text from the internet for training machine learning systems
- Uses sequence-to-sequence models to learn to map between utterances and their transcribed form
- Uses a minimalist approach to data pre-processing, relying on the expressiveness of sequence-to-sequence models
- Constructs the dataset from audio that is paired with transcripts on the internet
- Diversity in audio quality can help train a model to be robust, while diversity in transcript quality is not similarly beneficial
- Filters transcripts to improve quality
- De-duplicates transcripts between the training dataset and the evaluation datasets
- Sorts by high error rate and data source size to identify and remove low-quality ones

### Model
- Uses an off-the-shelf architecture to avoid confounding findings with model improvements
- Uses an encoder-decoder Transformer (Vaswani et al., 2017)
- Uses a small stem consisting of two convolution layers with a filter width of 3
- Adds sinusoidal position embeddings to the output of the stem
- Uses pre-activation residual blocks (Child et al., 2019)
- Uses a final layer normalization

### Multitask Format
- Predicts language being spoken
- Predicts transcription or translation
- Predicts timestamps or not
- Predicts caption tokens
- Predicts start time token before each caption's text, and end time token after

### Training Details
- We train a suite of models of various sizes in order to study the scaling properties of Whisper.
- We train with data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing.
- Models were trained with AdamW (Loshchilov & Hutter, 2017) and gradient norm clipping (Pascanu et al., 2013) with a linear learning rate decay to zero after a warmup over the first 2048 updates.
- A batch size of 256 segments was used, and the models are trained for 2 20 updates which is between two and three passes over the dataset.
- Due to only training for a few epochs, over-fitting is not a large concern, and we do not use any data augmentation or regularization and instead rely on the diversity contained within such a large dataset to encourage generalization and robustness.

## Experiments

### Zero-shot Evaluation
- Whisper is a single robust speech processing system
- Whisper is able to generalize well across domains, tasks, and languages
- Whisper is evaluated in a zero-shot setting

### Evaluation Metrics
- Speech recognition research typically evaluates and compares systems based on the word error rate (WER) metric.
- However, WER, which is based on string edit distance, penalizes all differences between the model's output and the reference transcript including innocuous differences in transcript style.
- As a result, systems that output transcripts that would be judged as correct by humans can still have a large WER due to minor formatting differences.
- While this poses a problem for all transcribers, it is particularly acute for zero-shot models like Whisper, which do not observe any examples of specific datasets transcript formats.
- This is not a novel observation; the development of evaluation metrics that better correlate with human judgement is an active area of research, and while there are some promising methods, none have seen widespread adoption for speech recognition yet.
- We opt to address this problem with extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences.
- Our text normalizer was developed through iterative manual inspection to identify common patterns where naive WER penalized Whisper models for an innocuous difference.
- Appendix C includes full details.
- For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset's reference transcripts seperating contractions from words with whitespace.
- We caution this development procedure comes at a risk of overfitting to the transcription style of Whisper models which we investigate in Section 4.4.

### English Speech Recognition
- Deep Speech 2 reported a speech recognition system matched human-level performance when transcribing the LibriSpeech test-clean split.
- As part of their analysis they concluded: "Given this result, we suspect that there is little room for a generic speech system to further improve on clean read speech without further domain adaptation."
- Yet seven years later the SOTA WER on LibriSpeech test-clean has dropped another 73% from their 5.3% to 1.4% (Zhang et al., 2021), far below their reported human-level error rate of 5.8%.
- Despite this massive and unanticipated further improvement in performance on held-out but in-distribution data, speech recognition models trained on LibriSpeech remain far above human error rates when used in other settings.
- What explains this gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution?
- We suspect a large part of this gap between human and machine behavior is due to conflating different capabilities being measured by human and machine performance on a test set.
- This claim may seem confusing at first; if both humans and machines are taking the same test, how can it be that different skills are being tested?
- The difference arises not in the testing but in how they trained for it. Humans are often asked to perform a task given little to no supervision on the specific data distribution being studied. Thus human performance is a measure of out-of-distribution generalization.
- But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization.
- While both humans and machines are being evaluated on the same test data, two quite different abilities are being measured due to a difference in train data.
- Whisper models, which are trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, could potentially match human behavior much better than existing systems.
- To study whether this is the case (or whether the difference between machine and human performance is due to yet-to-be-understood factors) we can compare Whisper models with both human performance and standard fine-tuned machine learning models and check which they more closely match.
- Despite matching or outperforming a human on Lib-riSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness.
- The estimated robustness frontier of zero-shot Whisper models, however, includes the 95% confidence interval for this particular human. To quantify this difference, we examine both overall robustness, that is average performance across many distributions/datasets, and effective robustness, introduced by Taori et al. (2020), which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets.
- A model with high effective robustness does better than expected on out-of-distribution datasets as a function of its performance on the reference dataset and approaches the ideal of equal performance on all datasets.

### Multi-lingual Speech Recognition
- Whisper outperforms prior work on two low-data benchmarks: Multilingual LibriSpeech (MLS) and VoxPopuli
- Whisper is heavily penalized for having no training data for 20 of Fleurs languages, but still outperforms prior work
- Language identification performance is not competitive with prior supervised results

### Translation
- We study the translation capabilities of Whisper models by measuring their performance on the X→en subset of CoVoST2 (Wang et al., 2020b).
- We achieve a new state of the art of 29.
- For an additional analysis on an even wider set of languages, we also re-purpose Fleurs, which is a speech recognition dataset, as a translation dataset.
- Since the same sentences are transcribed for every language we use the English transcripts as reference translations.
- In Figure 4 we visualize the correlation between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs.
- While there is a clear trend of improvement with increasing training data, the squared correlation coefficient is much lower than the 0.83 observed for speech recognition and only 0.24.
- We suspect this is partly caused by the noisier training data due to errors in audio language identification.

### Language Identification
- The Fleurs dataset has 102 languages, but only 20 of them are in the Whisper dataset
- The Whisper dataset is not very good at language identification on the Fleurs dataset
- The best model on the Fleurs dataset achieves 80.3% accuracy

### Robustness to Additive Noise
- We tested the noise robustness of Whisper models and 14 LibriSpeech-trained models by measuring the WER when either white noise or pub noise from the Audio Degradation Toolbox (Mauch & Ewert, 2013) was added to the audio
- The pub noise represents a more natural noisy environment with ambient noise and indistinct chatter typical in a crowded restaurant or a pub
- Among the 14 models, twelve are pre-trained and/or fine-tuned on LibriSpeech, and the other two are NVIDIA STT models trained on a mixture dataset similar to prior work like SpeechStew that includes LibriSpeech
- The level of additive noise corresponding to a given signal-to-noise ratio (SNR) is calculated based on the signal power of individual examples
- Figure 5 shows how the ASR performance degrades as the additive noise becomes more intensive
- There are many models that outperform our zero-shot performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech, but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB

### Long-form Transcription
- Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once
- We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model
- We observed that it is crucial to have beam search and temperature scheduling based on the repetitiveness and the log probability of the model predictions in order to reliably transcribe long audio
- The full procedure is described in Section 4.5
- We compare the performance with open-source models as well as 4 commercial ASR services
- The results show that Whisper performs better than the compared models on most datasets, especially on the Meanwhile dataset which is heavy with uncommon words.

### Comparison with Human Performance
- There are different levels of irreducible error in each dataset
- Whisper's English ASR performance is not perfect but very close to human-level accuracy

## Analysis and Ablations

### Model Scaling
- Large amount of promise in weakly supervised training approaches
- However, this comes with the cost of using data that is possibly much noisier and lower quality than gold-standard supervision
- A concern with this approach is that although it may look promising to begin with, the performance of models trained on this kind of data may saturate at the inherent quality level of the dataset
- Related concern is that as capacity and compute spent training on the dataset increases, models may learn to exploit the idiosyncrasies of the dataset
- To check whether this is the case, we study the zero-shot generalization of Whisper models as a function of the model size. Our analysis is summarized in Figure 8.

### Dataset Scaling
- The Whisper dataset is one of the largest ever created in supervised speech recognition
- To study this, we trained a series of medium-sized models on subsampled versions of the dataset which are 0.5%, 1%, 2%, 4%, and 8% of the full dataset size and compared their performance with the same medium-sized model trained on the whole dataset
- Early stopping based on the validation loss was used to select model checkpoints for each dataset size
- Evaluation was performed on an exponential moving average estimate of the parameters (Polyak & Juditsky, 1992) using a smoothing rate of 0.9999 to help reduce the effect of the learning rate not fully decaying to zero for the models trained on the subsampled datasets due to early stopping
- Performance on English and multilingual speech recognition and X→en translation is reported in Table 6
- All increases in the dataset size result in improved performance on all tasks, although we see significant variability in improvement rates across tasks and sizes
- Performance improves rapidly on English speech recognition from 3,000 to 13,000 hours and then slows down noticeably between 13,000 and 54,000 hours
- Using the full dataset, which corresponds to another 12.5× increase in size results in only a further 1 point drop in WER
- This mirrors the diminishing returns observed with model size scaling for English speech recognition and could similarly be explained by saturation effects when approaching human-level performance
- Improvements in WER follow a power-law trend for multilingual speech recognition till 54,000 hours and then deviate from this trend, improving only a further 7 points when increasing to the full dataset size
- For X→en translation, performance is practically zero when training on 7,000 hours of audio or less, and then follows a roughly log-linear improvement trend till 54,000 hours before also showing diminishing returns when further scaling to the full dataset size

### Multitask and Multilingual Transfer
- There is negative transfer between tasks and languages when training models jointly
- For small models, performance on English speech recognition degrades when trained jointly
- However, multilingual and multitask models benefit more from scale and eventually outperform models trained on English data only
- For our largest experiments, joint models also slightly outperform English-only models

### Text Normalization
- Since we developed our text normalization jointly with Whisper, there is a risk that our normalizer is overfitted to fixing Whisper's peculiarities rather than addressing general variation in transcription.
- To check this, we compared the performance of Whisper using our normalizer versus an independently developed one from the FairSpeech project. In Figure 10, we visualize the differences.
- On most datasets the two normalizers perform similarly, without significant differences in WER reduction between Whisper and compared open-source models, while on some datasets, namely WSJ, CallHome, and Switchboard, our normalizer reduces the WER of Whisper models' significantly more.
- The differences in reduction can be traced down to different formats used by the ground truth and how the two normalizers are penalizing them.
- For example, in CallHome and Switchboard, our standardizer did not penalize differences in common English contractions such as "you're" versus "you are", and in WSJ, our normalizer standardized the written and spo-

### Strategies for Reliable Long-form Transcription
- Transcribing long-form audio using Whisper relies on accurate prediction of the timestamp tokens to determine the amount to shift the model's 30-second audio context window by
- Inaccurate transcription in one window may negatively impact transcription in the subsequent windows
- We have developed a set of heuristics that help avoid failure cases of long-form transcription
- First, we use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding
- We start with temperature 0, i.e. always selecting the tokens with the highest probability, and increase the temperature by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than −1 or the generated text has a gzip compression rate higher than 2.4
- Providing the transcribed text from the preceding window as previous-text conditioning when the applied temperature is below 0.5 further improves the performance
- We found that the probability of the <|nospeech|> token alone is not sufficient to distinguish a segment with no speech, but combining the no-speech probability threshold of 0.6 and the average log-probability threshold of −1 makes the voice activity detection of Whisper more reliable
- Finally, to avoid a failure mode where the model ignores the first few words in the input, we constrained the initial timestamp token to be between 0.0 and 1.0 second.

## Related Work
- Scaling compute, models, and datasets has been documented across speech recognition research
- Deep learning approaches to speech recognition perform better with larger models and datasets
- Multitask learning has been studied for a long time and is a foundational work in NLP
- Multitask learning is more robust when trained on multiple domains

## Limitations and Future Work
- Whisper has a decoder-less conditional language model
- Whisper's robustness is partially due to its strong decoder
- It's currently unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both

## Conclusion
- a model trained on the English Wikipedia data
- a model trained on the Chinese Wikipedia data
- a model trained on the Japanese Wikipedia data
- a model trained on the Spanish Wikipedia data
- a model trained on the German Wikipedia data
- a model trained on the French Wikipedia data
- a model trained on the Italian Wikipedia data
- a model trained on the Portuguese Wikipedia data
- a model trained on the Dutch Wikipedia data
- a model trained on the Swedish Wikipedia data
- a model trained on the Danish Wikipedia data
- a model trained on the Norwegian Wikipedia data
- a model trained on the Finnish Wikipedia data
- a model trained on the Romanian Wikipedia data
- a model trained on the Hungarian Wikipedia data
- a model trained on the Estonian Wikipedia data
- a model trained on the Latvian Wikipedia data
- a model trained on the Bulgarian Wikipedia data
- a model trained on the Croatian Wikipedia data
- a model trained on the Slovenian Wikipedia data
- a model trained on the Serbian Wikipedia data
- a model trained on the Turkish Wikipedia data
- a model trained on the American English Wikipedia data
- a model trained on the British English Wikipedia data
- a model trained on the Australian English Wikipedia data
- a model trained on the New Zealand English Wikipedia data
- LibriSpeech (Panayotov et al., 2015): We used the test-clean and test-other splits from the LibriSpeech ASR corpus.
- TED-LIUM 3 (Hernandez et al., 2018): We used the test split of TED-LIUM Release 3, using the segmented manual transcripts included in the release.
- Common Voice 5.1 (Ardila et al., 2019): We downloaded the English subset of Common Voice Corpus 5.1 from the official website.
- Artie bias corpus (Meyer et al., 2020): We used the Artie bias corpus. This is a subset of the Common Voice dataset.
- CallHome and Switchboard: We used the two corpora from LDC2002S09 and LDC2002T43.
- WSJ: We used LDC93S6B and LDC94S13B and followed the s5 recipe to preprocess the dataset.
- CORAAL: We used the 231 interviews from CORAAL (Kendall & Farrington, 2021) and used the preprocessing script from the FairSpeech project.
- CHiME-6: For CHiME-6 (Watanabe et al., 2020), we downloaded the CHiME-5 dataset and followed the stage 0 of the s5 track1 recipe to create the CHiME-6 dataset which fixes synchronization. We then used the binaural recordings ( * P??.wav) and the corresponding transcripts.
- AMI-IHM and AMI-SDM1: We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe.
