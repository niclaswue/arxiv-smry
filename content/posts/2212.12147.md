---
title: "The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes"
date: 2022-12-23T04:48:04.000Z
author: "Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, Cengiz Pehlevan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12147v1.webp" # image path/url
    alt: "The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12147)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12147).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/the-onset-of-variance-limited-behavior-for).

# Abstract
- For small training set sizes, the generalization error of wide neural networks is well-approximated by the error of an infinite width neural network
- However, after a critical sample size, we empirically find the finite-width network generalization becomes worse than that of the infinite width network.
- In this work, we empirically study the transition from infinite-width behavior to this variance limited regime as a function of sample size and network width. We find that finite-size effects can become relevant for very small dataset sizes on the order of $P^* \sim \sqrt{N}$. We discuss the source of these effects using an argument based on the variance of the NN's final neural tangent kernel.

# Paper Content

## INTRODUCTION
- Deep learning systems are achieving state of the art performance on a variety of tasks
- Exactly how their generalization is controlled by network architecture, training procedure, and task structure is still not fully understood
- One promising direction for deep learning theory in recent years is the infinite-width limit
- Under a certain parameterization, infinitewidth networks yield a kernel method known as the neural tangent kernel (NTK)
- Kernel methods are easier to analyze, allowing for accurate prediction of the generalization performance of wide networks in this regime
- Infinite-width networks can also operate in the meanfield regime if network outputs are rescaled by a small parameter α that enhances feature learning
- While infinite-width networks provide useful limiting cases for deep learning theory, real networks have finite width
- Analysis at finite width is more difficult, since predictions are dependent on the initialization of parameters
- While several works have attempted to analyze feature evolution and kernel statistics at large but finite width (Dyer & Gur-Ari, 2020;Roberts et al., 2021), the implications of finite width on generalization are not entirely clear
- Specifically, it is unknown at what value of the training set size P the effects of finite width become relevant, what impact this critical P has on the learning curve, and how it is affected by feature learning
- To identify the effects of finite width and feature learning on the deviation from infinite width learning curves, we empirically study neural networks trained across a wide range of output scales α, • We demonstrate that the learning curve for the NN is well-captured by the learning curve for kernel regression with the final empirical NTK, eNTK f , as has been observed in other works (Vyas et al., 2022;Geiger et al., 2020b;Atanasov et al., 2021;Wei et al., 2022)
- Using this correspondence between the NN and the final NTK, we provide a cursory account of how fluctuations in the final NTK over random initializations are suppressed at large width N and large feature learning strength
- In a toy model, we reproduce several scaling phenomena, including the P ∼ √ N transition and the improvements due to feature learning through an alignment effect. We validate that these effects qualitatively persist in the realistic setting of wide ResNets Zagoruyko & Komodakis (2017) trained on CIFAR in appendix E. Overall, our results indicate that the onset of finite-width corrections to generalization in neural networks become relevant when the scale of the variance of kernel fluctuations becomes comparable to the bias component of the generalization error in the bias-variance decomposition.

## PROBLEM SETUP AND NOTATION
- We consider a supervised task with a dataset of size P .
- The pairs of data points are drawn from a population distribution.
- Our experiments will focus on training networks to interpolate degree k polynomials on the sphere.
- For this task, the infinite width network learning curves can be found analytically.
- In particular at large P the generalization error scales as 1/P 2 .
- We take a single output feed-forward NN with hidden width N for each layer.
- We let θ denote all trainable parameters of the network.
- Using NTK parameterization (Jacot et al., 2018), the activations for an input x are given by h .
- At initialization we have W ij ∼ N (0, 1). Consequently, the scale of the output at initialization is O(σ L ).
- As a consequence of the positive homogeneity of the network, the scale of the output is given by α = σ L . α controls the feature learning strength of a given NN.
- Large α corresponds to a lazy network while small α yields a rich network with feature movement. More details on how α controls feature learning are given in Appendix C.1 and C.2.
- In what follows, we will denote the infinite width NTK limit of this network by NTK ∞ .
- We will denote its finite width linearization by eNTK 0 (x, x ) := θ ∂ θ f (x)∂ θ f (x )| θ=θ0 .
- Following other authors (Chizat et al., 2019;Adlam & Pennington, 2020a), we will take the output to be f θ (x) := fθ (x) − fθ0 (x).
- Consequently, at initialization the function output is 0.
- We explain this choice further in Appendix A.

## EMPIRICAL RESULTS
- Both eNTK 0 and sufficiently lazy networks perform strictly worse than NTK ∞
- However, the ensembled predictors approach the NTK ∞ test error
- For all networks, the transition to the variance-limited regime begins at a P * that scales sub-linearly with N
- For polynomial regression, we find P * ∼ √ N

## FINITE WIDTH EFFECTS CAUSE THE ONSET OF A VARIANCE LIMITED REGIME
- We probe the source of the discrepancy between finite width NNs and NTK ∞ by ensemble averaging network predictions fD (x) := f * θ0,D (x) θ0 over E = 20 initializations θ 0 .
- In Figures 1b and 1d, we calculate the error of fD (x), each trained on the same dataset. We then plot E g ( fD ). This ensembled error approximates the bias in a bias-variance decomposition (Appendix B).
- Thus, any gap between 1 (a) and 1 (b) is driven by variance of f θ,D over θ.
- We sharpen these observations with phase plots of NN generalization, variance and kernel alignment over P, α, as shown in Figure 2.
- In Figure 2a, generalization for NNs in the rich regime (small α) have lower final E g than lazy networks. As the dataset grows, the fraction of E g due to initialization variance (that is, the fraction removed by ensembling) strictly increases (2 (b)).
- We will show why this effect occurs in section 3.2.
- Figure 2b shows that, at any fixed P , the variance is lower for small α.
- To measure the impact of feature learning on the eNTK f , we plot its alignment with the target function, measured as y Ky |y| 2 TrK for a test set of targets [y] µ and kernel [K] µν = eNTK f (x µ , x ν ). Alignment of the kernel with the target function is known to be related to good generalization (Canatar et al., 2021).
- In Section 4, we revisit these effects in a simple model which relates kernel alignment and variance reduction. Following (Adlam & Pennington, 2020b), we discuss a symmetric decomposition of the variance in Appendix B, showing the contribution from dataset variance and the effects of bagging. We find that most of the variance in our experiments is due to initialization.

## FINAL NTK VARIANCE LEADS TO GENERALIZATION PLATEAU
- All networks have the same generalization error as kernel regression solutions with their final eNTKs.
- At large α, the initial and the final kernel are already close, so this follows from earlier results of Chizat et al. (2019).
- In the rich regime, the properties of the eNTK f have been studied in several prior works. Several have empirically demonstrated that the eNTK f is a good match to the final network predictor for a trained network (Long, 2021;Vyas et al., 2022;Wei et al., 2022) while others have given conditions under which such an effect would   hold true (Atanasov et al., 2021;Bordelon & Pehlevan, 2022).
- We comment on this in appendix C.4.
- Figure 3 how the final network generalization error matches the generalization error of eNTK f . As a consequence, we can use eNTK f to study the observed generalization behavior.
- Next, we relate the variance of the final predictor f * θ0,D to the corresponding infinite width network f ∞ D . The finite size fluctuations of the kernel at initialization have been studied in (Dyer & Gur-Ari, 2020;Hanin & Nica, 2019;Roberts et al., 2021). The variance of the kernel elements has been shown to scale as 1/N .
- We perform the following bias-variance decomposition: Take f θ0,D to be the eNTK 0 predictor, or a sufficiently lazy network trained to interpolation on a dataset D. Then, We demonstrate this equality using a relationship between the infinite-width network and an infinite ensemble of finite-width networks derived in Appendix B. There we also show that the O(1/N ) term is strictly positive for sufficiently large N . Thus, for lazy networks of sufficiently large N , finite width effects lead to strictly worse generalization error.

## FEATURE LEARNING DELAYS VARIANCE LIMITED TRANSITION
- We define the onset of the variance limited regime to take place at the value P * = P 1/2 where over half of the generalization error is due to variance over initializations.
- Equivalently we have E g ( f * )/E g (f * ) = 1/2.
- By using an interpolation method together with bisection, we solve for P 1/2 and plot it in Figure 4.
- Figure 4b shows that P 1/2 scales as √ N for this task.
- In the next section, we shall show that this scaling is governed by the fact that P 1/2 is close to the value where the infinite width network generalization curve E ∞ g is equal to the variance of eNTK f .

## SIGNAL PLUS NOISE CORRELATED FEATURE MODEL
- In Section 3.2 we have shown that in both the rich and lazy regimes, the generalization error of the NN is well approximated by the generalization of a kernel regression solution with eNTK f .
- This finding motivates an analysis of the generalization of kernel machines which depend on network initialization θ 0 .
- Unlike many analyses of random feature models which specialize to two layer networks and focus on high dimensional Gaussian random data (Mei & Montanari, 2022;Adlam & Pennington, 2020a;Gerace et al., 2020;Ba et al., 2022), we propose to analyze regression with the eNTK f for more general feature structures.
- This work builds on the kernel generalization theory for kernels developed with statistical mechanics (Bordelon et al., 2020;Canatar et al., 2021;Simon et al., 2021;Loureiro et al., 2021).
- We will attempt to derive approximate learning curves in terms of the eNTK f 's signal and noise components, which provide some phenomenological explanations .
- Starting with the final NTK K θ0 (x, x ) which depends on the random initial parameters θ 0 , we project its square root K 1/2 θ0 (x, x ) (as defined in equation 32) on a fixed basis {b k (x)} ∞ k=1 orthonormal with respect to p(x).
- This defines a feature map which can be reconstructed from these features .
- The kernel can be reconstructed from these features by performing linear regression with features ψ(x, θ 0 ).
- In general, since the rank of K is finite for a finite size network, the ψ k (x, θ 0 ) have correlation matrix of finite rank N H .
- Since the target function y does not depend on the initialization θ 0 , we decompose it in terms of a fixed set of features ψ M (x) ∈ R M (for example, the first M basis functions {b k } M k=1 ).
- In this random feature model, one can interpret the initialization-dependent fluctuations in K(x, x ; θ 0 ) as generating fluctuations in the features ψ(x, θ 0 ) which induce fluctuations in the learned network predictor f (x, θ 0 ).
- To illustrate the relative improvements to generalization from denoising these three different objects, in Figure 5, we compare averaging the final kernel K, averaging the induced features ψ, and averaging network predictions f directly.
- For all α, all ensembling methods provide improvements over training a single NN. However, we find that averaging the kernel directly and performing regression with this kernel exhibits the largest reduction in generalization error.
- Averaging features performs comparably. However, ensemble averaging network predictors does not perform as well as either of these other two methods.
- The gap between ensembling methods is more significant in the lazy regime (large α) and is negligible in the rich regime (small α).

## TOY MODELS AND APPROXIMATE LEARNING CURVES
- Characterizes the test error associated with a Gaussian covariate model in a high dimensional limit
- This model was also studied by Loureiro et al. (2021) and subsumes the classic two layer random feature models of prior works (Hu & Lu, 2020;Adlam & Pennington, 2020a;Mei & Montanari, 2022)
- The expected generalization error for any distribution of A(θ 0 ) has the form where . Details of the calculation can be found in Appendix D.5
- We also provide experiments showing the predictive accuracy of the theory in Figure 6

## EXPLAINING FEATURE LEARNING BENEFITS AND ERROR PLATEAUS
- The kernels exhibit fluctuations over initialization with variance O(1/N ), either in the lazy or rich regime
- In Figure 6 (a), we show learning curves for networks of different widths in the lazy regime. Small width networks enter the variance limited regime earlier and have higher error.
- Similarly, if we alter the scale of the noise Eg  Σ = σ 2 Σ M in our toy model, the corresponding transition time P 1/2 is smaller and the asymptotic error is higher.
- In Figure 6 (c), we show that our theory also predicts the onset of the variance limited regime at
- We stress that this scaling is a consequence of the structure of the task. Since the target function is an eigenfunction of the kernel, the infinite width error goes as 1/P 2 (Bordelon et al., 2020).
- Since variance scales as 1/N , bias and variance become comparable at P ∼ √ N .
- Often, realistic tasks exhibit power law decays where E N =∞ g = P −β with β < 2 (Spigler et al., 2020;Bahri et al., 2021), where we'd expect a transition around
- In Figure 6 (e)-(f), we plot the theoretical generalization for kernels with enhanced signal eigenvalue for the task eigenfunction y(x) = φ k (x). This enhancement, based on the intuition of kernel alignment, leads to lower bias and lower asymptotic variance. However, this model does not capture the fact that feature learning advantages are small at small P and that the slopes of the learning curves are different at different α.
- Following the observation of Paccolat et al. (2021a) that kernel alignment can occur with scale √ P , we plot the learning curves for signal enhancements that scale as √ P . Though this toy model reproduces the onset of the variance limited regime P 1/2 and the reduction in variance due to feature learning, our current result is not the complete story. A more refined future theory could use the structure of neural architecture to constrain the structure of the A distribution.

## CONCLUSION
- We performed an extensive empirical study for deep ReLU NNs learning a fairly simple polynomial regression problems.
- For sufficiently large dataset size P , all neural networks under-perform the infinite width limit, and we demonstrated that this worse performance is driven by initialization variance.
- We show that the onset of the variance limited regime can occur early in the learning curve with P 1/2 ∼ √ N , but this can be delayed by enhancing feature learning.
- Finally, we studied a simple random-feature model to attempt to explain these effects and qualitatively reproduce the observed behavior, as well as quantitatively reproducing the relevant scaling relationship for P 1/2 . This work takes a step towards understanding scaling laws in regimes where finite-size networks undergo feature learning.
