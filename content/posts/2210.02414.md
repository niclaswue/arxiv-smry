---
title: "GLM-130B: An Open Bilingual Pre-trained Model"
date: 2022-10-05T17:34:44.000Z
author: "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-02414v1.webp" # image path/url
    alt: "GLM-130B: An Open Bilingual Pre-trained Model" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.02414)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.02414).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model).

# Abstract
- Introduces GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.
- Facing numerous unexpected technical and engineering challenges, the model outperforms GPT-3 175B on a wide range of popular English benchmarks.
- The model also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks.
- Leveraging a unique scaling property of GLM-130B, the model reaches INT4 quantization, without quantization aware training and with almost no performance loss.

# Paper Content

## INTRODUCTION
- Large language models (LLMs), particularly those with over 100 billion (100B) parameters, have presented attractive scaling laws (Wei et al., 2022b), where emergent zero-shot and few-shot capabilities suddenly arouse.
- Among them, GPT-3 (Brown et al., 2020) with 175B parameters pioneers the studies of 100B-scale LLMs by strikingly generating better performance with 32 labeled examples than the fully-supervised BERT-Large model on a variety of benchmarks.
- However, both GPT-3 (and other 100B-scale ones)-the model itself-and how it can be trained, have been thus far not available to the public.
- It is of critical value to train a high-quality LLM of such scale with both the model and training process shared with everyone.
- We thus aim to pre-train an open and highly-accurate 100B-scale model with ethical concerns in mind.
- Over the course of our attempt, we come to realize that pre-training a dense LLM at such a scale raises numerous unexpected technical and engineering challenges compared to training 10Bscale models, in terms of pre-training efficiency, stability, and convergence.
- Similar difficulties have also been concurrently observed in training OPT-175B (Zhang et al., 2022) and BLOOM-176B (Scao et al., 2022), further demonstrating the significance of GPT-3 as a pioneer study.

## THE DESIGN CHOICES OF GLM-130B
- GLM-130B's architecture is unique, it is a backbone GLM with bidirectional attention.
- GLM-130B's architecture enables a more effective comprehension of contexts than GPT-style models.
- GLM-130B offers a record-high accuracy of 80.2% on zero-shot LAMBADA.
- We experiment with different options for positional encoding (PE) and FFN improvements, and find that Rotary Positional Encoding (RoPE) is more stable and effective than ALiBi.

## GLM-130B'S PRE-TRAINING SETUP
- GLM-130B pre-training includes self-supervised GLM autoregressive blank infilling and multi-task learning for a small portion of tokens.
- Self-Supervised Blank Infilling (95% tokens). Recall that GLM-130B uses both [MASK] and [gMASK] for this task. Specifically, [MASK] is used to mask consecutive spans in 30% of training tokens for blank infilling. The lengths of spans follow a Poisson distribution (λ = 3) and add up to 15% of the input.
- For the other 70% tokens, the prefix of each sequence is kept as context and [gMASK] is used to mask the rest of it. The masked length is sampled from the Uniform distribution.
- The pre-training data includes 1.2T Pile (Gao et al., 2020) English corpus, 1.0T Chinese Wudao-Corpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and QA) we crawl from the web, which form a balanced composition of English and Chinese contents.
- Multi-Task Instruction Pre-Training (MIP, 5% tokens). T5 (Raffel et al., 2020) and ExT5 (Aribandi et al., 2022) suggest that multi-task learning in pre-training can be more helpful than fine-tuning, we thus propose to include a variety of instruction prompted datasets including language understanding, generation, and information extraction in GLM-130B's pre-training.
- Compared to recent works (Wei et al., 2022a;Sanh et al., 2022) that leverage multi-task prompted fine-tuning to improve zero-shot task transfer, MIP only accounts for 5% tokens and is set in the pretraining stage to prevent spoiling LLMs' other general ability, e.g., unconditional free generation.

## PLATFORM-AWARE PARALLEL STRATEGIES AND MODEL CONFIGURATIONS
- GLM-130B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers with a 60-day access.
- The goal is to pass through as many tokens as possible, as a recent study (Hoffmann et al., 2022) suggests that most existing LLMs are largely under-trained.
- The 3D Parallel Strategy.
- The data parallelism (Valiant, 1990) and tensor model parallelism (Shoeybi et al., 2019) are the de facto practices for training billion-scale models (Wang & Komatsuzaki, 2021;Du et al., 2022).
- To further handle the huge GPU memory requirement and the decrease in overall GPU utilization resulted from applying tensor parallel between nodes-as 40G rather than 80G A100s are used for training GLM-130B, we combine the pipeline model parallelism with the other two strategies to form a 3D parallel strategy.
- The pipeline parallelism divides the model into sequential stages for each parallel group, and to further minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush (Narayanan et al., 2021) implementation from DeepSpeed (Rasley et al., 2020) to train GLM-130B with a relative big global batch size (4,224) to reduce time and GPU memory wasting.
- Through both numerical and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism, reaching 135 TFLOP/s per GPU (40G) (Cf. Appendix C.4 for details).
- GLM-130B Configurations.
- We aim to enable our 100B-scale LLM to run a single DGX-A100 (40G) node in FP16 precision.
- Based on the hidden state dimension of 12,288 we adopt from GPT-3, the resultant model size has to be no more than 130B parameters, thus GLM-130B.
- To maximize GPU utilization, we configure the model based on the platform and its corresponding parallel strategy.
- To avoid insufficient memory utilization in the middle stages due to the additional word embedding at both ends, we balance the pipeline partition by removing one layer from them, making 9×8-2=70 transformer layers in GLM-130B.
- During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens (roughly 200 billion each for Chinese and English) with a fixed sequence length of 2,048 per sample.
- For the [gMASK] training objective, we use a context window of 2,048 tokens.
- For the [MASK] and multi-task objectives, we use a context window of 512 and concatenate four samples together to 2022-10-06 (v1) cater the 2,048-sequence-length.
- We warm-up the batch size from 192 to 4224 over the first 2.5% samples.
- We use AdamW (Loshchilov & Hutter, 2019) as our optimizer with β 1 and β 2 set to 0.9 and 0.95, and a weight decay value of 0.1.
- We warm up the learning rate from 10 −7 to 8 × 10 −5 over the first 0.5% samples, then decay it by a 10× cosine schedule.
- We use a dropout rate of 0.1 and clip gradients using a clipping value of 1.0 (Cf. Table 15 for the full configurations).

## THE TRAINING STABILITY OF GLM-130B
- The training stability is the decisive factor in GLM-130B's quality
- There is a trade-off between efficiency and stability with regard to floatingpoint (FP) formats: low-precision FP formats (e.g., 16-bit precision-FP16) improve computing efficiency but are prone to overflow and underflow errors, resulting in training collapses.
- Similar to OPT-175B and BLOOM-176B (C.f. Figure 11 in Appendix), the training of GLM-130B faces frequent loss spikes resulted from this choice, which tends to become increasingly frequent as the training goes on.
- The precision related spikes are often without clear reasons: some recover on their own; others come with a portent of suddenly soaring gradient norm and eventually a spike or even NaN in loss.
- OPT-175B attempted to fix by manually skipping data and adjusting hyper-parameters; BLOOM-176B did so via the embedding norm technique (Dettmers et al., 2021).
- We spend months to empirically investigate the spikes and realize that a few issues emerge when transformers scale up: First, the transformer main branch's value scale can be extremely large in deeper layers if using Pre-LN. This is addressed in GLM-130B by using DeepNorm based Post-LN (Cf. Section 2.1), which makes the value scale always bounded.
- Second, the attention scores grow so large that they exceed FP16's range, as the model scales up. There are a few options to overcome this issue in LLMs.
- In CogView (Ding et al., 2021), PB-Relax is proposed to remove bias terms and deduct extremum value in attention computation to avoid the problem, which unfortunately does not help avoid disconvergence in GLM-130B.
- In BLOOM-176B, the BF16 format is used instead of FP16, due to its wide range of values on NVIDIA Ampere GPUs (i.e., A100). However, BF16 consumes ∼15% more run-time GPU memory than FP16 in our experiments due to its conversion to FP32 in gradient accumulation, and more importantly it is not supported on other GPU platforms (e.g., NVIDIA Tesla V100), limiting the accessibility of produced LLMs.
- Another option from BLOOM-176B is to apply embedding norm with BF16, but in sacrifice of a significant penalty on model performance (Scao et al., 2022).
- Embedding Layer Gradient Shrink (EGS).
- Our empirical search identifies that the gradient norm can serve as an informative indicator of training collapses. Specifically, we find that a training collapse usually lags behind a "spike" in gradient norm by a few training steps. Such spikes are usually caused by the embedding layer's abnormal gradients, as we observe that its gradient norm is often several magnitude larger that those of other layers in GLM-130B's early stage training (C.f. Figure 4 (a)).
- In addition, it tends to fluctuate dramatically in the early training.
- The problem is handled in vision models (Chen et al., 2021) via freezing the patch projection layer. Unfortunately, we cannot freeze the training of the embedding layer in language models.
- Finally, we find the gradient shrink on the embedding layer can help overcome loss spikes and thus stabilize the GLM-130B training. This is first used in the multi-modal transformer CogView. Specifically, let α be the shrinking factor, the strategy can be easily implemented via word_embedding = 2022-10-06 (v1) word_embedding * α + word_embedding.detach() * (1 − α).

## GLM-130B INFERENCE ON RTX 2080 TI
- GLM-130B is a new model size that is 130B in size
- To run the full GLM-130B model on a single A100 (40G×8) server, rather than the high-end A100 (80G×8) machine required by OPT-175B and BLOOM-176B
- To accelerate GLM-130B inference, we leverage FasterTransformer (Timonin et al., 2022) to implement GLM-130B in C++
- Compared to the PyTorch implementation of BLOOM-176B in Huggingface, GLM-130B's decoding inference is 7-8.4× faster on the same single A100 server

## THE RESULTS
- GLM-130B is evaluated on bilingual LLMs with Chinese
- GLM-130B is also evaluated on Chinese benchmarks
- "zero-shot" seems to have controversial interpretations without a consensus in the community
- We follow one of the influential related surveys (Xian et al., 2018) which says "At test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label"
- Therefore, we derive our criterion to pick GLM-130B's zero-shot (and few-shot) datasets as:
- English: 1) For tasks with fixed labels (e.g., natural language inference): no datasets in such tasks should be evaluated on; 2) For tasks without fixed labels (e.g., (multiple-choice) QA, topic classification): only datasets with an obvious domain transfer from those in MIP should be considered.
- Chinese: All datasets can be evaluated as there exists a zero-shot cross-lingual transfer.

## LANGUAGE MODELING
- LAMBADA is a dataset to test the last word language modeling capability
- The results previously shown in Figure 2 suggest GLM-130B achieves a zero-shot accuracy of 80.2 with its bidirectional attention, setting up a new record on LAMBADA
- Table 3: GLM-130B's average BPB on Pile evaluation (18 sub-datasets)
- Jurassic-1 GPT-3 GLM-130B Avg. BPB 0.650 0.742 0.634 Pile.

## MASSIVE MULTITASK LANGUAGE UNDERSTANDING (MMLU)
- MMLU is a diverse benchmark including 57 multi-choice question answering tasks concerning human knowledge ranging from high-school-level to expert-level.
- The GPT-3 result is adopted from MMLU and BLOOM-176B is tested by using the same prompts as GLM-130B's (Cf. Appendix D.4 and Table 14 for details).
- GLM-130B's few-shot (5-shot) performance on MMLU approaches GPT-3 (43.9) after viewing about 300B tokens in Figure 6. It continues moving up as the training proceeds, achieving an accuracy of 44.8 when the training has to end (i.e., viewing 400B tokens in total). This aligns with the observation (Hoffmann et al., 2022)

## BEYOND THE IMITATION GAME BENCHMARK (BIG-BENCH)
- GLM-130B outperforms GPT-3 175B and even PaLM 540B (4× larger) in zero-shot setting
- GLM-130B's bidirectional context attention and MIP could be responsible for its strong zero-shot performance
- The performance growth of GLM-130B is not as significant as GPT-3's

## CHINESE LANGUAGE UNDERSTANDING EVALUATION (CLUE)
- Evaluates GLM-130B's Chinese zero-shot performance on established Chinese NLP benchmarks
- Compared to the largest existing Chinese monolingual language model-the 260B ERNIE Titan 3.0
- GLM-130B consistently outperforms ERNIE Titan 3.0 across 12 tasks

## RELATED WORK
- Vanilla language modeling refers to decoder-only autoregressive models (e.g., GPT (Radford et al., 2018)), but it also recognizes any forms of self-supervised objectives on texts.
- Recently, transformer-based (Vaswani et al., 2017) language models present a fascinating scaling law: new abilities (Wei et al., 2022b) arouse as models scale up, from 1.5B (Radford et al., 2019), 10B-scale language models (Raffel et al., 2020;Shoeybi et al., 2019;Black et al., 2022), to 100Bscale GPT-3 (Brown et al., 2020).
- Later, despite many emerged 100B-scale LLMs (Lieber et al., 2021;Thoppilan et al., 2022;Rae et al., 2021;Smith et al., 2022;Chowdhery et al., 2022;Wu et al., 2021;Zeng et al., 2021;Wang et al., 2021) in both English and Chinese, they are not available to public or only accessible via limited APIs.
- The closeness of LLMs severely stymies its development.
- GLM-130B's efforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al., 2022), aim to offer high-quality open-sourced LLMs to our community.
- What additionally makes GLM-130B distinct from others is that, GLM-130B focuses on the inclusivity of LLMs to all researchers and developers.
- From our decision of its size, the choice of its architecture, and the expenditure to allow its fast inference on popularized GPUs, GLM-130B believes it is the inclusivity of all that be the key to realize LLMs' promised welfare to people.
- Transferring. Though fine-tuning has been a de facto way for transfer learning, the transfer learning for LLMs has been concentrated on prompting and in-context learning due to their tremendous sizes (Brown et al., 2020;Liu et al., 2021a).
- Following peer LLMs, in this work our evaluation is also under such setting. Nevertheless, some recent attempts has been on parameter-efficient learning on language models (Houlsby et al., 2019)
- In this work, we do not focus on them and will leave their testing on GLM-130B in future study.
- Inference. Most public-accessible LLMs nowadays are providing their services via limited APIs.
- In this work, an important part of our endeavor has been on LLMs' efficient and fast inference.
- Related work may include distillation (Sanh et al., 2019;Jiao et al., 2020;Wang et al., 2020), quantization (Zafrir et al., 2019;Shen et al., 2020;Tao et al., 2022), and pruning (Michel et al., 2019;Fan et al., 2019).
- Very recent work (Dettmers et al., 2022) shows that LLMs such as OPT-175B and BLOOM-176B can be quantized to 8 bit due to special distribution of outlier dimensions.
- In this work, however, we demonstrate that GLM's scaling law for INT4 weight quantization, according to our novel insights for LLM architecture choices. Such quantization, together with our engineering exertion on adapting GLM-130B to FasterTransformer, allows GLM-130B to be efficiently inferenced on as few as 4×RTX 3090 (24G) GPUs or 8×GTX 1080 Ti (11G) GPUs.

## CONCLUSION AND LESSONS
- Introduces GLM-130B, a bilingual pre-trained language model
- Its technical and engineering undertakings generate insight into LLMs' architectures
- Pre-training objectives, training stability and efficiency, and affordable inference are all factors that contribute to the high quality of GLM-130B
- Lessons learned for training 100B-scale LLMs include the bidirectional-attention GLM being a strong architecture alternative

## Lesson (Platform-aware Configuration).
- Configure LLMs based on the cluster and parallel strategy used to squeeze hardware potential.
- Lesson (Improved Post-LN).
- Counter-stereotypically, DeepNorm, a type of Post-LN, is the option to stabilize GLM-130B.

## Lesson (Training Stability Categorization).
- Unexpected training instability that LLMs suffer from arouses systematically and numerically.
- Lesson (Systematical Instability: FP16).
- Though FP16 induces more instability, it enables training and inference on diverse platforms.
- Lesson (Numerical Instability: Embedding Gradient Shrink).
- Shrinking embedding layer's gradient to its 0.1 can solve most numerical instability problems.

## Lesson (GLM's INT4 Quantization Scaling Law).
- GLM has a unique INT4 weight quantization scaling law
- This scaling law is unobserved in GPT-style BLOOM

## Lesson (Future Direction).
- The main focus of creating powerful LLMs can be on more and better data, better architectures and pre-training objectives, and more sufficient training.
- Ethical Evaluation and Improvements. We evaluate GLM-130B on a wide range of ethical evaluation benchmarks, including bias measurement (Nadeem et al., 2021;Nangia et al., 2020), hate speech detection (Mollas et al., 2020), and toxic generation estimation (Gehman et al., 2020).
- Notwithstanding their deficiency (Blodgett et al., 2021;Jacobs & Wallach, 2021), these datasets serve as a meaningful and initial step towards an open quantitative evaluation of LLMs. Our evaluation implies that the algorithm design choices of GLM-130B, especially its bilingual pretraining, can significantly mitigate the biases and toxicity that an LLM may present while keeping its strong language performance compared to other LLMs (Brown et al., 2020;Zhang et al., 2022).

## REPRODUCIBILITY
- Uses a novel algorithm for learning
- Faster training time
- Better generalization performance
- Uses a novel algorithm for learning
- Faster training time
- Better generalization performance

## A A BRIEF HISTORY OF GLM-130B
- Develop a Chinese-specific language model
- Use GPT-3
- Believe that the GLM-130B project is of value
- Pre-train a highly accurate language model

## 2022.4
- Optimize A100 kernel's computing efficiency
- A100 kernels prefer square-shaped inputs, and seq_len=2,048 is optimal for our hidden-state dimension
- GLM (Du et al., 2022) is chosen for the pre-training algorithm due to its high performance in practice
- The project was delayed for many times, but was successful in completing the pre-training algorithm in two months with the help of partners
- The timeline of GLM-130B covers most of the issues encountered and addressed

## B ETHICS: EVALUATION ON BIASES AND TOXICITY
- LLMs have strong abilities in language and beyond
- In GLM-130B, before granting model weight to applicants, in the model license, we demand them to agree that they will not use it for any deeds that may be harmful to society and human beings
- From a technical perspective, we argue that we must also understand LLMs' toxic and biased behaviors and ultimately eliminate them
- This aligns with our commitment to "LLM Inclusivity", as it is necessary to include more people in the open-sourced LLM research to facilitate the process
- Suppose an LLM is good at identifying toxic and biased content. In that case, techniques such as self-diagnoses (Schick et al., 2021) can help to reduce the harmful generation in a self-consistent post-processing procedure.
- Therefore, as an initial step, we evaluate GLM-130B over various related benchmarks to shed light on the challenging topic

## B.2 BIAS MEASUREMENT: STEREOSET
- GLM-130B is a high quality language model
- GLM-130B outperforms GPT-3 Davinci and OPT-175B on all metrics
- CrowS-Pairs bias evaluation found GLM-130B to have a high quality in both language modeling and social fairness

## B.3 HATE SPEECH DETECTION: ETHOS
- The social media corpus may contain hate speeches
- We adopt the ETHOS dataset originally proposed in (Mollas et al., 2020) to detect sexism and racism speech
- GPT-3 Davinci (a public-accessible variant of GPT-3 175B) and OPT 175B are also tested on the benchmark (whose results are reported in (Zhang et al., 2022))
- For binary classification including Zero-shot, One-shot, and Few-shot (binary) (which answers "yes" or "no"), we report binary F1; for multiclass classification (which answers "yes", "no", or "neither"), we report micro F1
- We assess the toxic generation of GLM-130B on the RealTox-icPrompts (Gehman et al., 2020) dataset
- Following its setting (Zhang et al., 2022), we use nucleus sampling (p = 0.9) to generate 25 continuations for each 10K random sampled prompts, limiting the maximum generated length to 128 tokens
- We report the mean toxicity probabilities of 25 continuations evaluated by Perspective API4

## B.4 TOXIC GENEARATION: REALTOXICPROMPTS
- GLM-130B is less toxic than GPT-3 Davinci
- GLM-130B is less prone to generating harmful content

## C TECHNICAL DETAILS
- Post-LN is jointly proposed with the transformer architecture and is placed between the residual blocks.
- Post-LN is later accused of transformers' slow and vulnerable converging.
- Pre-LN (Xiong et al., 2020)
- On the contrary, Pre-LN is located in the residual blocks to reduce exploding gradients and becomes dominant in existing language models, including all recent LLMs.
- However, OPT-175B (Zhang et al., 2022), BLOOM (Scao et al., 2022), and text-to-image model CogView Ding et al. (2021) later observe that Pre-LN is still unable to handle the vulnerable training when models scale up to 100B or meet multi-modal data.
- This is also justified in GLM-130B's preliminary experiments, where Pre-LN consistently crashes during the early training stage.
- Another problem rooted in Pre-LN transformers is that it may harm the model performance after tuning compared to Post-LN.
- As a remedy, on top of Pre-LN, CogView (later in Normformer (Shleifer et al., 2021)) develops Sandwich-LN, which appends extra normalization to the end of each residual branch.
- Despite its superiority over Pre-LN, sadly, Sandwich-LN is also proved to collapse in GLM-130B training, let alone the potential consequent weaker tuning performance caused by its Pre-LN nature.

## C.3 POSITIONAL ENCODING AND FEED-FORWARD NETWORK
- Vanilla transformer adopts absolute (or sinuous) position encoding and is later evolved into relative positional encoding (Dai et al., 2019).
- Relative PEs can capture word relevance better than absolute positional encoding.
- Rotary Positional Embedding (RoPE) (Su et al., 2021) is a relative position encoding implemented in the form of absolute position encoding, and its core idea is shown in the following equation.
- The product of q at position m and k at position n is related to their distance n − m, which reflects the relativity of the position encoding.
- The definition of R in the above equation is To allow its value to decay as the distance increases, we set θ to the value A.
- Two-dimensional absolute position encoding method is proposed in vanilla GLM for modeling intra-and inter-span position information.
- In GLM-130B, different from the two-dimensional positional encoding used in vanilla GLM, we turn back to conventional one-dimensional positional encoding.
- However, we initially thought that we could not directly apply the two-dimensional form to RoPE.
- As a substitute plan, in GLM-130B, we remove the second dimension used in the original GLM as we find that the unidirectional attention mask sub-matrices for [MASK] generation also indicate the token order. This observation results in our transforming GLM-130B's positional encoding into a one-dimensional one according to the following strategies:
- For sequences corrupted by short spans, we discard the second-dimensional position encoding.
- For sequences corrupted by a long span at the end, we change the positional ids to one-dimensional 0, 1, • • • , s − 1, and generated tokens will prolong the first-dimensional positional encoding from the last context token s − 1.
- Feed-forward Network.
- Some recent efforts to improve transformer architecture have been on the FFN, including replacing it with GLU (adopted in PaLM).
- Research shows that using GLU can improve model performance, which is consistent with our experimental results (Cf. Table 8).
- Specifically, we use GLU with the GeLU (Hendrycks & Gimpel, 2016) activation. as In order to keep the same parameter as the vanilla FFN, the feed-forward size d ffn (which is usually 4d H , where d H is the hidden dimension) is reduced to 8 3 d H as the V is additionally introduced.

## C.4 PIPELINE PARALLEL ANALYSIS
- In pipeline parallelism, each stage consists of three operations (Cf. Figure 12(a)): forward (denoted as F), backward (marked as B), and optimizer step (denoted as U).
- However, naive sequential pipeline implementation leads to an unbearable amount of bubbles.
- The improved Gpipe (Huang et al., 2019) (Cf. Figure 12(b)) strategy reduces bubbles drastically via splitting data into microbatches; the more micro-batches there are, the more stages can compute simultaneously in an iteration.
- The recent PipeDream-Flush (Narayanan et al., 2021) (Cf. Figure 12(c)) additionally optimizes the GPU memory usage by interweaving forward and backward from different stages to reduce forward activation's memory occupation.
- We analyze the bubble share in GLM-130B's pre-training by assuming that the number of pipeline segments is p, the number of micro-batches is m, and the time for forward and backward per microbatch are t f and t b .
- In ideal case, forward and backward take t ideal = m(t f +t b ). But in practice, the default pipeline delivery strategy causes p − 1 forward propagation and p − 1 backward propagation bubbles, respectively, for a total time of t bubble = (p − 1)(t f + t b ), so that the bubble occupancy is bubble-ratio =
- For larger numbers of micro-batches, the bubble percentage will be reduced to an acceptable level.
- In particular, experiments in GPipe Huang et al. (2019) show that when m ≥ 4p, the total percentage of pipeline bubble time is reduced to a negligible level due to the forward recomputation technique in backpropagation that allows some overlap in computational communication, thus showing that the bubbles introduced in parallel by the pipeline model do not seriously deplete the training efficiency.
- In general, to make full use of the hardware, it is common to place models into model parallel groups consisting of multiple nodes and try to use the full memory of each node.
- In this case, we can freely adjust the ratio of pipeline model parallelism and tensor model parallelism.
- Since data parallelism hardly affects the computation time, we assume that the scale of data parallelism is d = 1, the total number of nodes is n, the scale of tensor model parallelism is t, and the scale of pipeline model parallelism is p, and satisfies n = t × p, the bubble share in this case is bubble.

## C.5 INFERENCE ACCELERATION
- Model's plain PyTorch implementation is easy to read and run, but it can be intolerably slow for LLMs
- Based on NVIDIA's FasterTransformer7 we spend two months implementing GLM-130B into C++ to speed up inference
- Our implementation for GLM-130B can be 7.0 to 8.4 times faster than BLOOM-176B's Pytorch implementation
- The exertion to accelerate LLM for tolerable response speed could be crucial to its popularization

## C.6 ACTIVATION OUTLIER ANALYSIS
- GLM-130B's weight can be quantized into INT4 to reduce parameter redundancy in the inference drastically
- However, we also find that GLM-130B's activations (i.e., hidden states between layers) cannot be appropriately quantized, as they contain value outliers, as is also suggested in concurrent literature
- What is unique in GLM-130B is that 30% of its dimensions may present value outliers (Cf. Figure 13), while other GPT-based LLMs (e.g., OPT-175B and BLOOM 176B) only have very few outlying dimensions
- Therefore, the solution to decompose matrix multiplication for higher-precision computation in outlying dimensions proposed in (Dettmers et al., 2022) does not apply to GLM-130B
- Figure 14: GLM-130B's activation outliers' absolute value scale
- We study whether these outliers can be ignored in LLM quantization, and the answer is "no". These values can be several orders of magnitude larger than typical activation values
- While most values (accounts for 99.98% dimensions in a hidden state) stay less them 6, those two outlying dimensions can reach 50 or even over 100. They are speculated to be some important clues for GLM-130B and potentially other LLMs to memorize some fixed world or language knowledge. Thus, removing or omitting them in quantization can lead to significant performance degradation

## C.8 QUANTIZATION SETTINGS
- Quantize linear layers only
- Use Absmax quantization
- Inference only uses quantized weights

## C.8.1 QUANTIZATION RESULTS AT SCALES
- GLM models are from a paper published in 2022
- GLM-130B models are similar, but the architecture is different
- Almost all models perform well at INT8 precision
- GLM maintains better performance than BLOOM at INT4 precision

## C.8.2 WEIGHT DISTRIBUTION ANALYSIS
- To achieve INT4 weight quantization, we analyze the weight value distribution of primary linear layers in GLM-130B and a counterpart BLOOM-176B in a histogram.
- The horizontal axis denotes the weight value, and the vertical axis indicates the number of weights of such value on the log scale.
- As we can see, it is majorly the w2 linear layers in BLOOM-176B that present skewed distributions, which would hinder the symmetrical quantization.
- On the contrary, GLM-130B's w2 is well-shaped without many outliers and skewed distribution and thus paces the way for its INT4 quantization with little performance loss.
- Following practices in (Raffel et al., 2020;Wei et al., 2022a;Sanh et al., 2022;Aribandi et al., 2022), we include many prompted instruction datasets in GLM-130B's MIP training, which accounts for 5% of the training tokens.
- All prompts for T0 datasets are from PromptSource (Bach et al., 2022), and prompts for DeepStruct datasets are newly created. Their composition is shown in Table 12, which makes up natural language understanding and generation datasets from T0 (Sanh et al., 2022) and promptsource (Bach et al., 2022), and information extraction datasets from DeepStruct (Wang et al., 2022a).
- In GLM-130B's training, we calculate that approximately 36% of the samples in each dataset have been seen.
- T0 originally splits datasets for 1) multi-task prompted training and 2) zero-shot task transfer in two sections. We initially planned to only include training sets of T0's multi-task prompted training section and DeepStruct (Wang et al., 2022a), but by mistake, we included both multi-task prompted training and zero-shot task transfer sections' datasets in MIP and excluded DeepStruct datasets. We fixed the mistake at around 23k steps, and our model continued to train on the correct version.

## D.2 DATA AND PROMPTS IN MIP FOR DEEPSTRUCT
- The dataset DeepStruct is manually created by the authors
- The introduction, task description, and full prompts for each dataset are attached
- To allow template infilling, the prompts are written into Jinja10 templates
- When a dataset sample is provided in our format, the Jinja engine will render it into a prompted sample with instructions

## D.2.1 DIALOGUE STATE TRACKING
- We adopt Multiwoz 2.1 (Eric et al., 2020) dialogue state tracking dataset.
- The dataset is reformulated into two tasks, each with one prompt correspondingly: • Dialogue state tracking: which asks the model to extract information from dialogues given a list of certain slots, e.g., taxi_arrival_time and destination. • Slot filling: which model should fill in one provided slot and identify situations without an answer.
- In this work, we adopt three classical joint entity and relation extraction datasets: CoNLL04 (Roth & Yih, 2004), NYT (Riedel et al., 2010), and ACE2005 (Walker & Consortium, 2005).
- In GLM-130B, we follow (Wang et al., 2022a) to formulate such challenges into sequence-to-sequence generation, where our inputs are raw texts and outputs are triples.
- We only conduct relation-related tasks for these datasets here and leave the entity-related ones in the named entity recognition section.
- Relation Extraction: here we extract knowledge triples consisting of "head entity", "relation", and "tail entity", given a list of relation candidates.
- For example, given the input "In Kunming the 800-some faculty and student established the National Southwestern Associated University.", the model output could be (National Southwestern Associated University, location of formation, Kunming).
- Recent works (Wei et al., 2022c;Wang et al., 2022c)
- Here we elaborate the prompts we use for CLUE (Xu et al., 2020) and FewCLUE (Xu et al., 2021) evaluation.
- In Chinese datasets, prompting meets some challenges as Chinese texts are organized by single characters rather than words, leading to unequal length of verbalizers in many cases.
- Albeit dataset-specific calibration (Wang et al., 2021;Wu et al., 2021) can help to mitigate the issue, the too-specified technique can be complicated in implementation.
- Our evaluation in this paper adopts a more easy-to-solve method leveraging GLM-130B's unique features.
- As GLM-130B is a bilingual LLM with English MIP, we adopt English prompts and verbalizers from similar tasks in (Bach et al., 2022) for Chinese dataset evaluation and find such strategies to be quite effective.
- In terms of evaluation metrics, except for DRCD and CMRC2018, two question answering datasets report EM, and other datasets report accuracy.
- Table 13: Details results of GLM-130B, GPT-3 175B (Brown et al., 2020), and PaLM 540B (Chowdhery et al., 2022) on BIG-bench-lite in 0, 1, and 3-shots.
- "Normalized preferred metric" is reported for each task.
- GPT-3 and PaLM's results are reported in BIG-bench's GitHub repository, and PaLM 540B's 3-shot results are not found.

## E.1 IMPACT ON AI RESEARCH
- Most research institutions cannot afford the substantial cost of pre-training large language models
- With the inference APIs, researchers can only analyze the outputs of models as black boxes, which limits the scope of potential work
- With GLM-130B, researchers can analyze the model parameters and internal states corresponding to specific inputs, leading to in-depth studies of LLMs' theory, capacity, and flaws
- Researchers can also modify the model architecture and weights to validate the proposed algorithms to improve LLMs

## E.2 IMPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES

## F ENVIRONMENTAL IMPACT
- GPT-3 is estimated to use 500 tons of carbon emissions
- The carbon emission is roughly the equivalent of the yearly emissions of 18 average Americans
- GLM-130B is estimated to use less carbon emissions
