---
title: "NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior"
date: 2022-12-14T18:16:41.000Z
author: "Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, Victor Adrian Prisacariu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07388v1.webp" # image path/url
    alt: "NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07388)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07388).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/nope-nerf-optimising-neural-radiance-field).

# Abstract
- Training a Neural Radiance Field (NeRF) is challenging
- Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes
- However, these methods still face difficulties during dramatic camera movement
- We tackle this challenging problem by incorporating undistorted monocular depth priors
- These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames
- This constraint is achieved using our proposed novel loss functions
- Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy

# Paper Content

## Introduction
- 3D geometry reconstruction and view-dependent appearance modelling are required
- Recently, Neural Radiance Fields (NeRF) have demonstrated the ability to build high-quality results for generating photo-realistic images from novel viewpoints given a sparse set of images
- A current go-to option is the popular Structure-from-Motion (SfM) library COLMAP, which is easy to use but has a long processing time
- Recent works such as NeRFmm, BARF and SC-NeRF propose to simultaneously optimise camera poses and the neural implicit representation to address these issues
- However, these methods can only handle forward-facing scenes when no initial parameters are supplied
- To handle the limitation of large camera motion, we seek help from monocular depth estimation
- Our system is enabled by three contributions. First, we propose a novel way to integrate monodepth into unposed-NeRF training by explicitly modelling scale and shift distortions. Second, we supply relative poses to the camera-NeRF joint optimisation via an inter-frame loss using undistorted mono-depth maps. Third, we further regularise our relative pose estimation with a depth-based surface rendering loss.

## Related Work
- Novel View Synthesis: Early approaches used interpolations between pixels, later works rendered images from 3D reconstruction.
- NeRF: A popular scene representation for its photorealistic rendering.
- Methods to improve NeRF's performance with additional regularisation:
- Depth priors:
- Surface enhancements:
- Latent codes:
- Joint pose and scene optimisation:
- Mono-depth maps:
- SIREN:
- LLFF:
- BARF:
- SC-NeRF:
- GARF:
- SiNeRF:

## Method
- Our method recovers camera poses and optimizes a NeRF simultaneously
- We assume camera intrinsics are available in the image meta block
- We run an off-the-shelf monodepth network DPT to acquire mono-depth estimations
- Without repeating the benefit of mono-depth, we unroll this section around the effective integration of monocular depth into unposed-NeRF training
- The training is a joint optimisation of the NeRF, camera poses, and distortion parameters of each mono-depth map
- We detail our method in the following sections

### NeRF
- Neural Radiance Field (NeRF) is a representation of a scene as a mapping function F Θ : (x, d) → (c, σ)
- This mapping is usually implemented with a neural network parameterized by NeRF
- The NeRF can be optimised by minimising photometric error
- We refer to [24] for further details

### Joint Optimisation of Poses and NeRF
- prior works show that it is possible to estimate both camera parameters and a NeRF at the same time by minimising the above photometric error L rgb under the same volumetric rendering process in Eq. (2)
- the key lies in conditioning camera ray casting on variable camera parameters Π, as the camera ray r is a function of camera pose
- mathematically, this joint optimisation can be formulated as: where Π denotes camera parameters that are updated during optimising

### Undistortion of Monocular Depth
- Generates mono-depth sequence D = {D i | i = 0 . . . N − 1} from input images
- Without surprise, mono-depth maps are not multi-view consistent so we aim to recover a sequence of multi-view consistent depth maps, which are further leveraged in our relative pose loss terms
- Consider two linear transformation parameters for each mono-depth map, resulting in a sequence of transformation parameters for all frames Ψ = {(α i , β i ) | i = 0 . . . N − 1}, where α i and β i denote a scale and a shift factor
- With multi-view consistent constraint from NeRF, we aim to recover a multi-view consistent depth map D * i for D i : by joint optimising α i and β i along with a NeRF
- This joint optimisation is mostly achieved by enforcing the consistency between an undistorted depth map D * i and a NeRF rendered depth map Di via a depth loss

### Relative Pose Constraint
- Aforementioned unposed-NeRF methods [12,18,45] optimise each camera pose independently, resulting in an overfit to target images with incorrect poses.
- Penalising incorrect relative poses between frames can help to regularise the joint optimisation towards smooth convergence, especially in a complex camera trajectory.
- Therefore, we propose two losses that constrain relative poses.
- Point Cloud Loss.
- We back-project the undistorted depth maps D * using the known camera intrinsics, to point clouds where T ji = T j T −1 i represents the related pose that transforms point cloud P * i to P * j , tuple (i, j) denotes indices of a consecutive pair of instances, and l cd denotes Chamfer Distance:
- While the point cloud loss L pc offers supervision in terms of 3D-3D matching, we observe that a surface-based photometric error can alleviate incorrect matching.
- With the photometric consistency assumption, this photometric error penalises the differences in appearance between associated pixels.
- The association is established by projecting the point cloud P * i onto images I i and I j .
- The surface-based photometric loss can then be defined as:

### Overall Training Pipeline
- The loss function is a sum of terms that are weighted according to their importance.
- The weighting factors for the loss terms are determined by the user.
- The method minimizes the combined loss to find the optimised NeRF parameters, camera poses, and distortion parameters.

## Experiments

### Experimental Setup

### Comparing With Pose-Unknown Methods
- Our method outperforms all the baselines by a large margin.
- The quantitative results are summarised in Tab. 1.
- For ScanNet, we use the camera poses provided by the dataset as ground truth.
- For Tanks and Temples, not every video comes with ground truth poses, we use COLMAP estimations for reference.
- Our estimated trajectory is better aligned with the ground truth than other methods, and our estimated rotation is two order-of-magnitudes more accurate than others.
- We visualise the camera trajectories and rotations in Fig. 4.
- Our rendered depth maps achieve superior accuracy over the previous alternatives.
- We also compare with the mono-depth maps estimated by DPT. Our rendered depth maps, after undistortion using multiview consistency in the NeRF optimisation, outperform DPT by a large margin.

### Comparing With COLMAP Assisted NeRF
- Our method achieves on-par accuracy with COLMAP, as shown in Tab. 4
- Our method generates significantly better results than previous methods, which show visible artifacts
- The full rendered videos and details about generating novel views are provided in the supplementary.

### Ablation Study
- The results of ablation studies are shown in Tab. 6.
- Ignoring depth distortions (i.e., setting scales to 1 and shifts to 0 as constants) leads to a degradation in pose accuracy, as inconsistent distortions of depth maps introduce errors to the estimation of relative poses and confuse NeRF for geometry reconstruction.
- The inter-frame losses are the major contributor to improving relative poses.
- When removing the pairwise point cloud loss L pc or the surface-based photometric loss L rgb−s , there is less constraint between frames, and thus the pose accuracy becomes lower.

### Limitations
- Our proposed method optimizes camera pose and the NeRF model jointly
- The optimisation of the model is also affected by non-linear distortions and the accuracy of the mono-depth estimation

## Conclusion
- NoPe-NeRF is an end-to-end differentiable model for joint camera pose estimation and novel view synthesis from a sequence of images
- Previous approaches have difficulty with complex trajectories
- To tackle this challenge, we use mono-depth maps to constrain the relative poses between frames and regularise the geometry of NeRF, which leads to better pose estimation
- We show the effectiveness and robustness of NoPe-NeRF on challenging scenes
- The im-proved pose estimation leads to better novel view synthesis quality and geometry reconstruction compared with other approaches
- We believe our method is an important step towards applying the unknown-pose NeRF models to largescale scenes in the future
