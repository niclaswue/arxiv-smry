---
title: "Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations"
date: 2022-12-17T02:20:14.000Z
author: "Jifan Chen, Yuhao Zhang, Lan Liu, Rui Dong, Xinchi Chen, Patrick Ng, William Yang Wang, Zhiheng Huang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08780v1.webp" # image path/url
    alt: "Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08780)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08780).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/improving-cross-task-generalization-of).

# Abstract
- There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning.
- However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the model's ability to generalize to new domains or tasks that were not seen during training.
- In this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models. We design the task configurations to explicitly specify the task type, as well as its input and output types. We show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner. We demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively.

# Paper Content

## Introduction
- pre-trained transformer models (Raffel et al., 2020;Lewis et al., 2020;Xue et al., 2021),
- unifying multiple NLP tasks with a single encoder-decoder model (Khashabi et al., 2020;Sanh et al., 2022),
- recent popularity of pre-trained transformer models (Raffel et al., 2020;Lewis et al., 2020;Xue et al., 2021),
- lack of detailed information about the task,
- cross-task generalizability is not captured in the training methods or the evaluation setup,
- proposed use of compositional task configurations to improve cross-task generalizability.

## Method & Tasks
- Prompting is a natural and feasible way to impose explicit control over the behaviors of pre-trained language models
- In this work, we implement the task configurations as prompts of an encoder-decoder model
- Each task configuration contains the following four aspects: task type, input type, output type, and dataset name
- The task type is the end goal of a task, e.g., QA and summarization, as shown in Figure 2
- Input and output types specify the inputs of the encoder and the outputs of the decoder of table-to-text models, respectively
- These types can be compositional, for example, both long-form and short-form table QA in Figure 2 require the decoder to output a set of relevant cells and the final answer
- The dataset name specifies the dataset used for training
- When testing the model on a new dataset, we can simply omit the dataset name since it is not trained
- One of the major advantages of having explicit task configurations is that it enables the model to learn the mapping between a configuration and its behavior
- At test time, we can compose a new set of configurations which suits best for an unseen task using the trained configurations

### Datasets and Task Configurations
- WIKISQL
- WIKITQ
- SQUAD
- TOTTO
- TABFACT
- NQ-TABLES
- HYBRIDQA
- TAT-QA
- FETAQA
- FEVEROUS
- Conventionally, when using ToTTo, people feed the cells to the encoder to decode the summary.
- To make the task more compatible with other table-to-text tasks, we feed the relevant cells as inputs to the decoder and the model is not trained on those cells.
- We consider 5 datasets as in-domain datasets for both training and testing: WIKISQL (Zhong et al., 2017), WIKITQ (Pasupat and Liang, 2015), SQUAD (Rajpurkar et al., 2016), TOTTO (Parikh et al., 2020)
- For SQUAD and TOTTO, since no official test set is released, we follow UnifiedSKG (Xie et al., 2022) and report results on the official development sets.
- In addition, we consider 5 datasets for test only: NQ-TABLES (Kwiatkowski et al., 2019;Herzig et al., 2021), HYBRIDQA (Chen et al., 2020), TAT-QA (Zhu et al., 2021), FETAQA (Nan et al., 2022b) and FEVEROUS (Aly et al., 2021).
- The test-only evaluation setup aims to assess the effectiveness of our method in enabling the model to generalize to unseen tasks with new compositional configurations, as well as to a new dataset with existing configurations.
- Specifically, we test if the model can benefit from a combination of input configurations for tasks such as HYBRIDQA, TAT-QA, and FEVROUS, which involve both passages and tables as inputs, despite the model is only trained on one or the other during training.
- Similarly, we examine if the configuration for FETAQA, a combination of WIKISQL and TOTTO as shown in Figure 2, allows for explicit control over the model's behaviors, resulting in improved generalization.
- Finally, we assess the model's ability to generalize to a new dataset, NQ-TABLES, which has the same configurations as WIKISQL and WIKITQ.

## Experiments
- Experimental settings: We evaluate our method by following the experimental setup shown in Table 1.
- Valuation: For the in-domain tasks, we simply train on their training sets and evaluate on their test sets. For the test-only tasks, we evaluate our method in two settings: 1) a zero-shot setting, where we directly apply the model trained on indomain datasets and use a new set of task configs

## Results

### Main Results
- The in-domain and zero-shot evaluation results for all datasets in Table 2 show that using compositional task configs improves performance on zero-shot datasets
- The in-domain and zero-shot evaluation results for all datasets in Table 2 show that using compositional task configs improves performance on zero-shot datasets unseen at training time
- In most cases, using compositional task configs consistently improves the indomain performance over the UnifiedSKG baseline and single-task training
- In few-shot evaluation (as depicted in Figure 4), using task configurations has improved few-shot learning performance for most test-time tasks

### Ablation of Task Configs at Training time
- The impact of individual configurations on model performance was evaluated by removing one configuration at a time during training
- The removal of the output type resulted in the largest performance drop, as the model was only able to guess the desired output type based on learned parameters
- The removal of the input type had the least impact on performance
- This is likely due to the fact that learning the representation of the two input types was not difficult for the model, and explicitly informing the model about the input type does not provide significant benefit, as observed in the previous section
- The removal of the dataset type also results in a performance drop, particularly on the NQ-TABLES dataset, indicating that even when the task type, input, and output are the same, including the dataset type helps the model learn dataset-independent knowledge more effectively
- The removal of the task type results in a complete failure on the FETAQA dataset

### Ablation of Task Configs at Test time
- The method demonstrates much stronger zero-shot task performance
- The performance drop is quite noticeable when the table and passage input configurations are removed together
- The model can generate different outputs by combining the output configurations

### Human Evaluation of Generated Cells
- The proposed task configurations can be applied to table-to-text tasks
- An example of this is for the table-based fact verification task, TABFACT, where the output configuration can include a cell component that can serve as supporting evidence of the binary prediction
- Overall we found that the model is able to generate cells with high relevance (with 72% examples being fully relevant generations), but struggle with full completeness (with 34% fully complete)

## Related Work
- Table-to-text tasks are mainly focused
- The proposed framework is capable of generalizing to a broader range of tasks and datasets
- Various efforts have been made to improve the ability of unified models to generalize to new tasks and datasets
- Our proposed method is most relevant to Macaw (Tafjord and Clark, 2021), ProQA (Zhong et al., 2022b), and SchemaPro (Zhong et al., 2022a)

## Conclusion
- We introduced compositional task configurations for unified table-to-text task
- We used both versions of TOTTO at training time
- At test time, the model is able to identify relevant cells in the table based on the input question
- For table-based tasks, including table-based question answering, we see better performance when we remove task configurations at training time
