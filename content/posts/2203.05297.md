---
title: "BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis"
date: 2022-03-10T11:19:52.000Z
author: "Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2203-05297v5.webp" # image path/url
    alt: "BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.05297)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.05297).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/beat-a-large-scale-semantic-and-emotional).

# Abstract
- 76 hours of high-quality, multi-modal data was captured from 30 speakers talking with eight different emotions and in four different languages
- 32 million frame-level emotion and semantic relevance annotations were used to train the statistical analysis
- The baseline model, Cascaded Motion Network (CaMN), was built using six modalities modeled in a cascaded architecture for gesture synthesis
- Semantic relevancy was evaluated using Semantic Relevance Gesture Recall (SRGR)
- Qualitative and quantitative experiments were conducted to validate the metrics' validity and ground truth data quality

# Paper Content

## Related Work
- Conversational gestures dataset: There are a few different mo-cap and pseudo-label conversational gestures datasets.
- Volkova et al. [48] built a mo-cap emotional gestures dataset in 89 mins with text annotation.
- Takeuchi et al. [46] captured an interview-like audio-gesture dataset in total 3.5-hour with two Japanese speakers.
- Ferstl and Mcdonnell [18] collected a 4-hour dataset, Trinity, with a single male speaker discussing hobbies, etc.
- On the other hand, Ginosar et al. [21] used OpenPose [12] to extract 2D poses from YouTube videos as training data for 144 hours.
- Habibie et al. [22] extended it to a full 3D body with facial landmarks.
- Yoon et al. [53] used VideoPose3D [40] to build on the TED dataset, which is 97 hours with 9 joints on upper body.
- The limited data amount of mo-cap and noise in ground truth makes a trade-off for the trained network's generalization capability and quality.
- Recently, Bhattacharya [7] extracted emotional cues from text and used them for gesture synthesis.
- However, the proposed method has limitations in the accuracy of the emotion classification algorithm and the diversity of emotion categories in the dataset.
- Conditional conversational gestures synthesis: Early baseline models were released with datasets such as text-conditioned gesture [54], audio-conditioned gesture [21,46,18], and audio-text-conditioned gesture [53].
- These baseline models were based on CNN and LSTM for end-to-end modelling.
- Several efforts try to improve the performance of the baseline model by input/output representation selection [31,20], adversarial training [19] and various types of generative modeling techniques [51,37,52,1], which can be summarized by "Estimating a better distribution of gestures based on the given conditions.".

## BEAT: Body-Expression-Audio-Text Dataset
- The motion capture system is based on 16 synchronized cameras
- The facial capture system uses ARKit with a depth camera on iPhone 12 Pro
- The audio is recorded in a 48KHz stereo

### Data Acquisition
- The BEAT dataset consists of 120 1-minute self-talk recordings
- The dataset is divided into conversation and self-talk sessions, which consist of 10-min and 1-min sequences, respectively
- The conversation is between the speaker and the instructor remotely, i.e., to ensure only the speaker's voice is recorded
- The self-talk sessions consist of 120 1-minute self-talk recordings
- The speakers answer questions about daily conversation topics, e.g., personal experiences or hobbies
- The answers were written and proofread by three English native speakers
- The phonetic coverage was controlled to be similar to the frequently used 3000 words
- The dataset consists mainly of English data: 60h (81%), 12h of Chinese, 2h of Spanish and Japanese
- The Spanish and Japanese are also 50% of the size of the previous mo-cap dataset

### Data Annotation
- Text alignment: We use an in-house-built Automatic Speech Recognizer (ASR) to obtain the initial text for the conversation session and proofread it by annotators.
- Emotion and semantic relevance: The 8-class emotion label of self-talk is confirmed, and the on-site supervision guarantees the correctness.
- Annotators: We pay ∼ $10 for each annotator per hour in this task.

### Data Analysis
- BEAT has made it possible to analyze correlations between conversational gestures and other modalities
- Facial expressions and emotions were strongly correlated (excluding some of the lip movements), and we first analyze the correlation between conversational gestures and emotional categories here
- As shown in Figure 3a, We visualized the gestures in T-SNE based on a 2s-rotation representation, and the results showed that gestures have different characteristics in different emotions
- For example, as shown in Figure 3b, speaker-2 has different gesture styles when angry and happy, e.g., the gestures are larger and faster when angry
- The T-SNE results also significantly differ between happy (blue) and angry (yellow)
- However, the gestures for the different emotions are still not perfectly separable by the rotation representation
- Furthermore, the gestures of the different emotions appear to be confounded in each region, which is also consistent with subjective perceptions
- There is large randomness for the semantic relevance between gestures and texts, which is shown in Figure 4, where the frequency, position and content of the semantic-related gestures vary from speaker to speaker when the same text content is uttered
- In order to better understand the distribution of the semantic relevance of the gestures, we conducted a semantic relevance study based on four hours of two speakers' data
- As shown in Figure 4b, for the overall data, 83% of the gestures have low semantic scores (≤ 0.2)
- For the words-level, the semantic distribution varied between words, e.g., i and was which are sharing a similar semantic score but different in the score distribution
- Besides, Figure 4c shows the average semantic scores of nine high-frequency words in the text corpus
- Ultimately, it presents a different probability distribution to the semantically related gestures.

## Multi-Modal Conditioned Gestures Synthesis Baseline
- The text, audio and speaker ID encoders network selection are referred to [53] and customized for better performance.
- All input data have the same time resolution as the output gestures so that the synthesized gestures can be processed frame by frame through a sequential model.
- The gesture and facial blendshape weights are downsampled to 15 FPS, and the word sentence is inserted with padding tokens to correspond to the silence time in the audio.
- Text Encoder. First, words are converted to word embedding set v T ∈ R 300 by pre-trained model in FastText [10] to reduce dimensions. Then, the word sets are fine-tuned by customized encoder E T , which is a 8-layer temporal convolution network (TCN) [6] with skip connections [23], as

## Metric for Semantic Relevancy
- The Semantic-Relevant Gesture Recall (SRGR) is a metric to evaluate the semantic relevancy of gestures
- The SRGR metric can be calculated as follows: where 1 is the indicator function and T, J is the set of frames and number of joints.
- We think the SRGR, which emphasizes recalling gestures in the clip of interest, is more in line with the subjective human perception of gesture's valid diversity than the L1 variance of synthesized gestures.

## Experiments
- The SRGR metric is evaluated
- The dataset's data quality is assessed based on subjective experiments
- The validity of the baseline model is demonstrated using subjective and objective experiments
- The contribution of each modality is discussed based on ablation experiments

### Data quality
- The proposed dataset is compared to the widely used mocap dataset Trinity and in-the-wild dataset S2G-3D.
- The user study compares clips sampled from ground truth and generated results using motion synthesis networks trained in each dataset.
- The Trinity dataset has a total of 23 sequences, with 10 minutes each. We randomly divide the data into 19:2:2 for train/valid/test since there is no standard for splitting.
- We used S2G [21], as well as the SoTA algorithm A2G [33], to cover both GAN and VAE models.
- The output layer of the S2G model was adapted for outputting 3D coordinates.
- In the ablation study, the final generated 3D skeleton results were rendered and composited with audio for comparison in the user study.
- A total of 120 participant subjects compared the clips randomly sampled from Trinity and our dataset, with 5-20s in length.
- The participants were asked to evaluate gestures correctness, i.e., physical correctness, diversity and gestureaudio synchrony.
- Furthermore, the body and hands were evaluated separately for the gesture correctness test.
- The results are shown in Table 2, demonstrating that our dataset received higher user preference in all aspects. Especially for the hand movements, we outperformed the Trinity dataset by a large margin.
- Based on the score, the model trained on the BEAT dataset would be fitted into a more physically correct, diverse, and attractive distribution.

### Evaluation of the baseline model

### Ablation Study.
- The cascaded connection can achieve better performance because we introduce prior human knowledge to help the network extract features of different modalities.
- We gradually removed the data of one modality during the experiment (cf. Table 5).
- Synchrony would significantly be reduced after removing the audio, which is intuitive. However, it still maintains some synchronizations, such as the padding and time-align annotation of the text and the lip motion of the facial expression.
- In contrast, eliminating weighted semantic loss improves synchrony, which means that semantic gestures are usually not strongly aligned with audio perfectly.
- There is also a relationship between emotion and synchrony, but speaker ID only has little effect on synchrony.
- The removal of audio, emotion, and facial expression does not significantly affect the semantic relevant gesture recall, which depends mainly on the text and the speaker ID.
- Data from each modality contributed to improving the FGD, which means using different modalities of data enhances the network's mapping ability.
- The unities of audio and facial expressions, especially facial expressions, improve the FGD significantly.

### Limitation
- The impact of acting is inevitable and controlled.
- SRGR now is calculated based on semantic annotation, which has a limitation for an un-labelled dataset.

## Conclusion
- Annotation Interface and Measurement of Agreement
- Details of Data Release Format
- Facial Mesh Data Release
- BeatAlign
