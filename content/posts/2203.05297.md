---
title: "BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis"
date: 2022-03-10T11:19:52.000Z
author: "Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2203-05297v5.webp" # image path/url
    alt: "BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.05297)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.05297).


# Abstract
- 76 hours of high-quality, multi-modal data was captured from 30 speakers talking with eight different emotions and in four different languages
- 32 million frame-level emotion and semantic relevance annotations were used in the dataset
- Statistical analysis on the BEAT dataset demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics
- A baseline model, Cascaded Motion Network (CaMN), was proposed which consists of six modalities modeled in a cascaded architecture for gesture synthesis
- Semantic relevancy was evaluated using a metric, Semantic Relevance Gesture Recall (SRGR)

# Paper Content

## Related Work
- Datasets used for conversational gestures synthesis:
- Volkova et al. [48] built a mo-cap emotional gestures dataset in 89 mins with text annotation
- Takeuchi et al. [46] captured an interview-like audio-gesture dataset in total 3.5-hour with two Japanese speakers
- Ferstl and Mcdonnell [18] collected a 4-hour dataset, Trinity, with a single male speaker discussing hobbies, etc.
- Ginosar et al. [21] used OpenPose [12] to extract 2D poses from YouTube videos as training data for 144 hours
- Habibie et al. [22] extended it to a full 3D body with facial landmarks
- Yoon et al. [53] used VideoPose3D [40] to build on the TED dataset, which is 97 hours with 9 joints on upper body
- The limited data amount of mo-cap and noise in ground truth makes a trade-off for the trained network's generalization capability and quality.
- Recently, Bhattacharya [7] extracted emotional cues from text and used them for gesture synthesis. However, the proposed method has limitations in the accuracy of the emotion classification algorithm and the diversity of emotion categories in the dataset.
- Conditional conversational gestures synthesis:
- Early baseline models were released with datasets such as text-conditioned gesture [54], audio-conditioned gesture [21,46,18], and audio-text-conditioned gesture [53].
- These baseline models were based on CNN and LSTM for end-to-end modelling.
- Several efforts try to improve the performance of the baseline model by input/output representation selection [31,20], adversarial training [19] and various types of generative modeling techniques [51,37,52,1], which can be summarized by "Estimating a better distribution of gestures based on the given conditions.".

## BEAT: Body-Expression-Audio-Text Dataset
- The motion capture system is based on 16 synchronized cameras
- The facial capture system uses ARKit with a depth camera on iPhone 12 Pro
- The audio is recorded in a 48KHz stereo
- The BEAT dataset is a collection of conversations between people
- The dataset is composed of text, emotion, and semantic relevance annotations
- The motion capture system is used to analyze the correlation between conversational gestures and emotions

### Data Acquisition
- The BEAT dataset consists of 120 1-minute self-talk recordings
- The conversation is between the speaker and the instructor remotely, i.e., to ensure only the speaker's voice is recorded
- The self-talk recordings cover 8 emotions, neutral, anger, happiness, fear, disgust, sadness, contempt and surprise
- The speakers are from different ethnicities and have different language ratios
- The speakers are asked to read answers in self-talk sections proficiently, but are not guided to perform a specific style of gesture
- Before talking with an emotion, the speakers watch 2-10 minutes of emotionally stimulating videos corresponding to different emotions

### Data Annotation
- We use an in-house-built Automatic Speech Recognizer (ASR) to obtain the initial text for the conversation session and proofread it by annotators.
- Then, we adopt Montreal Forced Aligner (MFA) aligner [38] for temporal alignment of the text with audio.
- For the conversation session, annotators would watch the video with corresponding audio and gestures to perform frame-level annotation.
- For the semantic relevance, we get the score on a scale of 0-10 from assigned 600 annotators from Amazon Mechanical Turk (AMT).
- We paid ∼ $10 for each annotator per hour in this task.

### Data Analysis
- BEAT has made it possible to analyze correlations between conversational gestures and other modalities
- Facial expressions and emotions were strongly correlated (excluding some of the lip movements), and we first analyze the correlation between conversational gestures and emotional categories here
- As shown in Figure 3a, We visualized the gestures in T-SNE based on a 2s-rotation representation, and the results showed that gestures have different characteristics in different emotions
- For example, as shown in Figure 3b, speaker-2 has different gesture styles when angry and happy, e.g., the gestures are larger and faster when angry
- The T-SNE results also significantly differ between happy (blue) and angry (yellow)
- However, the gestures for the different emotions are still not perfectly separable by the rotation representation
- Furthermore, the gestures of the different emotions appear to be confounded in each region, which is also consistent with subjective perceptions
- There is large randomness for the semantic relevance between gestures and texts, which is shown in Figure 4, where the frequency, position and content of the semantic-related gestures vary from speaker to speaker when the same text content is uttered
- In order to better understand the distribution of the semantic relevance of the gestures, we conducted a semantic relevance study based on four hours of two speakers' data
- As shown in Figure 4b, for the overall data, 83% of the gestures have low semantic scores (≤ 0.2)
- For the words-level, the semantic distribution varied between words, e.g., i and was which are sharing a similar semantic score but different in the score distribution. Besides, Figure 4c shows the average semantic scores of nine high-frequency words in the text corpus. It is to be mentioned that the scores of the Be-verbs showed are comparatively lower than that Pronouns and Prepositions which are shown in blue and yellow, respectively. Ultimately, it presents a different probability distribution to the semantically related gestures.

## Multi-Modal Conditioned Gestures Synthesis Baseline
- The text, audio and speaker ID encoders network selection are referred to [53] and customized for better performance.
- All input data have the same time resolution as the output gestures so that the synthesized gestures can be processed frame by frame through a sequential model.
- The gesture and facial blendshape weights are downsampled to 15 FPS, and the word sentence is inserted with padding tokens to correspond to the silence time in the audio.
- Text Encoder. First, words are converted to word embedding set v T ∈ R 300 by pre-trained model in FastText [10] to reduce dimensions. Then, the word sets are fine-tuned by customized encoder E T , which is a 8-layer temporal convolution network (TCN) [6] with skip connections [23], as

## Metric for Semantic Relevancy
- The Semantic-Relevant Gesture Recall (SRGR) is a metric to evaluate the semantic relevancy of gestures
- The SRGR metric can be calculated as follows: where 1 is the indicator function and T, J is the set of frames and number of joints
- We think the SRGR, which emphasizes recalling gestures in the clip of interest, is more in line with the subjective human perception of gesture's valid diversity than the L1 variance of synthesized gestures

## Experiments
- The SRGR metric is evaluated
- The dataset's data quality is assessed based on subjective experiments
- The validity of the baseline model is demonstrated using subjective and objective experiments
- The contribution of each modality is discussed based on ablation experiments

### Data quality
- To evaluate the captured ground truth motion data quality, we compare our proposed dataset with the widely used mocap dataset Trinity [18] and in-the-wild dataset S2G-3D [21,22].
- We conducted the user study by comparing clips sampled from ground truth and generated results using motion synthesis networks trained in each dataset.
- The Trinity dataset has a total of 23 sequences, with 10 minutes each. We randomly divide the data into 19:2:2 for train/valid/test since there is no standard for splitting.
- We used S2G [21], as well as the SoTA algorithm A2G [33], to cover both GAN and VAE models.
- The output layer of the S2G model was adapted for outputting 3D coordinates.
- In the ablation study, the final generated 3D skeleton results were rendered and composited with audio for comparison in the user study.
- A total of 120 participant subjects compared the clips randomly sampled from Trinity and our dataset, with 5-20s in length.
- The participants were asked to evaluate gestures correctness, i.e., physical correctness, diversity and gestureaudio synchrony.
- Furthermore, the body and hands were evaluated separately for the gesture correctness test.
- The results are shown in Table 2, demonstrating that our dataset received higher user preference in all aspects. Especially for the hand movements, we outperformed the Trinity dataset by a large margin. This is probably due to the noise of the past motion capture devices and the lack of markers on the hands.
- Based on the score, the model trained on the BEAT dataset would be fitted into a more physically correct, diverse, and attractive distribution.

### Evaluation of the baseline model
- We use the Adam optimizer to train at a learning rate of 2e-4
- The 4-speaker data is trained in an NVIDIA V100 environment
- For evaluation metrics, L1 has been demonstrated unsuitable for evaluating the gesture performance
- We adopt FGD to evaluate the generated gestures' distribution distance with ground truth
- It computes the distance between latent features extracted by a pretrained network, we use an LSTM-based autoencoder as the pretrained network
- In addition, we adopt SRGR and Beat-Align to evaluate diversity and synchrony
- Beat-Align is a Chamfer Distance between audio and gesture beats to evaluate gesture-audio beat similarity
- The final results are shown in Table 4

### Ablation Study.
- The effectiveness of cascaded connection
- The effectiveness of each modality
- The synchrony of emotional gestures
- The data from each modality that contributes to the FGD
- The unities of audio and facial expressions
- The classifier is trained and tested on speaker-4's ground truth data

### Limitation
- The impact of acting is inevitable and controlled.
- SRGR now is calculated based on semantic annotation, which has a limitation for an un-labelled dataset.
- To solve this problem, training a scoring network or semantic discriminator is a possible direction.

## Conclusion
- We build a large-scale, high-quality, multi-modal, semantic and emotional annotated dataset to generate more human-like, semantic and emotional relevant conversational gestures.
- Together with the dataset, we propose a cascade-based baseline model for gesture synthesis based on six modalities and achieve SoTA performance.
- Finally, we introduce SRGR for evaluating semantic relevancy.
