---
title: "Training language models to follow instructions with human feedback"
date: 2022-03-04T07:04:42.000Z
author: "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2203-02155v1.webp" # image path/url
    alt: "Training language models to follow instructions with human feedback" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.02155)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.02155).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/training-language-models-to-follow).

# Abstract
- Making language models bigger does not inherently make them better at following a user's intent
- For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user
- In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.

# Paper Content

## Introduction
- Large language models (LMs) can be "prompted" to perform a range of natural language processing (NLP) tasks, given some examples of the task as input.
- However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021;Bommasani et al., 2021;Kenton et al., 2021;Weidinger et al., 2021;Tamkin et al., 2021;Gehman et al., 2020).
- This is because the language modeling objective used for many recent large LMs-predicting the next token on a webpage from the internet-is different from the objective "follow the user's instructions helpfully and safely" (Radford et al., 2019;Brown et al., 2020;Fedus et al., 2021;Rae et al., 2021;Thoppilan et al., 2022).
- Averting these unintended behaviors is especially important for language models that are deployed and used in hundreds of applications.
- We make progress on aligning language models by training them to act in accordance with the user's intention (Leike et al., 2018).
- This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful.
- Using the language of Askell et al. (2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn't fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment).
- We elaborate on the evaluation of these criteria in Section 3.6.
- We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017;Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions (see Figure 2).
- This technique uses human preferences as a reward signal to fine-tune our models.
- We first hire a team of 40 contractors to label our data, based on their performance on a screening test (see Section 3.4 and Appendix B.1 for more details).
- We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API 3 and some labeler-written prompts, and use this to train our supervised learning baselines.
- Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts.
- We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer.
- Finally, we use this RM as a reward function and fine-tune our supervised learning baseline to maximize this reward using the PPO algorithm (Schulman et al., 2017).
- We illustrate this process in Figure 2.
- This procedure aligns the behavior of GPT-3 to the stated preferences of a specific group of people (mostly our labelers and researchers), rather than any broader notion of "human values"; we discuss this further in Section 5.2.
- We call the resulting models InstructGPT.
- We mainly evaluate our models by having our labelers rate the quality of model outputs on our test set, consisting of prompts from held-out customers (who are not represented in the training data).
- We also conduct automatic evaluations on a range of public NLP datasets.
- We train three model sizes (1.3B, 6B, and 175B parameters), and all of our models use the GPT-3 architecture.
- Our main findings are as follows:
- Labelers significantly prefer InstructGPT outputs...

## Related work
- Research on alignment and learning from human feedback.
- We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feedback (RLHF).
- Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017;Ibarz et al., 2018), it has recently been applied to fine-tuning language models to summarize text (Ziegler et al., 2019;Stiennon et al., 2020;BÃ¶hm et al., 2019;Wu et al., 2021).
- This work is in turn influenced by similar work using human feedback as a reward in domains such as dialogue (Jaques et al., 2019;Yi et al., 2019;Hancock et al., 2019), translation (Kreutzer et al., 2018;Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019).
- Madan et al. (2022) use written human feedback to augment prompts and improve the performance of language models.
- There has also been work on aligning agents in text-based environments using RL with a normative prior (Nahian et al., 2021).
- Our work can be seen as a direct application of RLHF to aligning language models on a broad distribution of language tasks.
- The question of what it means for language models to be aligned has also received attention recently (Gabriel, 2020).
- Kenton et al. (2021) catalog behavioral issues in LMs that result from misalignment, including producing harmful content and gaming misspecified objectives.
- In concurrent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study some simple baselines, and their scaling properties.
- Training language models to follow instructions.
- Our work is also related to research on crosstask generalization in language models, where LMs are fine-tuned on a broad range of public NLP datasets (usually prefixed with an appropriate instruction) and evaluated on a different set of NLP tasks.
- There has been a range of work in this domain (Yi et al., 2019;Mishra et al., 2021;Wei et al., 2021;Khashabi et al., 2020;Sanh et al., 2021;Aribandi et al., 2021), which differ in training and evaluation data, formatting of instructions, size of pretrained models, and other experimental details.
- A consistent finding across studies is that fine-tuning LMs on a range of NLP tasks, with instructions, improves their downstream performance on held-out tasks, both in the zero-shot and few-shot settings.
- There is also a related line of work on instruction following for navigation, where models are trained to follow natural language instructions to navigate in a simulated environment (Bahdanau et al., 2018;Abramson et al., 2020;Zhao et al., 2021).
- Evaluating the harms of language models.
- A goal of modifying the behavior of language models is to mitigate the harms of these models when they're deployed in the real world.
- Language models can produce biased outputs (Dhamala et al., 2021;Liang et al., 2021;Manela et al., 2021;Caliskan et al., 2017;Kirk et al., 2021),...

### High-level methodology
- Pretrained language model
- Distribution of prompts on which we want our model to produce aligned outputs
- Team of trained human labelers
- Step 1: Collect demonstration data, and train a supervised policy
- Step 2: Collect comparison data, and train a reward model
- Step 3: Optimize a policy against the reward model using PPO

### Dataset
- The dataset consists of text prompts submitted to the OpenAI API, specifically those using an earlier version of the InstructGPT models (trained via supervised learning on a subset of our demonstration data)
- The dataset is heuristically deduplicated by checking for prompts that share a long common prefix, and it is limited to 200 prompts per user ID
- To avoid the models learning potentially sensitive customer details, the dataset is filtered for personally identifiable information (PII)
- To train the very first InstructGPT models, the labelers were asked to write prompts themselves
- The dataset contains about 13k training prompts (from the API and labeler-written), 33k training prompts (from the API and labeler-written), and 31k training prompts (only from the API)

### Tasks
- 96% of the training data is English
- The training data includes prompts from a dataset of written prompts by the labelers and a dataset of prompts submitted to early models on the API
- The task is most often specified directly through a natural language instruction (e.g. "Write a story about a wise frog"), but could also be indirectly through either few-shot examples (e.g. giving two examples of frog stories, and prompting the model to generate a new one) or implicit continuation (e.g. providing the start of a story about a frog)
- The labelers take into account the implicit intentions such as truthfulness of the response, and potentially harmful outputs such as biased or toxic language

### Human data collection
- To produce our demonstration and comparison data, and to conduct our main evaluations, we hired a team of about 40 contractors on Upwork and through ScaleAI.
- Compared to earlier work that collects human preference data on the task of summarization (Ziegler et al., 2019;Stiennon et al., 2020;Wu et al., 2021), our inputs span a much broader range of tasks, and can occasionally include controversial and sensitive topics.
- Our aim was to select a group of labelers who were sensitive to the preferences of different demographic groups, and who were good at identifying outputs that were potentially harmful.
- Thus, we conducted a screening test designed to measure labeler performance on these axes.
- We selected labelers who performed well on this test; for more information about our selection procedure and labeler demographics, see Appendix B.1.
- During training and evaluation, our alignment criteria may come into conflict: for example, when a user requests a potentially harmful response.
- During training we prioritize helpfulness to the user (not doing so requires making some difficult design decisions that we leave to future work; see Section 5.4 for more discussion).
- However, in our final evaluations we asked labelers prioritize truthfulness and harmlessness (since this is what we really care about).
- As in Stiennon et al. (2020), we collaborate closely with labelers over the course of the project.
- We have an onboarding process to train labelers on the project, write detailed instructions for each task (see Appendix B.2), and answer labeler questions in a shared chat room.
- As an initial study to see how well our model generalizes to the preferences of other labelers, we hire a separate set of labelers who do not produce any of the training data.
- These labelers are sourced from the same vendors, but do not undergo a screening test.
- Despite the complexity of the task, we find that inter-annotator agreement rates are quite high: training labelers agree with each-other 72.6 Â± 1.5% of the time, while for held-out labelers this number is 77.3 Â± 1.3%.
- For comparison, in the summarization work of Stiennon et al. (2020) researcher-researcher agreement was 73 Â± 4%.

### Models
- GPT-3 pretrained language models from Brown et al. (2020)
- Starting from these models, we then train models with three different techniques: Supervised fine-tuning (SFT). We fine-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2.
- We do our final SFT model selection based on the RM score on the validation set.
- Similarly to Wu et al. (2021), we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings, despite this overfitting.
- Starting from the SFT model with the final unembedding layer removed, we trained a model to take in a prompt and response, and output a scalar reward.
- In this paper we only use 6B RMs, as this saves a lot of compute, and we found that 175B RM training could be unstable and thus was less suitable to be used as the value function during RL (see Appendix C for more details).
- In Stiennon et al. (2020), the RM is trained on a dataset of comparisons between two model outputs on the same input. They use a cross-entropy loss, with the comparisons as labels-the difference in rewards represents the log odds that one response will be preferred to the other by a human labeler.
- In order to speed up comparison collection, we present labelers with anywhere between K = 4 and K = 9 responses to rank. This produces K 2 comparisons for each prompt shown to a labeler. Since comparisons are very correlated within each labeling task, we found that if we simply shuffle the comparisons into one dataset, a single pass over the dataset caused the reward model to overfit.
- Instead, we train on all K 2 comparisons from each prompt as a single batch element. This is much more computationally efficient because it only requires a single forward pass of the RM for each completion (rather than K 2 forward passes for K completions) and, because it no longer overfits, it achieves much improved validation accuracy and log loss.
- Specifically, the loss function for the reward model is:
- where r Î¸ (x, y) is the scalar output of the reward model for prompt x and completion y with parameters Î¸, y w is the preferred completion out of the pair of y w and y l , and D is the dataset of human comparisons.
- Finally, since the RM loss is invariant to shifts in reward, we normalize the reward model using a bias so that the labeler demonstrations achieve a mean score of 0 before doing RL.

### Evaluation
- To evaluate how "aligned" our models are, we first need to clarify what alignment means in this context.
- Following Leike et al. (2018), our aim is to train models that act in accordance with user intentions.
- More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who define models to be aligned if they are helpful, honest, and harmless.
- To be helpful, the model should follow instructions, but also infer intention from a few-shot prompt or another interpretable pattern such as "Q: {question}\nA:".
- Since a given prompt's intention can be unclear or ambiguous, we rely on judgment from our labelers, and our main metric is labeler preference ratings.
- However, since our labelers are not the users who generated the prompts, there could be a divergence between what a user actually intended and what the labeler thought was intended from only reading the prompt.
- It is unclear how to measure honesty in purely generative models; this requires comparing the model's actual output to its "belief" about the correct output, and since the model is a big black box, we can't infer its beliefs.
- Instead, we measure truthfulness-whether the model's statements about the world are true-using two metrics: (1) evaluating our model's tendency to make up information on closed domain tasks ("hallucinations"), and (2) using the TruthfulQA dataset (Lin et al., 2021).
- Needless to say, this only captures a small part of what is actually meant by truthfulness.
- Similarly to honesty, measuring the harms of language models also poses many challenges.
- In most cases, the harms from language models depend on how their outputs are used in the real world.
- For instance, a model generating toxic outputs could be harmful in the context of a deployed chatbot, but might even be helpful if used for data augmentation to train a more accurate toxicity detection model.
- Earlier in the project, we had labelers evaluate whether an output was 'potentially harmful'. However, we discontinued this as it required too much speculation about how the outputs would ultimately be used; especially since our data also comes from customers who interact with the Playground API interface (rather than from production use cases).
- Therefore we use a suite of more specific proxy criteria that aim to capture different aspects of behavior in a deployed model that could end up being harmful: we have labelers evaluate whether an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content.
- We also benchmark our model on datasets intended to measure bias and toxicity, such as RealToxicityPrompts (Gehman et al., 2020) and CrowS-Pairs (Nangia et al., 2020).
- To summarize, we can divide our quantitative evaluations into two separate parts: Evaluations on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by customers we haven't included in training. However, given that our training prompts are designed to be used with InstructGPT models, it's likely that they disadvantage the GPT-3 baselines. Thus, we also evaluate on prompts submitted to GPT-3 models on the API; these prompts are generally not in an 'instruction following' style, but are designed specifically for GPT-3.
- In both cases, for each model we calculate how often its outputs are preferred to a baseline policy; we choose our 175B SFT model as the baseline since its performance is near the middle of the pack. Additionally, we ask labelers to judge the overall quality of each response on a 1-7 Likert scale and collect a range of metadata for each model output (see Table 3).

## Results
- The paper provides experimental evidence for their claims in Section 1
- The paper provides results on the API prompt distribution, results on public NLP datasets, and qualitative results
- The paper omits GPT (prompted) from the evals on prompts submitted to GPT-3 models as these prompts are already designed to perform well for GPT-3

### Results on the API distribution
- Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.
- On our test set of prompts, our labelers significantly prefer InstructGPT outputs across model sizes.
- These results are shown in Figure 1.
- We find that GPT-3 outputs perform the worst, and one can obtain significant step-size improvements by using a well-crafted few-shot prompt (GPT-3 (prompted)), then by training on demonstrations using supervised learning (SFT), and finally by training on comparison data using PPO.
- Adding updates on the pretraining mix during PPO does not lead to large changes in labeler preference.
- To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs 85 Â± 3% of the time, and preferred 71 Â± 4% of the time to few-shot GPT-3.
- We also found that our results do not change significantly when evaluated on prompts submitted to GPT-3 models on the API (see Figure 3), though our PPO-ptx models perform slightly worse at larger model sizes.
- In Figure 4 we show that labelers also rate InstructGPT outputs favorably along several more concrete axes.
- Specifically, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints defined in the instruction (e.g. "Write your answer in 2 paragraphs or less."), are less likely to fail to follow the correct instruction entirely, and make up facts ('hallucinate') less often in closed-domain tasks.
- These results suggest that InstructGPT models are more reliable and easier to control than GPT-3.
- We've found that our other metadata categories occur too infrequently in our API to obtain statistically significant differences between our models.
- Our models generalize to the preferences of "held-out" labelers that did not produce any training data. Held-out labelers have similar ranking preferences as workers who we used to produce training data (see Figure 3).
- In particular, according to held-out workers, all of our InstructGPT models still greatly outperform the GPT-3 baselines.
- Thus, our InstructGPT models aren't simply overfitting to the preferences of our training labelers.
- We see further evidence of this from the generalization capabilities of our reward models.
- We ran an experiment where we split our labelers into 5 groups, and train 5 RMs (with 3 different seeds) using 5-fold cross validation (training on 4 of the groups, and evaluating on the held-out group).
- These RMs have an accuracy of 69.6 Â± 0.9% on predicting the preferences of labelers in the held-out group, a small decrease from their 72.4 Â± 0.4% accuracy on predicting the preferences of labelers in their training set.
- Public NLP datasets are not reflective of how our language models are used.
- In Figure 5, we also compare InstructGPT to our 175B GPT-3 baselines fine-tuned on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets (see Appendix C for details).
- We find that these models perform better than GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline.
- This indicates that these datasets are not sufficiently diverse to improve performance on our API prompt distribution.

### Results on public NLP datasets
- InstructGPT models show improvements in truthfulness over GPT-3
- As measured by human evaluatoins on the TruthfulQA dataset, our PPO models show small but significant improvements in generating truthful and informative outputs compared to GPT-3 (see Figure 6)
- This behavior is the default: our models do not have to be specifically instructed to tell the truth to exhibit improved truthfulness
- Interestingly, the exception is our 1.3B PPO-ptx model, which performs slightly worse than a GPT-3 model of the same size
- When evaluated only on prompts that were not adversarially selected against GPT-3, our PPO models are still significantly more truthful and informative than GPT-3 (although the absolute improvement decreases by a couple of percentage points. Following Lin et al. (2021), we also give a helpful "Instruction+QA" prompt that instructs the model to respond with "I have no comment" when it is not certain of the correct answer
- In this case, our PPO models err on the side of being truthful and uninformative rather than confidently saying a falsehood; the baseline GPT-3 model aren't as good at this
- Our improvements in truthfulness are also evidenced by the fact that our PPO models hallucinate (i.e. fabricate information) less often on closed-domain tasks from our API distribution, which we've shown in Figure 4
- InstructGPT shows small improvements in toxicity over GPT-3, but not bias
- We first evaluate our models on the RealToxicityPrompts dataset (Gehman et al., 2020)
- We do this in two ways: we run model samples through the Perspective API8 to obtain automatic toxicity scores, which is the RealToxicityPrompts. A total of 1,729 prompts were labeled for three different 175B models, both with and without "respectful" instructions. The automatic evaluations shown here are calculated over the same set of prompts as the human evaluations, and thus differ slightly from the full set of evaluations recorded in Table 14 in Appendix D
- standard evaluation procedure for this dataset, and we also send these samples to labelers to obtain ratings on absolute toxicity, toxicity relative to the prompt, continuity, and overall output preference
- Our results are in Figure 7
- We find that, when instructed to produce a safe and respectful output ("respectful prompt"), InstructGPT models generate less toxic outputs than those from GPT-3 according to the Perspective API. This advantage disappears when the respectful prompt is removed ("no prompt"). Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than those from GPT-3 (see Figure 39)
- These results are confirmed in our human evaluations: InstructGPT is less toxic than GPT-3 in the "respectful prompt" setting, but performs similarly in the "no prompt" setting
- We provide extended results in Appendix E
- To summarize: all of our models are rated as less toxic than expected given the prompt (they get a negative score on a scale from -1 to 1, where 0 is 'about as toxic as expected'). Our SFT baseline is the least toxic out of all of our models, but also has the lowest continuity and is the least preferred in our rankings, which could indicate that the model generates very short or degenerate responses
- We want an alignment procedure that avoids an alignment tax, because it incentivizes the use of models that are unaligned but more capable on these tasks

### Qualitative results
- InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution.
- In particular, we find that InstructGPT shows ability to follow instructions in non-English languages, and perform summarization and question-answering for code.
