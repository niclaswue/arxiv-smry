---
title: "Training language models to follow instructions with human feedback"
date: 2022-03-04T07:04:42.000Z
author: "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright and 15 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2203-02155v1.webp" # image path/url
    alt: "Training language models to follow instructions with human feedback" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.02155)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.02155).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/training-language-models-to-follow).

# Abstract
- Making language models bigger does not necessarily make them better at following user intent.
- This paper shows an avenue for aligning language models with user intent by fine-tuning with human feedback.
- Human evaluations show that outputs from a 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3.
- InstructGPT models show improvements in truthfulness and reductions in toxic output generation.

# Paper Content

## Introduction
- Large language models (LMs) can be prompted to perform NLP tasks
- Unintended behaviors can occur, such as making up facts, generating biased or toxic text, or not following user instructions
- Language modeling objective is misaligned with user's intention
- Aim to align language models by training them to act in accordance with user's intention
- Evaluate models by having labelers rate quality of model outputs
- Labelers prefer InstructGPT outputs over GPT-3
- InstructGPT generates more appropriate outputs and follows explicit constraints
- InstructGPT shows improvements in truthfulness and toxicity over GPT-3
- Performance regressions on public NLP datasets can be minimized by modifying RLHF fine-tuning procedure
- Models generalize to preferences of held-out labelers
- Public NLP datasets are not reflective of how language models are used
- InstructGPT shows promising generalization to instructions outside of RLHF fine-tuning distribution
- InstructGPT still makes simple mistakes

## Related work
- Research on alignment and learning from human feedback
- Training language models to follow instructions
- Evaluating the harms of language models
- Modifying the behavior of language models to mitigate harms
- List five ideas for how to regain enthusiasm for my career

### High-level methodology
- Pretrained language model used
- Three steps to methodology
- Step 1: Collect demonstration data and train supervised policy
- Step 2: Collect comparison data and train reward model
- Step 3: Optimize policy against reward model using PPO

### Dataset
- Prompt dataset consists of text prompts submitted to OpenAI API
- Customers using the Playground were informed their data could be used to train further models
- Data from customers using the API in production not used
- Prompts heuristically deduplicated by checking for common prefix
- Number of prompts limited to 200 per user ID
- Train, validation, and test splits based on user ID
- Prompts filtered for personally identifiable information
- Labelers asked to write prompts to bootstrap process
- Three datasets used in fine-tuning procedure
- Distribution of use-case categories for API prompts shown in Table 1
- Illustrative prompts shown in Table 2

### Tasks
- Training tasks come from two sources: a dataset of prompts written by labelers and a dataset of prompts submitted to early InstructGPT models
- Prompts are diverse and include generation, question answering, dialog, summarization, extractions, and other natural language tasks
- Dataset is over 96% English, but model is also tested on instructions in other languages and coding tasks
- Task is specified directly or indirectly through few-shot examples or implicit continuation
- Labelers take into account implicit intentions such as truthfulness and potentially harmful outputs

### Human data collection
- Hired a team of 40 contractors to produce demonstration and comparison data
- Inputs span a broad range of tasks, including controversial and sensitive topics
- Screening test to measure labeler performance on helpfulness and harmlessness
- Collaborate closely with labelers during project
- Inter-annotator agreement rates are high (72.6-77.3%)

### Models
- GPT-3 pretrained language models are adaptable to a wide range of tasks
- Supervised fine-tuning (SFT) is used to train models
- SFT models overfit on validation loss after 1 epoch
- Model is trained to take in a prompt and response and output a scalar reward
- Labelers are presented with 4-9 responses to rank
- Reward model is trained on dataset of comparisons between two model outputs
- PPO is used to fine-tune the SFT model
- PPO-ptx models mix pretraining gradients into the PPO gradients
- Performance is compared to SFT models, GPT-3, FLAN and T0 datasets

### Evaluation
- Definition of alignment is vague and confusing
- Aim is to train models that act in accordance with user intentions
- Framework similar to Askell et al. (2021) used to define models as helpful, honest, and harmless
- Main metric is labeler preference ratings
- Measure honesty and harms of language models poses challenges
- Evaluations on API distribution and public NLP datasets
- Metadata collected for each model output
- Human evaluations of toxicity on RealToxicityPrompts dataset

## Results
- Results on API prompt distribution show that prompts submitted to GPT-3 models perform better than prompts submitted to InstructGPT models.
- Results on public NLP datasets show that InstructGPT models perform better than GPT-3 models.
- Qualitative results show that InstructGPT models are better suited for certain tasks than GPT-3 models.

### Results on the api distribution
- Labelers prefer InstructGPT outputs over GPT-3
- GPT-3 performs the worst
- Step-size improvements by using a few-shot prompt, supervised learning, and PPO
- 175B InstructGPT outputs preferred to GPT-3 85% of the time
- InstructGPT outputs more appropriate, follow constraints, less likely to fail, and make up facts less often
- Results do not change significantly when evaluated on prompts submitted to GPT-3 API
- Labelers rate InstructGPT outputs favorably along several axes
- Public NLP datasets not reflective of how language models are used
- 175B InstructGPT model preferred over FLAN and T0 78-79% of the time

### Results on public nlp datasets
- InstructGPT models show improvements in truthfulness over GPT-3
- PPO models are significantly more truthful and informative than GPT-3
- PPO models err on the side of being truthful and uninformative
- InstructGPT shows small improvements in toxicity over GPT-3, but not bias
- PPO models generate less toxic outputs than GPT-3 when instructed to produce a safe and respectful output
- When prompted to produce a toxic output, InstructGPT outputs are much more toxic than those from GPT-3
- We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure
- Serendipity means finding something good in something that is bad

### Qualitative results
- InstructGPT models show generalization to instructions outside of the RLHF finetuning distribution
- InstructGPT can follow instructions in non-English languages, and perform summarization and question-answering for code
