---
title: "Speaking Style Conversion With Discrete Self-Supervised Units"
date: 2022-12-19T18:53:04.000Z
author: "Gallil Maimon, Yossi Adi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09730v1.webp" # image path/url
    alt: "Speaking Style Conversion With Discrete Self-Supervised Units" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09730)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09730).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/speaking-style-conversion-with-discrete-self).

# Abstract
- Voice conversion is the task of making a spoken utterance sound as if uttered by a different speaker
- Current VC methods focus primarily on spectral features like timbre, while ignoring the unique speaking style of people which often impacts prosody
- In this study, we introduce a method for converting not only the timbre, but also prosodic information (i.e., rhythm and pitch changes) to those of the target speaker
- The proposed approach is based on a pretrained, self-supervised, model for encoding speech to discrete units, which makes it simple, effective, and easy to optimise
- We consider the many-to-many setting with no paired data and empirically demonstrate the proposed approach is significantly superior to the evaluated baselines

# Paper Content

## Introduction
- Traditional voice conversion methods were mainly focused on changing the timbre of the source speaker while leaving the speaking style unchanged (Stylianou et al., 1998;Kain and Macon, 1998;Nakashika et al., 2013;Chou et al., 2019;Huang et al., 2021).
- Recent methods propose to additionally convert the speaking style of the target speaker.
- The proposed method uses a discrete self-supervised speech representation to convert the speaking style of the target speaker.
- The proposed method is effective and simple, requiring only a single GPU to be trained.

## Model
- Our approach is based on DIscrete units for Speaking Style Conversion, denoted as DISSC.
- We suggest using a decomposed representation of the speech signal to synthesise speech in the target speaking style.
- We consider three components in the decomposition: phonetic-content, prosodic features (i.e., F0 and duration) and speaker identity, denoted by z c , (z dur , z F 0 ), z spk respectively.
- We propose the following cascaded pipeline: (i) extract z c from the raw waveform using a SSL model; (ii) predict the prosodic features of the target speaker from z c and z spk ; (iii) synthesize the speech from the content, predicted prosody and target speaker identity.
- See Figure 2 for a visual description of the method.

### Speech Input Representation
- To represent speech phonetic-content we extract a discrete representation of the audio signal using a pre-trained SSL model, namely HuBERT (Hsu et al., 2021).
- We use a SSL representation for phonetic-content to avoid limitation to transcribed samples, and to support non-written languages.
- We discretise this representation to use as a rhythm proxy (by number of repetitions) and for ease of modelling. This paradigm allows us to benefit from recent advances in NLP.
- We chose HuBERT for the phoneticcontent units as it was shown to better disentangle between speech content and both speaker and prosody compared to other SSL-based models (Polyak et al., 2021).
- Denote the domain of audio samples by X ⊂ R. The representation for an audio waveform is therefore a sequence of samples x = (x 1 , . . . , x T ), where each x t ∈ X for all 1 ≤ t ≤ T .
- The content encoder E c is a HuBERT model pre-trained on the LibriSpeech corpus (Panayotov et al., 2015).
- HuBERT is a self-supervised model trained on the task of masked prediction of continuous audio signals, similarly to BERT (Devlin et al., 2018).
- During training, the targets are obtained via clustering of MFCCs features or learned representations from earlier iterations.
- The input to the content encoder E c is an audio waveform x, and the output is a spectral representation sampled at a lower frequency z = (z 1 c , . . . , z L c ) where L = T /n.
- Since HuBERT outputs continuous representations, an additional k-means step is needed in order to quantize these representations into a discrete unit sequence denoted by z c = (z 1 c , . . . , z L c ) where z i c ∈ {1, . . . , K} and K is the size of the vocabulary.
- For the rest of the paper, we refer to these discrete representations as "units".
- We extracted representations from the final layer of the HuBERT model and set K = 100.
- When we wish to predict the rhythm, we must decompose it from the sequence, therefore repeated units were omitted (e.g., 0, 0, 0, 1, 1, 2 → 0, 1, 2) and the number of repetitions correlates to the rhythm.
- We denote such sequences as "deduped".
- Speaker representation. Our goal is to convert the speaking style while keeping the content fixed. To that end, we construct a speaker-representation z spk , and include it as an additional conditioner during the prosody prediction and waveform synthesis phases.
- To learn z spk we optimize the parameters of a fixed size look-up-table. Although such modeling limits our ability to generalize to new and unseen speakers, it produces higher quality generations (Polyak et al., 2021).
- Prosody representation. As explained in the content representation section, the number of repetitions of units indicates the rhythm of the speaker in the context. More repetitions means a sound was produced for a longer time. Therefore when converting rhythm the sequence is "deduped", and when only converting the pitch the full z c is used.
- Following Polyak et al. (2021) we encode the pitch contour of a sample using YAAPT (Kasi and Zahorian, 2002).
- We mark this pitch contour z F 0 = (z 1 F 0 , . . . , z L F 0 ), where z t F 0 ∈ R ≥0 .
- We mark the per-speaker normalised version as z F 0 .

### Speaking Style Conversion
- Several methods exist which aim to change the prosody of a given spoken utterance
- However, many convert them based on a single target utterance (Qian et al., 2020;Chen and Duan, 2022) which does not truly capture the general speaking style of a target speaker, and often creates artifacts when the contents do not match.
- Other methods only linearly change the speaking rate (Kuhlmann et al., 2022) thus ignoring the change of rhythm for different content.
- Leading methods which actually perform speaking-style conversion focus only on the rhythm (Qian et al., 2021;Lee et al., 2022c)
- Unlike the proposed method, these methods operate over a continuous speech representation, while in addition, Lee et al. (2022c) require text transcriptions.

### Speech Synthesis
- Sum of squared errors between predicted and actual values
- Mean squared error between predicted and actual values
- Cross-entropy between predicted and actual values

## Experimental Setup
- There is no standardised framework for evaluating SSC.
- In this section, the authors describe their setup which uses several datasets and fine-grained offline metrics for each part of the conversion -timbre, pitch, rhythm.

### Datasets
- The authors use VCTK as a dataset because it has many speakers
- The authors leave all paired utterances for the test set, so the setup is truly unpaired
- The authors follow Qian et al. in selecting the two fastest and two slowest speakers
- The authors take the two fastest and two slowest speakers for rhythm metric evaluation
- After preliminary experiments the authors found that the speakers in these datasets do not have clear pitch patterns
- To effectively evaluate pitch conversion the authors created a synthetic dataset based on VCTK

### Metrics
- We introduce several new metrics which aim to capture the rhythm in a fine-grained manner, and objectively evaluate the pitch contour (even when the rhythm does not match).
- For regular VC we use the common Equal Error Rate (EER) of speaker verification using Speech-Brain's (Ravanelli et al., 2021) ECAPA-TDNN (Desplanques et al., 2020) which achieves 0.8% EER on Voxceleb.
- For each sample, we randomly select n positive and n negative samples. Positive samples are random utterances of other speakers converted to the current speaker, and negative samples are random utterances of the current speaker converted to a random target speaker.
- For phonetic content preservation we use the standard Word Error Rate (WER) and Character Error Rate (CER), with Whisper (Radford et al., 2022) as our Automatic Speech Recognition (ASR) model.
- In order to evaluate the rhythm we wish to go beyond the length of the recording, which only indicates the average rhythm and not which sounds are uttered slowly and which quickly. We therefore use the text transcriptions of the samples, and the Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to get start and end times for each word and phoneme in the recording.
- We then compare the lengths between the converted speech and the target speech. Thus defining Phoneme Length Error (PLE), Word Length Error (WLE) and Total Length Error (TLE).
- We do not compare the length of silences as they can appear differently for different recordings thus breaking the alignment.
- Formally, for each input recording and matching text, MFA returns a list of tuples T w = ((s w 1 , e w 1 , c w 1 ) . . . (s wm , e wm , c wm )) for word level and another for phoneme level -T p = ((s p 1 , e p 1 , c p 1 ) . . . (s pm , e pm , c pm )).
- s {w,p} i indicates the start time of the sound at index i in the recording (word or phoneme according to the level), and e {w,p} i indicates the end time.
- c {w,p} i represents the content of the time segment, i.e the word or phoneme.
- We filter only non-silence time segments leaving T {w,p} = ((s {w,p} i , e {w,p} i , c {w,p} i ) ∀i s.t. c {w,p} i = silence).
- Thus formally, the errors per sample compare durations of a reference sample and a synthesised sample: The TLE is just the duration difference between the entire recordings.
- All rhythm metrics are in time units, in this case seconds.
- In addition, on rare occasions the number of phonemes for the same sound by different speakers is different (due to accents etc.), we ignore these samples for a more well-defined metric.
- When the phonetic content is harmed dramatically in the conversion, MFA might fail to align the text. We penalise such samples by assuming a linear split of the recording to phonemes and words.
- Popular evaluation methods for F0 similarity include the Voicing Decision Error (VDE) and F0 frame error (FFE).
- Intuitively, VDE measures the portion of frames with voicing decision error, while FFE measures the percentage of frames that contain a deviation of more than 20% in pitch value or have a voicing decision error.
- Formally, and, where p, p are the pitch frames from the target and...

## Results
- We compare our baselines to Speech Resynthesis (SR) which is a strong VC baseline for our setup, which works similarly to DISSC but does not convert prosody.
- In addition we compare to AutoPST which is considered the state of the art in prosodic-aware VC (i.e. SSC).
- We use the official Github implementations and train the models on our datasets.
- We demonstrate three variants of our approach: DISSC_Rhythm, DISSC_Pitch and DISSC_Both which predict duration, pitch contour and both respectively.
- Objective results for VCTK and ESD appear in Table 1.
- On both datasets DISSC improves the length errors across all scales: phonemes, words and utterances.
- The relative improvement compared to the next best baseline is 56.8% on ESD TLE.
- At word level we get 27.6% and 26.7% relative improvement on ESD and VCTK respectively.
- We can see a minor decrease in content quality (by WER and CER) of DISSC compared to SR which does not convert prosody, but still better than Au-toPST.
- The harm in quality can be explained additional information bottlenecks by de-duplicating the speech units, and F0 prediction.
- We additionally provide subjective evaluation considering both naturalness of the converted samples and their similarity to the target recording considering speaking style.
- These results are shown in Figure 4.
- We found that the naturalness of both DISSC variants was comparable to SR, and not far from the reference recordings while far outperforming AutoPST.
- In addition, we see a noticeable increase in speaking style similarity when converting prosody with DISSC compared to SR. However, also converting the pitch did not noticeably improve the results.
- We believe that this has to do with the lack of distinct speaking style which people can notice, especially from a single recording.
- To properly evaluate the ability of DISSC to learn the pitch speaking style, when such exists, we use the synthetic VCTK data (as explained in section 3.1) with known pitch trends.
- The results in Table 2 highlight the control-ability of our approach -when predicting the rhythm, only length errors improve, likewise for pitch.
- We can see that when a pitch pattern exists in the speaking style -DISSC learns it, providing 88-113% relative increase in the metrics.
- We also see that they can be converted jointly without noticeable drop in pitch and length performance.
- Lastly, to demonstrate that DISSC performs content dependent rhythm correction, and not only on average -we create a sample with abnormal rhythm patterns.
- We take an utterance and artificially stretch one of the words using speech resynthesis. We then attempt to "convert" the sample to the original speaker thus checking the ability to only change the rhythm of the abnormal part.
- The results of DISSC and AutoPST are brought in Figure 5.
- We can see that DISSC manages to mostly correct the irregular rhythm, while hardly changing the other parts, compared to AutoPST which only slightly changes the rhythm.
- Given optimal performance of E c we are guaranteed perfect results, as z c would be identical for irregular and original after dedup. However, HuBERT may introduce additional errors specifically when considering signal variations as seen in (Gat et al., 2022).

## Related Work

### Unpaired Voice Conversion
- Many existing methods perform VC in the setting of unpaired utterances
- Contains a Vocoder which generates the audio (or some intermediate representation) from several representations, one meant to capture the speaker information and others to capture the remaining information
- The disentanglement was encouraged through information bottlenecks by neural network architecture (Chen et al., 2021;Qian et al., 2019), mutual information and adversarial losses (Lee et al., 2021;Yuan et al., 2021), pretrained models (Huang et al., 2022;Polyak et al., 2021) or combinations of these.
- Notable is that Polyak et al. (2021) used discrete HuBERT (Hsu et al., 2021) tokens, pitch representations based on YAAPT (Kasi and Zahorian, 2002) and learned speaker representations to resynthesise the audio waveform.

## Conclusion & Future Work
- The proposed method is superior to the evaluated baselines considering both objective and subjective metrics.
- The model suffers from content loss when using the deduped units, together with repeated utterances getting slightly different units (in the abnormal rhythm experiment).
- For future work, we aim to improve the robustness and disentanglement of the speech representation considering speaker and prosodic information.
