---
title: "DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching"
date: 2023-01-08T07:15:09.000Z
author: "Tao Xie, Kun Dai, Ke Wang, Ruifeng Li, Lijun Zhao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02993v1.webp" # image path/url
    alt: "DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02993)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02993).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/deepmatcher-a-deep-transformer-based-network).

# Abstract
- Local feature matching between images remains a challenging task, especially in the presence of significant appearance variations
- In this work, we propose DeepMatcher, a deep Transformer-based network built upon our investigation of local feature matching in detector-free methods
- The key insight is that local feature matcher with deep layers can capture more human-intuitive and simpler-to-match features
- Based on this, we propose a Slimming Transformer (SlimFormer) dedicated for DeepMatcher, which leverages vector-based attention to model relevance among all keypoints and achieves long-range context aggregation in an efficient and effective manner
- A relative position encoding is applied to each SlimFormer so as to explicitly disclose relative distance information, further improving the representation of keypoints
- A layer-scale strategy is also employed in each SlimFormer to enable the network to assimilate message exchange from the residual block adaptively, thus allowing it to simulate the human behaviour that humans can acquire different matching cues each time they scan an image pair
- To facilitate a better adaption of the SlimFormer, we introduce a Feature Transition Module (FTM) to ensure a smooth transition in feature scopes with different receptive fields
- By interleaving the self- and cross-SlimFormer multiple times, DeepMatcher can easily establish pixel-wise dense matches at coarse level
- Finally, we perceive the match refinement as a combination of classification and regression problems and design Fine Matches Module to predict confidence and offset concurrently, thereby generating robust and accurate matches.

# Paper Content

## I. INTRODUCTION
- Local feature matching is the prerequisite for a variety of geometric computer vision applications
- Detector-based matching is typically accomplished by (i) detecting and describing a set of sparse keypoints, (ii) instituting point-to-point correspondences via nearest neighbour search, or more advanced matching algorithms, and (iii) comparing the results against a reference image
- DeepMatcher is a deep local feature matching network that can produce more human-intuitive and simpler-to-match features for accurate correspondence with less computational complexity
- By interleaving the self-and cross-SlimFormer multiple times, DeepMatcher learns the discriminative features to construct dense matches at the coarse level by Coarse Matches Module (CMM).

## II. RELATED WORK

## A. Detector-based Methods
- The conventional pipeline of detector-based matching systems detects two sets of keypoints, describes them with high-dimensional vectors, and then implements a matching algorithm to generate matches between the two sets of keypoints.
- There are numerous handcrafted methods that seek to strike a balance between accuracy and efficiency, but the handcrafted descriptors are fragile when coping with image pairs with extreme appearance variations.
- With the development of deep learning, numerous approaches leverage elaborate convolution neural network (CNN) to extract robust feature representations, hence achieving superior performance.
- SuperPoint builds a large dataset of pseudoground truth interest point locations in real images, supervised by the interest point detector itself, as opposed to a largescale human annotation.
- D2-Net makes the collected keypoints more stable by delaying the detection to a later stage.
- Subsequently, the aforementioned methods utilize the nearest neighbor search, followed by a robust estimator, such as RANSAC or its variants to find matches between the retrieved keypoints.
- Recent researches has interpreted local feature matching as a graph matching problem involving two sets of features.
- These methods utilize keypoints as nodes to construct graph neural network (GNN), employ the self-and crossattention layers in Transformer to exchange global visual and geometric messages across nodes, and then generate the matches in accordance with soft assignment matrixes.
- Typically, SuperGlue utilizes self-and cross-attention in Transformer to integrate global context information, followed by the Sinkhorn algorithm to generate matches according to the soft assignment matrix.
- Nonetheless, the matrix multiplication in vanilla Transformer results in quadratic complexity with respect to the number of keypoints, making SuperGlue costly to deal with substantial keypoints.
- To tackle this problem, many approaches attempt to ameliorate the structure of SuperGlue.
- SGMNet exploits the sparsity of graph neural network to lower the computation complexity.
- ClusterGNN employs a progressive clustering module adaptively to divide keypoints into different subgraphs to reduce computation.

## B. Detector-free Methods
- Detector-free methods exclude the feature detector and generate dense matches directly from the original images.
- Earlier detector-free matching researches [21]- [25] generally utilize convolutional neural network (CNN) based on correlation or cost volume to identify probable neighbourhood consensus.
- DRC-Net [23] generates a 4D correlation tensor from the coarse-resolution features, which is refined by a learnable neighbourhood consensus module to generate matches.
- Patch2Pix [24] proposes a weakly supervised approach to learn matches that are consistent with the epipolar geometry of image pairs.
- DFM [25] uses pre-trained VGG architecture as a feature extractor and captures matches without any additional training strategy.
- Although elevating the matching accuracy, these methods extract ambiguous feature representations and fail to discriminate incorrect matches due to the limited receptive field of CNN.
- To handle this issue, LoFTR [2], the pioneering detector-free GNN method, utlizes Transformer to realize global context information exchange and extracts matches in a coarse-to-fine manner.
- Matchformer [26] proposes a human-intuitive extractand-match scheme that interleaves self-and cross-attention in each stage of the hierarchical encoder.
- Such a match-aware encoder releases the overloaded decoder and makes the model highly efficient.
- QuadTree [43] proposes a novel Transformer structure that builds token pyramids and computes attention in a coarse-to-fine manner.
- Then, the QuadTree Transformer is integrated into LoFTR and achieves superior matching performance.
- TopicFM [27] applies a topic-modeling strategy to encode high-level contexts in images, which improves the robustness of matching by focusing on the same semantic areas between the images.
- ASpanFormer [28] proposes a Transformer-based detector-free architecture, in which the flow maps are regressed in each cross-attention phase to perform local attention.

## C. Efficient Transformer
- In the vanilla Transformer, the memory cost is quadratic to the length of sequences due to the matrix multiplication, which has become a bottleneck for Transformer when dealing with long sequences.
- Recently, several approaches have been proposed to improve the efficiency of Transformer [30], [44], [45].
- Linear Transformer [30] uses linear dot product of kernel feature maps and makes use of the associativity property of matrix products to reduce the computational complexity.
- BigBird [44] combines local attention and global attention at certain positions and utilizes random attention on several randomly selected token pairs.
- FastFormer [45] uses additive attention mechanism to model global contexts, achieving effective context modeling with linear complexity.

## III. METHODOLOGY

## A. Overall
- Intuitively, when humans match images, they scan the images back and forth, and the more times they scan them, the simpler it is for them to recall the easier-to-match features.
- Thus, as shown in Fig. 2, we consider depth of the network is more prominent than width and present a deep Transformer-based network, namely DeepMatcher.
- As shown in Fig. 3, given the image pair I A and I B , our network produces reliable and accurate matches across images in an end-to-end manner.
- The matching process starts with a CNN-based encoder to extract the fine-level features FA , FB and coarse-level features FA , FB .
- Before feeding these features to Slimming Transformer (SlimFormer), we utilize the Feature Transition Module (FTM) to guarantee a smooth transition to SlimFormer.
- Then, we utilize SlimFormer to achieve long-range global context aggregation intra-/inter-images in an efficient and effective way.
- A relative position encoding is applied on each SlimFormer to explicitly model relative distance information, hence enhancing the DeepMatcher's ability to convey information, particularly in deeper layers.
- A layer-scale strategy is also leveraged in each SlimFormer to enables the network to assimilate message exchange from the feed-forward modules adaptively, thus allowing it to simulate the human behavior that human can receive different matching information each time they scan an image pair.
- After interleaving SlimFormer by L times, the enhanced features L F seq A and L F seq B are utilized to establish coarse matches H c , which are further optimized to fine matches H f using Correspondence Refine Module (CRM).

## B. Local Feature Extractor
- We use a standard convolutional neural network (CNN) with FPN to extract the coarse-level features FA , FB ∈ R Ĉ×H/8×W/8 and the fine-level features FA , FB ∈ R C×H/2×W/2 for the image pair I A and I B
- For convenience, we denote N = H/8 × W/8 as the number of pixel tokens
- Since each pixel in FA , FB represents an 8 × 8 grid in the original images I A , I B , we view the central position of all grids as the pixel coordinates P A , P B ∈ R N ×2 of keypoints

## C. Feature Transition Module (FTM)
- In the subsequent steps, we construct graph neural network (GNN) and propose SlimFormer that leverages self-/crossattention in Transformer to aggregate global context information intra-/inter-image.
- Nevertheless, there is apparently a gap between the feature extractor and SlimFormer in terms of context ranges, which is deleterious to subsequent steps involving deep feature interaction.
- Besides, representing features at multiple scales is so critical for discriminating objects or regions of varying sizes that it can ensure prominent features at various scales can be preserved for deep features aggregation.
- Thus, we propose a Feature Transition Module (FTM) inserted between the local feature extractor and SlimFormer to adjust the receptive fields of the extracted features, ensuring effective deep feature interaction in SlimFormer.
- Specifically, instead of directly using (1 × 1, 3 × 3, 5 × 5, 7 × 7) convolution, FTM adopts (1 × 1, 3 × 3, 5 × 5, 7 × 7) depth-wise convolution [47] followed by 1 × 1 point-wise convolution to reduce model parameters and computation, obtaining multi-scale feature representations.
- Then, we concatenate the features along channel dimension, hence enlarging the receptive fields of FA , FB .
- Finally, we derive the updated , which can be formulated as: where C represents the context vector at scale i.

## D. Slimming Transformer (SlimFormer)
- We flatten the updated enhanced features to be the input sequence for deep feature aggregation
- We view keypoints with features in image pairs as nodes to construct GNN
- Intuitively, more observations between images can result in more precise matches, indicating that deep feature interaction is essential for local features matching task
- However, the ablation study of LoFTR demonstrates that the matching performance has not been significantly improved with more Transformer layers
- We attribute this phenomenon to the following reasons: (i) LoFTR only utilizes absolute position encoding before Transformer layers, where the position information would disappear when the Transformer layers grow deeper
- To handle this dilemma, we propose SlimFormer that leverages relative position information and global context information to boost the capability of DeepMatcher to convey abundant information, hence extracting discriminative feature representations
- L F seq A , L F seq B ∈ R Ĉ×H/8×W/8 are the discriminative feature representations
- Vector-based Attention (VAtt) Layer is introduced to model the context information of the input features
- Instead of using a context-agnostic manner to approximate self-attention, we convert query vector to global query contexts and leverage element-wise product to model relevance among all keypoints
- Technically, during each feature enhancement process, we utilize self-/cross-attention to aggregate long-range context information intra-/inter-images
- For self-attention, the input features U and R are same (either (F seq A , F seq A ) or (F seq B , F seq B )). For cross-attention, the input features U and R are different (either (F seq A , F seq B ) or (F seq B , F seq A ))
- SlimFormer transforms the input features U and R into the query, key, and value vectors where W Q , W K , W V ∈ R Ĉ× Ĉ denote learnable weights for feature transformation
- Then, we perform relative position encoding on query vector Q and key K. where DP E(•) means relative position encoding operation, described below
- Next, modeling the context information of the input features based on the interactions among Q, K, and V is a critical problem for Transformer-like architectures
- In the vanilla Transformer, dot-product attention mechanism leads to quadratic complexity, making it unrealistic to establish deep Transformer layers
- A potential method to reduce the computational complexity is to summarize the attention matrices before modeling their interactions. Inspired by [45], we introduce vector-based attention that effectively models long-range interactions among pixel tokens to alleviate this bottleneck
- Instead of computing a quadratic attention map QK T that encodes all possible interactions between candidate matches, we form a compact representation of query-key interactions via vector-based attention that computes the correlation between global query vector and each key vector
- Specifically, we firstly leverage MLP to calculate the weight Qimp ∈ R 1×N of each query vector: where Sof tmax(•) means softmax operation
- The global query vector Q ∈ R 1× Ĉ is set to be a linear combination of Q: where ⊗ means matrix multiplication
- Then, we utilize the element-wise multiplication between the global query vector Q and each key vector to model their interaction, obtaining context-aware key vector KQ ∈ R N × Ĉ : where denotes element-wise multiplication
- We utilize a similar vector-based attention to extract global context-aware key vector KQ and model the interaction between KQ and V
- Subsequently, we employ a MLP and short-cut structure to...

## F. Fine Matches Module (FMM)
- After establishing coarse matches, a coarse-to-fine module is applied to refine these matches to the original picture resolution.
- However, the coarse-to-fine module in LoFTR only predicts the offset of the coarse matches without appraising whether the predicted matches are reliable.
- To tackle this issue, we view the match refinement as a combination of classification and regression problems and design Fine Matches Module to predict confidence and offset concurrently.
- As shown in Fig. 3, for each coarse match, we locate its position at fine-level feature maps and crop two sets of local image patches with the size of w × w, obtaining local features F w A , F w B ∈ R K×C×w×w , where K is the number of coarse matches.
- Then, we flatten F w A , F w B to be sequences, implement SlimFormer to perform L 2 times of global information passing, and rearrange the sequences into 2D feature maps, The feature maps are concatenated along channel dimension and fed into a network, which is comprised of two convolution layers, a max pooling layer, and four convolution layers.
- The network predicts the offset ∆ ∈ R K×2 of the P c B and the confidence c ∈ R K×1 of the predicted coarse matches: where P max means global max pooling operation; C 1 (•) means 1 × 1 convolution; [•||•] denotes concatenation along the channel dimension; Sig(•) means sigmoid function.
- Ultimately, we obtain the fine matches H f = {(P f A , P f B )}:

## G. Loss
- DeepMatcher generates final dense matches according to soft assignment matrix G and offset ∆.
- The total loss L all of DeepMatcher comprises of matching loss L m , regression loss L r , and classification loss L c .
- Matching loss. Following [2], we calculate the index E gt of the ground truth matches, which are utilized in conjunction with soft assignment matrix G to calculate matching loss L m defined as focal loss [50].
- Regression loss. For predicted matches {(P f A , P f B )}, we project P f A in the first image to second image, deriving P gt B . Then, the ground truth offset ∆ gt is formulated as:
- According to predicted offset ∆ and ground truth offset ∆ gt , we define the regression loss L r as: where K is the number of predicted matches.
- Notably, we ignore the predicted matches with ∆ gt larger than predefined threshold ψ.
- Classification loss. For the predicted matches with ground truth offset less than ψ, we regard them as positive and define the classification label as 1, while other matches are viewed as negative. Ultimately, we obtain the ground truth confidence c gt , while are utilized to calculate classification loss L c together with predicted confidence c.

## B. Indoor Pose Estimation
- The DeepMatcher performs well on indoor scenes with motion blur and low textures
- The DeepMatcher-L outperforms the other methods by a large margin

## Local features
- AUC@(5 • , 10 • , 20 • ) is superior to LoFTR
- DeepMatcher-L achieves superior performance with the improvement of (5.26%, 5.45%, 4.87%)
- Compared with the pioneering method LoFTR, DeepMatcher-L realizes superior performance with the improvement of (5.26%, 5.45%, 4.87%)
- DeepMatcher-L outperforms the state-of-theart detector-free method ASpanFormer by (1.72%, 0.25%) in terms of AUC@(5 • , 10 • )

## C. Outdoor Pose Estimation
- Outdoor pose estimation remains a challenging task
- To demonstrate the efficacy of DeepMatcher in overcoming these obstacles, an outdoor pose estimation experiment is conducted
- Dataset: We utilize MegaDepth [52] to conduct the outdoor pose estimation experiment
- MegaDepth contains 1M internet images from 196 different scenes
- Following [2], [14], we select 100 image pairs each scene for training and 1500 image pairs for testing
- Images are resized such that their longer dimensions are equal to 840
- Evaluation Protocol: We use the same evaluation metrics AUC@(5 • , 10 • , 20 • ) as the indoor pose estimation task
- Results: As shown in Table II, we can observe that DeepMatcher families surpass other methods in all evaluation metrics. Specifically, DeepMatcher-L noticeably outperforms the cutting-edge detector-based method ClusterGNN by (12.79%, 14.57%, 13.82%) in AUC@(5 • , 10 • , 20 • ) since the detector struggles to extract repeatable keypoints in image pairs with extreme viewpoint change. Besides, compared with the baseline approach LoFTR, DeepMatcher-L achieves superior performance with the improvement of (4.18%, 3.92%, 2.97%). Moreover, DeepMatcher-L also surpasses the state-of-the-art detector-free method ASpanFormer by (1.68%, 1.61%, 1.05%), further validating the rationality of the deep Transformer structures.

## D. Image Matching
- DeepMatcher is a deep learning algorithm that is better than other detector-free methods at image matching
- DeepMatcher achieves superior performance under varying illumination conditions
- DeepMatcher-L is a deep learning algorithm that is better than other detector-free methods at image matching for robotness

## E. Homography Estimation
- Conducts a homography estimation experiment to comprehensively evaluate the performance of DeepMatcher
- Dataset: HPatches
- Evaluation Protocol: Following the corner correctness metric (CCM) utilized in [24], we report the percentage of image pairs with average corner errors ε smaller than 1/3/5 pixels.
- Results: DeepMatcher achieves the best performance among the detector-free methods under extreme viewpoint changes.

## F. Understanding DeepMatcher
- DeepMatcher is a detector-free method that is able to accurately match image pairs with extreme appearance settings, e.g., sparse texture, motion blur, large viewpoint and illumination changes.
- Visual Descriptors Enhancement Efficacy Analysis shows that DeepMatcher-L is able to achieve dense and accurate matching performance.
- Efficiency Analysis shows that DeepMatcher is able to reduce the dominant computational complexity of the attention layer from O(N C 2 ) to O(N C).
- Weight Analysis shows that SlimFormer pays attention to the prominent keypoints at object boundaries that involve tremendous visual and geometry information.

## G. Ablation Study
- The proposed modules improve the performance of DeepMatcher.
- The learnable scale factor ξ affects the performance of the modules.
- The relative position encoding is more conducive to realize elaborate scene parsing.
- The fine matches module is more effective than the coarse-to-fine module.
