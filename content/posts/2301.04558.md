---
title: "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing"
date: 2023-01-11T16:35:33.000Z
author: "Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C. Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, Anton Schwaighofer, Maria Wetscherek, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, Ozan Oktay"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04558v1.webp" # image path/url
    alt: "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04558)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04558).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/learning-to-exploit-temporal-structure-for).

# Abstract
- Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities
- Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data.
- In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model.
- It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) report generation, whilst offering consistent improvements on disease classification and sentence-similarity tasks. We release a novel multi-modal temporal benchmark dataset, MS-CXR-T, to quantify the quality of vision-language representations in terms of temporal semantics. Our experimental results show the advantages of incorporating prior images and reports to make most use of the data.

# Paper Content

## Introduction

## Related work
- Vision-language processing: Self-supervised VLP can significantly reduce the need for manual labels required for the training of image encoders [18,52].
- Objectives include contrastive and discriminative image-text matching [39,52,68] including local variants [31,75], auto-regressive (AR) captioning [4,38,76] and multi-modal masked modelling objectives [13,39,60].
- Biomedical vision-language processing: Paired medical image-report datasets were originally used for supervised learning via (typically) automated label extraction from clinical reports [32,62,69]. Using such datasets, advances in general-domain self-supervised VLP have been demonstrated to benefit biomedical imaging applications [9,31,80].
- Work has incorporated ideas from general-domain VLP such as the original CLIP-style cross-modal contrastive objective [80], multi-modal masking with merged co-attention on image-text representations [45], and adaptations to the data of the domain. For example, a radiology report may have sparse image-specific details, prompting a local modification to the contrastive loss enabling alignment between text tokens and image patches [31].
- Domainspecific pre-training of the text model is shown to benefit biomedical VLP [9], and preferential masking of medical terms during masked language modelling (MLM) was explored [74]. Here we use a local loss and domain-specific pre-training of the text model, but did not find a benefit to preferential masking. Similarly, cross-attention [21] is used rather than merged co-attention for image-guided MLM.
- Longitudinal modelling of medical images: While prior images are used in unimodal supervised longitudinal analysis of medical images [36,57,67,73], temporal information has not directly been employed for self-supervision. The closest work exploits patient metadata to select positive or negative examples in unimodal contrastive learning [66,78].
- Existing models typically employ either late fusion of global image representations [57,63,67,73], which can miss fine-grained localised changes [31], or explicit spatial correspondence of features, using fixed spatial grids [47] or object detection [36].
- Registering image pairs is commonly used for change detection in other contexts [17,51,58], and has been applied to medical imaging [5,22]. For chest X-rays (CXRs) however, registration entails the ill-posed problem of aligning 2D projections of 3D geometry, which inevitably results in residual misalignment. Our approach does not rely on bounding boxes or explicit graph construc-tion as it uses self-attention of visual tokens across time to handle any spatial misalignment.
- Self-supervision across time: Self-supervision has found applications on densely-sampled time series data (e.g., video) to capture temporal information [29,54,77,79]. Our problem setting involves sparsely and sporadically sampled data where temporal pretext tasks are less applicable [2]. Similarly, it requires text supervision to enable both static and temporal learning, when temporal structure is present.

## BioViL-T training framework
- multi-image encoder
- designed to extract spatio-temporal features
- text encoder incorporating optional cross-attention on image features
- trained jointly with image-guided MLM and cross-modal global and local contrastive objectives
- resulting image and text models are later adapted for uni-or multimodal downstream tasks
- implementation details are presented in Appendices E and F

### Extracting spatio-temporal image features
- Clinical findings are often observed across different image regions and co-occur simultaneously, which requires dense level visual reasoning across time to capture both static and temporal features.
- In contrast to late global fusion [63] and bounding-box based approaches [36], BioViL-T leverages local correspondences between image regions across time using transformer self-attention blocks [20].
- Thus our method does not require an explicit image registration step between time points.
- We propose a hybrid CNN-Transformer encoder model due to its data efficiency and spatial flexibility of cross-attention across time points: where W , H, and T correspond to spatiotemporal dimensions, L = W ′ H ′ is the number of visual tokens per image, and D img is the embedding dimension.
- Here E img (e.g., ResNet-50 [30]) serves as a stem network [50] to provide visual token features of individual images.
- The CNN's inductive biases [23,50] ensure data efficiency of our hy-brid model, making it ideal for smaller scale biomedical datasets.
- E img is initialised with BioViL weights [9].
- The main purpose of A img is to capture patch embedding interactions across time when a prior image x prior img is available and to aggregate them into a fixed-length token representation.
- Input visual tokens, H curr 0 = P curr ∶= E img (x curr img ), H prior 0 ∶= E img (x prior img ) are augmented with spatial and temporal positional encodings and flattened across the spatial dimensions.
- They are then processed by K transformer encoder [65] layers A as follows: for k = 1, . . . , K, where S ∈ R L×D img denotes 2D sinusoidal positional encodings [12] and T = [t curr ; t prior ] ∈ R 2×D img is its temporal counterpart, which is learnt (Fig. 2) [4].
- The layer-normalised (LN) [6] output of the final transformer encoder block P diff ∶= LN(H curr K ) is an 'aggregated' representation of patch-level progression information anchored on the current image.
- Figure 3 shows attention roll-out [1] applied to P diff after pre-training, showing how the prior image contributes to the fused representation.
- Figure A.3 further highlights the robustness to variations in pose underlining that registration is not necessary for this encoder.
- When no prior scan is available (x ∈ D s ), A img is not used and P diff is replaced by a learnable token p miss ∈ R D img , replicated across the spatial dimensions.
- Section 4.5 later demonstrates that A img highlights the value of feature decomposition for tasks such as phrase grounding which require well-localised features [10].
- Hereafter, downstream tasks that require solely single image features, P curr , are referred to as static tasks, and the ones that benefit from additional progression information, P diff , as temporal tasks, e.g., report decoding.

### Text-supervision for spatio-temporal learning
- Let w = (w 1 , . . . , w M ) denote a vector of M tokens of a report x txt after tokenisation.
- We first obtain contextualised token features E txt (w) ∈ R M ×Dtxt by passing a sequence of text tokens w = (w 1 , . . . , w M ) through a BERT encoder E txt [19].
- The input sequence is prepended with either a [CLS] or [MLM] token associated with a downstream training objective, conditioning the output features Image representations V are extracted from single and multiple input scans (whenever available) using a hybrid CNN and transformer encoder [23,50].
- This design choice is to increase the dataefficiency and enable the fusion of temporal content without requiring image registration.
- They are later matched with their corresponding text representations obtained with CXR-BERT [9] using local [31] and global InfoNCE [48] training objectives.
- As an additional model supervision, multi-modal fused representations, obtained with cross-attention, are used for image-guided masked language modelling.
- Similar to [38,41], during training, we do two forward passes through E txt : once with masking at 45% probability (for the MLM objective) and once without masking for contrastive learning, as shown in Figure 2.
- The text encoder is initialised with the weights of CXR-BERT3 [9], a model pre-trained on domain-specific vocabulary and corpora.
- Both text and image features are later projected into a joint latent space with φ txt ∶ R Dtxt → R D , and similarly v proj w,h ∶= φ img (v w,h ) where φ img ∶ R D img → R D , with φ being a two-layer perceptron in our experiments.
- [CLS] denote the global representation of w, with r proj ∶= φ txt (r) its projected version.
- Given projected patch embeddings v proj w,h , we can compute a global cosine similarity S C (v proj , r proj ) and a local similarity using weighted pairwise cosine similarities across text tokens and projected patch embeddings [31,75].
- These similarities are used in both global and local contrastive objectives with the InfoNCE loss [48,52].
- The local loss proves crucial both for static phrase-grounding and temporal image classification (see Table 4), highlighting the importance of localised self-supervision.

### Adaptations to downstream tasks
- Adapts BioViL-T to various downstream tasks
- Uses S C (r proj , v proj w,h ) similar to [9,31]
- Uses marginalization prior to 2 -normalization for phrase-grounding and zero-shot inference
- Conditions on prior reports to contextualize report generation task
- Uses a dedicated separation token [SEP] to enhance the quality of the temporal classification dataset

## Datasets & experiments
- BioViL-T is a data efficient and adaptable VLP model that achieves SOTA performance on a diverse range of downstream tasks.
- The model is evaluated on a new multi-modal benchmark dataset, MS-CXR-T, which comprises multi-image and ground-truth label pairs across 5 findings in the field of disease progression.
- The model is compared against a range of domain-specific SOTA pre-training frameworks, including internal ablations such as removing the past report during report generation and masking prior images during phrase grounding.

### Temporal pre-training yields data efficiency
- Downstream tasks are enabled with minimal labels
- Temporal image classification results (repeated for 4 random seeds) on the MS-CXR-T benchmark for fully-supervised and zero-/few-shot (Z&F) learning settings, in terms of macroaccuracy across the three classes for each finding
- Affine registration is performed for the baseline method (denoted with suffix 'w/reg'), to partially address the pose variations across scans.
- For zero-shot classification we prompt the AR fine-tuned model with prefix: "[FINDING] is" and compare the next-token probability of words meaning 'improving', 'stable', and 'worsening' (Appendix F.4).
- Without using any labelled data, Table 2 shows that the proposed AR-based approach already yields performance superior to prior fully-supervised work [36] on temporal image classification.
- With only 10% of labels, classification fine-tuning provides a further boost, indicating that BioViL-T produces a multi-image encoder readily adapted to temporal tasks.
- Similarly, in a zero-shot report-retrieval setting, the findings show that compared to temporallyagnostic pre-training, BioViL-T leveraging prior images improves across all metrics.
- Consistent with prior work [24], the retrieved reports already preserve factuality with high CheXbert scores, more-so than the other metrics which measure fine-grained specifics of phrasing.
- This demonstrates that the latent space captures the high-level semantics of the clinical features.
- Fine-grained phrasing however will be substantially improved by AR fine-tuning.

### Static tasks benefit from temporal learning
- BioViL-T broadens the range of downstream tasks that can be applied to images
- Performance improvements are not restricted to temporal tasks - static tasks also benefit
- Text models are benchmarked against SOTA domainspecific BERT models and the results show that the proposed framework greatly increases the sensitivity of sentence embeddings to temporal content and learns to better capture the static content

### Which tokens require a prior image in MLM?
- We leverage the MLM objective in an inference setting to analyse the influence of prior images in predicting masked tokens
- Inspired by the ∆ image loss of [8], we define ∆ prior img as the change in loss by conditioning the estimation with a prior image for a given token w as follows: where l(w, x curr img , x prior img ) is the cross-entropy of predicting the masked token w given visual features (MLM loss for a single token), averaged over sentences in which w appears. ∆ prior img is a measure of how much that token benefits from access to the prior image, as well as an assessment of the contribution of the prior image to the image representation.
- In Figure 4 we show the distribution of ∆ prior img as a function of token category (e.g., Anatomy, Positional; see F.5 for annotation details). For Progression-type terms in particular, the model heavily relies on the prior image for image-guided MLM.

## Conclusion
- BioViL-T is a visionlanguage pre-training framework enabling alignment between text and multiple images
- BioViL-T makes use of a novel multi-image encoder and explicitly decomposes static-temporal features to augment the current image representation with information from prior images
- This enables the grounding of temporal references in the text
- incorporating such multi-modal temporal content provides strong learning signals to the model, resulting in richer representations and improved downstream performance
- We demonstrate the value of this paradigm through extensive experiments: BioViL-T excels on both static and temporal tasks, establishing new SOTA on report generation, temporal image classification, few/zero-shot pneumonia detection, and phrase grounding
- Furthermore, we release a new multi-modal benchmark (MS-CXR-T) to measure the quality of image and text representations in terms of temporal semantics, enabling more diverse evaluation of biomedical VLP models
