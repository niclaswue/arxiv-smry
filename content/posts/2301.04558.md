---
title: "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing"
date: 2023-01-11T16:35:33.000Z
author: "Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C. Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, Anton Schwaighofer, Maria Wetscherek, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, Ozan Oktay"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04558v1.webp" # image path/url
    alt: "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04558)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04558).


# Abstract
- Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities
- Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs
- This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data
- In this work, we explicitly account for prior images and reports when available during both training and fine-tuning
- Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model
- It is designed to be versatile to arising challenges such as pose variations and missing input images across time
- The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) report generation, whilst offering consistent improvements on disease classification and sentence-similarity tasks

# Paper Content

## Introduction
- Self-supervision from image-text pairs has enabled the development of flexible general-purpose vision-language models
- In such settings, discarding the temporal connectivity of images limits the alignment of image-text pairs, leading to suboptimal pre-training and missed opportunity to create additional model supervision for free
- Our approach exploits this domain knowledge by learning to incorporate a series of images and correlate them to reports, leading to pre-trained models that can generalise to a wider range of downstream tasks whilst achieving SOTA performance

## Related work
- Vision-language processing can significantly reduce the need for manual labels required for the training of image encoders
- The availability of large-scale paired image-text datasets has led to rapid development of general-purpose VLP models
- Objectives include contrastive and discriminative image-text matching, including local variants, auto-regressive captioning, and multi-modal masked modelling objectives
- Biomedical vision-language processing can benefit from self-supervised learning
- Cross-attention is used rather than merged co-attention for image-guided MLM
- Longitudinal modelling of medical images requires text supervision

## BioViL-T training framework
- multi-image encoder
- designed to extract spatio-temporal features
- text encoder incorporating optional cross-attention on image features
- trained jointly with image-guided MLM and cross-modal global and local contrastive objectives
- resulting image and text models are later adapted for uni-or multimodal downstream tasks
- implementation details are presented in Appendices E and F

### Extracting spatio-temporal image features
- Clinical findings are often observed across different image regions and co-occur simultaneously, which requires dense level visual reasoning across time to capture both static and temporal features.
- In contrast to late global fusion [63] and bounding-box based approaches [36], BioViL-T leverages local correspondences between image regions across time using transformer self-attention blocks [20].
- Thus our method does not require an explicit image registration step between time points.
- We propose a hybrid CNN-Transformer encoder model due to its data efficiency and spatial flexibility of cross-attention across time points: where W , H, and T correspond to spatiotemporal dimensions, L = W ′ H ′ is the number of visual tokens per image, and D img is the embedding dimension.
- Here E img (e.g., ResNet-50 [30]) serves as a stem network [50] to provide visual token features of individual images.
- The CNN's inductive biases [23,50] ensure data efficiency of our hy-brid model, making it ideal for smaller scale biomedical datasets.
- E img is initialised with BioViL weights [9].
- The main purpose of A img is to capture patch embedding interactions across time when a prior image x prior img is available and to aggregate them into a fixed-length token representation.
- Input visual tokens, H curr 0 = P curr ∶= E img (x curr img ), H prior 0 ∶= E img (x prior img ) are augmented with spatial and temporal positional encodings and flattened across the spatial dimensions.
- They are then processed by K transformer encoder [65] layers A as follows: for k = 1, . . . , K, where S ∈ R L×D img denotes 2D sinusoidal positional encodings [12] and T = [t curr ; t prior ] ∈ R 2×D img is its temporal counterpart, which is learnt (Fig. 2) [4].
- The layer-normalised (LN) [6] output of the final transformer encoder block P diff ∶= LN(H curr K ) is an 'aggregated' representation of patch-level progression information anchored on the current image.
- Figure 3 shows attention roll-out [1] applied to P diff after pre-training, showing how the prior image contributes to the fused representation.
- Figure A.3 further highlights the robustness to variations in pose underlining that registration is not necessary for this encoder.
- When a prior scan is available (x ∈ D s ), A img is not used and P diff is replaced by a learnable token p miss ∈ R D img , replicated across the spatial dimensions.
- Section 4.5 later demonstrates that A img highlights the value of feature decomposition for tasks such as phrase grounding which require well-localised features [10].
- Hereafter, downstream tasks that require solely single image features, P curr , are referred to as static tasks, and the ones that benefit from additional progression information, P diff , as temporal tasks, e.g., report decoding.

### Text-supervision for spatio-temporal learning
- The BERT encoder is used to obtain contextualized token features
- The text encoder is initialized with the weights of CXR-BERT3, a model pre-trained on domain-specific vocabulary and corpora
- Both text and image features are later projected into a joint latent space with φ txt ∶ R Dtxt → R D , and similarly v proj w,h ∶= φ img (v w,h )
- Cross-attention is used to the image features v proj w,h during this task

### Adaptations to downstream tasks
- Adapts BioViL-T to various downstream tasks
- Uses S C (r proj , v proj w,h ) similar to [9,31]
- For multiple-text prompts, projected text embeddings are marginalised prior to 2 -normalisation
- To enable language decoding, v proj w,h inputs are cross-attended by text queries w, and causal-attention is utilised between text tokens
- Different from [9,31,80], we show that report generation tasks can greatly benefit from temporal joint latent space
- Conditioning on prior reports In contrast to existing work, we incorporate the prior report as a prompt to contextualise the report generation task
- Candidate ẑ is selected as follows: ẑ = arg max z∈Z S C (v proj z , r proj ) s.t. s ẑ − s Z ẑ > δ for a margin δ

## Datasets & experiments
- BioViL-T achieves SOTA performance on a diverse range of downstream tasks by learning from data instances linked across time
- The model is evaluated on a diverse set of datasets, including a new multi-modal benchmark dataset MS-CXR-T
- Downstream evaluations are performed on a disjoint held-out test set shared across all tasks

### Temporal pre-training yields data efficiency
- Downstream tasks are enabled with minimal labels
- Temporal image classification results (repeated for 4 random seeds) on the MS-CXR-T benchmark for fully-supervised and zero-/few-shot (Z&F) learning settings, in terms of macroaccuracy across the three classes for each finding
- Affine registration is performed for the baseline method (denoted with suffix 'w/reg'), to partially address the pose variations across scans
- Fine-tuning of the AR-based approach yields performance superior to prior fully-supervised work [36] on temporal image classification
- With only 10% of labels, classification fine-tuning provides a further boost, indicating that BioViL-T produces a multi-image encoder readily adapted to temporal tasks
- In temporal image classification (Tab. 2), BioViL-T pretraining outperforms the non-temporal baseline (BioViL) and improves on previously-reported results [36] by up to 20 percentage points (pp)
- Curation of temporal training data is observed to improve the classification results by .68 pp aggregated across the findings, see Appendix A.4 for details

### Static tasks benefit from temporal learning
- BioViL-T broadens the range of applicable downstream tasks
- In this section, we demonstrate that performance improvements afforded by BioViL-T are not restricted to temporal tasks -static tasks also benefit
- Below, we report results on zero-and few-shot pneumonia classification from single images
- where BioViL-T establishes a new SOTA compared to prior work
- We see a similar trend on the MS-CXR phrase grounding benchmark (below)
- This task can be solved with single images, however we show that the inclusion of the prior image (where available) does not impair the performance of BioViL-T
- MS-CXR benchmark results (5-runs with different seeds)
- Language models acquire increased temporal sensitivity
- We hypothesise that text encoders learn temporal semantics through supervision from longitudinal image series
- To verify this, RadNLI [44] and MS-CXR-T datasets are used in a zero-shot binary classification setting
- Cosine similarity of sentence pair embeddings [55] are treated as class-logits to label each pair either as paraphrase or contradiction. See Appendix F.6 for further details
- Our text model is benchmarked against SOTA domainspecific BERT models. The results below (MS-CXR-T), collected over 4 runs, show that the proposed framework greatly increases the sensitivity of sentence embeddings to temporal content. At same time, it learns to better capture the static content (RadNLI)

### Ablation experiments
- Decomposition of static and progression features is essential to ensure good performance on single-image tasks, such as phrase grounding
- For temporal representations, on the other hand, positional encodings are essential to disambiguate the order of scans, i.e., permutation variance across time
- The local contrastive loss proves crucial to ensure meaningful language supervision during pre-training
- Use of the FINDINGS section results in only minor performance gains as the key findings are already captured in the IMPRESSION section
- The importance of prior image and report is demonstrated by the substantial drop in the "no prior image & report" ablation especially on TEM metric

### Which tokens require a prior image in MLM?
- We leverage the MLM objective in an inference setting to analyse the influence of prior images in predicting masked tokens
- Inspired by the ∆ image loss of [8], we define ∆ prior img as the change in loss by conditioning the estimation with a prior image for a given token w as follows: where l(w, x curr img , x prior img ) is the cross-entropy of predicting the masked token w given visual features (MLM loss for a single token), averaged over sentences in which w appears. ∆ prior img is a measure of how much that token benefits from access to the prior image, as well as an assessment of the contribution of the prior image to the image representation.
- In Figure 4 we show the distribution of ∆ prior img as a function of token category (e.g., Anatomy, Positional; see F.5 for annotation details). For Progression-type terms in particular, the model heavily relies on the prior image for image-guided MLM.

## Conclusion
- BioViL-T is a visionlanguage pre-training framework that enables alignment between text and multiple images
- It makes use of a novel multi-image encoder and explicitly decomposes static-temporal features to augment the current image representation with information from prior images
- This enables the grounding of temporal references in the text
- To our knowledge, this is the first method capable of leveraging the temporal content commonly present in biomedical text
- It addresses an important limitation in existing VLP approaches, which simply discard such context
- Incorporating such multi-modal temporal content provides strong learning signals to the model, resulting in richer representations and improved downstream performance
- We demonstrate the value of this paradigm through extensive experiments: BioViL-T excels on both static and temporal tasks, establishing new SOTA on report generation, temporal image classification, few/zero-shot pneumonia detection, and phrase grounding
- Furthermore, we release a new multi-modal benchmark (MS-CXR-T) to measure the quality of image and text representations in terms of temporal semantics

## A. Additional Results and Analyses

### A.1. Qualitative analysis of generated reports
- BioViL-T is able to incorporate prior study information and is able to provide factually more accurate reports especially in terms of describing temporal progression of the findings.
- This is showcased in the first two examples in the table: In the first row, BioViL-T is able to comment on not only the presence of the pleural effusion but also its improvement while BioViL fails to mention the change. In the second example, BioViL-T is able to correctly identify that there is no relevant change by comparing with the previous study, while BioViL wrongly hallucinates the tube in the current image as a new placement. BioViL-T can also avoid hallucination of the temporal information when there is no prior study.
- For instance, in the third example, BioViL-T correctly acknowledges that there is no prior image and generates the report based on information from the single current image, while BioViL hallucinates a non-exisistent prior study and wrongly generates temporal descriptions in the report.

### A.2. Further analysis on temporal classification
- The subset of the MS-CXR-T benchmark dataset is reannotated by an expert radiologist
- The expert radiologist blinds them to the existing ground-truth labels and displays only pairs of images obtained from each subject
- The analysis focuses on measuring the correlation between inter-rater agreement and image model's prediction errors
- Figure A.1 shows the dependency between the two where the x-axis corresponds to the cross entropy loss between the MS-CXR-T benchmark labels and model predictions
- We observe lower model performance on cases with smaller inter-rater reliability for the three classes in the dataset, indicating that the model's prediction errors occur more often for the cases where experts may disagree with each other.

### A.3. Self-attention visualisation
- self-attention rollout maps for pleural effusion and consolidation
- average attention weight matrices across all heads
- add the identity matrix
- keep the top 10% of attention weights per block
- in good agreement with radiologist-annotated bounding boxes
- robust to pose variations

### A.4. Data curation of imaging datasets
- The large datasets often contain instances that are mislabelled or out of distribution
- We used BioViL-T to perform pairwise ranking of instances in MIMIC-CXR and selected representative examples found in the dataset
- Our method is able to select the most appropriate image for a range of different image-acquisition or image-processing issues
- We found that many lateral acquisitions in the dataset were unexpectedly labelled as frontal
- In comparison with the study of , there has been placement of a right ij port -a -cath that extends to the lower svc. No evidence of post procedure pneumothorax. The cardiac silhouette is within normal limits and there is no vascular congestion or pleural effusion. There is some asymmetry of opacification at the bases, more prominent on the left. In the appropriate clinical setting, this could possibly represent a developing consolidation. As compared to the previous radiograph, no relevant change is seen
- The lung volumes are normal
- Normal size of the cardiac silhouette
- Normal hilar and mediastinal structures
- No pneumonia, no pulmonary edema, no pleural effusions. No previous images

## B. Temporal aspects of the MIMIC-CXR v.2 dataset
- Subjects in the MIMIC-CXR dataset often have multiple associated studies that happened at different times.
- A study, sometimes referred to as an 'exam' or 'procedure', refers to "one or more images taken on a single visit to a medical facility"7 .
- To assess pathology progression, radiologists compare images (also referred to as 'scans' or 'series') from different studies.
- In the MIMIC-CXR dataset, each study (with one or more images) is accompanied by the report written by the radiologist.
- Figure B.1 represents the distribution of studies per subject within MIMIC-CXR and the corresponding cumulative distribution function, showing that 67 % of the subjects have at least two different associated studies (and therefore at least two images acquired at different stages of the disease).
- Another way to quantify temporal information in MIMIC-CXR is through the progression labels provided by the Chest ImaGenome dataset [71].
- These progression labels are extracted from the reports and thus identify the cases when the radiologist explicitly describes changes.
- We found that in MIMIC, around 40 % of the reports are associated with a progression label from any of the available findings defined by ImaGenome.
- The MS-CXR-T temporal image classification contains progression labels for five findings (Consolidation, Edema, Pleural Effusion, Pneumonia and Pneumothorax) across three progression classes (Improving, Stable, and Worsening).
- This benchmark builds on the publicly available Chest ImaGenome gold and Chest ImaGenome silver datasets [71] which provide progression labels automatically derived from radiology reports.
- We collected a set of studies that are part of the ImaGenome silver dataset, excluding any studies that had been previously verified as part of the ImaGenome gold dataset. Additionally, we excluded studies where there are multiple progression labels for a single pathology (e.g. left pleural effusion has increased, right pleural effusion remains stable).
- We conducted a review process of the selected candidates, asking a board certified radiologist to either accept or reject the label.
- To inform their review of the labels, the radiologist was given access to the radiology report for the current image, and the sentence from which the auto generated label had been extracted.
- After collecting our curated labels and labels from the ImaGenome gold dataset, we matched the report-based labels to specific image pairs, performing a second data curation step to create the image dataset.
- To ensure the diagnostic quality of all images in the dataset, if a study had multiple frontal scans we performed a quality control step asking a radiologist to select the best image for each study.
- The class distribution for the image classification task in MS-CXR-T is shown in Tab. C.1.

### C.2. Temporal sentence similarity
- The sentence "A study, sometimes referred to as an 'exam' or 'procedure', refers to "one or more images taken on a single visit to a medical facility" (adapted from https://ncithesaurus. nci.nih.gov/)" is a paraphrase of the sentence "A study refers to one or more images taken on a single visit to a medical facility."
- The sentence "Note that 67 % of subjects have at least two studies that happened at different times." is a contradiction of the sentence "Only 33 % of subjects have at least two studies that happened at different times."
- The sentence "After creating candidate sentence pairs, we manually filtered out sentence pairs with ambiguous differences in terms of disease progression." is a contradiction of the sentence "We only created sentence pairs if the sentences had no ambiguous differences in terms of disease progression."
- The sentence "A board certified radiologist then annotated each sentence pair as either paraphrase or contradiction." is a contradiction of the sentence "A board certified radiologist only annotated sentence pairs as either paraphrase or contradiction."

## D. Temporal entity matching
- Extract entities (tagged as "observation" or "observation modifier") from the text
- Review a list of temporal entities that indicate progression by an expert radiologist
- Calculate global precision (p E ) and global recall (r E )

### D.2. List of temporal keywords
- The list of temporal keywords used to compute the TEM score are as follows: {bigger, change, cleared, constant, decrease, decreased, decreasing, elevated, elevation, enlarged, enlargement, enlarging, expanded, greater, growing, improved, improvement, improving, increase, increased, increasing, larger, new, persistence, persistent, persisting, progression, progressive, reduced, removal, resolution, resolved, resolving, smaller, stability, stable, stably, unchanged, unfolded, worse, worsen, worsened, worsening, unaltered}.
- The models are trained in a distributed setting across 8 GPU cards.
- For pre-training, we use a batch size of 240 (30 * 8 GPUs) and the AdamW optimizer [42].
- We use a linear learning rate scheduler with a warm-up proportion of 0.03 and base learning rate of 2 × 10 −5 .
- We train for a maximum of 50 epochs and use validation set loss for checkpoint selection.
- The overall loss is a sum of components with weighting factors: global contrastive (1.0), local contrastive (0.5), and image-guided MLM (1.0) respectively, see Sec. 3.1 for further details on their formulation.
- Following [9] we use sentence permutation as text-based data augmentation.
- Similarly, spelling errors in the reports are corrected prior to tokenisation of the text data
- For image augmentations, note that we apply the same augmentation to current and prior images to prevent severe misalignment.
- We resize the shorter edge to 512 and centrecrop to (448, 448).
- We apply random affine transformations (rotation up to 30 ○ and shear up to 15 ○ ) and colour jitter (brightness and contrast).
- We train with distributed data processing (DDP) on eight NVIDIA Tesla V100s with 32GB of memory each.
- To handle inconsistently-present prior images with DDP, we define a custom batch sampler.
- This sampler is a mixture of two samplers, in proportion to their dataset coverage: a sampler which produces batches with only multi-image examples -(x curr img , x prior img , x curr txt ) ∈ D m and one with only singleimage examples -(x curr img , ∅, x curr txt ) ∈ D s .
- Each GPU then processes a batch which is entirely single or multi-image, avoiding branching logic within the forward pass and enabling an efficient single pass through the CNN to process all input images (current or prior) by concatenating them along the batch dimension.
- We confirmed that although the custom sampler theoretically impacts the order in which the dataset is traversed, it has a negligible effect on training metrics relative to fully random sampling.
- Since we train on eight GPUs and collect negatives across all GPUs during contrastive training, each update involves on average a representative mixture of both single-image and multi-image samples.
- Finally, following [9] we use the DICOM images from MIMIC-CXR to avoid JPEG compression artefacts.
- During fine-tuning of BioViL-T for report generation, we minimise the cross entropy loss to maximise the log likelihood of the report in an autoregressive manner given the input images.
- The model is initialised from the pretrained weights of the image encoder and the text encoder.
- Similar to the cross-modal masked language modelling task, we additionally train a linear projection layer to map the projected patch embeddings to the same hidden dimension of the text encoder, and we cross-attention layers in each transformer block.
- The difference from the masked language modelling task is that we change the bidirectional self-attention to unidirectional causal attention that can only access the past tokens.
- If trained with prior report, we pass the prior report as prefix to condition the generation of the current report (the current and prior report are separated by [SEP]), and we only back-propagate the gradients from the loss on the tokens in the current report.
- For all experiments, we train the model for 100 epochs and we chose the best checkpoint according to metrics on the validation set.
- We performed grid search for learning rate in [10 −5 , 2 × 10 −5 , 5 × 10 −5 ] and found 2 × 10 −5 to be optimal.
- We ran each experiment with 3 random seeds and report mean and standard deviation.

### F.2. Nearest-neighbour-based report retrieval
- The joint latent space learnt by BioViL-T can be used to directly perform report retrieval
- Given the test image, we retrieve its semantically closest report from the training set in the joint latent space.
- Specifically, we encode each test image with the image model in BioViL-T and collect its projected image embeddings, and similarly we encode all the reports in the training data with their projected text embeddings.
- For each test study, we compute cosine similarity between the test image embedding and all the text embeddings from the training set in the joint latent space, and we retrieve the closest text embedding and use its correspond- ing report as the prediction.
- To evaluate the retrieval performance, we use the same decoding metrics on the retrieved reports and report results in the top section of Table 1.

### F.3. Fine-tuning for temporal image classification
- The training dataset has 5 different pathology labels and is filtered to only include image pairs with multiple directions of progression
- The fully supervised and few-shot settings of BioViL-T are finetuned on a subset of the Chest ImaGenome silver dataset to predict progression labels
- The training dataset has a size of 128 and the fully supervised setting uses a multilayer classification head
- The learning rate for the fully supervised setting is 1 × 10 −3 and for the few-shot setting it is 1 × 10 −5

### F.4. Auto-regressive prompting for zero-shot temporal image classification
- The GPT-3 style language prompting [11]
- The AR language decoding model is used to decode the fine-tuned language
- The model is characterised by a short list of tokens provided in Table F.3
- The posterior for each token in the list is normalised across the three classes, and the class with the highest score is selected as the prediction
- The overall ∆ prior img (m) is computed across all samples

### F.6. Sentence similarity experiment
- The text models are evaluated in isolation to observe if their encoding is sensitive to key clinical observations
- To achieve this, the models are probed in a zero-shot setting and the BERT output token embeddings are utilised
- The models are evaluated on two different datasets, RadNLI and MS-CXR-T

### F.7. Image registration algorithm
- Image registration is applied to pairs of images as a preprocessing step to enable a fair comparison for the baseline approaches.
- We performed bidirectional multi-scale registration between image pairs optimising an affine transformation (4 degrees of freedom), using mutual information (MI) with 128 bins as the similarity criterion.
- In more detail, the spatial transformation is characterised by four parameters: two for translation, one for isotropic scaling, and one for rotation.
- The optimisation is repeated five times with different random seeds for initialisation, and the run with the highest MI is selected to determine the final spatial alignment.
- To better identify the correspondences between the scans, bilateral filtering is applied to each image before registration to remove detailed texture whilst preserving edge information.
- Our implementation is based on the SimpleITK library.
- Figure1. (a) Existing visual-language pre-training approaches[9,31,80] often use only a single image for contrastive learning (e.g., InfoNCE[48]).
- In such settings, discarding the temporal connectivity of images limits the alignment of image-text pairs as shown with the affinity matrix, leading to suboptimal pre-training and missed opportunity to create additional model supervision for free.
- Our approach exploits this domain knowledge by learning to incorporate a series of images and correlate them to reports, leading to pre-trained models that can generalise to a wider range of downstream tasks whilst achieving SOTA performance.
- Figure2. The proposed self-supervised VLP training framework BioViL-T: Image representations V are extracted from single and multiple input scans (whenever available) using a hybrid CNN and transformer encoder[23,50].
- This design choice is to increase the dataefficiency and enable the fusion of temporal content without requiring image registration.
- They are later matched with their corresponding text representations obtained with CXR-BERT[9] using local[31] and global InfoNCE[48] training objectives.
- As an additional model supervision, multi-modal fused representations, obtained with cross-attention, are used for image-guided masked language modelling.
- Attention rollout maps[1] from the reference patch (marked with ★) to the current and prior images.
- The bounding boxes, annotated by a radiologist, show the extent of consolidation.
- Note that the reference patch attends to its anatomical neighbourhood in the prior image despite the misalignment between prior and current images.
- The grid (14 × 14) represents the visual tokens processed in the transformer encoder blocks.
- Figure A.4a). Some images contained only noise (Figure A.4b), non-human samples (Figures A.4d and A.4e) or incorrect anatomy (Figure A.4g).
- Often, acquisitions with an incomplete field of view (FOV) (i.e., the lungs are not completely visible) were repeated (Figure A.4c).
- Lastly, post-processed images were Reference (by radiologist) BioViL BioViL-T No evidence of acute cardiopulmonary process. Decreased right pleural effusion. Small right pleural effusion. Small right pleural effusion, decreased since . No relevant change as compared to the previous image. No evidence of pneumonia or other parenchymal pathology. Normal size of the heart. No pleural effusions.
- Figure A.2. Self-attention rollout maps[1] from the reference patch (marked with ★) to the current and prior images, overlaid on example cases of (a) improving, (b) stable and (c) worsening pleural effusion (top row) and consolidation (bottom row).
- The bounding boxes, annotated by a radiologist, show the area corresponding to the pathology.
- The centre patch in the bounding box for the current image was selected as reference.
- The grid (14 × 14) represents the visual tokens processed in the transformer encoder blocks.
- Fig. F.1 shows examples from the benchmark across different pathologies and progression labels.
- Figure A.4. Pairwise ranking of images performed by the proposed data curation method (see Section 3.3) on images from the MIMIC-CXR v2 dataset. Images highlighted with dashed green rectangles are automatically selected by our method and used for training to improve model's downstream performance.
- The rejected image samples may not be appropriate for training due to image acquisition or image processing issues as shown in each subfigure above.
