---
title: "Data Distillation: A Survey"
date: 2023-01-11T02:25:10.000Z
author: "Noveen Sachdeva, Julian McAuley"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-04272v1.webp" # image path/url
    alt: "Data Distillation: A Survey" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.04272)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.04272).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/data-distillation-a-survey).

# Abstract
- The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets.
- Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as high model-training time; slow research iteration; and poor eco-sustainability.
- As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc.
- In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.

# Paper Content

## Introduction
- Data distillation is a task that aims to synthesize tiny and high-fidelity data summaries which distill the most important knowledge from a given target dataset.
- A terse, high-quality data summary has use cases from a variety of standpoints, including a faster model-training procedure.
- Faster model training equates to (1) computecost saving and expedited research iterations, i.e., the investigative procedure of manually experimenting different ideas; and (2) improved eco-sustainability, i.e., lowering the amount of compute time directly leads to a lower carbon footprint from running power-hungry accelerated hardware.

## The Data Distillation Framework
- Outline notation for data distillation
- Define an -approximate data summary, and the data distillation task as follows: existing data distillation approaches.
- Inspired by the definition of coresets (Bachem et al., 2017), we formally define an -approximate data summary.

### Data Distillation by Meta-model Matching
- Meta-model matching-based data distillation approaches optimize for the transferability of models trained on the data summary when generalized to the original dataset.
- The key simplifying assumption for this family of methods is that a perfect classifier exists and can be estimated on Plugging the second assumption along with the iid assumption of D in Equation ( 2) directly translates to Equation (3).
- Despite the assumption, Equation ( 3) is highly expensive both in terms of computation time and memory, due to which, methods from this family typically resort to making further assumptions.
- A separate line of work focuses on using Neural Tangent Kernel (NTK) based algorithms to solve the inner-loop in closed form.
- As a brief side note, the infinite-width correspondence states that performing Kernelized Ridge Regression (KRR) using the NTK of a given neural network, is equivalent to training the same ∞-width neural network with L2 reconstruction loss for ∞ SGD-steps.
- These "∞-width" neural networks have been shown to perform reasonably compared to their finite-width counterparts, while also being solved in closed-form.
- KIP uses the NTK of a fully-connected neural network (Nguyen et al., 2021a), or a convolutional network (Nguyen et al., 2021b) in the inner-loop of Equation (3) for efficient data distillation.
- More formally, given the NTK K : X × X → R of a neural network architecture, KIP optimizes the following objective: arg min
- FRePO (Zhou et al., 2022b) decouples the feature extractor and a linear classifier in Φ, and alternatively optimizes (1) the data summary along with the classifier, and (2) the feature extractor.

### Data Distillation by Gradient Matching
- Gradient matching based data distillation, at a high level, performs one-step distance matching on (1) the network trained on the target dataset (D) vs. (2) the same network trained on the data summary (D syn ).
- In contrast to the meta-model matching framework, such an approach circumvents the unrolling of the inner-loop, thereby making the overall optimization much more efficient.
- First proposed by Zhao et al. (2021) (DC), data summaries optimized by gradient-matching significantly outperformed heuristic data samplers, principled coreset construction techniques, as well as TBPTT-based data distillation proposed by Wang et al. (2018).
- Formally, given a learning algorithm Φ, DC solves the following optimization objective: where T accounts for model similarity T -steps in the future, and ), i.e., approximate θ D t as a single gradient-descent update on θ Dsyn t−1 using D rather than θ D t−1 (Figure 3).
- Subsequently, numerous other approaches have been built atop this framework with subtle variations.
- DSA (Zhao & Bilen, 2021) improves over DC by performing the same image-augmentations (e.g., crop, rotate, jitter, etc.) on both D and D syn while optimizing Equation (7).
- Since these augmentations are universal and are applicable across data distillation frameworks, augmentations performed by DSA have become a common part of all methods proposed henceforth, but we omit them for notational clarity.
- DCC (Lee et al., 2022b) further modifies the gradient-matching objective to incorporate class contrastive signals inside each gradient-matching step and is shown to improve stability as well as performance.
- With θ t evolving similarly as in Equation ( 7), the modified optimization objective for DCC can be written as:
- Most recently, Kim et al. (2022) (IDC) extend the gradient matching framework by: (1) multi-formation: to synthesize a higher amount of data within the same memory budget, store the data summary (e.g., images) in a lower resolution to remove spatial redundancies, and upsample (using e.g., bilinear, FSRCNN (Dong et al., 2016)) to the original scale while usage; and (2) matching gradients of the network's training trajectory over the full dataset D rather than the data summary D syn .
- To be specific, given a k× upscaling function f : R d×d → R kd×kd , the modified optimization objective for IDC can be formalized as:
- Update by minimizing: Minimize: where D : R |θ| × R |θ| → R is a distance metric of choice (typically L2 distance).
- Such an optimization can intuitively be seen as optimizing for similar quality models trained with N SGD steps on D syn , compared to M N steps on D, thereby invoking long-horizon trajectory matching.
- Notably, calculating the gradient of Equation ( 10) w.r.t. D syn encompasses gradient unrolling through N -timesteps, thereby limiting the scalability of MTT.
- On the other hand, since the trajectory of training Φ θ on D, i.e., {θ D t } T t=0 is independent of the optimization of D syn , it can be pre-computed for various θ 0 ∼ P θ initializations and directly substituted.
- Similar to gradient matching methods (Section 2.2), the trajectory matching framework also optimizes the first-order distance between parameters, thereby inheriting the local smoothness assumption.
- As a scalable alternative, Cui et al. (2022b) proposed TESLA, which re-parameterizes the parameter-matching loss of MTT in Equation (10) (specifically when D is set as the L2 distance), using linear algebraic manipulations to make the bilevel optimization's memory complexity independent of N .
- Furthermore, TESLA uses learnable soft-labels (Y syn ) during the optimization for an increased compression efficiency.

### Data Distillation by Distribution Matching
- Even though the aforementioned gradient-matching or trajectory-matching based data distillation techniques have been empirically shown to synthesize high-quality data summaries, the underlying bilevel optimization, however, is oftentimes an expensive procedure both in terms of computation time and memory.
- To this end, distribution-matching techniques solve a correlated proxy task which restricts the optimization to a single-level, leading to a much improved scalability.
- More specifically, instead of matching the quality of models on D vs. D syn , distribution-matching techniques directly match the distribution of data in D vs. D syn .
- The key assumption for this family of methods is that two datasets which are similar according to a particular distribution divergence metric, also lead to similarly trained models.
- First proposed by Zhao & Bilen (2023), DM uses (1) numerous parametric encoders to cast high-dimensional data into respective low-dimensional latent spaces; and (2) an approximation of the Maximum Mean Discrepancy to compute the distribution mismatch between D and D syn in each of the latent spaces.
- More precisely, given a set of k encoders E {ψ i : X → X i } k i=1 , the optimization objective can be written as: DM uses a set of randomly initialized neural networks (with the same architecture) to instantiate E. They observe similar performance when instantiated with more meaningful, task-optimized neural networks, despite it being much less efficient.
- CAFE (Wang et al., 2022) further refines the distribution-matching idea by: (1) solving a bilevel optimization problem for jointly optimizing a single encoder (Φ) and the data summary, rather than using a pre-determined set of encoders (E); and (2) assuming a neural network encoder (Φ), match the latent representations obtained at all intermediate layers of the encoder instead of only the last layer.
- Formally, given a (L + 1)-layer neural network Φ θ : X → Y where Φ l θ represents Φ's output at the l th layer, the optimization problem for CAFE can be specified as: where p(•|•, θ) intuitively represents the nearest centroid classifier on D syn using the latent representations obtained by last layer of Φ θ .
- Analogously, IT-GAN (Zhao & Bilen, 2022) also uses the distribution-matching framework in Equation ( 11) to generate data that is informative for model training, in contrast to the traditional GAN (Goodfellow et al., 2014) which focuses on generating realistic data.

### Data Distillation by Factorization
- All data distillation frameworks intrinsically maintain the synthesized data summary as a large set of free parameters, which are in turn optimized.
- Factorization-based data distillation techniques parameterize the data summary using two separate components: (1) bases: a set of mutually independent base vectors; and (2) hallucinators: a mapping from the bases' vector space to the joint data-and label-space.
- In turn, both the bases and hallucinators are optimized for the task of data distillation.
- Formally, let B {b i ∈ B} |B| i=1 be the set of bases, and H {h i : B → X × Y} |H| i=1 be the set of hallucinators, then the data summary is parameterized as D syn {h(b)} b∼B, h∼H .
- Even though such a two-pronged approach seems similar to generative modeling of data, note that unlike classic generative models, (1) the input space consists only of a fixed and optimized set of latent codes and isn't meant to take any other inputs; and (2) given a specific B and H, we can generate at most |B| • |H| sized data summaries.
- Notably, such a hallucinator-bases data parameterization can be optimized using any of the aforementioned data optimization frameworks (Sections 2.1 to 2.4)

## Applications
- Data distillation is a promising solution for differential privacy
- Data distillation can perform better than existing state-of-the-art differentially-private data generators
- Data distillation does not perform well when evaluating diverse architectures on bigger NAS test-beds

## Challenges & Future Directions
- data distillation is a field of computer science that aims to reduce the size of a dataset to make it easier to train a machine learning model
- existing data distillation techniques are limited to image classification, due to the availability of datasets and the ease of data optimization
- there are many important tasks that can be done with a high-quality data summary, but current data distillation techniques are not able to scale to larger data sets
- a unifying thread across data distillation techniques is an underlying bilevel optimization that is NP-hard
- data distillation by trajectory matching was proposed by Cazenavette et al. and is able to match the training trajectories of models trained on D vs. D syn in polynomial time.
