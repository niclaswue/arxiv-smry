---
title: "Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages"
date: 2023-03-23T18:16:30.000Z
author: "Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde, Skyler Wang, Samuel Cahyawijaya and 6 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-13592v1.webp" # image path/url
    alt: "Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.13592)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.13592).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/prompting-large-language-models-to-generate).

# Abstract
- Code-mixing is a common linguistic practice in many parts of the world.
- Collecting high-quality and low-cost code-mixed data is a challenge for NLP research.
- LLMs can be used for data generation.
- LLMs can generate code-mixed data for 5 languages in SEA.
- ChatGPT shows the most potential for code-mixing.
- Word choice errors lead to semantic inaccuracies.
- Other multilingual models are unable to produce code-mixed texts.
- LLMs have limited promises in data-scarce NLP contexts.

# Paper Content

## Introduction
- Code-mixing is the practice of alternating between two or more languages in an utterance or conversation.
- It allows individuals to express culturally-specific ideas, connect with or differentiate from other interlocutors, and reify their identities.
- Computational research into code-mixing has only recently picked up steam.
- Acquiring high-quality and low-cost code-mixed data is logistically demanding and costly.
- Large Language Models (LLMs) can be used to ameliorate data scarcity in code-mixing research.
- We test whether multilingual LLMs can be prompted to create code-mixed data.
- We hone in on languages in South East Asia (SEA) due to their extended histories of language and culture cross-fertilization.
- Multilingual NLP research has devoted scant attention to examining code-mixing in this region.
- We prompt LLMs to automatically generate code-mixed text.
- Chat-GPT is able to follow instructions and generate syntactically sound code-mixed texts for the five SEA languages up to 68% success rate.
- Code-mixing capabilities of these models are dampened by word choice errors that lead to semantic inaccuracies.
- Relying on LLMs to generate synthetic code-mixed data without extensive human supervision is discouraged.

## Methodology

### Prompting language models
- Collected synthetic code-mixed data by prompting LLMs with natural language requests
- Prompts specified code-mixing between English and Indonesian, Malay, Mandarin, Tagalog, or Vietnamese
- Focused on code-mixing English with SEA languages
- Used OpenAI and HuggingFace's API for prompting
- 180 unique prompts per language model
- Evaluated LLMs capability of producing intrasentential code-mixed text
- Native speakers manually annotated responses on a scale from 0 to 3
- 0 -No code-mixing
- 1 -Loanword usage
- 2 -Topic-related entities
- 3 -Beyond entity
- Native speakers flagged outputs suffering from semantic inaccuracies or fluency issues

### Accurateness
- ChatGPT may fail to produce correctly code-mixed text when the nationality of the speaker is mentioned.
- ChatGPT may hallucinate words that do not exist in the code-mixed text.
- ChatGPT may assign words or phrases to the wrong language.
- ChatGPT performs poorly on language identification tasks for low-resource languages.

## Results

### English-sea languages
- ChatGPT outperforms other language models in generating code-mixed data
- ChatGPT is better at code-mixing beyond topic-related entities
- InstructGPT has some capability to generate code-mixed texts
- Other two publicly available multilingual LLMs perform poorly
- ChatGPT and InstructGPT can use either English or a SEA language as the matrix language
- ChatGPT and InstructGPT tend to code-mix with loanwords when the topic is about AI
- ChatGPT mentions the word "nasi goreng" for all English-Indonesian responses
- ChatGPT may generate unnatural conversations with more than two speakers

### Singlish
- Chat-GPT and InstructGPT (davinci-003) have up to a 96% success rate in generating Singlish sentences.
- Flan-T5-XXL and BLOOMZ have a near-zero success rate.
- Chat-GPT is capable of translating Singlish expressions into Standard English expressions.
- ChatGPT and davinci-003 significantly outperform davinci-002 and other multilingual LLMs in generating Singlish text.
- ChatGPT outputs Singlish expressions and code-mixed phrases.
- Native speakers found the analogy between familial connections and the dish to be semantically confusing.

## Discussion
- Deploying LLMs for low-resourced data generation
- Multilingual â‰  code-mix compatible
- Research transparency needed
- Multimodal applications
- Code-mixing in LLMs can help people communicate more naturally
- Removing need to adjust speech patterns to be legible to machines

## Related work
- Limited number of human-curated code-mixed data in SEA
- Existing code-mixing studies in SEA cover several language pairs
- Current corpus does not scratch the surface of code-mixedness in SEA
- Attempt to close gap by exploring potential of generating synthetic code-mixed data for SEA region
- Previous attempts to generate synthetic code-mixed sentences through word alignment and candidate selection
- Assess novel way of generating synthetic code-mixed sentences without need for parallel corpora or machine translation models

## Conclusion
- ChatGPT and Instruct-GPT outperform other multilingual LLMs when generating code-mixed texts for South East Asian languages
- BLOOMZ and Flan-T5-XXL are not able to generate code-mixed data
- Chat-GPT has issues with accuracy, reliability, and fluency
- 180 prompts across five topics and six SEA languages were used
- Automatically generate more codemixed samples
- Compare multilingual models that are not finetuned with instructions
- Involvement of native speakers in tasks is key
- Use code-mixed prompt templates
- Analyze data with automated metrics
- Generate code-mixed data for non-English language pairs
- Talk about weather using both English and Vietnamese in a single sentence
