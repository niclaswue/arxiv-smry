---
title: "A Survey on Transformers in Reinforcement Learning"
date: 2023-01-08T14:04:26.000Z
author: "Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, Deheng Ye"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03044v1.webp" # image path/url
    alt: "A Survey on Transformers in Reinforcement Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03044)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03044).


# Abstract
- Transformer neural architecture is the most dominant in NLP and CV, but it is facing unique design choices and challenges in RL
- Recent surge of using Transformers in RL has not been well unraveled, so in this paper, we seek to systematically review motivations and progress on using Transformers in RL
- We provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.

# Paper Content

## Introduction
- Reinforcement learning provides a mathematical formalism for sequential decision-making
- By utilizing RL, we can acquire intelligent behaviors automatically
- While RL has provided a general framework for learning-based control, the introduction of deep neural networks, as a way of function approximation with high capacity, is enabling significant progress along a wide range of domains
- Among its notable benefits, the Transformer architecture enables modeling long dependencies and has excellent scalability
- Inspired by the success of SL, there has been a surge of interest in applying Transformers in reinforcement learning, with the hope of carrying the benefits of Transformers to the RL field
- The use of Transformers in RL dates back to Zambaldi et al. [2018b], where the self-attention mechanism is used for relational reasoning over structured state representations
- Afterward, many researchers seek to apply self-attention for representation learning to extract relations between entities for better policy learning
- Besides leveraging Transformers for state representation learning, prior works also use Transformers to capture multi-step temporal dependencies to deal with the issue of partial observability
- More recently, offline RL [Levine et al., 2020] has attracted attention due to its ability to leverage offline large-scale datasets
- Motivated by offline RL, recent efforts have shown that the Transformer architecture can serve directly as a model for sequential decisions
- In this paper, we seek to provide a comprehensive overview of TransformRL, including a taxonomy of current methods and the challenges. We also discuss future perspectives, as we believe the field of TransformRL will play an important role in unleashing the potential impact of reinforcement learning

## Problem Scope

### Reinforcement Learning
- In general, Reinforcement Learning (RL) considers learning in a Markov Decision Process (MDP)
- M = S, A, P, r, γ, ρ 0 , where S and A denote the state space and action space respectively, P (s |s, a) is the transition dynamics, r(s, a) is the reward function, γ ∈ (0, 1) is the discount factor, and ρ 0 is the distribution of initial states
- Typically, RL aims to learn a policy π(a|s) to maximize the expected discounted return J(π) = E π,P,ρ0 [ t r(s t , a t )].
- There are many important topics in this area, for instance, meta RL, multi-task RL, and multi-agent RL.
- In contrast to model-free RL which directly learns the policy and value functions, model-based RL learns an auxiliary dynamic model of the environment
- Such a model can be directly used for planning algorithms [Schrittwieser et al., 2020], or it can be used as a generator to produce imaginary trajectories and enlarge the training data for any model-free algorithm [Hafner et al., 2019].
- Learning a model is non-trivial, especially in large or partially observed environments where we first need to construct the representation of the state.
- Some recent methods propose to use latent dynamics [Hafner et al., 2019] or value models [Schrittwieser et al., 2020] to address these challenges and improve the sample efficiency of RL.

### Transformers
- Transformer models sequential data by incorporating self-attention
- The key idea is to incorporate a mechanism that can capture dependencies within long sequences
- Formally, given a sequential input with n tokens, the self-attention layer maps each token to a query, a key, and a value.

### Combination of Transformers and RL
- Transformers can be used as one component for RL algorithms
- Transformers can also serve as one whole sequential decision-maker

## Network Architecture in RL
- reviewed early progress of network architecture design in RL
- summarized their challenges
- found that general techniques of neural networks (e.g., regularization, skip connection, batch normalization) can be applied to RL
- found that increasing input dimensionality while using an online feature extractor to boost state representation helps improve the performance and sample efficiency of DRL algorithms

### Architectures for function approximators
- 2020: propose a deep dense architecture for DRL agents
- 2021: use DenseNet with decoupled representation learning to improve flows of information and gradients for large networks
- Recently, due to the superior performance of Transformers, some researchers have attempted to apply Transformers architecture in policy optimization algorithms, but found that the vanilla Transformer design fails to achieve reasonable performance in RL tasks

### Challenges
- Transformer-based architectures have made rapid progress in SL domains, but applying them in RL is not straightforward
- There exist several unique challenges, such as the sensitivity of RL algorithms to the architecture of deep neural networks
- Recently, many researchers aim to make improvements around computational and memory efficiency upon the original Transformer architecture, but most of these works focus on SL domains

## Transformers in RL
- Most early attempts of TransformRL apply Transformers for state representation learning or providing memory information while still applying the standard RL algorithms for agent learning such as temporal difference learning and policy optimization.
- Offline RL makes it possible to learn optimal policy from large-scale offline data. Inspired by offline RL, recent works further treat the RL problem as a conditional sequence modeling problem on fixed experiences.
- By doing so, it helps to bypass the challenges of bootstrapping error in traditional RL, consequently enabling the Transformer architecture to unleash its powerful sequential modeling ability.

### Transformers for representation learning
- Various sequences in RL tasks require processing, such as local per-timestep sequence (multi-entity sequence [Vinyals et al., 2019;Baker et al., 2019], multi-agent sequence [Wen et al., 2022]), temporal sequence (trajectory [Parisotto et al., 2020;Banino et al., 2021]) and so on.
- The early notable success of this method is embodied in using Transformers to process complex information from a variable number of entities scattered in the agent's observation.
- Zambaldi et al. [2018a] first propose to capture relational reasoning over structured observation with multi-head dot-product attention, which is subsequently used in AlphaStar [Vinyals et al., 2019] to process multi-entity observation in the challenging multi-agent StarCraft II environment.
- In such a mechanism, called entity Transformer, the observation is encoded in the form: where e i represents the agent's observation on entity i either directly sliced from the whole observation or given by an entity tokenizer.
- Several follow-up works have enriched entity Transformer mechanisms.
- Hu et al. [2020] propose a compatible decoupling policy to explicitly associate actions to various entities and exploit an attention mechanism for policy explanation.
- To solve the challenging one-shot visual imitation, Dasari and Gupta [2021] use Transformers to learn a representation focusing on task-specific elements.
- Similar to entities scattered in observation, some works exploit Transformers to process other local per-timestep sequences.
- Tang and Ha [2021] leverage the attention mechanism of Transformers to process sensory sequence and construct a policy that is permutation invariant w.r.t. inputs.
- In the incompatible multi-task RL setting, Transformer is proposed to extract morphological domain knowledge [Kurin et al., 2020].
- Meanwhile, it is also reasonable to process temporal sequence with Transformers. Such a temporal encoder works as a memory architecture, where o t represents the agent's observation at timestep t and Emb 0:t represents the embedding of historical observations from initial observation to current observation.
- In the early work, Mishra et al. [2018] fail to process temporal sequence with vanilla Transformers and find it even worse than random policy in some certain tasks.
- Gated Transformer-XL (GTrXL) [Parisotto et al., 2020] is the first efficacious scheme to use Transformer as a memory architecture to process trajectories.
- GTrXL modifies Transformer-XL architecture [Dai et al., 2019] with Identity Map Reordering to provide a 'skip' path from temporal input to the Transformer output, which may conduce to a stabilizing training procedure from the beginning.
- Furthermore, Loynd et al. [2020] propose a shortcut mechanism with memory vectors for long-term dependency and Irie et al. [2021] combine the linear Transformer with Fast Weight Programmers for better performance.
- In addition, Melo [2022] proposes to use the self-attention mechanism to mimic memory reinstatement for memory-based meta RL.
- While Transformer outperforms LSTM/RNN as the memory horizon grows and parameter scales, it suffers from poor data efficiency with RL signals.
- Follow-up works exploit some auxiliary (self-)supervised tasks to benefit learning [Banino et al., 2021] or use pre-trained Transformer architecture as a temporal encoder [Li et al., 2022;Fan et al., 2022].

### Transformers for model learning
- Transformer architecture is the backbone of the environmental model in some model-based algorithms
- The success of Dreamer and subsequent algorithms [Hafner et al., 2020[Hafner et al., , 2021;;Seo et al., 2022] has demonstrated the benefits of the world model conditioned on history in some partially observable environments or in some tasks that require a memory mechanism.
- A world model conditioned on history consists of an observation encoder to capture abstract information and a transition model to learn the transition in latent space
- It is true that both RNN and Transformer are compatible with learning a world model conditioned on historical information, but Transformer architecture is more data-efficient

### Transformers for sequential decision-making
- Transformer is an expressive architecture to be plugged into components of traditional RL algorithms
- Transformer can serve as a model that conducts sequential decisionmaking directly
- Transformer as a milestone for offline RL
- One challenge for Transformers to be widely used in RL is that the non-stationarity during the training process may hinder its optimization
- However, the recent prosperity in offline RL motivates a growing number of works focusing on training a Transformer model on offline data that can achieve state-of-the-art performance
- Decision Transformer (DT) [Chen et al., 2021] first applies this idea by modeling RL as an autoregressive generation problem to produce the desired trajectory: where Rt = T t =t r(s t , a t ) is the return-to-go.
- By conditioning on the proper target return value at the first timestep, DT can generate desired actions without explicit TD learning or dynamic programming.
- Concurrently with this work, Trajectory Transformer (TT) [Janner et al., 2021] adopts a similar Transformer structure, but alternatively proposes to use beam search for planning during execution.
- The empirical results demonstrate that TT performs well on longhorizon prediction. Moreover, TT shows that with mild adjustments on vanilla beam search, TT can perform imitation learning, goal-conditioned RL, and offline RL under the same framework.
- Regarding the behavior cloning setting, Behavior Transformer (BeT) [Shafiullah et al., 2022] proposes a similar Transformer structure as TT to learn from multi-modal datasets.
- In light of Transformer's superior accuracy on sequence prediction, Bootstrapped Transformer (BooT) [Wang et al., 2022] proposes to bootstrap Transformer to generate data while optimizing it for sequential decision-making.
- Bootstrapping Transformer for data augmentation can expand the amount and coverage of offline datasets, and hence achieve performance improvement.
- More specifically, BooT compares different data generation schemes and bootstrapping schemes to analyze how BooT can benefit policy learning. The results show that it can generate data consistent with the underlying MDP without additional explicit conservative constraints.
- While conditioning on return-to-go is a practical choice to incorporate future trajectory information, one natural question is whether other kinds of hindsight information can benefit sequential decision-making. To this end, Furuta et al. [2021] propose Hindsight Information Matching (HIM), a unified framework that can formulate variants of hindsight RL problems.
- More specifically, HIM converts hindsight RL into matching any pre-defined statistics of future trajectory information w.r.t. the distribution induced by the learned conditional policy.
- Furthermore, this work proposes Generalized DT (GDT) for arbitrary choices of statistics and demonstrates its applications in two HIM problems: offline multitask state-marginal matching and imitation learning.
- Specifically, one drawback of conditioning on return-to-go is that it will lead to sub-optimal actions in stochastic environments. This is because the training data may contain suboptimal actions that result in high rewards by luck due to the stochasticity of transitions.
- Paster et al. [2022] [Janner et al., 2021] IL/GCRL/Offline return-to-go beam search basic Transformer structure BeT [Shafiullah et al., 2022] BC none conditioning basic Transformer structure BooT [Wang et al., 2022] Offline return-to-go beam search data augmentation GDT [Furuta et al., 2021] HIM arbitrary conditioning anti-causal aggregator ESPER [Paster et al., 2022] Offline (stochastic) expected return conditioning adversarial clustering DoC [Yang et al., 2022] Offline (stochastic) learned representation conditioning additional latent value func. QDT [Yamagata et al., 2022] Offline relabelled return-to-go conditioning additional Q func. StARformer [Shang et al., 2022] IL/Offline return-to-go/reward conditioning Step Transformer ConDT [Konan et al., 2022] Offline learned representation conditioning return-dependent transformation SPLT [Villaflor et al., 2022] Offline none min-max search separate models for world and policy ODT [Zheng et al., 2022] Online finetune return-to-go conditioning trajectory-based entropy MADT [Meng et al., 2021] Online finetune (multi-agent) none conditioning separate models for actor and critic

### Transformers for generalist agents
- Decision transformer can be used to generalize to multiple tasks
- Some works draw on the ideas of pre-training on large-scale datasets in CV and NLP, and try to abstract a general policy from large-scale multi-task datasets
- Multi-Game Decision Transformer (MGDT) [Lee et al., 2022], a variant of DT, learns DT on a diverse dataset consisting of both expert and non-expert data and achieves close-to-human performance on multiple Atari games with a single set of parameters
- In order to obtain expert-level performance with a dataset containing non-expert experiences, the expert action inference mechanism is designed in MGDT, which calculates an expert-level return-to-go posterior distribution from the prior distribution of return-to-go and a preset expert-level return-to-go likelihood proportional according to Bayesian formula
- Likewise, Switch Trajectory Transformer (SwitchTT) [Lin et al., 2022], a multi-task extension to TT, exploits a sparsely activated model that replaces the FFN layer with a mixture-of-expert layer for efficient multi-task offline learning
- Besides, a distributional trajectory value estimator is adopted to model the uncertainty of value estimates
- With these two enhanced features, SwitchTT achieves improvement over TT across multiple tasks in terms of both performance and training speed
- MGDT and SwitchTT exploit experiences collected from multiple tasks and various performance-level policies to learn a general policy
- Yet, constructing a large-scale multi-task dataset is non-trivial. Unlike large-scale datasets in CV or NLP, which are usually constructed with massive public data from the Internet and simple manual labeling, action information is always absent from public sequential decision-making data and is not facile to label
- Baker et al. [2022] propose a semi-supervised scheme to utilize large-scale online data without action information and the key is to learn a Transformer-based Inverse Dynamic Model (IDM), which predicts the action information with past and future observations and is consequently capable of labeling massive unlabeled online video data
- IDM is learned on a small-scale dataset containing manually labeled actions and is accurate enough to provide action labels of videos for effective behavior cloning and fine-tuning
- The efficacy of prompting [Brown et al., 2020] for adaptation to new tasks has been proven in many prior works in NLP
- Following this idea, several works aim at leveraging prompting techniques for DT-based methods to enable fast adaptation
- Prompt-based Decision Transformer (Prompt-DT) [Xu et al., 2022] samples a sequence of transitions from the few-shot demonstration dataset as prompt, and shows that it can achieve few-shot policy generalization on offline meta RL tasks
- Reed et al. [2022] further exploit prompt-based architecture to learn a generalist agent (Gato) via auto-regressive sequence modeling on a super large-scale dataset covering natural language, image, temporal decisionmaking, and multi-modal data
- Gato is capable of a range of tasks from various domains, including text generation and decision-making. Specifically, Gato unifies multi-modal sequences in a shared tokenization space and adapts promptbased inference in deployment to generate task-specific sequences
- Despite being effective, Laskin et al. [2022] point out that one limitation of the prompt-based framework is that the prompt is demonstrations from a well-behaved policy, as contexts in both works are not sufficient to capture policy improvement
- Instead, they propose Algorithm Distillation (AD) [Laskin et al., 2022], which instead trains a Transformer on across-episode sequences of the learning progress of single-task RL algorithms

## Summary and Future Perspectives
- Transformers can serve as a powerful module of RL, e.g., acting as a representation module or a world model
- Transformers can serve as a sequential decision-maker
- Transformers can benefit generalization across tasks and domains
- While we cover representative works on this topic, the usage of Transformers in RL is not limited to our discussions
- Given the prosperity of Transformers in the broader AI community, we believe that combining Transformers and RL is a promising trend
