---
title: "A Survey on Transformers in Reinforcement Learning"
date: 2023-01-08T14:04:26.000Z
author: "Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, Deheng Ye"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03044v1.webp" # image path/url
    alt: "A Survey on Transformers in Reinforcement Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03044)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03044).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-survey-on-transformers-in-reinforcement).

# Abstract
- Transformer has been considered the dominating neural architecture in NLP and CV, mostly under a supervised setting.
- Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL.
- However, the evolution of Transformers in RL has not yet been well unraveled.
- Hence, in this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.

# Paper Content

## Introduction
- Reinforcement learning provides a mathematical formalism for sequential decision-making
- By utilizing RL, we can acquire intelligent behaviors automatically
- While RL has provided a general framework for learning-based control, the introduction of deep neural networks, as a way of function approximation with high capacity, is enabling significant progress along a wide range of domains
- Among its notable benefits, the Transformer architecture enables modeling long dependencies and has excellent scalability
- Inspired by the success of SL, there has been a surge of interest in applying Transformers in reinforcement learning, with the hope of carrying the benefits of Transformers to the RL field
- In Section 3, we describe the evolution of network architecture in RL and the challenges that prevent the Transformer architecture from being widely explored in RL for a long time
- In Section 4, we provide a taxonomy of Transformers in RL and discuss representative existing methods
- Finally, we summarize and point out potential future directions in Section 5.

## Problem Scope

### Reinforcement Learning
- In general, Reinforcement Learning (RL) considers learning in a Markov Decision Process (MDP)
- M = S, A, P, r, γ, ρ 0 , where S and A denote the state space and action space respectively, P (s |s, a) is the transition dynamics, r(s, a) is the reward function, γ ∈ (0, 1) is the discount factor, and ρ 0 is the distribution of initial states.
- Typically, RL aims to learn a policy π(a|s) to maximize the expected discounted return J(π) = E π,P,ρ0 [ t r(s t , a t )].
- There are many important topics in this area, for instance, meta RL, multi-task RL, and multi-agent RL.
- In the following part, we introduce several specific RL problems that are closely related to advances in Transformers in RL.
- Offline RL. In offline RL [Levine et al., 2020], the agent is not allowed to interact with the environment during training. Instead, it only has access to a static offline dataset D = {(s, a, s , r)} collected by arbitrary policies.
- Without exploration, modern offline RL approaches [Fujimoto et al., 2019;Kumar et al., 2020;Yu et al., 2021b] constrain the learned policy close to the data distribution, to avoid out-of-distribution actions that may lead to overestimation.
- Recently, in parallel with typical value-based methods, one popular trend in offline RL is RL via Supervised Learning (RvS) [Emmons et al., 2021], which learns an outcomeconditioned policy to yield desired behavior via SL.
- Goal-conditioned RL. Goal-conditioned RL (GCRL) extends the standard RL problem to goal-augmented setting, where the agent aims to learn a goal-conditioned policy π(a|s, g) that can reach multiple goals.
- Prior works propose to use various techniques, such as hindsight relabeling [Andrychowicz et al., 2017], universal value function [Schaul et al., 2015], and self-imitation learning [Ghosh et al., 2019], to improve the generalization and sample efficiency of GCRL.
- GCRL is quite flexible as there are diverse choices of goals. We refer readers to [Liu et al., 2022] for a detailed discussion around this topic.
- Model-based RL. In contrast to model-free RL which directly learns the policy and value functions, model-based RL learns an auxiliary dynamic model of the environment. Such a model can be directly used for planning algorithms [Schrittwieser et al., 2020], or it can be used as a generator to produce imaginary trajectories and enlarge the training data for any model-free algorithm [Hafner et al., 2019].
- Learning a model is non-trivial, especially in large or partially observed environments where we first need to construct the representation of the state.
- Some recent methods propose to use latent dynamics [Hafner et al., 2019] or value models [Schrittwieser et al., 2020] to address these challenges and improve the sample efficiency of RL.

### Transformers
- Transformer is an effective and scalable neural network for sequential data
- The key idea of Transformers is to incorporate self-attention mechanism
- Formally, given a sequential input with n tokens, the self-attention layer maps each token to a query, a key, and a value via linear transformations
- The output of the self-attention layer is a weighted sum of all values

### Combination of Transformers and RL
- Transformers can be used as one component for RL algorithms
- Transformers can also serve as one whole sequential decision-maker

## Network Architecture in RL
- Transformer is an advanced neural network
- Designing appropriate neural networks contributes to the success of DRL
- [Sunehag et al., 2017] or sub-reward [Lin et al., 2019]
- Increasing input dimensionality while using an online feature extractor to boost state representation helps improve the performance and sample efficiency of DRL algorithms

### Architectures for function approximators
- 2020: propose a deep dense architecture for DRL agents
- 2021: use DenseNet with decoupled representation learning to improve flows of information and gradients for large networks
- Recently, due to the superior performance of Transformers, some researchers have attempted to apply Transformers architecture in policy optimization algorithms, but found that the vanilla Transformer design fails to achieve reasonable performance in RL tasks

### Challenges
- Transformer-based architectures have made rapid progress in SL domains, but applying them in RL is not straightforward
- There exist several unique challenges, such as the sensitivity of RL algorithms to the architecture of deep neural networks, and the high inference latency of Transformers
- So far, the idea of efficient or lightweight Transformers has not yet been fully explored in the RL community

## Transformers in RL
- Transformers as function approximators
- Approximate policy learning
- Memory-based learning
- Sequential decisionmaking
- Transformers as model approximators
- Policy iteration
- Memory-based learning
- Sequential decisionmaking
- Transformers as sequential decisionmakers
- Online learning
- Offline learning
- Policy iteration
- Memory-based learning

### Transformers for representation learning
- Various sequences in RL tasks require processing, such as local per-timestep sequence (multi-entity sequence [Vinyals et al., 2019;Baker et al., 2019], multi-agent sequence [Wen et al., 2022]), temporal sequence (trajectory [Parisotto et al., 2020;Banino et al., 2021]) and so on.
- The early notable success of this method is embodied in using Transformers to process complex information from a variable number of entities scattered in the agent's observation.
- Zambaldi et al. [2018a] first propose to capture relational reasoning over structured observation with multi-head dot-product attention, which is subsequently used in AlphaStar [Vinyals et al., 2019] to process multi-entity observation in the challenging multi-agent StarCraft II environment.
- In such a mechanism, called entity Transformer, the observation is encoded in the form: where e i represents the agent's observation on entity i either directly sliced from the whole observation or given by an entity tokenizer.
- Several follow-up works have enriched entity Transformer mechanisms.
- Hu et al. [2020] propose a compatible decoupling policy to explicitly associate actions to various entities and exploit an attention mechanism for policy explanation.
- To solve the challenging one-shot visual imitation, Dasari and Gupta [2021] use Transformers to learn a representation focusing on task-specific elements.
- Similar to entities scattered in observation, some works exploit Transformers to process other local per-timestep sequences.
- Tang and Ha [2021] leverage the attention mechanism of Transformers to process sensory sequence and construct a policy that is permutation invariant w.r.t. inputs.
- In the incompatible multi-task RL setting, Transformer is proposed to extract morphological domain knowledge [Kurin et al., 2020].
- Meanwhile, it is also reasonable to process temporal sequence with Transformers. Such a temporal encoder works as a memory architecture, where o t represents the agent's observation at timestep t and Emb 0:t represents the embedding of historical observations from initial observation to current observation.
- In the early work, Mishra et al. [2018] fail to process temporal sequence with vanilla Transformers and find it even worse than random policy in some certain tasks.
- Gated Transformer-XL (GTrXL) [Parisotto et al., 2020] is the first efficacious scheme to use Transformer as a memory architecture to process trajectories.
- GTrXL modifies Transformer-XL architecture [Dai et al., 2019] with Identity Map Reordering to provide a 'skip' path from temporal input to the Transformer output, which may conduce to a stabilizing training procedure from the beginning.
- Furthermore, Loynd et al. [2020] propose a shortcut mechanism with memory vectors for long-term dependency and Irie et al. [2021] combine the linear Transformer with Fast Weight Programmers for better performance.
- In addition, Melo [2022] proposes to use the self-attention mechanism to mimic memory reinstatement for memory-based meta RL.
- While Transformer outperforms LSTM/RNN as the memory horizon grows and parameter scales, it suffers from poor data efficiency with RL signals.
- Follow-up works exploit some auxiliary (self-)supervised tasks to benefit learning [Banino et al., 2021] or use pre-trained Transformer architecture as a temporal encoder [Li et al., 2022;Fan et al., 2022].

### Transformers for model learning
- The Transformer architecture is the backbone of the environmental model in some model-based algorithms.
- The success of Dreamer and subsequent algorithms [Hafner et al., 2020[Hafner et al., , 2021;;Seo et al., 2022] has demonstrated the benefits of the world model conditioned on history in some partially observable environments or in some tasks that require a memory mechanism.
- A world model conditioned on history consists of an observation encoder to capture abstract information and a transition model to learn the transition in latent space, formally:
- Besides, some works also try out Transformer-based world model with planning.
- Ozair et al. [2021] verify the efficacy of planning with a Transformer transition model to tackle stochastic tasks requiring long tactical look-ahead.
- Sun et al. [2022] propose a goal-conditioned Transformer-based transition model which is effective in visual-grounded planning for procedural tasks.

### Transformers for sequential decision-making
- Transformer is an expressive architecture to be plugged into components of traditional RL algorithms
- Transformer can serve as a model that conducts sequential decisionmaking directly
- Transformer as a milestone for offline RL
- One challenge for Transformers to be widely used in RL is that the non-stationarity during the training process may hinder its optimization
- However, the recent prosperity in offline RL motivates a growing number of works focusing on training a Transformer model on offline data that can achieve state-of-the-art performance
- Decision Transformer (DT) [Chen et al., 2021] first applies this idea by modeling RL as an autoregressive generation problem to produce the desired trajectory: where Rt = T t =t r(s t , a t ) is the return-to-go.
- By conditioning on the proper target return value at the first timestep, DT can generate desired actions without explicit TD learning or dynamic programming.
- Concurrently with this work, Trajectory Transformer (TT) [Janner et al., 2021] adopts a similar Transformer structure, but alternatively proposes to use beam search for planning during execution.
- The empirical results demonstrate that TT performs well on longhorizon prediction. Moreover, TT shows that with mild adjustments on vanilla beam search, TT can perform imitation learning, goal-conditioned RL, and offline RL under the same framework.
- Regarding the behavior cloning setting, Behavior Transformer (BeT) [Shafiullah et al., 2022] proposes a similar Transformer structure as TT to learn from multi-modal datasets.
- In light of Transformer's superior accuracy on sequence prediction, Bootstrapped Transformer (BooT) [Wang et al., 2022] proposes to bootstrap Transformer to generate data while optimizing it for sequential decision-making.
- Bootstrapping Transformer for data augmentation can expand the amount and coverage of offline datasets, and hence achieve performance improvement.
- More specifically, BooT compares different data generation schemes and bootstrapping schemes to analyze how BooT can benefit policy learning.
- The results show that it can generate data consistent with the underlying MDP without additional explicit conservative constraints.
- While conditioning on return-to-go is a practical choice to incorporate future trajectory information, one natural question is whether other kinds of hindsight information can benefit sequential decision-making.
- To this end, Furuta et al. [2021] propose Hindsight Information Matching (HIM), a unified framework that can formulate variants of hindsight RL problems.
- More specifically, HIM converts hindsight RL into matching any pre-defined statistics of future trajectory information w.r.t. the distribution induced by the learned conditional policy. Moreover, this work proposes Generalized DT (GDT) for arbitrary choices of statistics and demonstrates its applications in two HIM problems: offline multitask state-marginal matching and imitation learning.
- Specifically, one drawback of conditioning on return-to-go is that it will lead to sub-optimal actions in stochastic environments. This is because the training data may contain suboptimal actions that result in high rewards by luck due to the stochasticity of transitions.
- Paster et al. [2022] [Janner et al., 2021] IL/GCRL/Offline return-to-go beam search basic Transformer structure BeT [Shafiullah et al., 2022] BC none conditioning basic Transformer structure BooT [Wang et al., 2022] Offline return-to-go beam search data augmentation GDT [Furuta et al., 2021] HIM arbitrary conditioning anti-causal aggregator ESPER [Paster et al., 2022] Offline (stochastic) expected return conditioning adversarial clustering DoC [Yang et al., 2022] Offline (stochastic) learned representation conditioning additional latent value func. QDT [Yam...

### Transformers for generalist agents
- Decision transformer can be used to generalize to multiple tasks
- Some works draw on the ideas of pre-training on large-scale datasets in CV and NLP, and try to abstract a general policy from large-scale multi-task datasets
- Multi-Game Decision Transformer (MGDT) [Lee et al., 2022], a variant of DT, learns DT on a diverse dataset consisting of both expert and non-expert data and achieves close-to-human performance on multiple Atari games with a single set of parameters.
- In order to obtain expert-level performance with a dataset containing non-expert experiences, the expert action inference mechanism is designed in MGDT, which calculates an expert-level return-to-go posterior distribution from the prior distribution of return-to-go and a preset expert-level return-to-go likelihood proportional according to Bayesian formula.
- Likewise, Switch Trajectory Transformer (SwitchTT) [Lin et al., 2022], a multi-task extension to TT, exploits a sparsely activated model that replaces the FFN layer with a mixture-of-expert layer for efficient multi-task offline learning.
- Besides, a distributional trajectory value estimator is adopted to model the uncertainty of value estimates.
- With these two enhanced features, SwitchTT achieves improvement over TT across multiple tasks in terms of both performance and training speed.
- MGDT and SwitchTT exploit experiences collected from multiple tasks and various performance-level policies to learn a general policy.
- Yet, constructing a large-scale multi-task dataset is non-trivial.
- Unlike large-scale datasets in CV or NLP, which are usually constructed with massive public data from the Internet and simple manual labeling, action information is always absent from public sequential decision-making data and is not facile to label.
- Baker et al. [2022] propose a semi-supervised scheme to utilize large-scale online data without action information and the key is to learn a Transformer-based Inverse Dynamic Model (IDM), which predicts the action information with past and future observations and is consequently capable of labeling massive unlabeled online video data.
- IDM is learned on a small-scale dataset containing manually labeled actions and is accurate enough to provide action labels of videos for effective behavior cloning and fine-tuning.
- The efficacy of prompting [Brown et al., 2020] for adaptation to new tasks has been proven in many prior works in NLP.
- Following this idea, several works aim at leveraging prompting techniques for DT-based methods to enable fast adaptation.
- Prompt-based Decision Transformer (Prompt-DT) [Xu et al., 2022] samples a sequence of transitions from the few-shot demonstration dataset as prompt, and shows that it can achieve few-shot policy generalization on offline meta RL tasks.
- Reed et al. [2022] further exploit prompt-based architecture to learn a generalist agent (Gato) via auto-regressive sequence modeling on a super large-scale dataset covering natural language, image, temporal decisionmaking, and multi-modal data.
- Gato is capable of a range of tasks from various domains, including text generation and decision-making. Specifically, Gato unifies multi-modal sequences in a shared tokenization space and adapts promptbased inference in deployment to generate task-specific sequences.
- Despite being effective, Laskin et al. [2022] point out that one limitation of the prompt-based framework is that the prompt is demonstrations from a well-behaved policy, as contexts in both works are not sufficient to capture policy improvement.
- Instead, they propose Algorithm Distillation (AD) [Laskin et al., 2022], which instead trains a Transformer on across-episode sequences of the learning progress of single-task RL algorithms.
- Concretely, Reid...

## Summary and Future Perspectives
- Transformers can serve as a powerful module of RL, e.g., acting as a representation module or a world model
- Transformers can serve as a sequential decision-maker
- Transformers can benefit generalization across tasks and domains
- Combining Reinforcement Learning and (Self-) Supervised Learning
- Retracing the development of Trans-formRL, the training methods involve both RL and (self-)supervised learning
- When serving as a representation module that is trained under the conventional RL framework, optimization of the Transformer architecture is usually unstable
- When using Transformers to solve decision-making problems via sequence modeling, the "deadly triad problem" [Van Hasselt et al., 2018] is eliminated due to (self-)supervised learning paradigm
- Under the framework of (self-)supervised learning, the performance of policy is deeply bounded to offline-data quality and the explicit trade-off between exploitation and exploration no longer exists
- Therefore, a better policy may be learned when we combine RL and (self-)supervised learning in Transformer learning
- Some works [Zheng et al., 2022;Meng et al., 2021] have tried out the scheme of supervised pre-training and RL-involved finetuning. However, exploration can be limited with a relatively fixed policy [Nair et al., 2020], which is one of the bottlenecks to be resolved. Also, along this line, the tasks used for performance evaluation are relatively simple. It is worthwhile to further explore whether Transformers can scale such (self-)supervised learning up to larger datasets, more complex environments, and real-world applications
- Further, we expect future work to provide more theoretical and empirical insights to characterize under which conditions this (self-)supervised learning is expected to perform well [Brandfonbrener et al., 2022]
- Bridging Online and Offline Learning via Transformers
- Stepping into Offline RL is a milestone in TransformRL. Practically, utilizing Transformers to capture dependencies in decision sequence and to abstract policy is mainly inseparable from the support of considerable offline data used. However, it is unfeasible for some decision-making tasks to get rid of the online framework in real applications
- On the one hand, it is not that easy to obtain expert data in certain tasks. On the other hand, some environments are open-ended (e.g., Minecraft), which means the policy has to continually adjust to deal with unseen tasks during online interaction. Therefore, we believe that bridging online learning and offline learning is necessary. However, most research progress following Decision Transformer focuses on the offline learning framework.
- Several works have attempted to adopt the paradigm of offline pre-training and online fine-tuning [Xie et al., 2022]. Yet, the distribution shift in online fine-tuning still exists as that in offline RL algorithms, we thereby expect some special designs for Decision Transformer to address this issue.
- In addition, how to train an online Decision Transformer from scratch is an interesting open problem.
- Transformer Structure Tailored for Decision-making Problems
- The Transformer structures in the current Decision Transformer series methods are mainly vanilla Transformer, which is originally designed for the text sequence and may not fit the nature of decision-making problems.
- For instance, is it appropriate to adopt the vanilla self-attention mechanism for trajectory sequences? Whether different elements in the decision sequence or different parts of the same elements need to be distinguished in position embedding? In addition, as there are many variants of representing trajectory as a sequence in different Decision Transformer algorithms, how to choose from them still lacks systematic research.
- For instance, how to select robust hindsight information when deploying such...
