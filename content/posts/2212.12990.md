---
title: "Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models"
date: 2022-12-26T02:37:38.000Z
author: "Zijian Zhang, Zhou Zhao, Zhijie Lin"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2212-12990v2.webp" # image path/url
    alt: "Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12990)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12990).


# Abstract
- Diffusion Probabilistic Models (DPMs) have shown a powerful capacity of generating high-quality image samples
- Recently, diffusion autoencoders (Diff-AE) have been proposed to explore DPMs for representation learning via autoencoding.
- Their key idea is to jointly train an encoder for discovering meaningful representations from images and a conditional DPM as the decoder for reconstructing images.
- Considering that training DPMs from scratch will take a long time and there have existed numerous pre-trained DPMs, we propose Pre-trained DPM Auto Encoding (PDAE), a general method to adapt existing pre-trained DPMs to the decoders for image reconstruction, with better training efficiency and performance than Diff-AE.
- Specifically, we find that the reason that pre-trained DPMs fail to reconstruct an image from its latent variables is due to the information loss of forward process, which causes a gap between their predicted posterior mean and the true one.
- From this perspective, the classifier-guided sampling method can be explained as computing an extra mean shift to fill the gap, reconstructing the lost class information in samples.
- These imply that the gap corresponds to the lost information of the image, and we can reconstruct the image by filling the gap.
- Drawing inspiration from this, we employ a trainable model to predict a mean shift according to encoded representations and train it to fill as much gap as possible, in this way, the encoder is forced to learn as much information as possible from images to help the filling.
- By reusing a part of network of pre-trained DPMs and redesigning the weighting scheme of diffusion loss, PDAE can learn meaningful representations from images efficiently.

# Paper Content

## Introduction

### Denoising Diffusion Probabilistic Models
- DDPMs employ a forward process that starts from the data distribution q(x 0 ) and sequentially corrupts it to N (0, I) with Markov diffusion kernels q(x t |x t−1 ) defined by a fixed variance schedule {β t } T t=1 .
- The process can be expressed by: q(x t |x t−1 ) = N (x t ; 1 − β t x t−1 , β t I) q(x 1:T |x 0 ) = T t=1 q(x t |x t−1 ) , where {x t } T t=1 are latent variables of DDPMs.
- According to the rule of the sum of normally distributed random variables, we can directly sample x t from x 0 for arbitrary t with q(x t |x 0 ) = N (x t ; √ ᾱt x 0 , (1 − ᾱt )I), where α t = 1 − β t and ᾱt =
- The forward process of DDPMs can be expressed as a sum of normally distributed random variables.

### Denoising Diffusion Implicit Models
- DDIMs define a non-Markov forward process that leads to the same training objective as DDPMs, but the corresponding reverse process can be much more flexible and faster to sample from.
- Specifically, one can sample x t−1 from x t using the θ of some pre-trained DDPMs via: where t ∼ N (0, I) and σ t controls the stochasticity of forward process.
- When σ t = 0, the generative process becomes deterministic, which is named as DDIMs.

### Classifier-guided Sampling Method
- Classifier-guided sampling method
- Shows that one can train a classifier on noisy data and use its gradient to guide some pre-trained unconditional DDPM to sample towards specified class y
- The conditional reverse process can be approximated by a Gaussian similar to that of the unconditional one in Eq.(2), but with a shifted mean: p θ,φ (x t−1 |x t , y) ≈ N (x t−1 ; µ θ (x t , t) + Σ θ (x t , t) • ∇ xt log p φ (y|x t ) , Σ θ (x t , t))

### Forward Process Posterior Mean Gap
- Generally, one will train unconditional and conditional DPMs by respectively learning p θ (x t−1 |x t ) = N (x t−1 ; µ θ (x t , t), Σ θ (x t , t)) and p θ (x t−1 |x t , y) = N (x t−1 ; µ θ (x t , y, t), Σ θ (x t , y, t)) to approximate the same forward process posterior q(x t−1 |x t , x 0 ) = N (x t−1 ; µ t (x t , x 0 ), 1− ᾱt−1 1− ᾱt β t I).
- The experiment in Figure 1 can prove this fact, which means that µ θ (x t , y, t) is closer to µ t (x t , x 0 ) than µ θ (x t , t).
- This implies that there exists a gap between the posterior mean predicted by the unconditional DPMs µ θ (x t , t) and the true one µ t (x t , x 0 ) .
- If we introduce some knowledges of x 0 for DPMs, like y here, the gap will be smaller.
- The more information of x 0 that y contains, the smaller the gap is.
- Moreover, according to Eq.( 5), the Gaussian mean of classifier-guided conditional reverse process contains an extra shift item compared with that of the unconditional one.
- From the perspective of posterior mean gap, the mean shift item can partially fill the gap and help the reverse process to reconstruct the lost class information in samples.
- In theory, if y in Eq.( 5) contains all information of x 0 , the mean shift will fully fill the gap and guide the reverse process to reconstruct x 0 .
- On the other hand, if we employ a model to predict mean shift according to our encoded representations z and train it to fill as much gap as possible, the encoder will be forced to learn as much information as possible from x 0 to help the filling.
- The more the gap is filled, the more accurate the mean shift is, the more perfect the reconstruction is, and the more information of x 0 that z contains.

### Unsupervised Representation Learning by Filling the Gap
- Employs an encoder z = E ϕ (x 0 ) to learn compact and meaningful representations from input images
- Uses a gradient estimator G ψ (x t , z, t) to simulate ∇ xt log p(z|x t ), where p(z|x t ) is some implicit classifier that we will not use explicitly
- Uses pre-trained DPMs so that θ are frozen during the optimization
- Usually sets 2 , which forces the predicted mean shift Σ θ (x t , t) • G ψ (x t , E ϕ (x 0 ), t) to fill the posterior mean gap µ t (x t , x 0 ) − µ θ (x t , t)

### Network Design
- Figure 2: Network and data flow of PDAE
- For encoder E ϕ , unlike Diff-AE that uses the encoder part of U-Net [40], we find that simply stacked convolution layers and a linear layer is enough to learn meaningful z from x 0 .
- For gradient estimator G ψ , we use U-Net similar to the function approximator θ of pre-trained DPM.
- Considering that θ also takes x t and t as input, we can further leverage the knowledges of pre-trained DPM by reusing its trained encoder part and time embedding layer, so that we only need to employ new middle blocks, decoder part and output blocks of U-Net for G ψ .
- To incorporate z into them, we follow [8] to extend Group Normalization [53] by applying scaling & shifting twice on normalized feature maps.

### Weighting Scheme Redesign

## Experiments
- To compare PDAE with Diff-AE, we follow their experiments with the same settings.
- Moreover, we also show that PDAE enables some added features.
- For fair comparison, we use the baseline DPMs provided by official Diff-AE implementation as our pre-trained models (also as our baselines), which have the same network architectures (hyperparameters) with their Diff-AE models.
- For brevity, we use the notation such as "FFHQ128-130M-z512-64M" to name our model, which means that we use a baseline DPM pre-trained with 130M images and leverage it for PDAE training with 64M images, on 128 × 128 FFHQ dataset [21], with the semantic latent code z of 512-d.

### Training Efficiency
- We think that the reason that PDAE needs less training time than Diff-AE is because Diff-AE needs to model the posterior mean gap, which is easier to do with pre-trained DPMs.
- The network reuse and weighting scheme redesign also help.

### Learned Mean Shift Fills Posterior Mean Gap
- We train a model of "FFHQ128-130M-z512-64M"
- We show that our learned mean shift can fill the posterior mean gap with qualitative and quantitative results
- Specifically, we select some images x 0 from FFHQ, sample x t = √ ᾱt x 0 + √ 1 − ᾱt for different t and predict x0 from x t by denoising them for only one step (i.e., ), using pre-trained DPM and PDAE respectively
- As we can see in the figure (left), even for large t, PDAE can predict accurate noise from x t and reconstruct plausible images, which shows that the predicted mean shift fills the posterior mean gap and the learned representation helps to recover the lost information of forward process

### Autoencoding Reconstruction
- We use "FFHQ128-130M-z512-64M" to run some autoencoding reconstruction examples using PDAE generative process of DDIM and DDPM respectively.
- As we can see in Figure 6, both methods generate samples with similar contents to the input.
- Some stochastic variations [36] occur in minor details of hair, eye and skin when introducing stochasticity.
- Due to the similar performance   7,168 0.827 0.114 0.006 VQ-GAN [10] 65,536 0.782 0.109 3.61e-3 VQ-VAE2 [37] 327,680 0.947 0.012 4.87e-4 NVAE [47] 6,005,760 0.984 0.001 4.85e-5 Diff-AE @130M (T=100, random xT ) [36] 512 0.677 0.073 0.007 PDAE @64M (T=100, random xT ) 512 0.696 0.094 0.005 DDIM @130M (T=100) [44] 49,152 0.917 0.063 0.002 Diff-AE @130M (T=100, inferred xT ) [36] 49,664 0.991 0.011 6.07e-5 PDAE @64M (T=100, inferred xT ) 49,664 0.993 0.008 5.48e-5
- Between DDPM and DDIM with random x T , we will always use DDIM sampling method in later experiments.
- We can get a near-exact reconstruction if we use the stochastic latent code inferred from aforementioned ODE, which further proves that the stochastic latent code controls the local details.

### Interpolation of Semantic Latent Codes and Trajectories
- Given two images x 1 0 and x 2 0 from FFHQ, we use "FFHQ128-130M-z512-64M" to encode them into (z 1 , x 1 T ) and (z 2 , x 2 T ) and run PDAE generative process of DDIM starting from Slerp(x 1 T , x 2 T ; λ) under the guidance of G ψ x t , Lerp(z 1 , z 2 ; λ), t with 100 steps
- From the view of the diffusion trajectories, PDAE generates desired samples by shifting the unconditional sampling trajectories towards the spatial direction predicted by G ψ (x t , z, t)
- This enables PDAE to directly interpolate between two different sampling trajectories
- Intuitively, the spatial direction predicted by the linear interpolation of two semantic latent codes, G ψ x t , Lerp(z 1 , z 2 ; λ), t , should be equivalent to the linear interpolation of two spatial directions predicted by respective semantic latent code, Lerp G ψ (x t , z 1 , t), G ψ (x t , z 2 , t); λ

### Attribute Manipulation
- We can explore the learned semantic latent space in a supervised way
- To illustrate this, we train a model of "CelebA-HQ128-52M-z512-25M" and conduct attribute manipulation experiments by utilizing the attribute annotations of CelebA-HQ dataset
- Specifically, we first encode an image to its semantic latent code, then move it along the learned direction and finally decode it to the manipulated image
- Similar to Diff-AE, we train a linear classifier to separate the semantic latent codes of the images with different attribute labels and use the normal vector of separating hyperplane (i.e. the weight of linear classifier) as the direction vector
- We present some attribute manipulation examples in Figure 8

### Truncation-like Effect
- The truncation-like effect for "ImageNet64-77M-y-38M" by scaling G ψ (x t , y, t) with 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0 respectively.
- In theory, it can also be applied in truncation-like effect.
- To illustrate this, we directly incorporate the class label into G ψ (x t , y, t) and train it to fill the gap.

### Few-shot Conditional Generation
- D2C is a method that uses a model to predict the label of a new image
- D2C is better than Diff-AE and the naive approach when it comes to predicting the label of a new image
- PDAE is a method that uses a model to predict the label of a new image and it is better than Diff-AE and the naive approach

### Improved Unconditional Sampling
- z is used to improve the unconditional sampling of pre-trained DPMs
- PDAE uses an independent gradient estimator as a corrector of the pre-trained DPM for sampling
- PDAE can be applied for any pre-trained DPMs as an auxiliary booster to improve their sample quality

## Related Work
- Our work is based on an emerging latent variable generative model known as Diffusion Probabilistic Models (DPMs)
- Numerous studies [34,24,8,15,44,19,46,30] and applications [5,26,18,32,57,6,29,41,3,16,17] have further significantly improved and expanded DPMs
- Unsupervised representation learning via generative modeling is a popular topic in computer vision
- Latent variable generative models, such as GANs [13], VAEs [25,39], and DPMs, are a natural candidate for this, since they inherently involve a latent representation of the data they generate
- For GANs, due to its lack of inference functionality, one have to extract the representations for any given real samples by an extra technique called GAN Inversion
- Existing inversion methods [58,35,4,1,2,51] either have limited reconstruction quality or need significantly higher computational cost
- VAEs explicitly learn representations for samples, but still face representation-generation trade-off challenges [49,42]
- VQ-VAE [49,37] and D2C [42] overcome these problems by modeling latent variables post-hoc in different ways
- DPMs also yield latent variables through the forward process, but these latent variables lack high-level semantic information because they are just a sequence of spatially corrupted images
- In light of this, diffusion autoencoders (Diff-AE) [36] explore DPMs for representation learning via autoencoding. Specifically, they jointly train an encoder for discovering meaningful representations from images and a conditional DPM as the decoder for image reconstruction by treating the representations as input conditions. Diff-AE is competitive with the state-of-the-art model on image reconstruction and capable of various downstream tasks.
- Compared with Diff-AE, PDAE leverages existing pre-trained DPMs for representation learning also via autoencoding, but with better training efficiency and performance.

## Conclusion
- PDAE is a general method for representing data using pre-trained DPMs
- PDAE is more efficient and performs better than Diff-AE
- The key idea behind PDAE is based on the concept of posterior mean gap and its connections with classifier-guided sampling method

### C.2 Autoencoding Reconstruction
- Figure 5 shows autoencoding reconstruction examples using different models.
- As we can see, the deterministic method can almost reconstruct the input images even with only 100 steps and both stochastic methods can generate samples with similar contents to the input except some minor details, such as sheet pattern and wrinkle for LSUN-Bedroom; horse eye, spot and mane for LSUN-Horse.

### C.3 Interpolation of Semantic Latent Codes and Trajectories
- Figure 8 shows two interpolation methods, one using a model and one using a manual selection of spatially-similar image pairs.
- Both methods generate similar samples that smoothly transition from one endpoint to the other.

### C.4 Attribute Manipulation
- PDAE can move semantic latent codes
- PDAE can change attribute-relevant features
- PDAE can keep other irrelevant details almost stationary

### C.5 Few-shot Conditional Generation
- PDAE can generate samples belonging to specified class for different few-shot scenarios
- Semantic latent codes are easy to classify even with a very small number of labeled samples

### C.6 Visualization of Mean Shift
- The gradient estimator learns a mean shift direction towards x 0 for each x t .
- The gradient estimator is a linear function of the input variables and the output variables.
- The gradient estimator is unbiased and consistent.

## D Limitations and Potential Negative Societal Impacts
- PDAE has a slower inference speed than Diff-AE due to an extra gradient estimator
- Slow generation speed is a common problem for DPM-based works
- Although many studies have been able to achieve decent performance with few reverse steps, they still lag behind VAEs and GANs, which only need a single network pass
- Almost perfect PDAE reconstruction needs hundreds of extra forward steps to infer the stochastic latent code
- Moreover, we have found that the weighting scheme of diffusion loss is indispensable to PDAE, but we haven't explored its mechanism, which may help to further improve the efficiency and performance of PDAE
- Potential negative impacts of our work mainly involve deepfakes, which leverage powerful generative techniques from machine learning and artificial intelligence to create synthetic media, which may be used for hoaxes, fraud, bullying or revenge
- Although some synthetic samples are hard to distinguish, researchers have developed algorithms similar to the ones used to build the deepfake to detect them with high accuracy
- Some other techniques such as blockchain and digitally signing can help platforms to verify the source of the media
