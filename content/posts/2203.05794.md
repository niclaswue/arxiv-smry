---
title: "BERTopic: Neural topic modeling with a class-based TF-IDF procedure"
date: 2022-03-11T08:35:15.000Z
author: "Maarten Grootendorst"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2203-05794v1_cH8MfpRSn.jpg" # image path/url
    alt: "BERTopic: Neural topic modeling with a class-based TF-IDF procedure" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.05794)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.05794).


# Abstract
- Topic models can be useful tools to discover latent topics in collections of documents
- Recent studies have shown the feasibility of approach topic modeling as a clustering task.
- We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF.
- BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure.
- BERTopic remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.

# Paper Content

## Introduction
- Text embedding techniques have rapidly become popular in the natural language processing field
- Bidirectional Encoder Representations from Transformers (BERT) and its variations (e.g., Lee et al., 2020;Liu et al., 2019;Lan et al., 2019) have shown great results in generating contextual word-and sentence vector representations
- Semantic properties of these vector representations allow the meaning of texts to be encoded in such a way that similar texts are close in vector space
- Although embedding techniques have been used for a variety of tasks, ranging from classification to neural search engines, researchers have started to adopt these powerful contextual representations for topic modeling
- In this paper, we introduce BERTopic, a topic model that leverages clustering techniques and a class-based variation of TF-IDF to generate coherent topic representations.

## Related Work
- Neural topic models have been increasingly successful in leveraging neural networks to improve upon existing topic modeling techniques
- The incorporation of word embeddings into classical models, such as LDA, demonstrated the viability of using these powerful representations
- Foregoing incorporation into LDAlike models, there has been a recent surge of topic modeling techniques built primarily around embedding models
- CTM, for example, demonstrates the advantage of relying on pre-trained language models
- Several approaches have started simplifying the topic building process by clustering word-and document embeddings
- BERTopic builds on top of the clustering embeddings approach and extends it by incorporating a class-based variant of TF-IDF for creating topic representations.

## BERTopic
- First, each document is converted to its embedding representation
- Second, the dimensionality of the resulting embeddings is reduced to optimize the clustering process
- Lastly, from the clusters of documents, topic representations are extracted

### Document embeddings
- Embeds documents to create representations in vector space
- Uses SBERT framework to convert sentences and paragraphs to dense vector representations
- Can be used to cluster semantically similar documents
- As a result, the quality of clustering in BERTopic will increase as new and improved language models are developed.

### Document clustering
- As data increases in dimensionality, distance to the nearest data point has been shown to approach the distance to the farthest data point
- In high dimensional space, the concept of spatial locality becomes ill-defined and distance measures differ little
- PCA and t-SNE are well-known methods for reducing dimensionality, UMAP has shown to preservers more of the local and global features of high-dimensional data in lower projected dimensions
- Reducing the dimensionality of document embeddings generated in 3.1 with UMAP can improve the performance of well-known clustering algorithms

### Topic Representation
- The topic representations are modeled based on the documents in each cluster where each cluster will be assigned one topic.
- For each topic, we want to know what makes one topic, based on its clusterword distribution, different from another?
- For this purpose, we can modify TF-IDF, a measure for representing the importance of a word to a document, such that it allows for a representation of a term's importance to a topic instead.
- The classic TF-IDF procedure combines two statistics, term frequency, and inverse document frequency (Joachims, 1996):
- Where the term frequency models the frequency of term t in document d. The inverse document frequency measures how much information a term provides to a document and is calculated by taking the logarithm of the number of documents in a corpus N divided by the total number of documents that contain t.
- We generalize this procedure to clusters of documents. First, we treat all documents in a cluster as a single document by simply concatenating the documents. Then, TF-IDF is adjusted to account for this representation by translating documents to clusters:
- Where the term frequency models the frequency of term t in a class c or in this instance. Here, the class c is the collection of documents concatenated into a single document for each cluster. Then, the inverse document frequency is replaced by the inverse class frequency to measure how much information a term provides to a class.
- It is calculated by taking the logarithm of the average number of words per class A divided by the frequency of term t across all classes. To output only positive values, we add one to the division within the logarithm. Thus, this class-based TF-IDF procedure models the importance of words in clusters instead of individual documents.
- Finally, by iteratively merging the c-TF-IDF representations of the least common topic with its most similar one, we can reduce the number of topics to a user-specified value.

## Dynamic Topic Modeling
- Table 3: The topic coherence and topic diversity scores were calculated on dynamic topic modeling tasks.
- For Trump, it outperforms LDA on all measures whereas it only achieves the top score on topic coherence for the UN dataset.
- On both datasets, there seems to be no effect of the assumption of linearly evolving topics on both topic coherence and topic diversity indicating that from an evaluation perspective, the proposed assumption does not impact performance.

### Smoothing
- The topic representation at timestep t is independent of timestep t-1
- To overcome this, we can leverage the c-TF-IDF matrices that were created at each timestep to incorporate this linear assumption
- For each topic and timestep, the c-TF-IDF vector is normalized by dividing the vector with the L1-norm
- When comparing vectors, this normalization procedure prevents topic representations from having disproportionate effects as a result of the size of the documents that make up the topic

## Experimental Setup
- OCTIS was used to preprocess the datasets
- BERTopic was evaluated on three datasets
- The Trump dataset was binned to 10 timesteps and the UN datasets to 9 timesteps
- The Trump dataset was used to evaluate BERTopic

### Models
- BERTopic will be compared to LDA, NMF, CTM, and Top2Vec
- LDA and NMF were run through OCTIS with default parameters
- The "all-mpnet-base-v2" SBERT model was used as the embedding model for BERTopic and CTM (Song et al., 2020)
- Two variations of Top2Vec were modeled, one with Doc2Vec and one with the "all-mpnet-base-v2" SBERT model
- For fair comparisons between BERTopic and Top2Vec, the parameters of HDBSCAN and UMAP were fixed between topic models
- To measure the generalizability of BERTopic across language models, four different language models were used in the experiments
- BERTopic, with and without the assumption of linearly-evolving topics, was compared with the original dynamic topic model, referred hereto as LDA Sequence

### Evaluation
- The performance of the topic models in this paper is reflected by two widely-used metrics, namely topic coherence and topic diversity.
- For each topic model, its topic coherence was evaluated using normalized pointwise mutual information (NPMI, (Bouma, 2009)).
- This coherence measure has been shown to emulate human judgment with reasonable performance (Lau et al., 2014).
- The measure ranges from [-1, 1] where 1 indicates a perfect association.
- Topic diversity, as defined by (Dieng et al., 2020), is the percentage of unique words for all topics.
- The measure ranges from [0, 1] where 0 indicates redundant topics and 1 indicates more varied topics.
- Ranging from 10 to 50 topics with steps of 10, the NPMI score was calculated at each step for each topic model.
- All results were averaged across 3 runs for each step.
- To evaluate the dynamic topic models, the NPMI score was calculated at 50 topics for each timestep and then averaged.
- All results were averaged across 3 runs.
- Validation measures such are topic coherence and topic diversity are proxies of what is essentially a subjective evaluation.
- One user might judge the coherence and diversity of a topic differently from another user.
- As a result, although these measures can be used to get an indication of a model's performance, they are just that, an indication.
- It should be noted that although NPMI has been shown to correlate with human judgment, recent research states that this may only be the case for classical models and that this relationship might not exist with neural topic models (Hoyle et al., 2021).

## Results
- Table 1: The main results
- Table 2: TC and TD for different language models
- Average TC and TD across 3 runs

### Performance
- BERTopic has high topic coherence scores across all datasets
- It has the highest scores on the slightly preprocessed dataset, Trump's tweets
- Whilst remaining competitive on the thoroughly preprocessed datassets, 20 NewsGroups and BBC News
- Although BERTopic demonstrates competitive topic diversity scores, it is consistently outperformed by CTM

### Language Models
- The stability of BERTopic across different language models
- The poor performance of Doc2Vec on the Trump dataset
- The competitive performance of Top2Vec with Doc2Vec when MPNET embeddings are used

### Wall time
- CTM using a MP-NET SBERT model is quite slow
- If we remove that model from the results, we can more easily compare the wall time of the topic models that are more close in speed
- NMF and LDA are faster than the neural network-based topic modeling techniques
- BERTopic and Top2Vec are quite similar in wall times if they are using the same language models
- MiniLM SBERT model seems to be similar in speed compared with Doc2Vec indicating that in BERTopic, MiniLM is a good trade-off between speed and performance
- However, it should be noted that in the environment used in this experiment a GPU was available for creating the embeddings. As a result, the wall time is expected to increase significantly when embedding documents without a GPU.

## Discussion
- BERTopic can be validated through many other evaluation metrics
- Topic modeling techniques can be used across a variety of use cases
- Strengths and weaknesses of BERTopic are discussed

### Strengths
- BERTopic is competitive regardless of the language model used
- Stability across language models allows it to be used in a wide range of situations
- Class-based TF-IDF allows for dynamic and evolutionary topics to be modeled

### Weaknesses
- The model is not perfect
- There are several weaknesses to the model
- One of the weaknesses is that it does not take into account that documents may contain multiple topics
- The model also does not account for the importance of words in a topic

## Conclusion
- Developed BERTopic, a topic model that extends the cluster embedding approach by leveraging state-of-the-art language models
- By separating the process of clustering documents and generating topic representations, significant flexibility is introduced in the model
- Shows competitive and stable performance across a variety of tasks
