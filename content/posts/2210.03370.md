---
title: "GNM: A General Navigation Model to Drive Any Robot"
date: 2022-10-07T07:26:41.000Z
author: "Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, Sergey Levine"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-03370v1.webp" # image path/url
    alt: "GNM: A General Navigation Model to Drive Any Robot" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.03370)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.03370).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/gnm-a-general-navigation-model-to-drive-any).

# Abstract
- Learning provides a powerful tool for vision-based navigation
- Combining data from multiple sources can train more powerful navigation models
- This paper studies how a general goal-conditioned model can be trained on data from multiple robots
- 60 hours of navigation trajectories from 6 robots were used to train the model
- The model was deployed on a range of new robots, including an underactuated quadrotor
- Training on diverse data leads to robustness against degradation in sensing and actuation

# Paper Content

## I. introduction
- Machine learning methods have enabled broad generalization in natural language processing, visual perception, and other domains.
- Large-scale models can be adapted for new tasks by reusing representations from broader, larger, and more general datasets.
- Robotics is difficult to apply machine learning methods to due to the sheer diversity of environments and platforms.
- This paper proposes a framework for training a general omnipolicy from multirobot datasets.

## Ii. related work
- Learning from large, diverse robotic datasets has been studied
- Current approaches rely on learning from small datasets
- Our paper proposes learning navigation behavior from heterogeneous robot datasets
- Related to transfer learning, domain adaptation, and hand-engineered augmentations
- Focus is on generalization of simple, high-capacity models
- Complementary to learning from passive data, such as YouTube videos
- Combination of topological graphs and image-goal policies for navigation
- Can be coupled with topological graphs to scale image-goal navigation to new robots

## Iii. multi-robot training dataset
- Aim is to train a general visual navigation model for robots
- Dataset contains over 60 hours of real-world navigation trajectories from 6 distinct robotic platforms
- Trajectories contain widely varying robot dynamics and top speeds
- Environments include office buildings, hallways, suburban, off-road trails, university campus etc.
- Dataset contains forward-facing RGB images paired with robot's commanded actions and local odometry measurements

## Iv. training a general navigation model
- Image-goal navigation is a general framework that does not rely on ground truth localization or semantic labels
- Goal is to train a goal-reaching policy that can navigate solely from egocentric visual observations
- Two key ingredients in training multi-robot policies: action representation and conditioning policies on a "summary" vector

## A. a shared abstraction across robots
- Navigation from egocentric images is a common task for robots
- Inputs and outputs vary between robots
- High variance outputs can make it difficult to learn a common control policy
- A shared abstraction is proposed to make data points look similar and easier to learn from
- Relative waypoints and yaw change are used as a mid-level action space
- Temporal distance is used to measure traversability
- A normalized action space is proposed to account for varying dynamics across robots

## B. embodiment context
- Policy must infer capabilities of robot
- Hand-designed parameters provide summary of capabilities
- Propose automatic approach to infer learned embodiment context
- Context contains information about robot's configuration and dynamics
- Experiments show same policy can be deployed on novel robot configurations
- Most effective representation achieved by using temporally consistent context

## C. implementation details
- Combining conditioning policies on embodiment context and transforming action space allows a simple goal-reaching policy to be trained from heterogeneous datasets.
- Architecture uses a goal-conditioned policy that takes current and goal observations as input and predicts normalized waypoints and distances.
- Training data pairs are obtained by using image-goal pairs from the same trajectory as "positives" and from different trajectories as "negatives".
- Policy is combined with a topological map and Dijkstra's algorithm to compute optimal sequence of subgoals.

## V. deploying the gnm across robots
- Deployed GNM omnipolicy in challenging indoor and outdoor environments on four different robot platforms
- Designed experiments to answer four questions about multi-robot training, GNM policies, design choices, and policy robustness

## A. meet the robots
- Deployed GNM on four distinct robotic platforms, including a quadrotor and two novel robots
- Custom-built robot platform inspired by Niwa et. al. [46], based on a Roomba
- Commercially available quadrotor equipped with a forward-facing camera
- Commercially available offroad platform equipped with an off-the-shelf PCB-mounted fisheye camera

## Locobot:
- Open-source platform based on Kobuki with fisheye camera
- No training data from LoCoBot
- GS collected on TurtleBot2 with different spherical camera at lower height

## B. zero-shot deployment
- Trained GNM deployed on four distinct robotic platforms without fine-tuning
- GNM outperforms single robot policies up to 5x
- GNM generalizes to novel environment-robot pairs and new robots
- Performance evaluated in 20 indoor and outdoor environments
- Increasingly diverse datasets improve performance
- GNM outperforms single-domain policies in a variety of environments

## C. a systematic analysis of the design space

## D. robustness to degradation
- Training on heterogeneous datasets encourages policy to learn shared affordances across robots.
- GNM offers robustness to small variation in robot parameters.
- GNM can compensate for steering degradation by taking a longer, smoother path.
- GNM can successfully reach goals despite camera position perturbation and physical damage.

## Vi. discussion
- Demonstrated that a general goal-conditioned navigation policy can control new robots in challenging environments
- Simple decisions, such as including a temporal context and standardizing the action space, are sufficient to enable broad generalization from heterogeneous data
- Real-world navigation for a range of robots, including some not seen in training, and even an underactuated quadrotor
