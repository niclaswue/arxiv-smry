---
title: "GNM: A General Navigation Model to Drive Any Robot"
date: 2022-10-07T07:26:41.000Z
author: "Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, Sergey Levine"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-03370v1.webp" # image path/url
    alt: "GNM: A General Navigation Model to Drive Any Robot" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.03370)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.03370).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/gnm-a-general-navigation-model-to-drive-any).

# Abstract
- Learning provides a powerful tool for vision-based navigation
- The capabilities of learning-based policies are constrained by limited training data
- If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models
- In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments
- We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset
- We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor
- We find that training on diverse data leads to robustness against degradation in sensing and actuation

# Paper Content

## I. INTRODUCTION
- Machine learning methods have enabled broad generalization with real-world applicability in natural language processing
- Such generalization typically requires learning general patterns from diverse datasets, which are usually collected once and then reused for various purposes
- Such large-scale models also support the ability to be adapted for new tasks by reusing the representations learned from broader, larger, and more general datasets
- Although this paradigm has been very successful, it is difficult to apply in robotics due to the sheer diversity of environments and platforms across researchers
- Control policies learned end-to-end usually require separate data collection for each robotic platform, leading to "fragmentation" in progress, where every researcher works with their own robot-specific dataset and policies
- Can we overcome this challenge by training models on more general and reusable cross-robot datasets?
- We study this question in the context of visual navigation, where heterogeneity between robots might include different camera hardware, viewpoints, dynamics, and more broadly, embodiments, but where the over-arching navigation objective looks similar irrespective of these differences
- A wheeled robot, quadruped, or a drone all have the same abstract objectives: to explore the environment, plan a path to the goal, and avoid collisions
- Leveraging this shared abstraction across robots and training a general navigational omnipolicy from large-scale data could enable broad generalization to novel environments, unseen sensor parameters (e.g., camera intrinsics and extrinsics), and new robot configurations
- In this paper, we propose to take a step towards this kind of data sharing by training an embodiment-agnostic general navigation model (GNM) from an aggregated multirobot dataset
- The primary contribution of our work is a framework for training a general omnipolicy from multirobot datasets, with empirical evidence that such an omnipolicy can effectively learn from heterogeneous datasets and generalize to novel robot platforms
- To facilitate this, we aggregate a large heterogeneous dataset of navigation trajectories collected across 6 robots, spanning 60 hours of interactions in challenging indoor and outdoor environments
- We train the GNM on this dataset and deploy it on 4 distinct robot platforms, including 2 new robots
- We show that a single learned policy can be used across multiple robots to perform goal-reaching in challenging indoor and outdoor environments, outperforming policies trained with any single dataset
- We also report robustness to degradation in camera parameters, tire damage, and other gradual changes that the robot may experience over its lifetime
- We will be releasing the trained GNM policy, code used to train and deploy our models on various popular robot platforms, as well as the dataset used to train these models at our project page

## II. RELATED WORK
- Learning navigation behavior from heterogeneous robot datasets, collected across multiple embodiments
- Our work is closely related to transfer learning, where the objective is to train policies that transfer across domains
- Towards this, our work is also closely related to DroNet, which imitates expert on-road driving data to control a quadrotor
- Prior work has also extended this framework for complex tasks beyond goal-reaching, such as exploration, instruction following, and reinforcement learning

## III. MULTI-ROBOT TRAINING DATASET
- The GNM dataset contains over 60 hours of real-world navigation trajectories
- The GNM dataset is heterogeneous and contains 8 datasets from different robotic platforms
- The GNM dataset is used to train a general visual navigation model
- The GNM dataset is released to the public

## IV. TRAINING A GENERAL NAVIGATION MODEL
- Image-goal navigation is a general framework that does not rely on ground truth localization or semantic labels
- To provide a general task representation for this policy, we condition it on the desired goal
- Our goal is to train a goal-reaching policy π(o t , o G ) that can navigate solely from egocentric visual observations
- To provide a general task representation for this policy, we condition it on the desired goal and integrate it into a navigational system based on topological graphs
- Such systems have shown great navigation results in a variety of indoor and outdoor environments

## A. A Shared Abstraction Across Robots
- Differential drive robots have different outputs than an ATV
- It is difficult to learn a common control policy for robots with different outputs
- We propose using a normalized action space to make the data look similar across robots

## B. Embodiment Context
- The policy must infer the capabilities of a particular robot
- A simple way to provide such awareness to the policy is to condition it on hand-designed parameters that provide a concise "summary" of capabilities
- Defining these parameters by hand presents a barrier to fast and easy deployment of the policy to new robots
- Instead, we propose a simple and automatic approach: rather than manually defining parameters that fully identify the robot, we use a sequence of consecutive past observations from the robot's viewpoint to infer a learned embodiment context
- This context contains information about the robot's configuration and dynamics, which can be used to condition the behavior of the policy
- While this context may not contain all information to fully identify the robot, we hypothesize that it is sufficient to effectively control the robot
- Our experiments show that the embodiment context allows the same policy to be deployed on novel robot configurations without designing any handengineered robot representation

## C. Implementation Details
- We use a goal-conditioned policy architecture that takes as input the current observation o t and goal observation o G , and predicts normalized waypoints and distances.
- Additionally, we condition on temporal context C t , which is constructed by stacking the past k = 5 consecutive observations.
- Visual inputs to the network are provided as 85 × 64 RGB images for all observations.
- Following prior work [32,44], we train context-conditioned representations by using separate MobileNetv2 encoders for (i) the current observation {o t , C t }, and (ii) conditional goal observation, as shown in Fig. 2.
- The two embeddings are concatenated and passed through three fully-connected layers to two prediction heads: normalized temporal distance dt and a sequence of τ = 5 normalized future waypoints {p i , ψ i } τ i=1 .
- Training: Following the procedure of Shah et. al. [27], we use a combination of image-goal pairs sampled from the same trajectory in the dataset as "positives", and "negatives" sampled from different trajectories, to obtain training data pairs.
- The distance head is trained on both positives and negatives, whereas the action head is only trained on positives.
- We train the two heads jointly with supervised learning using an 2 regression loss.
- We use multi-GPU training with batch sizes between 400-1200 and perform gradient updates using the Adam optimizer [45] with a learning rate of 5 × 10 −4 .
- We combine this goal-reaching policy with a topological map M, where nodes are represented by the robot's observations (augmented with the embodiment context), and edges are computed using the temporal distance estimates d from the trained policy, following the setup of ViNG [27].
- At every time step, the robot associates its current and goal observations in M, i.e., finds the node with smallest temporal distance to it, and computes the optimal sequence of subgoals {s i } using Dijkstra's algorithm.
- The policy π is queried with the current observation {o t , C t } and immediate subgoal s 1 to obtain a sequence of waypoints {p i , ψ i } τ i=1 , which are tracked by a robot-specific low-level controller.

## V. DEPLOYING THE GNM ACROSS ROBOTS
- We deploy our learned GNM omnipolicy in a variety of challenging indoor and outdoor environments on four different robot platforms
- GNM policies outperform policies trained solely on single-domain data
- Are policies trained with multiple datasets more robust to degradation than single-domain policies

## A. Meet the Robots
- Deploys GNM on four distinct robotic platforms
- Vizbot: a custom-built robot platform inspired by the design of Niwa et. al. [46], based on a Roomba
- DJI Tello: a commercially available quadrotor equipped with a forward-facing camera
- Clearpath Jackal UGV: a commercially available offroad platform equipped with an off-the-shelf PCB-mounted fisheye camera
- GNM is deployed on four distinct robotic platforms
- Vizbot: a custom-built robot platform inspired by the design of Niwa et. al. [46], based on a Roomba
- DJI Tello: a commercially available quadrotor equipped with a forward-facing camera
- Clearpath Jackal UGV: a commercially available offroad platform equipped with an off-the-shelf PCB-mounted fisheye camera

## LoCoBot:
- A popular open-source platform based on a Kobuki, equipped with an off-the-shelf PCB-mounted fisheye camera.
- There is no training data from a LoCoBot, although GS was collected on a similar TurtleBot2, albeit with a different spherical camera at a lower height.

## B. Zero-Shot Deployment
- GNM outperforms single robot policies across all tested robots
- GNM can generalize to novel environment-robot pairs
- Adding more and diverse datasets (GNM-Large) contributes towards improvements in performance

## C. A Systematic Analysis of the Design Space
- We compare the three action spaces discussed in Sec. IV-A by training three different policies on GNM-Mid and evaluating them in 10 environments
- While using velocities as an action space works well for most easy environments, often outperforming the policy using waypoints, both these policies struggle in environments requiring dynamic maneuvers like sharp turns
- A policy based on normalized waypoints, on the other hand, significantly outperforms the others, including in the challenging environments
- We also compared different policy architectures to encode the goal information
- The choice of architecture significantly affects the navigation performance, with the conditional model being the most performant

## D. Robustness to Degradation
- Heterogeneous datasets allow for robust learning across robots
- The shared GNM can offer robustness to small variation in robot parameters

## VI. DISCUSSION
- A general goal-conditioned navigation policy can control new robots in challenging environments
- The design of our learning framework is simple, and largely follows prior work
- Empirically, we show that our approach can enable realworld navigation for a range of robots, including some not seen in training
- Our specific instantiation of this principle does have some limitations, most prominently that our system does not explicitly account for differences in capabilities
- While we observe exciting generalization from 60 hours of data, a much larger and broader dataset could enable even better generalization in the future
