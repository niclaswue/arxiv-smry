---
title: "Fake it till you make it: Learning(s) from a synthetic ImageNet clone"
date: 2022-12-16T11:44:01.000Z
author: "Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08420v1.webp" # image path/url
    alt: "Fake it till you make it: Learning(s) from a synthetic ImageNet clone" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08420)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08420).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/fake-it-till-you-make-it-learning-s-from-a).

# Abstract
- Recent large-scale image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a very simple text prompt.
- Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by questioning the need for real images when training models for ImageNet classification.
- More precisely, provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful they are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering those ImageNet clones we denote as ImageNet-SD are able to close a large part of the gap between models produced by synthetic images and models trained with real images for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data.

# Paper Content

## Introduction
- The rise of shallow and generic machine learning models has shifted the focus of computer vision research from methods to the training data itself.
- Datasets, initially of hundreds of images and dozens of classes, have grown in size and complexity, and started becoming contributions in their own right.
- They have been fueling the progress of computer vision as much as, if not more than, the methods themselves.
- ImageNet [19], and mainly its ImageNet-1K [86] subset of about 1 million annotated images, has impacted the field in an unprecedented way.
- Yet, curating, and annotating such a dataset comes at a very high time, money and labor cost.
- The last couple of years has seen the rise of large and generic models, trained on data which is less curated but orders of magnitude larger.
- Those proved to be easily applicable, either directly, or combined with a tailored model, to a wide range of computer vision transfer tasks.
- They have also been used beyond prediction tasks, e.g., for text-conditioned image generation.
- Models such as DALL-E [80] or Stable Diffusion [85] have demonstrated impressive image generation ability.
- They produce fairly realistic synthetic images and exhibit an impressive degree of compositionality.
- Such generative models are trained on billion-scale datasets composed of noisy image-text pairs scraped from the internet.
- Although training such models is out of reach for most institutions, a few of them have been made available to the community, and they will most likely fundamentally impact computer vision research.
- Given the impressive ability of these generative models, it is only natural to ask provocative questions such as: Is there still a need for real images when training image prediction models?
- In this paper we explore this question through one of the most iconic computer vision datasets, ImageNet [19].
- We study to which extent this dataset can be entirely replaced by synthetic images when learning deep models.
- More precisely, we assume that we are provided with a set of classes, and the Stable Diffusion v1.4 [85] model, 1 a generator that can produce realistic images from a textual prompt.
- Our task is to learn an image classification model from scratch using a dataset composed only of synthetic images.
- We then evaluate the performance of this model on several datasets.
- First and foremost, we measure how well models and classifiers trained only on synthetic images recognize the training classes in real images from the standard Ima-geNet validation set.
- Then, we evaluate them on common datasets that test their resilience to domain shifts or adversarial examples, still for the ImageNet training classes.
- Finally, we consider several transfer learning scenarios where we measure the generalization performance of our models to novel sets of classes.

### Distillation of datasets and models

## Preliminaries
- The task is to learn an image classification model given a set of class names and a text-to-image generator.
- We use the recent Stable Diffusion model [85] as text-to-image generator G.
- The text-to-image generation process takes a textual prompt as input and generates an image.
- Two important parameters control the quality and speed of text-conditioned diffusion: the number of diffusion steps and the coefficient that weights the textual conditioning vector.

## Generating synthetic ImageNet clones
- For our study, we create clones of the ImageNet dataset by synthesizing images depicting the classes it contains.
- We refer to all synthetic datasets of ImageNet classes that are created using Stable Diffusion as ImageNet-SD.
- Sec. 4.1 describes different ways of creating ImageNet-SD datasets starting from simply using the class name as the prompt.
- We then present generic, class-agnostic ways for tackling issues that arise with respect to semantics and diversity in Secs. 4.2 and 4.3, respectively.
- Note that we only present a limited set of qualitative results in Fig. 2 because of space constraints, but a far more extensive set can be found in the Appendix.

### Generating datasets using class names
- The generator G is used to synthesize images for each class in the class set C.
- To do so, the generator needs at least one prompt per class.
- When used as an input, this class-conditioned prompt p c triggers the generation of a synthetic image x c = g(p c ) from class c.
- Some issues become apparent when generating images for some classes, such as semantic errors, lack of diversity, and visual domain issues.

### Addressing issues with semantics and domain
- The class names generated by the synset prompt do not always match the real images from ImageNet
- To fix this, the authors augment the prompt with additional information from WordNet
- This additional information helps to reduce the semantic ambiguity of the prompts

### Increasing the diversity of generated images
- Generating images using more expressive prompts, e.g., by appending class hypernym or definition, reduces semantic errors, and increases the visual diversity of the output images.
- This is visible, for example, in the "lorikeet" and "pirate ship" classes in Figs. 2c and 2d when compared to Fig. 2b: The pose and viewpoints are slighly more diverse.
- However, images still tend to display the class instance centered and in a prominent position.
- The real Im-ageNet images feature significantly more diversity, several different settings and backgrounds, and, in several cases, multiple instances of the same class (e.g., see Fig. 2a).
- Although class-specific prompt engineering is an appealing option, in this study we chose to remain generic, and increase diversity in ways that can be applied regardless of the nature of the classes.
- To that end, we only make two assumptions about the class: a) There exists the notion of instance for that class; and b) this class can be "inside" a scene or background.
- These are generic assumptions that in practice can be satisfied by all object-centric classes.
- Generating multiple instances.
- We assume that there exists the notion of instance for class c.
- We leverage this to improve diversity by considering the following two simple, class-agnostic prompts: 1) p c = "a photo of multiple c, h c " and 2) p c = "a photo of multiple different c, h c ".
- Although basic, we observed these two to produce the most realistic images over the variants we tried.
- We show results for these two prompts in Figs. 2e and 2f.
- Both prompts improve diversity in all three depicted classes, and are even more successful for classes whose instances tend to appear as groups, as for e.g., parrots or dogs.
- Adding "different" gave a small but noticeable boost in diversity upon visual inspection (see also Appendix C for more qualitative results).
- We diversify the background.
- We assume that class c can be seen "inside" a scene or background.
- Once again, we approach prompt engineering in a class-agnostic way.
- We use all scene classes from the Places [121] dataset as background for every class.
- We generate images for every possible combination of a class c and a scene b âˆˆ B from the set B of 365 scenes in Places.
- We found that "c inside b" generally gave the best-looking results among a few prepositions we tried.
- However, we found that semantic and domain errors that arise from generating by only using class name remained after specifying a background.
- We therefore build on top of the second simplest, but more semantically correct prompt variant and use p c = "c, h c inside b" as the prompt for generating images in diverse scenes and backgrounds.
- Although we do not consider this in our study, selecting backgrounds tailored for each class, e.g., by matching class names to scenes using features from a text encoder, seems like a promising future direction.
- Label noise and visual realism.
- A large portion of the generated images, especially those with random backgrounds (e.g., see Fig. 2g) contain unlikely but believable combinations of objects and scenes.
- Some other images miss the foreground object completely (e.g., see the bottom row in the middle column of Fig. 2g) or contain physically impossible combinations.
- Yet, we see such noisy or unrealistic synthetic images as a way of providing additional stochasticity during training, similar to what strong, non-realistic data augmentation achieves [27,114].
- In fact, it was recently shown [27] that diverse data augmentations, even when inconsistent with the data distribution, can be very valuable (even more than additional training data) for outof-distribution scenarios.
- The experimental validation that we perform next corroborates this claim.

## Experiments
- The different synthetic datasets are created
- The ImageNet-100 dataset is used for most of the study
- The ImageNet-1K-SD dataset is used for measuring the impact of design choices
- The encoder and classifier are trained on the ImageNet-100-SD dataset
- The encoder is used to generate images for the different transfer learning experiments
- The classifier is used to predict labels for the different transfer learning experiments

### Results on ImageNet
- Models trained on real or synthetic images perform better on ImageNet-1K
- By simply using the class name as a prompt, one can synthesize images and learn a visual encoder from scratch that achieves more than 70% Top-5 accuracy (43% Top-1 accuracy) on a challenging 100way classification task like ImageNet-100 that contains many fine-grained classes.
- Prompts that address some of the semantic and domain issues translate to performance gains.
- Generating objects on diverse backgrounds, even in a simple and class-agnostic way, gives the best results so far, reaching over 50% Top-1 and 76% Top-5 accuracy on the challenging ImageNet-100 validation set.
- Results on ImageNet-1K show that generating synthetic images has potential to improve performance on real-world tasks.

### Resilience to domain shifts
- We investigate the performance of our models on three challenging evaluations sets for ImageNet-1K classes: ImageNet-Sketch [104] (IN-Sketch), ImageNet-R [34] (IN-R) and ImageNet-A [35] (IN-A).
- Results are reported in the right-most columns of Tab. 3.
- Top-1 accuracy on ten transfer learning datasets for encoders trained on real and synthetic images.
- We treat encoders as feature extractors and train linear classifiers on top for each dataset.
- We make the remarkable observation that representations from models trained on synthetic data can match the generalization performance of representations from models trained on millions of real images.

### Transfer learning
- Encoders are used to learn representations of images
- Representations are used to predict labels of test images
- Representations learned on synthetic images perform comparably to representations trained on real images

## Discussion
- The process we followed to create ImageNet-SD requires minimal assumptions and can be applied to a wider set of classes
- To disambiguate semantics, we only assume access to a short textual description of the class
- This is generally easy to acquire even at a larger scale, e.g., in semi-automatic ways from Wikipedia
- To improve diversity, we assumed that the notion of instance exists for every class, and that the class can be found "inside" a scene or a background
- Conceptually, there is no reason to restrict our approach to training with a finite dataset of synthetic images
- We could devise a training process which sees each image only once, following the standard continual or stream learning [73] protocols
- However, Fig. 4 suggests that generating more images with basic prompts might not be enough, and that a performance leap will require advanced prompt engineering
- We consider a study on scaling synthetic datasets important, but beyond the scope of this paper
- Despite this scaling potential, the quality of the resulting classifier is bounded by the expressivity of the generator and the concepts it can reliably reproduce
- Data and model bias
- Because of its pioneering role as a source of images to train generic models, and all it has done to advance the computer vision field, ImageNet and some of its bias has been under heavy scrutiny [20,58,119]
- Its synthetic counterparts have no reason to be immune to bias
- The main advantage of training with synthetic dataset is also its biggest flaw
- On top of the bias contained in the data, the architecture itself constraints the generated images, and as such propagates, and potentially amplifies [5] existing bias
- A major one that we have already discussed is the lack of diversity.
- An obvious corollary is the fact that stereotypes are reinforced.

## Conclusions
- The sparsity ratio for the models trained on synthetic images increases as the "diversity" of a synthetic dataset increases, i.e., we see gradual increase in sparsity scores from p c = "c" and p c = "c, h c inside b" to ImageNet-100-SD-10x. This observation aligns with their performance as well, i.e., in the main paper we show that ImageNet-100-SD-10x performs best in general while p c = "c" performs worst.
- Intra-class distance. In the main paper, we present simple ways to increase the diversity of synthetic images. Now we check if these efforts increase the variance of samples in the representation space. To do that, we compute the average 2 -distance between samples from the same class (i.e., intra-class distance). We see in Fig. 5b that models trained with more diverse images indeed learn representations with higher intra-class variance.
- Feature redundancy. Following [107], we compute feature redundancy, i.e., average pairwise Pearson correlation among dimensions.
