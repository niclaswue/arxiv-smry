---
title: "Teaching Small Language Models to Reason"
date: 2022-12-16T11:24:42.000Z
author: "Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08410v2.webp" # image path/url
    alt: "Teaching Small Language Models to Reason" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08410)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08410).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/teaching-small-language-models-to-reason).

# Abstract
- Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets.
- However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters.
- In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.

# Paper Content

## Introduction
- Chain of thought prompting encourages language models to break down a reasoning task into a series of intermediate steps
- Wei et al. (2022) demonstrate that this style of prompting significantly increases the task accuracy of large language models (LLMs) across a range of commonsense, symbolic and mathematical reasoning datasets
- Here, LLMs are models with at least tens of billions of parameters, such as PaLM-540B (Chowdhery et al., 2022), GPT-3 175B (Brown et al., 2020), or UL2 20B (Tay et al., 2022)
- However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT
- Notable, CoT prompting even reduces the accuracy of models with less than 10 billion parameters
- Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales
- This leads us to our research question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?
- This work explores CoT knowledge distillation (Hinton et al., 2015) from the LLMs PaLM-540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020), such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively
- As a result of our work, we make three recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a larger teacher model; (2) generate the CoT from an LLM, as proposed by Wei et al. (2022), but crucially provide the solution to the task in the few-shot prompt; and (3) scope the knowledge distillation to a single task due to the limited capacity of the smaller model.

## Related Work
- The work is inspired by the seminal work of Wei et al. (2022) on CoT prompting
- The underlying idea of CoT prompting is to encourage the model to break down a problem into a series of intermediate reasoning steps, which can be solved iteratively
- This is especially appealing for reasoning tasks with multiple steps, such as mathematical reasoning
- Wei et al. (2022) demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning for a similar task encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021)
- Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting
- Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote
- Subsequent work by Chung et al. (2022) explores finetuning on CoT data in combination with instruction tuning
- They demonstrate that a FLAN-based (Wei et al., 2021) version of PaLM (Chowdhery et al., 2022) benefits from additional finetuning on CoT data
- In contrast to our work, the CoT data explored in (Wei et al., 2021) is manually generated by human annotators
- Concurrent to our work, Huang et al. ( 2022) explore the ability of LLMs to self-improve by finetuning on the self-labelled solutions to an unlabelled dataset
- In their work, they also briefly explore student-teacher knowledge distillation on one dataset

## Method
- Annotating an existing supervised dataset with CoT reasoning generated by a teacher model
- Using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale
- Adapting the CoT prompts proposed by Wei et al. (2022) to provide the model with the solution to the task after posing the question and before providing example CoT in the few-shot prompts
- Removing all incorrect CoT based on the target answer to prevent the student model from learning from bad examples

## Experimental Setup
- Wei et al. (2022) used a different experimental setup
- We focus on a different set of tasks
- Wei et al. (2022) found that AI is not able to solve problems that humans can solve
- We found that AI is able to solve problems that humans can solve

### Arithmetic Reasoning
- We benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021), (2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021).
- We use the official training and testing split for GSM8K.
- For MAWPS and ASDiv, we perform 5-fold cross validation, as in the original works proposing the datasets.
- We do not evaluate our method on SVAMP (Patel et al., 2021), as the dataset does not have a training split.
- Moreover, we do not perform an evaluation on AQuA (Ling et al., 2017), as the dataset has 100,000 examples, which is too costly to infer with an LLM.
- We evaluate task accuracy by checking whether the target answer is provided as the final answer in the CoT produced.
- In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct.

### Commonsense Reasoning
- We benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset
- As a testing split is not available, we refrain from shuffling the dataset to allow for a reproducible training, validation and test split
- We take the first 80% as training data, the following 10% as validation data, and the final 10% as testing data
- We do not benchmark on the CSQA dataset
- Different from Wei et al. (2022), we also do not evaluate on the Date and Sports tasks from the Big-bench effort (Srivastava et al., 2022) or the SayCan (Ahn et al., 2022) dataset
- Similar to the evaluation of the arithmetic reasoning benchmarks, we compute task accuracy by checking for the target answer in the CoT

### Symbolic Reasoning
- The model is able to replicate the desired CoT for one example
- The model is able to generalize to sequences of length three and four

### Baselines and setup
- The knowledge distillation setup selects PaLM 540B and GPT-3 175B as the teacher models
- The teacher models are prompted as described in Section 3
- T5 XXL is the finetuned model for the original target
- For the MAWPS and ASDiv datasets, 5-fold cross validation is performed
- For all remaining datasets, 10% of the training set is used as a validation set

## Results

### Arithmetic reasoning
- We perform an ablation study to confirm that providing a LLM with the answer to the question during CoT generation is beneficial.
- We found that for the GSM8K dataset, if PaLM 540B is prompted without the answer only 59.98% are correct. On the contrary, when prompting PaLM 540B with the answer being provided, the accuracy is 79.37%.
- A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting its CoT that had a single step missing or was wrong.

### Commonsense reasoning
- For the commonsense reasoning dataset Strate-gyQA, using CoT finetuning improves accuracy from 68.12% to 71.98%.
- Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires.

### Symbolic reasoning
- Table 2: Results obtained for the synthetic symbolic reasoning datasets
- Traditional finetuning does not lead to an improvement
- The proposed method increases accuracy for the Coinflip dataset with regard to generalising to three coinflips.
- In comparison, the finetuning on CoT does not improve generalisation to four coinflips, as the baseline model performs strongly in this.

### Replicating Results using different Teacher Models
- The proposed method is robust to different teacher models
- The proposed method is also robust to different reasoning steps

### Ablation study on model size
- Figure 1: Performance gain achieved when fine-tuning T5 of different sizes on the GSM8K dataset.
- Our results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data.
- Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.

### Ablation study on dataset size
- Student-teacher knowledge distillation via finetuning results in a performance improvement
- The performance gain is trade-off with dataset size
- To investigate the trade-off, T5 XXL is trained on a random, smaller subset of the original finetuning data.

## Discussion
- We demonstrate that finetuning smaller LMs on the CoT data generated by LLMs of over 100 billion parameter can significantly improve task accuracy.
- Even a small number of CoT examples appears to suffice for this. However, such improvements appear to be task dependent.
- For example, the effects of knowledge distillation were limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller language models may not have memorised due to their limited capacity.
- Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks.
- Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy, which may be beneficial in certain settings where model size is constrained or the data available is limited.
- Future work could explore improving the reasoning of small models via knowledge distillation in multi-task settings.

## Conclusion
- CoT knowledge distillation from LLMs of over 100 billion parameters to smaller language models
- Proposes a knowledge distillation pipeline consisting of two key steps: (1) generate CoT for existing datasets using LLMs and (2) finetune smaller LMs on the CoT
- Results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets
