---
title: "StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles"
date: 2023-01-03T13:16:24.000Z
author: "Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Zhidong Deng, Xin Yu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-01081v1.webp" # image path/url
    alt: "StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01081)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01081).


# Abstract

# Paper Content

## Introduction
- Audio-driven photo-realistic talking head generation has drawn growing attention due to its broad applications in virtual human creation, visual dubbing, and short video creation.
- The past few years have witnessed tremendous progress in accurate lip synchronization (Prajwal et al. 2020;Wang et al. 2022), head pose generation (Zhou et al. 2021;Wang et al. 2021) and high-fidelity video generation (Zhang et al. 2021c;Yin et al. 2022). However, existing one-shot based works pay less attention to modeling diverse speaking styles, thus failing to produce expressive talking head videos with various styles.
- In real-world scenarios, different people speak the same utterance with significantly diverse personalized speaking styles. Due to such significant diversities, creating style-controllable talking heads is still a great challenge, especially in the one-shot setting.
- In previous work (Wang et al. 2020;Sinha et al. 2022), the speaking style is merely denoted as discrete emotion classes. Such a formulation is far from representing flexible speaking styles.
- Although some recent methods (Ji et al. 2022;Liang et al. 2022) can control facial expressions by involving an additional emotional source video, they mainly transfer the facial motion characteristics in a frame-by-frame fashion without modeling the temporal dynamics of facial expressions. Therefore, a universal spatio-temporal representation of speaking styles is highly desirable.
- Here, we denote speaking styles as personalized dynamic facial motion patterns. We aim to generate stylized photorealistic talking videos for a one-shot speaker image, in which the speaker speaks the given audio content with the speaking style extracted from a style reference video (style clip).
- Specifically, we design a novel style-controllable talking head generation framework, called StyleTalk. Our framework first encodes the style clip and the input audio into the corresponding latent features, and then uses them as the input of a style-controllable dynamic decoder to obtain the stylized 3D facial animations. Finally, an image renderer (Ren et al. 2021) takes the 3D facial animations and the reference image as input to generate talking faces.
- To be specific, our primary goal is to obtain a universal style encoder that is able to model the facial motion patterns from arbitrary style clips. Here, we employ a transformerbased (Vaswani et al. 2017) style encoder with self-attention pooling layers (Safari, India, and Hernando 2020) to extract the latent style code from the sequential 3D Morphable Model (3DMM) (Blanz and Vetter 1999) expression parameters of one style clip.
- In particular, we introduce a triplet constraint on the style code space, enabling the universal style extractor to be applicable to unseen style clips. Furthermore, we observe that the learned style codes would lie in a semantically meaningful space.
- Driving a one-shot talking head with different speaking styles is also a challenging one-to-many mapping problem. Although style codes can serve as the condition and transform an ambiguous one-to-many mapping into a conditional one-to-one mapping (Qian et al. 2021), we still observe unsatisfactory lip-sync and visual artifacts when talking faces exhibit large facial motions.
- To solve this issue, we propose a style-controllable dynamic transformer as our decoder. Inspired by Wang and Tu (2020), we found that the feed-forward layers following the multi-head attention are of great importance to style manipulation. Hence, we propose to adaptively generate the kernel weights of the feedforward layers based on the style code. Specifically, we apply an attention mechanism over K kernels conditioning on style codes to modulate stylized facial expressions of the target face adaptively. Thanks to this adaptive mechanism, our method turns the one-to-many mapping problem (Wang et al. 2022) to the style-controllable one-to-one mapping in the one-shot setting, thus effectively improving the lip-sync in different styles and producing more convincing facial expressions.
- Extensive experiments demonstrate that our method can generate photo-realistic talking faces with diverse speaking styles while achieving accurate lip synchronization.

## Proposed Method
- The proposed framework can create photo-realistic taking videos in which the target speaker speaks the speech content with the speaking style reflected in the style clip.
- The proposed framework consists of four components: (1) an audio encoder which extracts the sequential pure articulation-related features from phoneme labels; (2) a style encoder which encodes the facial motion patterns in the style clip into the compact style code; (3) a style-controllable dynamic decoder which produces the stylized 3DMM expression parameters from the audio features and the style code; (4) an image renderer which generates the photo-realistic talking faces using the reference image and the expression parameters.

### Audio Encoder
- The audio encoder E a is expected to extract the articulation-related information from the audio.
- However, We observe that audio contains some articulation-irrelevant information, such as the emotion and the intensity, that affects the speaking style of the output.
- To remove such information, we adopt the phoneme labels instead of acoustics features (e.g., Mel Frequency Cepstrum Coefficients (MFCC)) to represent the audio signals.
- The phoneme labels a t−w:t+w are converted to word embeddings and then fed to a transformer encoder to obtain audio features a t−w:t+w , a t ∈ R 256 .
- The phoneme is extracted by a speech recognition tool.

### Style Encoder
- The style encoder extracts the speaking style reflected in the style clip
- Since the speaking style is the dynamic facial motion patterns, it is irrelevant to the style clip's face shape, texture, and illumination
- To remove such information, we employ the 3DMM (Deng et al. 2019) to convert the style video clip to the the sequential expression parameters
- Unlike previous methods that merely transfer the static expressions of the static images (Ji et al. 2022;Liang et al. 2022), we design a style encoder to model the dynamic facial motion patterns
- A transformer encoder takes the sequential 3DMM expression parameters as the input tokens
- After modeling the temporal correlation between tokens, the encoder outputs the style vectors of each token, s 1:N
- Intuitively, the speaking style in the video clip can be identified by a few typical frames, so we employ a self-attention pooling layer (Safari, India, and Hernando 2020) to aggregate the style information over the style vectors. Specifically, this layer adopts an additive attention-based mechanism, which computes the token-level attention weights using a feedforward network. The token-level attention weights represent the frame-level contributions to the video-level style code. We sum all the style vectors multiplied by the attention weights to get the final style code s ∈ R ds

### Style-Controllable Dynamic Decoder
- At the early stage, the vanilla transformer decoder is employed
- The style tokens serve as the query of the transformer decoder, and the latent articulation representations serve as the key and value
- The middle output token is fed into a feed-forward network to generate the output expression parameters
- When utilizing the aforementioned decoder, we observe the defective lip movements and facial expressions when 2020) reveals that the feed-forward layers play the most important role in transformer decoder, we replace the feed-forward layers with novel style-aware adaptive feed-forward layers
- The styleaware adaptive layer utilizes K = 8 parallel sets of weights W k , bk . Such parallel weights are expected to be the experts for modeling the distinct facial motion patterns of the different speaking styles
- Then we introduce the additional layers followed by Softmax to adaptively compute the attention weights over each set of weights depending on the style code
- The feed-forward layer weights are aggregated dynamically via the attention weights: where π k is the attention weight for k th feed-forward layer weights W k , bk . The output of style-controllable dynamic feed-forward layers is then obtained by: where g is an activation function.

### Disentanglement of Upper and Lower faces
- Experiments show that the upper face and the lower face have different motion patterns
- We model the motion patterns of the two parts with separate networks
- We divide expression parameters into the lower face group and the upper face group
- We utilize two parallel style-controllable dynamic decoders, called the upper face decoder and the lower face decoder, to generate the corresponding group
- We select 13 out of 64 expression parameters that are highly related to mouth movements as the lower face group, and the other parameters as the upper face group
- The selected mouth-related PCA expression bases are reported in the supplementary materials

### Objective Function Design
- The 3DMM expression parameters δ 1:L are generated sequentially and fed into three discriminators: a temporal discriminator D tem , a vertex-based lip-sync discriminator D sync , and a style discriminator D style .
- The temporal discriminator D tem learns to distinguish the realness of the input sequences of 3DMM expression parameters δ 1:L .
- The style discriminator D style is trained to determine the speaking style of the input sequential 3DMM expression parameters δ 1:L .
- The style discriminator follows the structure of PatchGAN and is trained using crossentropy loss and then frozen.
- The total loss is given by a combination of the aforementioned loss terms: where we use λ rec = 88, λ trip = 1, λ sync = 1, λ tem = 1 and λ style = 1.

## Experiments

### Quantitative Evaluation
- We compare our method with state-of-the-art methods
- We conduct the experiments in the self-driven setting
- We select the first image of each video as the reference image
- The samples of the compared methods are generated either with their released codes or with the help of their authors
- Since Wav2Lip only generates movements of the mouth area, the head poses are fixed in its samples
- For other methods, poses are derived from ground truth videos
- The results of the quantitative evaluation are reported in Table 1
- Our method achieves the best performance among most metrics on MEAD and HDTF
- Since Wav2Lip is trained using SyncNet as a discriminator, it is reasonable for Wav2Lip to obtain the highest confidence score of SyncNet on MEAD

### Qualitative Evaluation
- Our method is better than speaker-agnostic methods
- Our method can generate talking faces with reference speaking style while achieving accurate lip-sync and preserving speaker identity better
- Among all methods, only EAMM, GC-AVT, and our method achieve speaking style control
- EAMM and GC-AVT can only control the speaking styles reflected in the upper face, i.e., eye, and eyebrow, while failing to control the stylized mouth shape
- GC-AVT cannot preserve the speaking identity well
- Besides, both EAMM and GC-AVT cannot produce plausible background
- In terms of lip-sync, only Wav2Lip, AVCT, PC-AVS, and GC-AVT are competitive with our method
- EAMM cannot achieve accurate lip-sync

### Ablation Study
- The adaptive feedforward layer is replaced with the vanilla feedforward layer
- K = 4/16 is set in the dynamic feedforward layer
- The style discriminator is removed
- The lip-sync discriminator is removed

### Style Space Inspection
- We project the style codes to a 2D space
- For clarity, we select the speaking styles of 4 speakers from the MEAD dataset
- Each speaker has 22 speaking styles (7 emotions × 3 levels plus one neutral style)
- For each speaking style, we randomly select 10 video clips to extract style codes
- In figure 4 (a), each speaker is marked with a distinct color
- As shown, the style codes of the same speaker cluster in the style space
- This implies that the speaking styles of one speaker are more similar than those with the same emotion
- Figure 4 (b) shows the style codes from one speaker in MEAD dataset
- Each style code is marked with a color corresponding to its emotion and intensity
- Each group of style codes with the same emotion first gathers into one cluster
- In each cluster, the style codes of emotions with low intensity are close to those of the neutral emotion
- Notably, some emotions show similar facial motion patterns, such as anger vs disgust and surprise vs fear
- Thus, their style codes are close in the style space
- The aforementioned observations prove that our model is able to learn a semantically meaningful style space
- Through interpolation, our method is able to control the style intensity (by interpolating the style with a neutral style) and create new speaking styles

## Conclusion
- Our framework effectively extracts the speaking style from an arbitrary style reference video and then injects the style into the facial animations of the target speaker using our proposed style-controllable modules.
- Our method captures the spatio-temporal co-activations of the facial expressions from the style reference videos, thus leading to authentic stylized talking head videos.
- Extensive experiments demonstrate that our method creates photorealistic talking head videos with a conditional speaking style while achieving more accurate lip-sync and better identity-preservation compared with the state-of-the-art.
