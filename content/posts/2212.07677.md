---
title: "Transformers learn in-context by gradient descent"
date: 2022-12-15T09:21:21.000Z
author: "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07677v1.webp" # image path/url
    alt: "Transformers learn in-context by gradient descent" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07677)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07677).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/transformers-learn-in-context-by-gradient).

# Abstract
- Transformers have become the state-of-the-art neural network architecture
- Partly due to their celebrated ability to transfer and to learn in-context based on few examples
- Nevertheless, the mechanisms by which Transformers become in-context learners are not well understood and remain mostly an intuition
- Here, we argue that training Transformers on auto-regressive tasks can be closely related to

# Paper Content

## Introduction
- Transformers have demonstrated their superiority in numerous benchmarks and various fields of modern machine learning,
- In-context learning, or the ability to flexibly adjust their computation based on additional data given in context, is a seemingly different approach to few-shot and meta-learning, but as of today the exact mechanisms of how it works are not fully understood,
- We aim to bridge the gap between in-context and meta-learning, and show that in-context learning in Transformers can be an emergent property approximating gradient-based few-shot learning within its forward pass,
- We summarize our contributions as follows:
- We construct explicit weights for a linear self-attention layer that induces an update identical to a single step of gradient descent (GD) on a regression loss. Additionally, we show how several selfattention layers can iteratively perform curvature correction improving on plain gradient descent.
- When optimized on linear regression datasets, we demonstrate that linear self-attention-only Transformers either converge to the construction and therefore the gradient descent algorithm or generate models that closely align with models trained by GD, both in in-and out-of-distribution validation tasks.
- By incorporating multi-layer-perceptrons (MLPs) into the Transformer architecture, we enable solving non-linear regression tasks within Transformers by showing its equivalence to learning a linear model on deep representations. Empirically, we compare meta-learned MLPs and a single step of GD on its output layer with trained Transformers and demonstrate striking similarities between the identified solutions.
- We resolve the dependency on the specific token construction by providing evidence that learned Transformers first encode incoming tokens into a format amenable to the in-context gradient descent learning that occurs in the later layers of the Transformer. These findings allow us to connect learning Transformer weights and meta-learning a learning algorithm (Schmidhuber, 1987;Bengio et al., 1990;Chalmers, 1991;Schmidhuber, 1992;Thrun & Pratt, 1998;Hochreiter et al., 2001;Andrychowicz et al., 2016), specifically meta-learning to learn a linear model on deep data representations by gradient descent for efficient adaptation.

### Data transformations induced by gradient descent
- We introduce a reference linear model y(x) = W x parameterized by the weight matrix W ∈ R Ny×Nx
- The goal of learning is to minimize the squared-error loss: One step of gradient descent on L with learning rate η yields the weight change
- Considering the loss after changing the weights, we obtain where we introduced the transformed targets y i − ∆y i
- We use the terms training and in-context data interchangeably, as well as query and test token/data, as we establish their equivalence now.

### Transformations induced by gradient descent and a linear self-attention layer can be equivalent
- We have re-cast the task of learning a linear model as directly modifying the data, instead of explicitly computing and returning the weights of the model (equation 5).
- We proceed to establish a connection between self-attention and gradient descent.
- We provide a construction where learning takes place simultaneously by directly updating all tokens, including the test token, through a linear self-attention layer. In other words, the token produced in response to a query (test) token is transformed from its initial value W 0 x test , where W 0 is the initial value of W , to the post-learning prediction ŷ = (W 0 + ∆W )x test obtained after one gradient descent step.
- Proposition 1: Given a 1-head linear attention layer and the tokens e j = (x j , y j ), for j = 1, . . . , N , one can construct key, query and value matrices W K , W Q , W V as well as the projection matrix P such that a Transformer step on every token e j is identical to the gradient-induced dynamics e j ← (x j , y j ) + (0, −∆W x j ) = (x i , y i ) + P V K T q j such that e j = (x j , y j − ∆y j ).
- For the test data token (x N +1 , y N +1 ) the dynamics are identical.
- We provide the weight matrices in block form: W K = W Q = I x 0 0 0 with I x and I y the identity matrices of size N x and N y respectively. Furthermore, we set W V = 0 0 W 0 −I y with the weight matrix W 0 ∈ R Ny×Nx of the linear model we wish to train and P = η N I with identity matrix of size N x + N y . With this simple construction we obtain the following dynamics for every token e j = (x j , y j ) including the query token e N +1 = e test = (x test , −W 0 x test ) which will give us the desired result.
- We denote the corresponding self-attention weights by θ GD . Below, we provide some additional insights on what is needed to implement the provided LSA-layer weight construction, and further details on what it can achieve.
- • Full self-attention. Our dynamics model training is based on in-context tokens only, i.e., only e 1 , . . . , e N are used for computing key and value matrices; the query token e N +1 (containing test data) is excluded. This leads to a linear function in x test as well as to the correct ∆W , induced by gradient descent on a loss consisting only of the training data. This is a minor deviation from full self-attention. In practice, this modification can be dropped, which corresponds to assuming that the underlying initial weight is zero, W 0 ≈ 0, which makes ∆W in equation 6 independent of the test token even if incorporating it in the key and value matrices.
- • Reading out predictions. When initializing the y-entry of the test-data token with −W 0 x N +1 , i.e. e test = (x test , −W 0 x test ), the test-data prediction ŷ can be easily read out by simply multiplying again by −1 the updated token, since −y . This can easily be done by a final projection matrix, which incidentally is usually found in Transformer architectures.
- • Uniqueness. We note that the construction is not unique; in particular, it is only required that the products P W V as well as W K W Q match the construction. Furthermore, since no nonlinearity is present, any rescaling s of the...

### One-step of gradient descent vs. a single trained self-attention layer
- Our first goal is to investigate whether a trained single, linear self-attention layer can be explained by the provided weight construction that implements GD.
- To that end, we compare the predictions made by a LSA layer with trained weights θ * (which minimize equation 7) and with constructed weights θ GD (which satisfy Proposition 1).
- We find that a simple scaling correction on the trained weights is enough to recover the weight construction implementing GD.
- This leads to an identical loss of GD, the trained Transformer and the linearly interpolated weights θ I = (θ + θ GD )/2.

### Multiple steps of gradient descent vs. multiple layers of self-attention
- We now turn to deep linear self-attention-only Transformers.
- The construction we put forth in Proposition 1, can be immediately stacked up over K layers; in this case, the final prediction can be read out from the last layer as before by negating the y-entry of the last test token:
- We measure the alignment between the GD as well as the GD ++ trained models and the model generated by the trained TF.
- In both cases the TF aligns well with GD in the beginning of training while converging to be much better aligned to GD ++ after training.
- Outer right: Transformer performance (in log-scale) mimics the one of GD ++ well when testing on out-of-distribution tasks (α = 1).
- K layers, we observe that these models generally outperform K steps of plain gradient descent, see Figure 3.
- Their behavior is however well described by a variant of gradient descent, for which we tune a single parameter γ defined through the transformation function H(X) which transforms the input data according to x j ← H(X)x j , with H(X) = (I − γXX T ).
- We explain how a LSA layer can implement this input transformation in Appendix A.7.
- We term this gradient descent variant GD ++ .
- To analyze the effect of adding more layers to the architecture, we first turn to the arguably simplest extension of a single SA layer and analyze a recurrent 2-layer LSA model.
- We find that the trained model surpasses plain gradient descent, which also results in decreasing alignment between the two models (see center left column), the recurrent Transformer realigns perfectly with GD ++ while matching its performance on in-and out-of distribution tasks.
- Again, we can interpolate between the Transformer weights found by optimization and the LSA-weight construction with learned η, γ, see Figure 3 & 6.
- We next consider deeper, non-recurrent 5-layer LSA-only Transformers, with different parameters per layer (i.e. no weight tying).
- We see that a different GD learning rate as well as γ per step (layer) need to be tuned to match the Transformer performance.
- This slight modification leads again to almost perfect alignment between the trained Transformer and GD ++ with in this case 10 additional parameters and loss close to 0, see Figure 3.
- Nevertheless, we see that the naive correction necessary for model interpolation used in the aforementioned experiments is not enough to interpolate without a loss increase.
- We leave a search for better weight corrections to future work.
- We further study Transformers with different depths for recurrent as well as non-recurrent architectures, and find qualitatively equivalent results, see Figure 7.
- Additionally, in Appendix A.6, we provide results obtained when using softmax self-attention layers as well as when employing LayerNorm, thus essentially retrieving the standard Transformer architecture.
- We again observe (after slight architectural modifications) good learning performance and good alignment with the construction of Proposition 1, though worse than when using linear self-attention.
- These findings suggest that the in-context learning abilities of the standard Transformer with these common architecture choices can be explained by the gradient-based learning hypothesis explored here.

### Transformers solve nonlinear regression tasks by gradient descent on deep data representations
- Large Transformers exhibit in-context learning flexibility that cannot be explained by gradient descent on linear models
- This limitation can be resolved by incorporating one additional element of full-fledged Transformers: preceding self-attention layers by MLPs
- Empirically, we demonstrate this by solving non-linear sine-wave regression tasks
- Experimental details can be found in Appendix A.5

## Do self-attention layers build regression tasks?
- Proposition 1: The construction provided in Proposition 1 and the previous experimental section relied on a particular token structure where both input and output data are concatenated into a single token.
- Proposition 2: Given a 1-head linear or softmax attention layer and the token construction e 2j = (x j ), e 2j+1 = (0, y j ) with a zero vector 0 of dim N x − N y and concatenated positional encodings, one can construct key, query and value matrix W K , W Q , W V as well as the projection matrix P such that all tokens e j are transformed into tokens equivalent to the ones required in Proposition 1.
- The construction and its discussion can be found in Appendix A.3.
- To provide evidence that copying is performed in trained Transformers, we optimize a two-layer self-attention circuit on in-context data where alternating tokens include input or output data i.e. e 2j = (x j ) and e 2j+1 = (0, y j ).
- We again measure the loss as well as the mean of the norm of the partial derivative of the first layer's output w.r.t. the input tokens during training, see Figure 5.
- First, the training speeds are highly variant given different training seeds, also reported in Garg et al. (2022). Nevertheless, the Transformer is able to match the performance of a single not two steps gradient descent. Interestingly, before the Transformer performance jumps to the one of GD, token e j transformed by the first self-attention layer becomes notably dependant on the neighboring token e j+1 while staying independent on the others which we denote as e other in Figure 5.
- We interpret this as evidence for a copying mechanism of the Transformer's first layer to merge input and output data into single tokens as required by Proposition 1.
- Then, in the second layer the Transformer perform a single step of GD. Notably, we were not able to train the Transformer with linear selfattention layers, but had to incorporate the softmax operation in the first layer.

## Discussion
- Transformers learn to learn by gradient descent based on their context.
- In-context learning is driven by gradient descent, in short -Transformers learn to learn by gradient descent based on their context.
- Learning Transformer weights corresponds to the outer-loop which then enables the forward pass to transform tokens by gradient-based optimization.
- To provide evidence for this hypothesis, we build on Schlag et al. (2021) that already linked linear selfattention layers with (fast-)inner loop learning by the delta rule.
- We slightly diverge from their settings and focus on in-context learning based on input-and target training pairs.
- Given these pairs present in single tokens, we provide a simple weight construction that shows the equivalence of a single layer of linear self-attention and a step of GD.
- Crucially, this construction is not suggestive but when training multi-layer self-attention-only Transformers on simple regression tasks, we provide strong evidence that it is actually found when optimizing Transformers.
- This allows, at least in our restricted problems settings, to explain mechanistically in-context learning in trained Transformers and its close resemblance to GD observed by related work.
- Further work is needed to incorporate regression problems with noisy data and weight regularization into our hypothesis.
- We speculate aspects of learning in these settings are meta-learned -e.g., the weight magnitudes to be encoded in the self-attention weights.
- Additionally, we did not analyse logistic regression for which one possible weight construction is already presented in Zhmoginov et al. (2022).
- Our refined understanding of in-context learning based on gradient descent motives us to investigate how to improve it.
- We are excited about several avenues of future research.
- First, to exceed upon a single step of gradient descent in every self-attention layer it could be advantageous to incorporate so called declarative nodes (Bai et al., 2019;Gould et al., 2021;Zucchet & Sacramento, 2022) into Transformer architectures.
- Second, our findings are restricted to small Transformer and simple regression problems.
- Further research is needed to mechanistically understand in-context learning in larger models and language modeling.
- Third, we are excited about targeted modifications to Transformer architectures, or their training protocols, leading to improved gradient descent based learning algorithms or allow for alternative in-context learners to be implemented within Transformer weights, augmenting their functionality.
- Finally, it would be interesting to analyze in-context learning in HyperTransformers (Zhmoginov et al., 2022) that produce weights for target networks and already offer a different perspective on merging Transformers and meta-learning.
- Here, Transformers transform weights instead of data and could potentially allow for gradient computations of weights deep inside the target network lifting the limitation of GD on linear models analyzed in the present study.
