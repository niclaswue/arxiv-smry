---
title: "Manifestations of Xenophobia in AI Systems"
date: 2022-12-15T14:58:32.000Z
author: "Nenad Tomasev, Jonathan Leader Maynard, Iason Gabriel"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07877v1.webp" # image path/url
    alt: "Manifestations of Xenophobia in AI Systems" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07877)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07877).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/manifestations-of-xenophobia-in-ai-systems).

# Abstract
- Xenophobia is one of the key drivers of marginalisation, discrimination, and conflict
- Many prominent machine learning fairness frameworks fail to comprehensively measure or mitigate the resulting xenophobic harms
- Here we aim to bridge this conceptual gap and help facilitate safe and ethical design of artificial intelligence solutions
- We ground our analysis of the impact of xenophobia by first identifying distinct types of xenophobic harms
- Then applying this framework across a number of prominent AI application domains, reviewing the potential interplay between AI and xenophobia on social media and recommendation systems, healthcare, immigration, employment, and biases in large pre-trained models

# Paper Content

## INTRODUCTION
- The increasing scope of the field of AI ethics and corresponding analyses of algorithmic fairness reflect the ubiquitous nature of AI deployments in our daily lives.
- There is an increasing need to operationalize algorithmic fairness in order to minimise the risks involved with the use of technology and ensure an equitable distribution of its benefits.
- This is especially important since studies have shown that the risks of algorithmic systems can be imbalanced across different characteristics of the people who are affected by classifiers and decision-making systems.
- Much attention has focused on issues of fairness and discrimination associated with race and legal gender, though these frameworks may sometimes be ill-suited for potentially unobserved and less frequently recorded protected characteristics including sexual orientation and gender identity.
- Machine learning fairness frameworks need to be grounded in concrete use-cases, lived experiences, and take into account structural and historical power asymmetries.
- With that in mind, in this paper we focus on a prominent and widespread phenomenon which has been given comparatively little attention to date in machine learning fairness -xenophobia, a discrimination directed at the other and the foreign.
- Xenophobia is a phenomenon that still shapes the political landscape of the modern world.
- The Covid-19 pandemic has led to a sharp rise in xenophobia across offline and online communities.
- Thinking about xenophobia opens up a critical line of inquiry by shifting the focus onto the source of xenophobic discrimination rather than leaving it implicitly unnamed and undefined, and allows us to examine the role of technology in the formation of identities and the amplification of prejudice, discrimination, and hate.
- Existing machine learning fairness mitigation strategies tend to focus on ensuring equitable outcomes for legally protected groups.
- Mitigations aimed at combating xenophobia may need to go beyond that, as not all groups commonly perceived as foreign are given such legal protections.
- An additional challenge lies in the need to make explicit normative choices when distinguishing between us and them, the familiar and the foreign.
- The institutions of power often define and impose such categorizations, and the history of the US census is an illustrative example of how the ways in which data on race have been collected have drastically changed over time.
- Thinking about xenophobia opens up a critical line of inquiry by shifting the focus onto the source of xenophobic discrimination rather than leaving it implicitly unnamed and undefined.
- It holds that "when thinking about social, national justice we need not care about foreign distant others. And sucked into this loophole are documented as well as undocumented residents and citizens who are presumed-alien. "

## ON XENOPHOBIA
- Xenophobia is a fear, dislike or hate of people who are perceived to be different, including their culture and customs
- Xenophobia is commonly understood as a kind of hostility or prejudice directed towards foreigners, immigrants or, more broadly, those construed as "others"
- Xenophobia can manifest as a fear, dislike or hate towards people who are perceived to be different, including their culture and customs, and has been associated with a range of social and psychological roots
- Xenophobia is distinct from other forms of intergroup prejudice, notably racism, and the strategies needed to mitigate racism and xenophobia may differ
- While the history of xenophobia can not be decoupled from the artifacts of the colonial past and racism, xenophobia is conceptually distinct from racism, and the strategies needed to mitigate racism and xenophobia may differ
- A framework which deems any differential outcomes between citizens and non-citizens to be xenophobic is, as such, unlikely to be accepted as offering relevant ethical guidance for AI systems.
- At the same time, differential outcomes for non-citizens can indeed arise from xenophobia, and may extend up to persecution, ethnic cleansing, and genocide.

## PRACTICAL CONSIDERATIONS
- Demonstrable discrimination against those perceived to be foreign exists
- Risks associated with the development of technological solutions include the potential for misuse or abuse
- Inclusive and xenophilic AI solutions are needed to address these risks

## Social media
- Social media platforms may act as an instrument of amplification of xenophobia
- Low barriers to entry, reliance on user-generated content, difficulties in moderation, and the emergence of filter bubbles present difficulties that may lead to an easier spread of fake news and potentially dangerous and hateful speech
- These platforms may also provide mechanisms for supportive discourse, expressions of positive and inclusive views towards those who are foreign, and a medium for communicating information relevant to the effective exercise of individual rights
- The dual role of social media platforms in shaping culture, both to foster cross-cultural understanding, as well as to be abused for structural and cultural violence

## Promise.
- Recent advances in understanding and mitigating social biases in language models
- Coupled with a development of benchmarks for identification of malignant stereotypes in AI models, may pave the way towards quantifying the extent of xenophobic bias and harm in large pre-trained AI systems
- AI systems need to be designed to transcend ethno-nationalistic exclusionary cultural narratives and challenge entrenched xenophobic views
- This would involve developing better ways of incorporating domain knowledge on the cross-cultural underpinnings of social friction, and breaking away from a stochastic re-enactment of historical patterns
- There is a pressing need to imbue general AI systems with capacities for understanding and reasoning about the world, and therefore the ability to integrate and reinterpret information, deriving less-biased and better reasoned conclusions about social phenomena
- As for contemporary base models, the solution likely needs to be multi-faceted and case-specific
- Multi-lingual models may help with incorporating a wider variety of viewpoints, especially if grounded in geospatially diverse imagery aimed at improving representation -though there are still open challenges when it comes to ensuring equal performance
- Self-supervised learning in computer vision has shown promise in terms of reducing the amount of bias
- Data curation plays a central role in mitigating the effects of historical bias
- Mechanisms for incorporating comparative and natural language human feedback to refine and improve model outputs
- Oftentimes, human feedback is necessary in order to mitigate the effects of historical bias
- A more holistic participatory approach is needed in order to avoid pitfalls of technosolutionism

## Immigration
- AI systems may exacerbate or mitigate xenophobic harms by managing migration flows
- Migration between culturally distinct countries and regions is often accompanied by a growing resentment of immigrants and refugees
- Complicity and denialism hamper the implementation of remedial measures and a coordinated strategy spanning international, national and regional efforts is required in order to recognise and tackle the magnitude of the problem
- States have already been quick to employ new digital and AI technologies in an effort to more tightly and effectively control migration

## Risks. Numerous technological solutions incorporating AI
- Proposed for border control and migrant assessment
- There is a basic risk that such data may then be used for achieving questionable geopolitical, socio-economic, demographic and other goals
- AI systems have also been proposed for automatic assessment of immigration forms
- This risks perpetuating the historical, national, ethnic, and racial stereotypes
- While the management of migration risks may be legitimate in principle, such an approach creates enormous potential for harmful and unfair outcomes to individuals based on associative assumptions about risk that are built into AI assessment systems
- Harms can potentially manifest in different walks of life, for example, systems for the assessment of the likelihood of the H1B visa approval in the US immigration process
- Migrants' human rights in face of the use of AI technology in immigration enforcement

## Healthcare
- Xenophobia has been identified as an important determinant of health
- Exposure to repeated xenophobic prejudice in a healthcare context may erode trust and adversely affect care-seeking patient behaviours
- Rather than being an exclusive artefact of misguided public health policy, medical xenophobia can manifest via negative attitudes and practices by health workers, and has been shown to be deeply entrenched in numerous public health systems
- Discriminatory practices are further amplified in case of particularly vulnerable migrant populations, like refugees and undocumented migrants
- Even prior to engaging with health services, xenophobia and racism may impact health via early exposure to adverse childhood experiences
- In Adja et al. [5], the authors point out that the World Health Organization (WHO) defines health as "a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity"
- A holistic approach for mitigating the effects of medical xenophobia must therefore incorporate notions of social well-being and account for the impact of the social determinants of health and the disparities faced by foreigners outside of the boundaries of direct medical care
- Medical xenophobia has also historically played an important role in shaping epidemiological perception related to the spread of infectious diseases.
- As highlighted in [104,269], the act of blaming foreigners for the spread of infectious disease could be readily seen in the accounts related to the devastating epidemic of venereal syphilis in Europe in the 15th and 16th century
- Paleopathology can help alert us to past instances of medical xenophobia, raise public awareness and reduce future xenophobic discrimination in times of public health emergency
- Participatory approaches play a key role in medicine [4,17,95,273], where engaging with patient focus groups helps the clinicians align on the desired outcomes and improve the overall patient experience.
- Participatory approaches to developing AI solutions have similarly been identified as a key ingredient in safe and inclusive AI system design [48], though their implementation is not without challenges [286]

## Employment
- AI systems are increasingly used in decision-making around employment
- Differential outcomes in employment translate to direct material harms
- Unequal access to employment opportunities and unfair compensation are among the key drivers of social inequality
- Labour market attachment is also critical to the livelihood and identity formation of individuals and groups
- There are a number of different ways in which employment discrimination manifests for foreign workers
- Citizens and non-citizens can suffer xenophobic harms in the employment process
- Historically, the worst jobs, with the hardest working conditions and the least pay had been reserved for immigrants
- Immigrants play an important role in the labour market, due to their diverse skill sets, life experience, educational profiles, and background
- Assimilation plays a key role in modulating xenophobic employment discrimination
- Language barriers, cultural differences, lack of networks, and social capital present challenges to securing employment
- Lack of elasticity contributes to the wage gap between the immigrant and native job seekers
- Existing legal frameworks are often insufficient in protecting the rights of foreign employees and addressing the problem of xenophobia in the workplace
- Technology should aim to complement and help coordinate numerous organisational initiatives, collective action, legislative and legal measures, administrative measures, political and educational action and international standards

## Risks.
- Recent work on enumerating risks in large language models categorized the potential downstream harms across six types: discrimination, exclusion and toxicity;
- these categories intersect with material, representational and wider societal harms through which we've been discussing the potential impact of xenophobia in sociotechnical systems, adding another layer of analysis.
- LLMs may potentially surface direct xenophobic discrimination, they may pose information hazard by revealing sensitive or private information about vulnerable individuals, spread misinformation that disparages outgroups, be incorporated in technologies weaponized by malicious xenophobic users, subtly reinforce discriminatory stereotypes via repeated human-computer interaction, and indirectly lead to disparate socioeconomic and environmental impact.
- Prior studies have identified a concerning aptitude of such systems towards accurately emulating extremist views when prompted [203], and numerous ethnic biases [182].
- As this type of content may potentially influence and radicalise people into far-right ideologies at an unprecedented scale and pace, more focused investments in data curation are needed to address underlying risks [32].

## Promise
- Assistive technologies have the potential to improve the overall candidate experience
- ML models may also be seen as a powerful lens into existing hiring practices
- Recently developed frameworks for ML explainability and fairness may be utilised towards this end
- Simulations may also prove helpful for policy and process design

## Stereotypes in Large Pretrained Models
- Large language models (LLM) can be fine-tuned and adapted to a range of applications and use cases, becoming components of even larger and more complex AI systems.
- These systems are sometimes referred to as foundation models (or base models) in recognition of their emerging role as building blocks in such systems.
- The development of base models usually involves large-scale pre-training on increasingly large datasets which are increasingly hard to curate.
- This poses unique safety and ethics concerns, due to a difficulty of anticipating the totality of harmful biases and stereotypes, as well as the variety of use cases stemming from their wide applicability.
- Large language models may exhibit varying degrees of positive or negative sentiment across national and religious groups, inheriting features of historical and contemporary discourse.
- Text-to-image models have recently been shown to amplify demographic stereotypes at scale.
- Multimodal systems introduce another layer of complexity, with cross-modal stereotypes becoming detectable when considering modalities in relation to each other.
- Any biases in base models would have the potential to emerge across their downstream applications, making it especially important to evaluate such models and their underlying training data prior to a wider release.

## TOWARDS XENOPHILIC AI SYSTEMS 4.1 A moral imperative
- The problem of xenophobic bias has the potential to negatively impact people's lives across a wide range of digital services and contexts.
- ML systems may be used or redesigned to address xenophobic bias in the domain of AI.
- When successful, xenophilic technology helps to build relationships between people of different backgrounds and to enable the rich and productive sharing of cultural differences.
- Efforts to tackle xenophobia in AI, and to build xenophilic technology, require at least two things: a commitment among those developing and deploying these technologies to address the problem of xenophobic discrimination, and technical innovation to develop tools that can assist these processes.
- In the case of AI services and products, the reasons for inaction may have certain domain-specific features.
- Cumulatively, these technologies therefore have a powerful shaping effect on the contours of social relationships at a global level.

## Measurement and mitigation
- There is a pressing need to move away from hegemonic machine learning fairness approaches
- Mitigating xenophobia may play a pivotal role in questioning and reassessing how technical ML fairness is defined and operationalised

## Measuring xenophobia.
- Central to most technical machine learning fairness mitigation approaches is the ability to measure and quantify the degree to which a given system violates fairness constraints.
- A thorough assessment of such harms may require multiple scopes of analysis [118], and a consideration of AI systems not only in isolation, but also within the pathways of their projected use, with humans in the loop [340].
- Most approaches to measuring fairness rely on an understanding of groups and identities across which performance should be measured, coupled with the availability of the corresponding information in the underlying data.
- This is why it's especially concerning that there are deep issues with the overall quality of race and ethnicity data across observational databases [244], which often contain insufficiently fine-grained and static racial and ethnic categories [130,294], involve discrepancies between recorded and self-reported identities [211] and come with data collection [16,99,263,316], privacy [259] and governance challenges.
- Despite recent integration efforts [216], better data [249] and model [75,209] documentation are required to promote the understanding of ethnic representation and data provenance.
- Fairness approaches rooted in comparing model performance and impact across groups need to be more cognizant of the normative nature of defining group identities [177], and there needs to be a deeper engagement with domain experts to establish consistent and meaningful ways of identifying in-groups and out-groups within the context of each AI application.
- This is especially relevant for systems with a potentially global scope.
- There is a pressing need for the development of frameworks and benchmarks for assessing xenophobic impact.
- Meanwhile, practitioners should be encouraged to consider approximate assessments by including ethnicity and nationality in their ML fairness assessments, considered intersectionally [320] with gender, race, socioeconomic status and other relevant sensitive attributes.
- Commitment to intersectional analysis may also act as a driver of technical innovation, inspiring the development of new methods that can adequately cope with such problems, as well as furthering the development of methods designed to operate outside the confines of pre-defined categorizations [146].
- A deeper understanding of in-groups and out-groups and intergroup power dynamics in sociotechnical systems is not only a prerequisite for group fairness assessment, but also for most counterfactual [150,174,241] approaches to ML fairness.
- Counterfactual and contrastive [63] methodologies may prove useful for more concretely identifying the pathways of discrimination, while leveraging expert knowledge.
- For unlocking their potential, it is crucial to ensure that the assumed causal model embodies a faithful representation of the world [164,264], avoiding the kinds of reductionist assumptions that may erase or fail to consider the entirety of out-group identities and experiences.
- Meanwhile, individual fairness [90,91,124,281] may provide a potentially usefully lens to apply to processes in the interim, in absence of reliable and applicable categorizations and frameworks.
- Yet, the requirement that similar people be treated similarly may itself encode xenophobic bias and inadvertently obscure and hide discriminatory practices.
- This problem would arise if the in-group members are considered to be similar to each other, the out-group members are considered to be similar to each other, whereas ingroup and out-group members are not seen as similar by the model.
- This raises deeper questions on what criteria would be permissible for determining similarity between people, and future research on eliciting [28,151] or learning [143,214] similarity measures for enforcing individual fairness will need to address these serious concerns.
- Investigations aimed at identifying xenophobic bias and (un)fairness need to not only improve and extend the current frameworks and categorizations, but also re-center the question of discrimination onto the perpetrators, rather than only concerning themselves with the affected marginalised out-groups.

## Mitigation strategies.
- The complexity and pervasiveness of xenophobia in society and consequently in the data used to train AI systems calls for the development of both technical and nontechnical mitigation strategies for preventing the adverse impact of such systems on the most vulnerable and marginalised groups in society, citizens and non-citizens alike.
- These are currently lacking, in part due to a systemic failure to recognize the importance of xenophobic harms, and in part due to already discussed deficiencies in recording and utilising the relevant sensitive information, compounded by insufficient investments in participatory approaches.
- Until such comprehensive frameworks for identifying and quantifying xenophobic harms are fully developed, practitioners may want to consider alternative recent approaches, tailored for systems with missing or incomplete data, that are capable of providing partial safety and robustness guarantees without an explicit specification of an underlying categorization of interest.
- This usually involves considering plausible groupings based on the available model inputs, and improving worst-case or average-case performance across such plausible group structures [131,159,166].
- The use of proxy targets [117,123] is an alternative, though it comes with risks of stereotyping.
- Practical practitioners need to ensure robust evaluation [197] while accounting for uncertainty and incompleteness [20] in the data.
- Purely technical problem specifications rarely map onto socially acceptable mitigation outcomes [284], making it imperative to avoid technosolutionism and to embrace interdisciplinary efforts.
- Transparency and model audits [253] need to complement such fairness assessments, especially given the difficulties involved with defining comprehensive quantitative measures, that would consistently capture all instances of xenophobic harms.
- Advances in AI explainability research [40,110] may be helpful in such audits [1,42,128], enabling a rapid identification of model harms [29] possibly enriched with counterfactual explanations [287,317].
- Explainability methods may also be helpful in identifying vulnerable groups [295] within each AI application context, and provide a fine-grained breakdown of model performance [282].
- These efforts need to be coupled with the appropriate escalation procedures and enable recourse -which may require a greater focus on designing appropriate interventions [157].

## CONCLUSIONS
- Despite being one of the key drivers of discrimination and conflict worldwide, xenophobic bias is yet to be formally recognised and comprehensively assessed in AI system development.
- This blind spot introduces a considerable risk of amplifying harms to marginalised communities and fuelling fear and intolerance towards foreigners, which may lead to violence and conflict both at the societal level and more widely.
- Our overview of several prominent technological use cases of social media, immigration, healthcare, employment and via foundation models -reveals how historical and contemporary xenophobic attitudes may manifest in data and AI systems.
- Moreover, we illustrate the effect of these tendencies by drawing upon three categories of potential xenophobic harms: those that cause direct material disadvantage to people perceived as foreign, representational harms that undermine their social standing or status, and the wider societal harms which include barriers to the successful exercise of civic and human rights.
- Significantly, given the scope and depth of AI services, people who experience xenophobic bias in one domain of automation can also expect to experience unequal treatment across a range of other domains, further compounding their situation.
- In this context, we suggest that the development of xenophilic technology is crucial for the ethical design and deployment of AI systems, understood as systems that treat people equitable at the societal level.
- AI should help promote civic inclusion, oppose the malignant "othering" of marginalised groups, and be centered around the potential to cultivate the kind of rich and productive cross-cultural discourse that is appropriate for a world like our own.
- Coordinated interdisciplinary action, participatory frameworks, and an unwavering commitment of technologists are required to fashion such a future, along with technical innovation in addressing the existing technological bases of xenophobia in ML systems.
