---
title: "Hierarchically branched diffusion models for efficient and interpretable multi-class conditional generation"
date: 2022-12-21T05:27:23.000Z
author: "Alex M. Tseng, Tommaso Biancalani, Max Shen, Gabriele Scalia"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10777v1.webp" # image path/url
    alt: "Hierarchically branched diffusion models for efficient and interpretable multi-class conditional generation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10777)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10777).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/hierarchically-branched-diffusion-models-for).

# Abstract
- Diffusion models have achieved justifiable popularity by attaining state-of-the-art performance in generating realistic objects from seemingly arbitrarily complex data distributions
- However, their iterative nature renders them very computationally inefficient during the sampling process
- For the multi-class conditional generation problem, we propose a novel, structurally unique framework of diffusion models which are hierarchically branched according to the inherent relationships between classes
- In this work, we demonstrate that branched diffusion models offer major improvements in efficiently generating samples from multiple classes
- We also showcase several other advantages of branched diffusion models, including ease of extension to novel classes in a continuous-learning setting and a unique interpretability that offers insight into these generative models

# Paper Content

## Introduction
- Diffusion models have gained major popularity as a method for generating data from complex data distributions
- In recent years, diffusion models have shown notable success in generating images and videos
- They have rapidly become a staple in the field of generative models
- To generate 50k ImageNet samples on an A100 GPU, for example, can take about 5 days
- Given the carbon emissions and energy consumption required by these neural networks, there is a dire need for more computationally efficient methods where possible

## Related Work
- Conditional generation by class label was first proposed in Song et al. (2021)
- This method effectively combines the external pre-trained classifier from Song et al. (2021) and the diffusion model into one neural network, where the class label guides the generation of objects to specific classes.
- This method of conditional generation has achieved state-of-the-art performance in sample quality (Rombach et al., 2021;Ho et al., 2022).
- In contrast with the previous method, this strategy can be applied to both continuous-and discrete-time diffusion models.
- As a tradeoff, however, it loses modularity (i.e. we can no longer swap in different classifiers).
- It also is of dubious utility to supply labels at later diffusion times, as heavily diffused objects of distinct classes are generally indistinguishable from each other.

## Hierarchically branched diffusion models
- A hierarchically branched diffusion structure is proposed
- The structure mirrors the hierarchical structure of class similarities in a dataset
- A branched diffusion model is created by forward diffusing each class and computing the time points at which classes become just noisy enough so that they are indistinguishable from each other
- The branched diffusion models achieve similar generative performance to the current state-of-the-art label-guided strategy

## Efficient sampling from branched diffusion models
- Branched diffusion models offer significant improvements in computational efficiency when sampling from multiple classes.
- In a branched diffusion model, much of the diffusion process is shared for many classes.
- Partially reverse-diffused intermediates at branch points can be cached and reused to generate samples for multiple classes.
- Note that although a branched diffusion model takes the same amount of computation to generate samples of a single class compared to a standard linear model, branched models enjoy significant savings in computational efficiency when sampling multiple classes.

## Extending branched diffusion models to novel classes
- The unique hierarchical structure of branched diffusion models allows new classes to be added easily
- After fine-tuning on a branched diffusion model, a new digit class can be accommodated without affecting the ability to generate pre-existing digits
- In contrast, a linear diffusion model is more difficult to extend to accommodate new classes

## Interpretability of branched diffusion models
- branched diffusion models offer unique interpretations into the generative process
- by encoding the relationship between classes explicitly as branch points, branched diffusion models can reveal insight into common features between classes
- this can even generate analogous objects between different classes

### Hybrids at branch points
- In a branched diffusion model, branch points act as "minimal times of indistinguishability".
- In the reverse-diffusion process, classes split off at a branch point and start reverse diffusing according to different processes.
- Symmetrically, in the forward-diffusion process, two branches meet at a branch point when the classes become sufficiently noisy such that they cannot be distinguished from each other.
- Thus, for two similar classes (or two sets of classes), the reverse-diffusion intermediate at a branch point naturally encodes features which are shared (or otherwise intermediate or interpolated) between the two classes (or sets of classes).
- Each hybrid digit we show is the reverse-diffusion starting point for both images above and below it.
- We applied a small amount of Gaussian smoothing to the hybrids for ease of viewing.
- Firstly, we can visualize hybrid intermediates as partially reverse-diffused objects right before a branch splits into two distinct digit classes.
- Formally, for two classes c 1 and c 2 , let t b be the first branch point (earliest in diffusion time) in which c 1 and c 2 are both in the same branch.
- We define a hybrid object between classes c 1 and c 2 as the generated object from reverse diffusion following Algorithm 2 from time T until time t b for either c 1 or c 2 (it does not matter which since we stop at t b ).
- For example, on our MNIST branched diffusion model, hybrids tend to show shared characteristics that underpin both digit distributions (Figure 5).
- On our branched model trained on tabular data, we see that hybrids tend to interpolate between distinct feature distributions underpinning distinct classes, acting as a smooth transition state between the two endpoints (Figure 6).
- Note that although a branched diffusion model can successfully generate distinct classes even with very conservative branch points (i.e. late in diffusion time), the interpretability of the hybrid intermediates is best when the branch points are minimal (earliest in diffusion) times of indistinguishability.

### Transmutation between classes
- In a diffusion model, we can traverse the diffusion process both forward and in reverse
- Because branched diffusion models encode shared or interpolated characteristics between classes at branch points, this allows for a unique ability to perform transmutation (or analogous conditional generation) between classes
- Formally, say we have an object x 1 of class c 1 . We wish to transmute this object into the analogous object of class c 2 .
- Let t b be the first branch point (earliest in diffusion time) in which c 1 and c 2 are both in the same branch. Then in order to perform transmutation, we first forward diffuse x 1 to x b âˆ¼ q t b (x|x 1 ).
- Then, we reverse diffuse to generate an object of class c 2 (Algorithm 2), as if we started at time t b with object x b .
- On our MNIST branched diffusion model, we transmuted between 4s and 9s (Figure 7).
- Intriguingly, the model learned to transmute based on the slantedness of a digit. That is, slanted 4s tend to transmute to slanted 9s, and vice versa.
- To further quantify the analogous conditional generation between classes, we transmuted between letters on our tabular branched diffusion model (Figure 8).
- Transmuting between V and Y (and vice versa), we found that all features showed a positive correlation when comparing the value of the feature before and after transmutation. That is to say, letters with a larger feature value tended to transmute to letters also with a larger feature value, even if the range of the feature is different between the two classes.
- Over all features, we found that many were strongly correlated before and after transmutation. This underscores the ability of branched diffusion models to transmute objects of one class to the analogous object in a different class.

## Discussion
- branched diffusion models represent a tradeoff between training and sampling complexity
- branched diffusion models require more parameters and more training time than their linear counterparts
- training branched diffusion models took more epochs than a linear model, but the training time was far lower than the expected time based solely on total branch length or model capacity
- branched diffusion models enjoy significant savings in computational efficiency during sampling
- branched diffusion models can easily be combined with the many other methods for improving sampling efficiency proposed in other works

## Conclusion
- The branched diffusion model offers an alternative framework of conditional generation for discrete classes
- Compared to the traditional linear diffusion models, branched models are more efficient to sample multiple classes from
- Branched models are easily extendable to new, never-before-seen classes through a short and efficient finetuning step which does not lead to catastrophic forgetting of pre-existing classes
- Branched diffusion models can offer some insights into interpretability. Namely, reverse-diffusion intermediates at branch points are hybrids which encode shared or interpolated characteristics of multiple data classes. Furthermore, this allows branched models to transmute objects from one class into the analogous object in another class.
