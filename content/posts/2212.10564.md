---
title: "Does unsupervised grammar induction need pixels?"
date: 2022-12-20T18:59:50.000Z
author: "Boyi Li, Rodolfo Corona, Karttikeya Mangalam, Catherine Chen, Daniel Flaherty, Serge Belongie, Kilian Q. Weinberger, Jitendra Malik, Trevor Darrell, Dan Klein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10564v1.webp" # image path/url
    alt: "Does unsupervised grammar induction need pixels?" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10564)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10564).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/does-unsupervised-grammar-induction-need).

# Abstract
- Extralinguistic signals such as image pixels are crucial for inducing constituency grammars
- Past work has shown substantial gains from multimodal cues, but these gains persist in the presence of rich information from large language models
- LC-PCFG outperforms previous multi-modal methods on the task of unsupervised constituency parsing, achieving state-of-the-art performance on a variety of datasets
- Moreover, LC-PCFG results in an over 50% reduction in parameter count, and speedups in training time of 1.7x for image-aided models and more than 5x for video-aided models, respectively

# Paper Content

## Introduction
- Recent work, based on the idea of semantic bootstrapping (Pinker, 1984), has shown that model performance on unsupervised grammar induction can be improved by pairing text data with extralinguistic inputs such as images (Zhao and Titov, 2020;Shi et al., 2019), videos (Zhang et al., 2022a), audio (Zhang et al., 2021) or facial semantics (Zhang et al., 2021).
- These studies suggest that pairing text with extralinguistic, multi-modal data may help to ground text-only models in the multi-modal world and thereby facilitate grammar acquisition.
- However, there is limited direct evidence for the hypothesized import of this grounding signal from other modalities into text, beyond the observed increase in grammar induction performance.
- Moreover, these studies used text-only baselines based  on simple lexical word embeddings that are learned from scratch. This benefit found from adding extralinguistic data might be due to an increase in the total volume of input available to the learned word embeddings.
- Perhaps similar improvements could be achieved simply with more textual data instead of incorporating data from other modalities.

### Unsupervised parsing
- Unsupervised parsing, the task of inducing syntactic structure from text, has been extensively studied over the past few decades
- Many methods for unsupervised parsing rely on signals from text alone (e.g., Lari and Young, 1990;Carroll and Charniak, 1992;Klein and Manning, 2002;Smith and Eisner, 2005)
- However, recent work has suggested that multi-modal signals may be needed for accurate grammar induction (Shi et al., 2019;Zhao and Titov, 2020;Zhang et al., 2021Zhang et al., , 2022a)
- Prior work has argued that the correspondence between visual features and certain words may facilitate identification of syntactic constituents and therefore serve as a key component for unsupervised grammar induction (Shi et al., 2019)
- Subsequent studies built upon this intuition. These studies showed that adding visual and auditory features to random word embeddings, or lexical word embeddings such as fastText, can improve model performance on unsupervised parsing tasks

### LLM representations for unsupervised parsing
- Recent advances in large pretrained language models have enabled vast improvements on a wide range of downstream tasks, including syntactic parsing (e.g., Devlin et al., 2019;Radford et al., 2019;Kitaev et al., 2018).
- Prior work that studied the effect of grounding for unsupervised parsing has focused on text-only baselines based on much weaker lexical representations.
- The stronger word representations learned by current pretrained language models could obviate the need to use multimodal grounding for unsupervised parsing.
- The goal in statistical grammar induction is to automatically induce syntactic structure over a text corpus.
- To do this, a grammar formalism is assumed, and the grammar parameters are optimized to fit the data.
- In this work, we use LLM representations to bootstrap such a model and provide a stronger textonly baseline to evaluate the benefit of multimodal data on the task of unsupervised constituency parsing (See Figure 2 for an overview).
- We use Compound Probabilistic Context-Free Grammars (C-PCFGs) (Kim et al., 2019) as a testbed, boosting their performance with LLM features while circumventing the need for multimodal regularization objectives as introduced by prior work (Zhao and Titov, 2020;Zhang et al., 2021Zhang et al., , 2022a)).
- Background. C-PCFGs extend the Probabilistic Context Free Grammar (PCFG) formalism, this formalism is defined by a 5-tuple G = (S, N , P, Σ, R), consisting of a start symbol S, a set of non-terminals N , a set of pre-terminals P, a set of terminals Σ, and a set of derivation rules R: PCFGs define a probability distribution over transformation rules π = {π r } r∈R , and inference may be performed efficiently over them using the inside algorithm (Baker, 1979).
- In neural PCFGs, this distribution may be formulated as follows: where u are transformation vectors for each production rule, w are learnable parameter vectors for each symbol, and f 1 and f 2 are neural networks. This formulation allows neural PCFGs to enjoy the benefits of fast inference as before, while additionally leveraging distributional representations through symbol embeddings.
- The strong context-free assumption does not allow PCFGs to leverage global information when computing production probabilities in inference.
- Compound Probabilistic Context-Free Grammars (C-PCFGs) (Kim et al., 2019) remedy this by formulating rule probabilities as a compound probability distribution (Robbins, 1956): Where z is a latent variable generated by a prior distribution (generally assumed to be spherical Gaussian) and E G = {w N |N ∈ {S} ∪ N ∪ P} denotes the set of symbol embeddings. Rule probabilities π z are additionally conditioned on this latent: Importantly, this formulation allows global information to be shared across production decisions during parsing through the latent z while maintaining the context-free assumption needed for efficient inference when z is fixed.
- C-PCFGs are optimized with variational methods (Kingma and Welling, 2013), since the introduction of z makes inference intractable.
- At inference time, given a sentence x, the variational inference network q φ is used to produce the latent z = µ φ (g(E(x))). Here, g is a sentence encoder used to generate a vector representation given token embeddings E(x).
- For a more thorough treatment of C-PCFGs, please see (Kim et al., 2019).

## Experiments

### Image-assisted Parsing
- LC-PCFG is compared against VG-NSL and VC-PCFG, two methods for unsupervised grammar induction which employ visual regularization in the form of a visual matching loss, which encourages the learned representations of caption constituents to be aligned with their matching captions, and requires image-caption pairs during training.
- Setup: We follow the experimental setup of Zhao and Titov (2020), evaluating on their splits of the MSCOCO 2014 dataset (Lin et al., 2014).
- Results: LC-PCFG achieves the best overall performance while maintaining comparable runtime with the text-only C-PCFG baseline (tested with a single RTX 2080 Ti GPU) and reducing model size by 85% compared to VC-PCFG.
- Figure 4 shows an example parse tree generated by our model.

### Video-aided Parsing
- LLMs help unsupervised parsing in comparison with image-regularized models
- Images are static and often fail to reflect all constituents encountered in sentences, such as verb phrases (i.e. temporal actions), etc.
- We display an example in Figure 5.
- We could identify "flip the sandwich over and press down" from the provided video frames and other modalities' features, while "flip" or "press" would be unidentifiable based on a single image.
- Given this, prior work has found greater gains leveraging multiple modalities found in video (Zhang et al., 2021(Zhang et al., , 2022a)), such as audio, action features, etc.
- To better understand grounding in perception and action, we compare our approach with other state-of-the-art methods which use multimodal information for parsing tasks in the video settings.
- We use F-1 scores to compare the parsing results.
- Setup. Following MMC-PCFG, we use three benchmarking video datasets for our experiments: Distinct Describable Moments dataset (DiDeMo) (Anne Hendricks et al., 2017), Youtube Cooking dataset (YouCook2) (Zhou et al., 2018) and MSRVideo to Text dataset (MSRVTT) (Xu et al., 2016) (Kim et al., 2019;Zhao and Titov, 2020).
- denotes results obtained by running their code.
- Note that our method, using features from OPT-2.7B, yields the best results despite not being visually regularized.
- Our method achieves the best performance while reducing the number of trainable parameters by 85% compared to VC-PCFG and maintaining comparable training time to C-PCFG.
- "W/ MD" denotes models using multimodal data (image information).
- Table 2: Comparison with multi-modalities on three benchmark datasets.
- Note that our method, using features from OPT-2.7B, yields the superior results despite not being regularized by multiple modalities.
- "W/ MD" denotes using multimodal data.
- 9940 and 59794 video-sentence pairs in the training, validation and test sets.
- The multiple modalities (Zhang et al., 2021) includes object features that are extracted by SENet (Xie et al., 2017), action features that are extracted from I3D (Carreira and Zisserman, 2017), scene features (Huang et al., 2017;Zhou et al., 2017), audio features (Hershey et al., 2017), OCR features (Deng et al., 2018;Liu et al., 2018), face features (Liu et al., 2016;He et al., 2016) and speech features (Mikolov et al., 2013).
- We run all experiments 4 times for 10 epochs with different random seeds and calculate mean & standard deviations of C-F1 and S-F1.
- Results. We compare our results with C-PCFG, VC-PCFG with a single modality, and MMC-PCFG with multiple modalities.
- We display our results in Table 2.
- Although learning from different modalities could bring substantial improve-ment, we find that learning embeddings from LLMs can significantly bridge this gap and even outperform the parsing performance of video-regularized models for all three benchmark datasets.
- For example, MMC-PCFG learns all modalities together and achieves 44.7 as the average C-F1 score on the YouCook2 dataset, which displays obvious improvement over previous methods.
- However, our approach achieves 52.7 as the average C-F1 score without using any multimodal data.
- The...

## Conclusion
- In this paper, we investigated whether multimodal grounding is necessary for unsupervised constituency parsing.
- We found that LC-PCFG performs as well as previous multi-modal models on the task of unsupervised constituency parsing, challenging the notion that multi-modal signals are necessary for unsupervised grammar induction.
- Figure 1: Are paired pixels redundant? (a) Prior work has suggested using paired images to improve unsupervised grammar induction performance through grounding sentences in pixels (53.6 → 59.3 Corpuslevel F1). (b) We propose replacing paired-modality datasets with large text-only corpora, building upon large-language model (LLM) sentence embeddings and observe significant parsing gains (53.6 → 67.2) that no longer benefit from paired pixels, which now in fact, become detrimental (67.2 → 59.2).
- Figure 2: Experimental Settings. In this work we explore the use of large language model features for unsupervised grammar induction. We investigate this through three main experimental settings -the standard setting in which word representations are learned from scratch (Text Only), methods from prior work which add a multimodal loss for regularization (+Pixels), and one where we introduce pre-trained LLM features instead (+LLM features).
- In this work, we show how state-of-the-art performance may be achieved by the use of LLM features without requiring multimodal regularization.
- Figure 4: Top: A parse tree generated by LC-PCFG.Bottom: The gold-parse tree. Note how the parser is generally able to form noun phrases. We note that the top learnedparsers tend to learn a right-branching bias in their parses, which follows intuition since English is a right-branching language.
- Figure 5: An example of multimodal features from YouCook2 videos. Corresponding sentence: "flip the sandwich over and press down".
- A cat is sitting under the table
