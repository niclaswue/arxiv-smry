---
title: "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering"
date: 2022-12-19T17:44:54.000Z
author: "Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09662v1.webp" # image path/url
    alt: "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09662)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09662).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/matcha-enhancing-visual-language-pretraining).

# Abstract
- Visual language data such as plots, charts, and infographics are ubiquitous
- State-of-the-art vision-language models do not perform well on these data
- We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data.
- Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling.
- We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model.
- On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%.
- We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.

# Paper Content

## Introduction
- Visual language is the system that uses tightly integrated textual and visual elements to convey meaning
- Visual language is ubiquitous in the human world with typical examples being charts, plots and diagrams existing in places such as textbooks, scientific papers web pages and many more
- Visual language is highly complex -besides texts, its structural units can include line, shape, color, orientation, scale, angle, space, etc.
- One needs to recognize patterns from these structural units, and perform spatial grouping and/or alignment to extract information for reasoning.
- Whilst being prevalent and important, there is little research on visual language understanding from the machine learning community.
- Visionlanguage models pretrained on natural images or image-text pairs crawled from the web perform badly on visual language tasks such as ChartQA (Masry et al., 2022) and PlotQA (Methani et al., 2020) due to the high complexity of jointly modeling language and symbols (more evidence in experiments).
- Pix2Struct (Lee et al., 2022) is a recently proposed pretraining strategy for visually-situated language that significantly outperforms standard vision-language models, and also a wide range of OCR-based pipeline approaches.
- Pix2Struct designs a novel masked webpage screenshot parsing task and also a variable-resolution input representation for pretraining an image-to-text encodedecoder Transformer (Vaswani et al., 2017).
- In this work, we use Pix2Struct as the base model and further pretrain it with chart derendering and math reasoning tasks.
- We argue that visual language understanding needs two key ingredients: (1) layout understanding (including number extraction and their organizations) and (2) mathematical reasoning.
- (1) is required to discover the underlying patterns of the image and organize the elements in the image in a logical form.
- (2) is needed to operate on the elements extracted from (1) and derive meaningful information demanded by a task or query.
- Based on these observations, we propose two complementary pretraining tasks for enhancing visual language understanding: chart derendering and math reasoning.
- In chart derendering, given a plot/chart, the image-to-text model is required to generate its underlying data table or the code used to render it.
- The second task is math reasoning pretraining. We pick two numerical reasoning dataset MATH (Saxton et al., 2019) and DROP (Dua et al., 2019), render the input into images and the image-to-text model needs to decode the answers.
- In math reasoning, given a math question rendered as an image, the model needs to decode its answer.
- Chart derendering teaches the models layout understanding (including number extraction and their organizations) and math reasoning teaches the models numerical reasoning capabilities.
- We use a suite of visual language tasks to test the effectiveness of our method. Most importantly, we test on ChartQA and PlotQA which are QA datasets about plots and charts.
- On both datasets, MATCHA surpasses even the SOTA model assuming access to charts' underlying data tables and can beat the prior SOTA without gold data table by as much as 20%.
- We also test MATCHA on chart-to-text summarization tasks and observe clear improvements over Pix2Struct and achieves SOTA on Chart-to-Text (Kantharaj et al., 2022)
- Pew split. Last but not least, to examine if the MATCHA pretraining generalizes to datasets beyond the standard plots and charts domain, we also test MATCHA on four additional domains where Pix2Struct was evaluated on: documents, illustrations, user interfaces, and natural images (including datasets, such as textbook QA, Widget Captioning, etc.).
- We demonstrate consistent improvement on most additional datasets compared with the base model Pix2Struct.

## Related Work
- Vision-language research and a lack of attention on visual language.
- Research on visionand-language has predominately been focusing on natural images.
- Visually-grounded reasoning datasets such as NLVR2 (Suhr et al., 2019) and MaRVL (Liu et al., 2021) are mostly in the natural image domain.
- Synthesized datasets such as SHAPES (Andreas et al., 2016), NLVR (Suhr et al., 2017), and CLEVR (Johnson et al., 2017) can be seen as in the visual language domain. However, their visual language systems are significantly simpler than those in the real world such as plots and charts.
- As a result, information extraction from these synthesized datasets is straightforward.
- Be-sides, the queries in the synthesized datasets are relatively naive and do not require complex reasoning (e.g., questions can usually be on spatial relations or counting objects).
- Consequently, current vision-language models can handle the above mentioned synthesized visual reasoning datasets quite well.
- However, they do not perform well on real-world visual language datasets where both the information extraction and reasoning becomes much more complex.
- OCR-based & end-to-end methods for visuallysituated language.
- 2 LayoutLM (Xu et al., 2020;Huang et al., 2022) leverages a patch-OCR alignment loss to inject an external OCR systems' knowledge into the Transformer model.
- PresSTU (Kil et al., 2022) and PaLI (Chen et al., 2022b) also design OCR-aware pretraining objectives where the model needs to predict texts obtained from offthe-shelf OCR systems.
- While OCR systems can be helpful for accurately extracting texts, running them is not cheap. Also, OCR systems do not cover visual language systems that do not explicitly use text.
- As examples, plots and charts do not always have numbers written explicitly.
- Donut (Kim et al., 2022), Dessurt (Davis et al., 2022), and Pix2Struct (Lee et al., 2022) are end-to-end pretrained models for visual language where Donut and Dessurt focus on document understanding and Pix2Struct aim to provide a generic pretrained checkpoint for all visual language tasks.
- MATCHA's architecture is identical to Pix2Struct -we continually pretrain a Pix2Struct checkpoint with new objectives.
- Learning to reason by designing novel pretraining tasks.
- MATCHA is related to the literature of designing better pretraining objectives to help the language models (LMs) to reason better since the skill is hard to require through naive language modeling objectives only (e.g, masked language modeling and autoregressive language modeling on raw text).
- Geva et al. (2020); Eisenschlos et al. (2020) generate additional pretraining data focused on (numerical) reasoning through human-written templates.
- Pi et al. (2022) synthesize data and programs, and then use program executors to simulate answers.
- LMs are pretrained to predict the answers given data and programs.
- Wu et al. (2022) explore a wide range of synthetic pretraining tasks and found that even just injecting knowledge as simple as induction and deduction rules could teach LMs to reason.

## Method
- Layout understanding and basic math operation capabilities are the key elements for performing visual language understanding/reasoning
- We inject such capabilities to the model by proposing two pretraining tasks chart derendering ( ยง3.1) and math reasoning ( ยง3.2)

### Chart Derendering
- Plotting and charts are usually generated by an underlying data table and a piece of code
- Code decides the overall layout of the figure (e.g., type, direction, color/shape scheme of the chart) and the underlying data table decides the actual numbers and the groupings of them
- Both the data and code are sent to a compiler/rendering engine to create the final image
- To understand a chart one needs to discover the visual patterns in the image, effectively parse and group them to extract the key information
- Reverse the plot rendering process demands all such capabilities and can thus serve as a perfect pretraining task
- In practice, it is challenging to simultaneously obtain charts, their underlying data tables, and their rendering code
- To collect sufficient pretraining data, we independently accumulate (chart, code) and (chart, table) pairs
- For (chart, code) pairs, we explored two sources. First is to manually write code for converting web-crawled Wikipedia tables from Herzig et al. (2020) to charts. We randomly combine several plotting options. The key random variables include: using either matplotlib or seaborn as the plotting package; using either bar, line, or pie chart; styles and colors of the charts; whether to show numbers explicitly on the graph; font and size of the texts. Besides our own synthetic data, we also add chart-table pairs generated by Methani et al. (2020)
- For (chart, table) pairs, we explored two sources. First is to manually write code for converting web-crawled Wikipedia tables from Herzig et al. (2020) to charts. We randomly combine several plotting options. The key random variables include: using either matplotlib or seaborn as the plotting package; using either bar, line, or pie chart; styles and colors of the charts; whether to show numbers explicitly on the graph; font and size of the texts. Besides our own synthetic data, we also add chart-table pairs generated by Methani et al. (2020)
- Having a diverse set of charttable pairs is always better, but using only our own synthetic data brings very significant improvement already

### Math Reasoning
- Effective recognition and grouping of visual elements
- Applying mathematical operations (such as sorting, min/max, etc.) on top of them
- Plot derendering addresses
- Learning math reasoning skills from textual math datasets
- MATH (Saxton et al., 2019) and DROP (Dua et al., 2019)
- Concatenating the context and question for DROP
- Keeping applying screenshot parsing pretraining from Pix2Struct (Lee et al., 2022)
- A mixture of all aforementioned tasks

## Experiment
- We detail our experimental setup in ยง4.1
- Introduce the main results in ยง4.2
- Results on Pix2Struct tasks in ยง4.3
- Pretraining ablations in ยง4.4

### Experimental Setups
- Pretraining datasets/tasks. Overall, we create a mixture of pretraining task that has 40% of math reasoning, 40% of chart derendering, and 20% screenshot parsing.
- For chart derendering, we have four sources of data: (1) chart-table pairs synthesized by ourselves, (2) from ChartQA, (3) synthesized in PlotQA, and (4) chart-to-code data. We initially assigned equal weight to the four tasks however noticed training instability since chart-to-code is very hard (the pretraining data is noisy). We thus lower chart-to-code to 4% and increase all chart-to-table tasks to 12%.
- For math
- human where the augmented set is machine generated and thus more extractive and the human set is human written and requires more complex reasoning. PlotQA also has two sets v1 and v2. Similarly, v1 focuses more on extractive questions and v2 requires more numerical reasoning. However, both v1 and v2 are machine generated.
- Chart-to-Text has two sets as well. They are "Pew" and "Statista" where the names describe the source of the image examples. For Pew, the gold summaries are automatically extracted from areas around the image. For Statista, the summaries are human written.
- Beyond chart domain datasets, we additionally evaluate on other datasets used in Pix2Struct (Lee et al., 2022). We follow the exact same setups and protocols of Pix2Struct by rerunning Pix2Struct experiments but replacing the initial checkpoint with MATCHA. See Lee et al. (2022) for more experimental details.
- For ChartQA and PlotQA, Following previous works (Masry et al., 2022;Methani et al., 2020;Lee et al., 2022), we use relaxed correct-6 There exists othear chart domain QA datasets such as DVQA (Kafle et al., 2018) and FigureQA (Kahou et al., 2017). However, they are both synthetic and SOTA models have already reached > 95% accuracy. We thus focus on more challenging datasets.
- For Chart-to-Text, we use BLEU4. For all Pix2Struct experiments, we use identical metrics introduced in Lee et al. (2022).
- We save checkpoint every 200 steps and keep the checkpoint that produces the highest validation score. Following Lee et al. (2022), we finetune models on the ChartQA aug. and human sets together (i.e., one checkpoint for two sets) and use the checkpoint selected on human val set as the final checkpoint for testing.
- For PlotQA and Chart-to-Text, we train standalone models for v1, v2, Pew, and Statista sets. For pretraining, we use a batch size of 512 and max sequence length of 192. We pretrain for 100k steps and the final MATCHA checkpoint is selected at the 90k step (where the average exact match validation score is the highest).
- For downstream tasks finetuning, we use a batch size of 256 and max sequence length of 128. For ChartQA and Chart-to-Text we finetune for 10k steps and for PlotQA we finetune for 20k steps (since it is significantly larger).
- Setups for Pix2Struct tasks are the same as the original paper. As for the PaLI baselines, we use the larger 17B variant and finetune for 5k steps and save checkpoints every 1000 steps. All MATCHA and Pix2Struct models are pretrained/finetuned with 64 GCP-TPUv3 while PaLI models are finetuned with 128 GCP-TPUv4.

### Main Results
- MATCHA outperforms the strongest baseline without gold table access Pix2Struct by โ 10%
- All baselines without access to gold tables lag behind significantly -MATCHA outperforms the strongest baseline without gold table access Pix2Struct by โ 10%
- Among the baselines, we would like to highlight PaLI which is the SOTA for a series of multimodal text-image tasks such as VQA and and captioning on natural images and is of a much larger size (i.e. 17B parameters vs. 300M in MATCHA)
- PaLI fails significantly on ChartQA and PlotQA since the challenge in the visual language is distinctly different from that in the natural image domain
- Increasing input resolution substantially helps the model's performance (likely due to the better text reading with higher resolution) but this also increases the sequence length (thus also memory and compute) quadratically.

### Results on Pix2Struct Tasks
- MATCHA outperforms Pix2Struct by 2.3% on average
- MATCHA can outperform Pix2Struct by 1.6% on average

### Ablations and Analyses
- The first type of ablation removes whole pretraining datasets
- The second type of ablation removes specific datasets/tasks
- The removal of any major component (math reasoning, chart derendering, and screenshot parsing) would cause a performance drop
- Chart derendering is the most important component and the removal of it causes a decrease of โ 4% averaging across the two sets
- The most important component is chart derendering and the removal of it causes a decrease of โ 2.4% on the human set and a decrease of โ 1.6% on the augmented set

## Conclusion
- Pretraining MATCHA with its original objective improves its performance on ChartQA and PlotQA.
- Continuing pretraining on Pix2Struct with its original objective improves its performance on Chart-to-Text.
