---
title: "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models"
date: 2023-02-14T13:09:23.000Z
author: "Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, Jesse Dodge"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2302-07027v1.webp" # image path/url
    alt: "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2302.07027)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2302.07027).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/adaptersoup-weight-averaging-to-improve).

# Abstract
- PLMs need to specialize to specific domains
- Training an adapter for each domain can be impractical
- AdapterSoup uses weight-space averaging of adapters trained on different domains
- AdapterSoup improves performance to new domains without extra training
- Weight averaging of adapters trained on the same domain preserves performance on new domains

# Paper Content

## Introduction
- Large language models are pre-trained using a lot of data in a self-supervised way
- To adapt them to a new domain, continuing training with in-domain data is helpful
- Efficient methods have been proposed to avoid fine-tuning all parameters
- Weight-space averaging can be used to improve performance on novel domains without extra training
- AdapterSoup ensembles adapters in the weight space to improve performance on novel domains
- Text clustering is used to select which adapters to use for each novel domain
- Weight-space averaging of PLMs adapted to the same domain with varied hyper-parameters can be used to obtain competitive in-domain scores and preserve the generalization ability of a PLM

## Proposed approach
- PLM adapted to k domains
- Model that performs well in novel domain without training more parameters
- Provenance of text used as proxy for textual domain
- Assume PLM fine-tuned on single domain
- Combine fine-tuned models to obtain good in-domain performance and preserve generalization ability

### Cross-domain adaptersoup
- An illustration of the cross-domain AdapterSoup is provided in Figure 1.
- A PLM (parameterized language model) is used with input data x and parameters θ m ∈ R d.
- Adapters are added with a parameter initialization θ α.
- The method is general and could be extended to other efficient fine-tuning methods.
- The adapters are finetuned without updating the parameters θ m of the PLM.
- The parameters are obtained by fine-tuning a PLM with adapters in a domain D, using hyper-parameters φ.
- The hyper-parameter configuration is fixed and only the textual domain is varied.
- k different adapters are trained, one for each of the training domains.
- The parameters of l finetuned models are combined by taking the average.

### Single-domain adaptersoup
- We want to learn a language model (LM) that performs well in one domain.
- We train adapters with different hyper-parameter configurations.
- We compute the weight-space average with l = 3.
- This is similar to logit ensembling, but only adds the inference cost of one adapter.

### Model selection for adaptersoup
- Two methods for selecting combination of models to create AdapterSoup
- Use small amount of validation data from novel domain
- Use pretrained sentence-BERT to obtain sentence embeddings
- Compute average cosine similarity between each of training domains
- Domain clustering approach follows Aharoni and Goldberg (2020)
- Exhaustively combine all models trained on single textual domain
- Use 21 training domains and 10 evaluation domains from C4
- Use GPT-2 and add adapter to each Transformer layer
- Train adapters for language modeling in each training domain
- Train all models with initial learning rate 1e-4

## Results
- Evaluated perplexity and efficiency for each experiment
- Results presented in Table 1

### Cross-domain
- GPT-2 (zero-shot) has worse perplexity than other approaches but is most efficient at inference
- Single Adapters show how well approaches measure similarity between novel and training domains
- Clustering outperforms Sentence similarity
- AdapterSoups are equally as efficient at inference as using a single adapter
- Uniform AdapterSoup performs worse than all approaches except GPT-2 (zero-shot)
- Sentence similarity AdapterSoup outperforms single adapter chosen by Sentence similarity
- Clustering AdapterSoup outperforms single adapter chosen by Clustering
- Best adapter per domain is the upper bound for a single adapter
- Clustering + 2 best shows the performance of adding two best models to AdapterSoup
- Smaller learning rates lead to better in-domain scores, while larger learning rates improve out-of-domain performance

## Related work
- Training large models from scratch has a high cost
- Efficient methods such as mixtures-of-experts, adapters, and LoRA layers have been proposed
- These methods have been used for domain adaptation
- Hierarchy adapter outperforms approach but is more expensive
- Averaging weights of models independently finetuned on the same task can improve in-domain performance
- AdapterSoup has the inference cost of a single adapter, while other approaches require inference time that scales linearly to the number of adapters

## Conclusion
- PLM can be adapted to new domains using adapters
- Training a new set of adapters for each domain is required
- Proposed method uses weight-space averaging of adapters selected using text clustering
- Improves performance on novel domains without updating parameters or increasing inference cost
- Results only supported by evidence on language modeling task with C4 dataset
