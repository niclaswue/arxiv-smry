---
title: "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing"
date: 2022-12-21T06:18:31.000Z
author: "Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10789v1.webp" # image path/url
    alt: "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10789)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10789).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multi-modal-molecule-structure-text-model-for).

# Abstract
- Artificial intelligence is being increasingly adopted in drug discovery, but existing works use machine learning to mainly utilize the chemical structures of molecules
- Textual knowledge is useful in realizing new drug design objectives, adapting to text-based instructions, and predicting complex biological activities
- A multi-modal molecule structure-text model, MoleculeSTM, is developed by jointly learning molecule's chemical structures and textual descriptions
- To train MoleculeSTM, a large multi-modal dataset, PubChemSTM, is constructed
- MoleculeSTM is effective and useful in two challenging zero-shot tasks based on text instructions

# Paper Content

## Introduction
- AI methods have been used to augment and accelerate current computational pipelines
- Existing machine learning (ML) methods mainly focus on modeling the chemical structure of molecules through onedimensional descriptions
- However, such a supervised setting requires expensive annotations on pre-determined label categories, impeding the application to unseen categories and tasks
- To overcome this issue, unsupervised pretraining on large-scale databases has been proposed
- Compared to the supervised setting, although such pretrained models have proven to be more effective in generalizing to various downstream tasks by fine-tuning on a few labeled examples, it is still an open challenge to generalize unseen categories and tasks without such labeled examples or fine-tuning
- Additionally, existing molecule pretraining methods mostly incorporate only chemical structures, leaving the multi-modal representation less explored
- We have a vast amount of textual data that is human-understandable and easily accessible
- This is now being harnessed in large-scale multi-modal models for images and videos
- A natural language interface is an intuitive way to enable open vocabulary and description of tasks
- Pretrained multi-modal models can generalize well to new categories and tasks, even in the zero-shot setting
- They also enable agents to interactively learn to solve new tasks and explore new environments
- We believe similar capabilities can also be obtained in molecular models by incorporating the vast textual knowledge available in the literature
- Previous work [30] has attempted to leverage the textual knowledge to learn the molecule representation
- However, it only supports modeling with the 1D description (the simplified molecular-input line-entry system or SMILES) and learns the chemical structures and textual descriptions on a small-scale dataset (10K structure-text pairs)
- Furthermore, it unifies two modalities into a single language modeling framework and requires aligned data, i.e., chemical structure and text for each sample, for training
- As a result, it cannot adopt existing powerful pretrained models, and the availability of aligned data is extremely limited
- Our approach: We design a multi-modal foundation model for molecular understanding that incorporates both molecular structural information and textual knowledge
- We demonstrate zero-shot generalization to new drug design objectives using textbased instructions and to the prediction of new complex biological activities without the need for labeled examples or fine-tuning
- We propose MoleculeSTM, consisting of two branches: the chemical structure branch and the textual description branch, to handle the molecules' internal structures and external domain knowledge, respectively
- Such a disentangled design enables MoleculeSTM to be integrated with the powerful existing models trained on each modality separately, i.e., molecular structural models [11,31] and scientific language models [32]

## Results

### Overview and Preliminaries
- MoleculeSTM consists of two branches: the chemical structure branch and the textual description branch.
- The chemical structure branch illustrates the arrangement of atoms in a molecule. We consider two types of encoders f c : Transformer [36] on the SMILES string and GNNs [7,8,15] on the 2D molecular graph.
- The textual description branch provides a high-level description of the molecule's functionality, and we use the language model from a recent work [37] as the encoder f t .
- Pretraining. Within this design, MoleculeSTM aims to map the representations extracted from two branches to a joint space using two projectors (p c and p t ) via contrastive learning [31,33].
- The essential idea of contrastive learning is to reduce the representation distance between the chemical structure and textual description pairs of the same molecule and increase the representation distance between the pairs from different molecules.
- Specifically, we initialize these two branch encoders with the pretrained single-modal checkpoints [11,31,32] and then perform an end-to-end contrastive pretraining on collected dataset PubChemSTM, which consists of 281K chemical structure and text pairs.
- Downstream: zero-shot structure-text retrieval. Given a chemical structure and T textual descriptions, the retrieval task is to select the textual description with the highest similarity to the chemical structure (or vice versa) based on a score calculated on the joint representation space.
- This is appealing for specific drug discovery tasks, such as drug re-purposing or indication expansion [30,38].
- We highlight that pretrained models are used for retrieval in the zero-shot setting, i.e., without model optimization for this retrieval task.
- Downstream: zero-shot text-based molecule editing. The objective of the molecule editing task is to modify the chemical structure of molecules such as functional group change [39] and scaffold hopping [40,41].
- Traditional methods for molecule editing highly rely on domain experts and could be subjective or biased [42,43].
- ML methods have provided an alternative strategy to solve this issue. Given a fixed pretrained molecule generative model (encoder f g and decoder h g ), the ML editing methods learn a semantically meaningful direction on the latent representation (or latent code) space.
- The decoder h g then generates output molecules with the desired properties by moving along the direction.
- In MoleculeSTM, with the pretrained joint representation space, we can accomplish this task by injecting the textual description in a zero-shot manner.
- As shown in Figure 3 (a, b), we need two phases. The first phase is space alignment, where we train an adaptor module to align the representation space of the generative model to the joint representation space of MoleculeSTM. The second phase is latent optimization, where we directly learn the latent code using two similarity scores as the objective function.
- Finally, decoding the optimized latent code can lead to the output molecules.

### Two Principles for Downstream Task Design
- The language model in the pretrained MoleculeSTM reveals certain appealing attributes for molecule modeling and drug discovery
- The large language model has proven its generalization ability in various art-related applications
- We find that it can also provide promising and insightful observations for drug discovery tasks
- In this vein, our method is not limited to a fixed set of pre-defined molecule-related annotations but can support the exploration of novel biochemical concepts with unbound vocabulary
- One example is the drug re-purposing. Suppose we have a textual description for a new disease or protein target functionality. In that case, we can obtain its similarity with all the existing drugs using MoleculeSTM and retrieve the drugs with the highest rankings, which can be adopted for the later stages, such as clinical trials.
- Another example is text-based lead optimization. We use natural language to depict an entirely new property, which can be reflected in the generated molecules after the optimization.
- Compositionality. Another attribute is compositionality. In natural language, a complex concept can be expressed by decomposing it into simple concepts. This is crucial for certain domain-specific tasks, e.g., multi-objective lead optimization [35] where we need to generate molecules with multiple desired properties simultaneously.
- Existing solutions are either (1) learning one classifier for each desired property and doing filtering on a large candidate pool [10] or (2) optimizing a retrieval database to modify molecules to achieve the multi-objective goal [12].
- The main limitation is that the success ratio highly depends on the availability of the labeled data for training the classifier or the retrieval database. While with the language model in MoleculeSTM, we provide an alternative solution. We first craft a natural text, called the text prompt, as the task description. The text prompt can be multi-objective and consists of the description for each property (e.g., "molecule is soluble in water and has high permeability").
- With the pretrained joint space between chemical structures and textual descriptions, MoleculeSTM can transform the molecule property compositionality problem into the language compositionality problem, which is more tractable using the language model.

## Downstream: Zero-shot Structure-text Retrieval
- The retrieval task can be viewed as a multiple-choice problem
- All the encoders and projectors are pretrained from MoleculeSTM
- An example for the retrieval task of setting is choosing the correct molecule from a set of molecules

### Downstream: Zero-shot Text-based Molecule Editing
- In the molecule editing task, both the MoleculeSTM ( f c , p c , f t , p t ) and a pretrained molecule generative model ( f g , h g ) are frozen.
- Our editing pipeline can be split into two phases: the space alignment phase and the latent optimization phase.
- Phase 1: space alignment. In this phase, the goal is to learn an adaptor module to align the representation space of the generative model to the joint representation space of MoleculeSTM. The objective function is where m g2 f is the adaptor module optimized to align the two latent spaces.
- Phase 2: latent optimization. In this phase, given an input molecule x x x c,in and a text prompt x x x t , the goal is to optimize a latent code w directly. The optimal w should be close to the representations of x x x c,in and x x x t simultaneously, as: where L cosine-sim is the cosine-similarity, and L l 2 is the l 2 distance, and λ is a coefficient to balance these two similarity terms.
- Finally, after we optimize the latent code w, we will do decoding using the decoder from the pretrained generative model to obtain the output molecule: x x x c,out = h g (w).
- Evaluation. The evaluation metric is the satisfactory hit ratio.
- Suppose we have an input molecule x x x c,in and a text prompt x x x t , the editing algorithm will generate an output molecule x x x c,out . Then we use the hit ratio to measure if the output molecule can satisfy the conditions as indicated in the text prompt.

## Downstream: Molecular Property Prediction
- Experiments: One advantage for MoleculeSTM is that the pretrained chemical structure representation shares information with the external domain knowledge, and such implicit bias can be beneficial for the property prediction tasks.
- Baselines: We consider two types of chemical structures, the SMILES string and the molecular graph.
- Results: As shown in Table 1, we first observe that pretraining-based methods improve the overall classification accuracy compared to the randomly-initialized ones. MoleculeSTM on the SMILES string has consistent improvements on six out of eight tasks compared to the three baselines. MoleculeSTM on the molecular graph performs the best on four out of eight tasks, while it performs comparably to the best baselines in other four tasks. In both cases, the overall performances (i.e., taking an average across all eight tasks) of MoleculeSTM are the best among all the methods.

## Discussion
- The multi-modal model, MoleculeSTM, is effective in incorporating textual descriptions for molecule representation learning
- On two newly proposed zero-shot tasks and one standard property prediction benchmark, MoleculeSTM was found to be consistently improved over existing methods
- The functionalities of MoleculeSTM can be used to accelerate various downstream drug discovery practices
- The outcomes of such downstream tasks are found to be consistent with feedback from chemistry experts

## Methods
- PubChem is a large public database of molecules
- The PubChem database has fields for synonyms and structure-text pairs
- We use the synonym field to match with an academic paper corpus
- This results in a dataset with 10K structure-text pairs
- Meanwhile, the PubChem database has another field called "string" with more comprehensive and versatile molecule annotations
- We utilize this field to construct a large-scale dataset called PubChemSTM
- In addition, we conduct the end-to-end pretraining
- The textual description branch provides a high-level description of the molecule's functionality
- We can view this branch as domain knowledge to strengthen the molecule representation
- Such domain knowledge is in the form of natural language
- We use the BERT model [37] as the text encoder
- We further adapt the pretrained SciBERT [2]
- The objective for EBM-NCE and InfoNCE is where x x x c and x x x t form the structure-text pair for each molecule, and x x x c and x x x t are the negative samples randomly sampled from the noise distribution
- E(•) is the energy function with a flexible formulation
- We use the dot product on the jointly learned space, i.e., E(x x x c , x x x t )
- We provide four examples of the PubChemSTM-raw and PubChemSTM-extracted in Table 2
