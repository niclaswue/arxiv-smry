---
title: "Multimodal Teacher Forcing for Reconstructing Nonlinear Dynamical Systems"
date: 2022-12-15T15:21:28.000Z
author: "Manuel Brenner, Georgia Koppe, Daniel Durstewitz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07892v1.webp" # image path/url
    alt: "Multimodal Teacher Forcing for Reconstructing Nonlinear Dynamical Systems" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07892)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07892).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multimodal-teacher-forcing-for-reconstructing).

# Abstract
- Many systems of interest in science are naturally described as nonlinear dynamical systems (DS).
- Empirically, we commonly access these systems through time series measurements, where often we have time series from different types of data modalities simultaneously.
- For instance, we may have event counts in addition to some continuous signal.
- While by now there are many powerful machine learning (ML) tools for integrating different data modalities into predictive models, this has rarely been approached so far from the perspective of uncovering the underlying, data-generating DS (aka DS reconstruction).
- Recently, sparse teacher forcing (TF) has been suggested as an efficient control-theoretic method for dealing with exploding loss gradients when training ML models on chaotic DS.
- Here we incorporate this idea into a novel recurrent neural network (RNN) training framework for DS reconstruction based on multimodal variational autoencoders (MVAE).
- The forcing signal for the RNN is generated by the MVAE which integrates different types of simultaneously given time series data into a joint latent code optimal for DS reconstruction.
- We show that this training method achieves significantly better reconstructions on multimodal datasets generated from chaotic DS benchmarks than various alternative methods.

# Paper Content

## Introduction
- DS reconstruction methods are based on approximating the unknown governing equations of the true DS
- In many cases, time series data from discrete random processes are in fact quite commonplace
- One major challenge in DS reconstruction is the 'exploding and vanishing gradient problem'
- A novel formulation of the multimodal data integration problem for DS reconstruction is proposed

## Related Work
- Nonlinear systems are difficult to learn and analyze
- RNNs are popular machine learning tools for modeling DS
- Various RNN architectures have been employed for DS reconstruction
- However, most of the previously designed methods for DS reconstruction focus solely on the case where all observations are continuous
- RNN models trained with classical BPTT suffer from the EVGP
- Variational generative models are powerful methods for learning latent representations of joint distributions across many data types
- Variational autoencoders (VAE) are one popular variant which naturally lends itself to multimodal settings
- Longitudinal autoencoders have been proposed to model temporal correlations in latent space with Gaussian process priors
- Other generative models for sequential data include state space models that have been applied for posterior inference of latent state paths z t âˆ¼ p(z t |x 1:T ) of DS given time series observations {x 1:T }
- Through their probabilistic formulation, they can account for uncertainty in the model formulation or latent process itself and yield the full distribution over latent states

## Method
- A specific type of dynamically interpretable and mathematically tractable RNN, the recently introduced 'dendritic PLRNN (dendPLRNN)' (Brenner et al. 2022);
- A specific TF algorithm, sparse identity-TF, for guiding the training process (Brenner et al. 2022;Mikhaeil, Monfared, and Durstewitz 2022);
- An MVAE for producing a multimodal TF signal, trained jointly with the dendPLRNN through a combined loss.
- The whole procedure is illustated in Fig. 1.

## Experiments
- Performance measures include overlap in attractor geometries, agreement in asymptotic temporal structure, short-term behavior, and challenging data situations.
- To assess overlap in attractor geometries, we employ a Kullback-Leibler divergence in state space.
- To assess agreement in asymptotic temporal structure, in the continuous-Gaussian case power spectra were first computed through the Fast Fourier Transform (Cooley and Tukey 1965) on all observed time series dimensions and slightly smoothed with Gaussian kernels to remove noise. The Hellinger distance D H between empirical and model-generated power spectra was then computed.
- To assess the agreement in temporal structure for ordinal and count data, we determined the MSE between autocorrelation functions (Wiener 1930) computed for up to 200 time lags based on the Spearman rank correlation coefficient for discrete ordinal data (see Appx. Fig. 9, OACF for ordinal and CACF for count data in Table 1). We also assessed how well the Spearman cross-correlation structure between observed ordinal variables was preserved within the generated time series (SCC, see Appx. Fig. 4).
- To assess short-term behavior, for Gaussian data the classical 10-step-ahead prediction error (PE) was included.
- To challenge the algorithm, we next tested a scenario where continuous observations from the Lorenz-63 system were heavily distorted by Gaussian noise with 50% of the data variance. At the same time, ordinal observations with 8 variables divided into 7 ordered categories each, were sampled using Eq.
- We then trained the dendPLRNN via MVAE-TF once with, and once without, ordinal observations. Figure 2(a) proves that with ordinal observations on board, DS reconstruction is, in principle, possible even under these challenging conditions. The cumulative histograms of the geometric measure D stsp in Figure 2(b) and the temporal measure D H in Figure 8 (b), comparing runs in the unimodal and multimodal settings, furthermore shows that inclusion of ordinal observations significantly improves reconstruction of the underlying system.
- Motivated by these results, we pushed the system even further and attempted DS reconstruction solely from ordinal data (created as above), i.e. completely omitting continuous observations. This is profoundly more challenging than the multimodal setting with Gaussian data, since the ordinal process considerably coarse-grains the underlying continuous dynamical process. Since in this case we do not have a direct linear mapping between ground truth state space and that of the trained dendPLRNN (which in the case of Gaussian observations would simply be given by Eq. 3), we construct one post-hoc by optimizing a linear operator given by a linear dimensionality reduction (PCA) concatenated with a geometry-preserving rotation operation (see Appx.).
- Fig. 3 shows that, using MVAE-TF, successful DS reconstruction is, in principle, even feasible in this situation. Comparable results could not be achieved by the multimodal SVAE (see Appx., Table 3).

## Conclusions
- The ordinal PE is defined as the MSE between predicted and true values on the interval [0,1], where 0 corresponds to the first predicted value and 1 corresponds to the last predicted value.
- The ordinal PE is then averaged over all test sets.
- The ordinal PE is sensitive to the number of predicted values, but not to the size of the test set.
- The ordinal PE is less sensitive to outliers than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to outliers than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to outliers than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less sensitive to noise than the mean squared PE.
- The ordinal PE is less...
