---
title: "Massive Language Models Can Be Accurately Pruned in One-Shot"
date: 2023-01-02T17:48:56.000Z
author: "Elias Frantar, Dan Alistarh"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-00774v1.webp" # image path/url
    alt: "Massive Language Models Can Be Accurately Pruned in One-Shot" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00774)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00774).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/massive-language-models-can-be-accurately).

# Abstract
- Large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining
- SparseGPT is a new pruning method that achieves this
- When executing SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60% sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time.

# Paper Content

## Introduction
- Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have shown remarkable performance on a wide range of tasks, but are difficult to deploy because of their massive size and computational costs.
- For instance, the top-performing GPT-175B model has 175 billion parameters, which total at least 320GB (counting multiples of 1024) of storage in half-precision (FP16) format, leading it to require at least five A100 GPUs with 80GB of memory each for inference.
- It is therefore natural that there has been significant interest in reducing these costs via model compression.
- To date, virtually all existing GPT compression approaches have focused on quantization [3,47,46,9], that is, reducing the precision of the numerical representation of individual weights.
- A complementary approach for model compression is pruning, which removes network elements, from individual weights (unstructured pruning) to higher-granularity components such as entire rows/columns of the weight matrices (structured pruning).
- This approach has a long history [28,19], and has been applied successfully in the case of vision and smaller-scale language models and tasks [21].
- Yet, the best-performing pruning methods require extensive retraining of the model to recover from the accuracy loss due to removed elements, which is extremely expensive in the case of GPT-scale models.
- While some one-shot pruning methods also exist [22,11], which compress the model without retraining, they are unfortunately too computationally-expensive to be applied to models with billions of parameters.
- Thus, to date, there is virtually no work on accurate pruning of GPT3-scale models.
- In this paper, we propose SparseGPT, the first accurate one-shot pruning method which works efficiently at the scale of models with 10-100 billion parameters.
- SparseGPT works by reducing the pruning problem to an extremely large-scale instance of sparse regression.
- It is based on a new approximate sparse regression solver, used to solve a layer-wise compression problem, which is efficient enough to execute in a few hours on the largest openly-available GPT models (175B parameters), using a single GPU.
- At the same time, SparseGPT is accurate enough to drop negligible accuracy post-pruning, without any fine-tuning.
- For example, when executed on the largest publicly-available generative language models (OPT-175B and BLOOM-176B), SparseGPT induces 50-60% sparsity in one-shot, with minor accuracy loss, measured either in terms of perplexity or zero-shot accuracy.
- Our experimental results, for which we provide a snapshot in Figure 1, illustrate the following two key points.
- First, as shown in Figure 1 (left), SparseGPT can induce uniform layer-wise sparsity of up to 60% in e.g. the 175-billionparameter variant of the OPT family [48], with minor accuracy loss.
- By contrast, the only known one-shot baseline which works at this scale, Magnitude Pruning [16,18], preserves accuracy only until 10% sparsity, and completely collapses beyond 30% sparsity.
- Second, as shown in Figure 1 (right), SparseGPT can also accurately impose sparsity in the more stringent, but hardware-friendly, 2:4 and 4:8 semi-structured sparsity patterns [32]. Although these patterns tend to lose additional accuracy relative to the dense baseline, especially for the smaller models, these sparsity patterns can be directly exploited to obtain computational speedups.
- Moreover, as we show later in the paper, the sparsity induced by our technique compounds well with additional compression obtained through quantization.
- One interesting fact is that our method is entirely local, in the sense that it relies solely on weight updates designed to preserve the input-output relationship for each layer, which are computed without any global gradient information.
- As such, it is remarkable that one can directly identify such sparse models in the "neighborhood" of dense pretrained models, whose output correlates extremely closely with that of the dense model.
- Another general finding, illustrated in Figure 1 (right), is that larger models are easier to sparsify: specifically, we found that, for a fixed sparsity level, the relative accuracy gap between the dense and sparse model variant narrows as we increase the model size, to the point where inducing 50% sparsity results in practically no accuracy decrease on the largest models.
- This observation, illustrated in full detail in the experimental section, should be seen as very encouraging for future work on compressing such massive models.

## Background
- Layer-wise pruning is a practical scenario where we are given a well-optimized model, together with some calibration data, and must obtain a compressed version of which satisfies some compression predicate, specifying a set of weight quantization or sparsity constraints.
- Post-training compression has traditionally been investigated in the context of quantization [23,34,29] to reduce the computational cost of quantization-aware training.
- More recently, it has been shown that it is possible to also perform accurate post-training pruning [22,11,27], although existing work focuses on classic CNN and Transformer models, which have less than 100 million parameters.
- Layer-Wise Pruning. Post-training compression usually works by splitting the full-model compression problem into layer-wise subproblems, whose solution quality is measured in terms of the 2-error between the output of the uncompressed layer, and that of the compressed one.
- Specifically, for each layer W with calibration input X, the objective is to solve the following constrained optimization problem:
- where W is a set of weights satisfying the compression constraint C.
- Specifically for pruning, Hubara et al. [22] posed this problem as that of finding, for each layer, a sparsity mask satisfying the constraint, and weights W such that
- where W is a possibly-updated version of the original dense weights W.
- Once each one of these layer-wise subproblems is solved, the model can then be "stitched back together" by re-composing the compressed layers.
- Intuitively, if the layer-wise errors are small enough, the resulting model should preserve the accuracy of the original dense model.
- A key aspect of the the layer-wise pruning problem in (2) is that both the mask M as well as the remaining weights W are optimized jointly, which makes this problem NP-hard [1].
- Thus, exactly solving it for larger layers is unrealistic, leading all existing approaches to resort to approximations.
- A particularly popular approach is to separate the problem into mask selection and weight reconstruction [20,27,22].
- Concretely, this means to first choose a pruning mask M according to some saliency criterion, like the weight magnitude [50], and then optimize the remaining unpruned weights while keeping the mask unchanged.
- Importantly, once the mask is fixed, (2) turns into a linear squared error problem, which is convex and thus easily optimized.
- It can even be solved in closed form by applying the standard linear regression formula to each matrix row w i :
- where X M i denotes only the subset of input features whose corresponding weights have not been pruned in row i, and w M i represents the respective weights.
- It is also worth noting that X M i X M i is the problem's Hessian matrix, which needs to be inverted.
- The AdaPrune approach [22] has shown good results for this problem in the context of post-training pruning via magnitude-based weight selection, followed by applying SGD steps to reconstruct the remaining weights.
- Follow-up works demonstrate that pruning accuracy can be further improved by removing the strict separation between mask selection and weight reconstruction.
- Iterative AdaPrune [8] performs pruning in gradual steps with reoptimization in between and OBC [11] introduces a greedy solver which removes weights one-at-a-time, fully reconstructing the remaining weights after each iteration, via efficient closed-form equations.
- Yet, these improvements also come with increased runtime, which, as we will discuss next, is particularly problematic in the context of extremely large models.
- Difficulty of Scaling to 100+ Billion Parameters. Prior post-training techniques have been successfully applied to models up to a few hundred million parameters [22,11,27], on which they are able to produce good results within a few hours of computation.
- However, our goal in this paper is to accurately sparsify models up to 1000× larger.
- The currently most accurate post-training method, OBC [11], exhibits runtime scaling to the 4th power of a transformer's hidden dimension, while taking > 1 hour to compress a 100M parameter model.
- Thus, this is unsuitable for 100Bparameter models.
- Even the fastest known accurate post-training method,...

### Fast Approximate Reconstruction
- The row-wise reconstruction of the unpruned weights is quadratic in the number of input features.
- The row-wise reconstruction of the unpruned weights is quadratic in the number of input features, and the cost of applying the OBS update is quadratic in the number of input features.
- The row-wise reconstruction of the unpruned weights can be improved by iteratively applying the OBS update to individually prune the weights until the cost of applying the OBS update is quadratic in the number of input features.
- The row-wise reconstruction of the unpruned weights can be improved by iteratively applying the OBS update to individually prune the weights until the cost of applying the OBS update is quadratic in the number of input features, and the restriction to a subset of the input features does not incur any extra approximation error.
- The row-wise reconstruction of the unpruned weights can be improved by iteratively applying the OBS update to individually prune the weights until the cost of applying the OBS update is quadratic in the number of input features, and the restriction to a subset of the input features does not incur any extra approximation error, provided that the subset is small enough.

### Adaptive Mask Selection
- AdaPrune [22] is a simple option to choose the mask for the whole layer in PREPRINT advance
- Recent work [8,11] has shown that the updates applied during the pruning process change weights significantly, and that taking this into account for the mask selection yields significantly more accurate results
- We propose to alleviate this disadvantage while still exploiting the significant accuracy gains of adaptive weight selection during pruning via iterative blocking

### Extension to Semi-Structured Sparsity
- SparseGPT is easily adapted to semi-structured patterns
- Every consecutive m weights should contain exactly n zeros
- Hence, we can simply choose blocksize B s = m and then enforce the zeros-constraint in the mask selection for each row

### Full Algorithm Pseudocode
- The SparseGPT algorithm is a column-wise greedy algorithm that prunes the layer matrix W to p% unstructured sparsity given inverse Hessian H −1 = (2XX + λI) −1 .
- The lazy batch-update blocksize B and adaptive mask selection blocksize B s are chosen to achieve the desired p% unstructured sparsity.
- The pseudocode in Algorithm 1 presents the unstructured sparsity version of the SparseGPT algorithm in its fully developed form, integrating also all relevant techniques from GPTQ discussed above.

### Joint Sparsification & Quantization
- Algorithm 1 operates in the column-wise greedy framework of GPTQ
- This makes it possible to merge both algorithms into a single joint procedure
- Specifically, all weights that are frozen by SparseGPT are additionally quantized, leading to the following generalized errors to be compensated in the following update step: where quant(w) rounds each weight in w to the nearest value on the quantization grid.
- Crucially, in this scheme, sparsification and pruning are performed jointly in a single pass at essentially no extra cost over just running SparseGPT.

## Experiments
- We implement SparseGPT in PyTorch and use the HuggingFace Transformers library for handling models and datasets.
- All experiments are conducted on a single NVIDIA A100 GPU with 80GB of memory.
- In this setup, SparseGPT can fully sparsify the 175-billion-parameter models in approximately 4 hours.
- Similar to [47,9], we sparsify Transformer layers sequentially in order. This significantly reduces memory requirements, and also noticeably improves accuracy over handling all layers in parallel.
- All our compression experiments are performed in one-shot, without any finetuning, following a similar setup to that of recent work on post-training quantization of GPT-scale models [9,47,3].
- For calibration data, following [9], we use 128 2048-token segments, randomly chosen from the C4 [40] dataset. This represents generic text data crawled from internet and makes sure that our experiments remain actually zero-shot since no task-specific data is seen during pruning.
- We primarily work with the OPT model family as it provides a suite of models ranging from 125 million to 175 billion parameters, allowing us to study the scaling behavior of pruning relative to model size.
- Additionally, we also consider the 176 billion parameter variant of BLOOM [42]. In general, our focus lies on the very largest variants but we also show some results on smaller models to provide a broader picture, and in particular ablations with respect to model size.
- In terms of datasets and evaluation, we mainly focus on perplexity on the raw WikiText2 test set [31], a popular benchmark in LLM compression literature [47,36,9,46].
- In the Appendix, we also show results on a set of text segments sampled from the C4 validation set. In general, perplexity is known be a challenging and stable metric that is well suited for evaluating the accuracy of compression methods [47,11,4].
- Yet, for additional interpretability, we also provide some ZeroShot accuracy results for LAMBADA [35], ARC (Easy and Challenge) [2], PIQA [44] and StoryCloze [33].
- We emphasize that the main focus of our evaluation lies on accuracy of the sparse models, relative to the dense baseline rather than on absolute numbers.
- We calculate perplexity in easily-reproducible fashion following the procedure described by HuggingFace6 : we concatenate all samples of the raw dataset with "\n\n" separators, encode and split the entire sequence into non-overlapping segments of 2048 tokens (the maximum window size of both OPT and BLOOM), on which the standard average causal language modelling loss is computed.
- The final perplexity is the exponentiated version of this result.
- Different preprocessing may influence absolute numbers, but does not affect our claims, as we mainly focus on performance relative to the dense model.
- Our ZeroShot evaluations are performed using GPTQ's [9] implementation, which is in turn based on the popular [46,4] EleutherAI-eval-harness 7 .

### Results
- The difficulty of sparsifying LLMs changes with their size
- For SparseGPT, the trend is very different: already at 2.7B parameters, the perplexity loss is < 1 point, at 66B, there is zero loss and at the very largest scale there is even a slight accuracy improvement over the dense baseline
- ZeroShot experiments show that sparsified variants of OPT-175B are more accurate than their dense counterparts

## Related Work
- Model compression aims to produce more efficient models
- To our knowledge, we are the first academic work to investigate pruning of massive GPT-scale models
- Post-training quantization has been explored for GPT-scale models, with 8-bit quantization feasible
- SparseGPT is complementary to quantization approaches and can be applied in conjunction with GPTQ

## Discussion
- We have provided a new post-training pruning method called SparseGPT, specifically tailored to massive language models from the GPT family.
- Our results show for the first time that large-scale generative pretrained Transformerfamily models can be compressed to high sparsity via weight pruning in one-shot, without any retraining, at low loss of accuracy, when measured both in terms of perplexity and zero-shot performance.
- Specifically, we have shown the largest open-source GPT-family models (e.g. OPT-175B and BLOOM-176B) can reach 50-60% sparsity with low accuracy fluctuations. Surprisingly, this means that more than 100 billion weights from these models can be ignored at inference time.
- Central to our approach is a new large-scale approximate sparse regression algorithm, which generalizes to semi-structured (2:4 and 4:8) patterns, and is also compatible with existing weight quantization approaches.
- Interestingly, our method is local: after each pruning step, it performs weight updates, designed to preserve the input-output relationship for each layer. These updates are computed without any global gradient information. Thus, it appears that the high degree of parametrization of massive GPT models allows our method to directly identify sparse accurate models in the "close neighborhood" of the dense pretrained model.
- Remarkably, since our main accuracy measure (perplexity) is extremely sensitive, it appears that the output of the generated sparse model correlates extremely closely with that of the dense model. Our second main finding is that larger models are easier to sparsify: at a fixed sparsity level, the relative accuracy drop for the sparse model, relative to the dense one, narrows as we increase the model size, to the point where inducing 50% sparsity results in practically no accuracy decrease on the largest models. This finding should be seen as very encouraging for future work on compressing such massive models.
- Another extension which we plan to investigate is the applicability of our approaches during training, to reduce the computational cost of pre-training these massive models.
