---
title: "Multi-View Knowledge Distillation from Crowd Annotations for Out-of-Domain Generalization"
date: 2022-12-19T12:40:18.000Z
author: "Dustin Wright, Isabelle Augenstein"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09409v1.webp" # image path/url
    alt: "Multi-View Knowledge Distillation from Crowd Annotations for Out-of-Domain Generalization" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09409)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09409).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multi-view-knowledge-distillation-from-crowd).

# Abstract
- Selecting an effective training signal for tasks in natural language processing is difficult: collecting expert annotations is expensive, and
- Crowd-sourced annotations may not be reliable.
- At the same time, recent work in machine learning has demonstrated that learning from soft-labels acquired from
- However, the best method for acquiring these soft labels is inconsistent across tasks.
- This paper proposes new methods for acquiring soft-labels from crowd-annotations by aggregating the distributions produced by
- In particular, we propose to find a distribution over classes by learning from multiple-views of crowd annotations via temperature scaling
- We demonstrate that using these aggregation methods leads to best or near-best performance
- We argue that aggregating different views of crowd-annotations as soft-labels is an effective way to ensure performance which is as good or better than the

# Paper Content

## Introduction
- There are a multitude of tradeoffs associated with how labels are collected for supervised machine learning, including the cost, accuracy, and time to collect those labels.
- One of the primary concerns in supervised machine learning is how to define, collect, and use labels as training data.
- There are a multitude of tradeoffs associated with this decision, including the cost, the number of labels to collect, the time to collect those labels, the accuracy of those labels with respect to the task under consideration, and how well those labels enable model generalization.
- Soft-labeling schemes are used to improve both model accuracy and uncertainty estimation.
- When using soft-labels, models are trained to minimize the divergence between their predictive distribution and a distribution over the labels obtained from crowd annotations.
- While this has been shown to potentially improve model generalization for vision tasks, little work has systematically compared how different soft-labeling schemes affect out-of-distribution performance and uncertainty estimation in NLP.
- We seek to fill this gap in this work, and propose new methods for aggregating soft-labels which yield more consistent and robust performance without requiring new annotations or learning methods.

## Methods
- We build upon a rich literature around the topic of learning from crowd annotations.
- As opposed to ( ( ( hard labels, soft-labeling schemes give us a distribution over the possible classes in a given dataset.
- In this, more difficult or ambiguous classes can have their probability mass distributed over multiple classes, which can help regularize a downstream classifier.
- This may also reflect potential "dark knowledge" (Hinton et al., 2015), or the relative similarity of an input sample to different classes, learnable from the crowd annotations.
- Multiple soft-labeling schemes have been demonstrated to provide good training signals on different NLP tasks, but none of these methods are consistently best across tasks (Uma et al., 2021).
- Given this, we start with several well-studied methods for learning from crowd-labels, described in Section 3.1 (Uma et al., 2020;Fornaciari et al., 2021;Hovy et al., 2013;Dawid and Skene, 1979), adding to this literature by providing a systematic analysis of their performance and best practices when considering generalization to out of domain data, which is under-explored in NLP.
- Then, we propose several methods for aggregating the distributions over class labels produced by each of these methods in Section 3.2, which we will demonstrate produce robust soft-labels across tasks.

### Soft Labeling Methods
- The standard normalization scheme presented in (Uma et al., 2020) obtains soft-labels for a given sample by transforming a set of crowd-sourced labels into a probability distribution.
- This is done by normalizing the number of votes given to each label by the total number of annotations for a given sample, as described in Equation 1.
- The works of Peterson et al. (2019); Fornaciari et al. (2021) propose to use the softmax function directly from label vote counts as a way to obtain soft labels for a given sample, as in Equation 2.
- To obtain a soft label for a given sample i from this model, we use the posterior distribution of the latent variable c i which models the true class for a given instance.

### Combining Soft Labels
- The goal for a single example x i is to find a soft target for example x i which will serve as a reference for classifying future examples.
- The most basic model to acquire an ensembled probability distribution is to take an average of the individual probabilities p 1:M .
- The JSC is a probability distribution which has the least average total divergence to the average between itself and each probability distribution in a set of probability distributions.
- Finding the JSC can be done efficiently using methods from convex optimization.
- In order to acquire labels which capture the multiple views of the annotations learned by individual soft-labeling methods, we develop and investigate novel methods for combining the soft-labels obtained from these different views.
- This is inexpensive, requiring zero additional annotations, and we will demonstrate that it is robust across tasks.
- The goal for a single example x i is to produce a soft target for example x i which will serve as a reference for classifying future examples.
- The most basic model to acquire an ensembled probability distribution is to take an average of the individual probabilities p 1:M .
- The JSC is a probability distribution which has the least average total divergence to the average between itself and each probability distribution in a set of probability distributions.
- Finding the JSC can be done efficiently using methods from convex optimization.
- In order to acquire labels which capture the multiple views of the annotations learned by individual soft-labeling methods, we develop and investigate novel methods for combining the soft-labels obtained from these different views.
- This is inexpensive, requiring zero additional annotations, and we will demonstrate that it is robust across tasks.
- The goal for a single example x i is to produce a soft target for example x i which will serve as a reference for classifying future examples.
- The most basic model to acquire an ensembled probability distribution is to take an average of the individual probabilities p 1:M .
- The JSC is a probability distribution which has the least average total divergence to the average between itself and each probability distribution in a set of probability distributions.
- Finding the JSC can be done efficiently using methods from convex optimization.
- In order to acquire labels which capture the multiple views of the annotations learned by individual soft-labeling methods, we develop and investigate novel methods for combining the soft-labels obtained from these different views.
- This is inexpensive, requiring zero additional annotations, and we will demonstrate that it is robust across tasks.
- The goal for a single example x i is to produce a soft target for example x i which will serve as a reference for classifying future examples.
- The most basic model to acquire an ensembled probability distribution is to take an average of the individual probabilities p 1:M .
- The JSC is a probability distribution which has the least average total divergence to the average between itself and each probability distribution in a set of probability distributions.
- Finding the JSC can be done efficiently using methods from convex optimization.
- In order to acquire labels which capture the multiple views of the annotations learned by individual soft-labeling methods, we develop and investigate novel methods for combining the soft-labels obtained from these different views.
- This is inexpensive, requiring zero additional annotations, and we will demonstrate that it is robust across tasks.

### Learning From Soft labels
- Learning from soft labels requires methods which minimize the divergence between the probability distribution produced by a classifier and the distribution over labels.
- To do this, we adopt the strategies used in knowledge distillation and learning from crowd-sourced labels, using the Kullback-Leibler divergence as a loss function between the probability distribution produced by a given classifier and the soft labels produced by the methods in Section 3.1 and Section 3.2.

## Experimental Setup
- RQ1: Which methods for learning from crowd-sourced labels are most robust in outof-domain settings?
- We focus our experiments on the following research questions:
- RQ2: Does aggregating multiple views of crowd annotations lead to more robust outof-domain performance?
- RQ3: Which soft-labeling methods lead to better uncertainty estimation?
- Our experiments focus on the out-of-domain setting, where there is distribution shift between training data and test data. We use pairs of datasets which capture the same high level tasks such that the training data has both gold and crowdannotations available while the testing data has only gold annotations. Thus, to perform well on the test set, a model must be able to generalize across these two factors: input data sourced from different corpora and/or labels acquired from different sources. Additionally, two of our experiments employ limited datasets for training, giving a very difficult out of domain setting. We choose this setup in order to understand the impact of crowd-sourced soft targets on model generalization, whereas in the in-domain setting where train and test data use gold labels obtained from the same source, performance is dominated by the use of expert labels.
- In other words, when does the knowledge contained in softtargets confer benefits over expert annotations in NLP?
- For all experiments we use RoBERTa as our base network (Liu et al., 2019) with the same training hyperparameters in order to provide a stable comparison across different soft-labeling techniques. Additionally, this allows us to observe how the same soft-labeling techniques on the same network perform on different tasks.
- For the soft-labeling experiments (labeled "KLD") we only use soft labels obtained using one of the crowd-labeling methods described in Section 3.1 and Section 3.2 and trained using the KL divergence as the loss. Additionally, we experiment with the multi-task learning setup used in Fornaciari et al. (2021); Uma et al. (2021), where the model is trained on both gold labels and soft crowd-sourced labels (labeled "Gold + KLD"). This allows us to differentiate performance between when gold annotations are available vs. not, which is clearly beneficial in the in-domain test setting where the same method of acquiring gold labels is used for test data (Fornaciari et al., 2021), but not necessarily in the out-of-domain setting (Peterson et al., 2019).
- The tasks and datasets used in our experiments are described in the following paragraphs.
- The first task we consider is recognizing textual entailment (RTE). In the RTE task, a model must predict whether a hypothesis is entailed (i.e. supported) by a given premise. For training, we use the Pascal RTE-1 dataset (Dagan et al., 2005) with crowd-sourced labels from Snow et al. (2008). The dataset consists of 800 premise-hypothesis pairs annotated by 164 different annotators with 10 an-notations per pair. As an out-of-domain test set, we use the Stanford Natural Langauge Inference dataset (SNLI) (Bowman et al., 2015), where we transform the task into binary classification by collapsing the "neutral" and "contradiction" classes into a single class.
- relations hold between different biomedical entities in sentences extracted from biomedical papers. The MRE dataset used for training in this work is the crowd-sourced dataset from (Dumitrache et al., 2018), which collected crowd annotations from 3,984 sentences from PubMed abstracts (Wang and Fan, 2014) annotated by at least 15 annotators for 14 different UMLS (Bod...

## Results and Discussion
- We evaluate the performance of each soft-labeling method across two metrics: F1 score and calibrated log-likelihood (CLL).
- F1 score provides an indication of the predictive ability for the classifiers trained on the soft-labels, while CLL describes how well calibrated each classifier is in terms of its predictive uncertainty.
- We use the CLL in order to obtain a fair comparison between methods, as it first performs temperature scaling on a held-out portion of the test set and measures the temperature-scaled negative log-likelihood on the rest of the test set, averaging results over 5 splits.
- Formal definitions of each metric can be found in Appendix A.
- Additionally, we show the average performance of the individual methods (µ) to illustrate how the aggregation methods compare with the average individual performance, as well as results using only expert annotations (Gold) and only majority vote (Silver).
- We generalize our findings to other domains.
- We provide answers for the research questions proposed in Section 4.

### Raw Performance
- Overall, the RTE and MRE datasets are much more difficult to generalize from than the POS and Jigsaw tasks, as reflected in the high variance of the results.
- In general, aggregation methods using the JSC (Centroid in Figure 2) yield best or near-best performance across datasets, while the hybrid method works slightly better on POS tagging.
- Adding gold labels for the RTE and MRE tasks leads to worse performance, potentially due to the limited amount of labeled data.
- For the toxicity detection task, all methods perform within reasonable ranges of each other, with the Bayesian methods and basic averaging conferring slightly better performance.

### Uncertainty Estimation
- Uncertainty estimation can be improved with the addition of soft-labels
- The benefits are more pronounced for the RTE and MRE tasks, where labeled data is limited
- There is inconsistency from the individual soft labeling methods across tasks
- The aggregation methods (JSC in particular) offer much more consistent uncertainty estimation which is better or approximately equal to the performance of the best performing individual method

### Research Questions
- Individual soft-labeling techniques produce inconsistent and often poor performance
- Aggregating multiple views of crowd-labels leads to better performance
- Uncertainty estimation is inconsistent across tasks

### Analysis
- The average JSD of the distributions produced by the JSC aggregation method to those of a given individual method is highly correlated with the average JSD of that individual method to all other individual methods.
- This suggests that aggregating using the JSC will lead to distributions closer to the hubs of an ensemble, where many of the individual distributions are similar.

## Conclusion
- In this work, we present a systematic comparison of soft-labeling techniques from crowd-sourced labels and demonstrate their utility on out-of-domain performance for several text-classification tasks.
- The out-of-domain setting allows us to observe how well learning from crowd-sourced soft-labels enables generalization to unseen domains of data, potentially reflecting the "dark knowledge" imparted by these labels.
- Given than no consistent best performing model appears, we propose four novel methods for aggregating multiple views of crowdsourced labels into a combined distribution, demonstrating that doing so leads to consistently robust performance across tasks despite fluctuations in performance shown by the constituent views.
- Concretely, we show that using the JSC between the constituent distributions yields consistently high raw performance and good uncertainty estimation, and is resilient to fluctuations in performance of the individual methods.
- This constitutes a lowcost solution to acquiring reliable soft-labels from crowd-annotations which oftentimes outperform expert labels on out-of-domain data.
