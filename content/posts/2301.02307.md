---
title: "What You Say Is What You Show: Visual Narration Detection in Instructional Videos"
date: 2023-01-05T21:43:19.000Z
author: "Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, Kristen Grauman"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2301-02307v1_sqHNYOdAS.jpg" # image path/url
    alt: "What You Say Is What You Show: Visual Narration Detection in Instructional Videos" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02307)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02307).


# Abstract
- Narrated "how-to" videos have emerged as a promising data source for a wide
- range of learning problems, from learning visual representations to training
- robot policies.
- However, this data is extremely noisy, as the narrations do not always describe the actions demonstrated in the video.
- To address this problem we introduce the novel task of visual narration detection, which entails
- determining whether a narration is visually depicted by the actions in the
- video.
- We propose "What You Say is What You Show" (WYS^2), a method that
- leverages multi-modal cues and pseudo-labeling to learn to detect visual
- narrations with only weakly labeled data.
- We further generalize our approach to operate on only audio input, learning properties of the narrator's voice that
- hint if they are currently doing what they describe.
- Our model successfully detects visual narrations in in-the-wild videos, outperforming strong
- baselines, and we demonstrate its impact for state-of-the-art summarization and
- alignment of instructional video.

# Paper Content

## Introduction
- human viewers can learn new skills by watching instructional videos
- computer vision models have a lot to learn from instructional videos
- instructional videos are rich data sources for training models
- non-visual narrations are present in 50% of in-the-wild how-to video clips
- the goal of visual narration detection is to infer whether a narration describes the actions depicted in a given video segment
- we propose What You Say is What You Show (WYS 2 ), an approach to detect visual narrations
- our model shows promising results for predicting visual narrations from audio alone

## Related Work
- Learning from instructional video can be used to improve task performance like text-to-video retrieval, temporal segmentation, activity localization, and question-answering.
- Associating vision and language is a fundamental goal of linking language and vision research.
- Multi-modal video embeddings can be used to learn enriched visual representations.
- Self-supervised models can be used to pretrain video encoders.
- Text-video(audio) embeddings can be used to improve downstream tasks in text-to-video retrieval or captioning.
- Predicting visualness is a challenge for text-to-video retrieval and captioning.
- The VQA system of [68] identifies non-visual questions in images to decide whether the image content should be relevant for answering the question.

## Technical Approach
- We propose a method to detect visual narration in instructional videos
- There are many instances where the narration digresses from what is being shown
- Our objective is to identify such video segments using as input the video-text pair (or alternatively, the narration audio alone)
- Formally, let v denote a video clip, a the audio from the same temporal segment, and t the text resulting from automatic speech recognition (ASR) applied to a
- We aim to learn a visual narration detector, V vt : (v, t) → {0, 1}, where V vt (v, t) = 1 when the narration t describes the activity demonstrated in the video v, and V vt (v, t) = 0 otherwise

### Visual Narration Training Data
- There are no current datasets that can be directly used to learn V.
- We propose a method that automatically infers visual narration labels from existing datasets annotated with demonstration key-steps.
- We show that a model trained on these derived visual narration labels can then be applied to pseudo-label large-scale uncurated data.
- We further boost the accuracy of our visual narration detector by retraining it on the aggregation of the smaller labeled dataset and the pseudo-labeled large-scale data.

### Multimodal Visual Narration Detector
- Next, we present the model and training objective for V.
- In our ablation experiments, we pair the training methodology below with each of the three variants for inferring labels presented above.
- Inferring Visual Narrations from Video-Text Pairs.
- Our visual narration detector V vt consists of two subnetworks-one for extracting the video embedding f V (v) and the other for obtaining a sentence embedding f T (t).
- The idea is to learn f V and f T such that clips with a visual narration have high similarity in the embedding space, while the others have low similarity.
- See We use the S3D [84] architecture to compute the visual embedding from video clips.
- To obtain text embeddings, we use a pre-trained Google News word2vec model [56].
- The word2vec embedding is passed through a linear layer (with ReLU activation) and then max-pooled to obtain the sentence embedding.
- The video architecture is learned while the sentence encoder is frozen.
- The choice of these embedding architectures is consistent with recent work [2,8,23,48,53,85] and suitable for our objective of multimodal embedding learning.
- Following recent success in contrastive learning, we use softmax-type Noise-Contrastive Estimation (NCE) [64] as our loss function:
- where N (i) is the sample-specific negative set chosen from the same batch.
- During inference, we compute the similarity score using a dot product, i.e., f V (v) T f T (t) and apply a threshold to detect instances of visual narration.
- Inferring Visual Narrations from Audio.
- We separately consider an alternative setting where the input to the model is solely the audio waveform. Here the goal is to predict whether the (unobserved) video is likely to show what is being described (i.e., does the audio suggest that the person is showing what they are describing?).
- We hypothesize that audio elements such as vocal intonation, cadence, and pacing may signal when the narrator's speech is demonstration-relevant versus merely color commentary.
- For example, the narrator may slow down or talk more deliberately when declaring a key step they are showing. Similarly, an instruction has a different tone than welcoming and thanking the viewers.
- This setting is attractive for lowpower scenarios, since audio streaming is much more efficient than for video.
- This variant could enable applications that stream only audio for the non-visual parts, thereby saving on storage and transmission requirements.
- Formally, for the audio-input case, we aim to learn an audio-based visual narration detector V a :
- a → {0, 1}, where a denotes the raw audio waveform from the narration, and the target labels are determined by the video-text agreement, as before.
- To train V a , we first extract MEL-Spectrograms from the audio waveform a and pass the 2D array to a standard ResNet-18 [29] architecture.
- To compute the binary classification output we attach a fully-connected layer with sigmoid activation function.
- The standard Binary Cross Entropy (BCE) loss is used to train the model to match the visual narration labels in D vn .
- Our insight differs from the more common observation that audio can enhance action recognition [24,35,61] (e.g., hearing a pan sizzling reinforces the recognition of the action stir-frying from the visual stream). Instead, we aim to capitalize on the narration properties that signal both the video and text content are aligned. Hence, it is significant that our input is audio for this variant, rather than the narration's transcribed text-the latter would not reveal these category-agnostic aspects of how the speech is delivered.

## Experiments
- Temporal Alignment (MIL-NCE): This method is used in [53] and subsequent work [1,2,48]. It assumes that each video clip has a corresponding narration that describes it, albeit potentially misaligned in time. In order to handle these potential misalignments, a multiple instance learning (MIL) variant of the NCE loss is proposed [53]. We set the number of candidate positives |P i | = 3 based on the validation set.
- Overlapping Clips: Proposed in [85], this method takes a narration t (i) centered at frame i, extracts a long video clip around it, and labels it as a positive sample. The idea is that misalignment is generally limited to a few seconds, and that is alleviated by taking a longer overlapping clip. Following [85], this baseline uses an NCE training objective and 16s clips.
- TAN [28]: A recent method to infer temporal alignment between video and narrations that allows for "unalignable" segments.
- CLIP [67]: A popular large-scale video-language embedding model. Implementation Details. Unless stated otherwise, all models are trained with a batch size of 128 on eight Quadro RTX GPU (23 GB each) with an Adam optimizer [37] and learning rate 10 −5 . Each training clip has a duration of 3s and c = 0.5 for the sentence similarity S. We generate 4 negative clips per positive based on validation performance. For training V a , we use a batch size of 16 and use one GPU with SGD optimizer, a learning rate of 10 −2 and dropout 0.1.

### Visual narration detection
- WYS 2 outperforms the baselines
- We attribute our advantage over Temporal Alignment (which adopts the MILobjective [53]) to the fact that many training clips do not have a visual narration, leading to false positives that cannot be overcome by the alignment tolerance of MIL and hence corrupt learning.
- Compared to Overlapping Clips (which adopts the video padding idea of [85]), our model more accurately learns visual narration associations because it does not assume a positive must always exist.
- Table 1(left) shows an ablation to understand the relative strength of the proposed training variants.
- WYS 2 -SS is strongest.
- Even though WYS 2 -VR uses visual annotation, it uses less supervision than WYS 2 -SS because the latter also exploits ASR sentences for visual narration detection.
- WYS 2 -MC does quite well, but (unlike WYS 2 -SS) has the disadvantage of being task-specific due to its reliance on task labels o.
- See Supp. for further ablations on the selectivity for positive/negative clips, batch size, learning rate.
- In all further results we deploy WYS 2 -SS since it performs best.
- Figure 5 shows examples using our model to detect visual narrations in unseen HowTo100M videos.
- We see that WYS 2 automatically highlights the clips where the narrations are acted out.
- This visualization suggests natural downstream applications of our idea for storyboarding and video summarization, which we will quantitatively show in Sec. 4.4.
- The fact that our model is category-agnosticable to generalize to new tasks without knowing their keysteps-is essential to this capability.

### Pseudo-labeling a large-scale unlabeled dataset
- Next, we use our best performing model WYS 2 -SS to pseudo-label the large-scale unlabeled HowTo100M dataset, introduced as D U in Sec. 3.1.
- Originally there are no temporal annotations in D U ; we use our model to identify clips with visual narrations to obtain a larger D vn , retrain on it, and then test on VND Test and HTM-Align.
- We compare to TAN [28], MIL-NCE [53]-original VND HTM-Align CLIP [67] 63.2 76.0 MIL-NCE [53] 73.8 79.5 TAN [28] 69.5 77.4 WYS 2 -SS (Ours) 75.5 80.7
- Table 2 shows the results. Our method outperforms the baselines on both datasets.
- Since TAN is originally designed for alignment, we adapt it to our VND task by providing it short clips (instead of a long video) and checking for alignability; if it reports "unalignable", we take this to mean "non-visual".
- This experiment illustrates that we can use manually labeled datasets (D L ) like COIN and CrossTask to pseudolabel large-scale data (D U ) like HowTo100M. Our framework allows us to enhance the power of large-scale unlabeled datasets without additional human labeling effort.

### Alignability detection in long videos
- Next, we apply a WYS 2 algorithm to detect alignability in videos.
- Recall that a video is aligned if the narration is relevant to what is shown, whereas it is a visual narration if the visual content actually depicts the narrated action.
- We provide our learned features as input to the TAN alignment head.
- Table 3 shows the results. We improve state of the art by 1.9%.

### Summarizing instructional videos
- WYS 2 can detect visual narrations
- The output summaries are measurably better with VND model
- This shows the real-world applicability of our idea

### Visual narration detection from audio
- Audio signals contain relevance cues that indicate when a narration is about the demonstration taking place
- Table 1 (right) shows the results
- The voice of the narrator itself lends clues to when the speech is about the action being performed-not what is being said, but how it is being said
- However, as expected, the absolute accuracy remains lower than when using both the visual and ASRtext inputs

## Conclusion
- We propose What You Say is What You Show, a novel approach for a new task to detect visual narrations for inthe-wild how-to videos
- Using modest annotated data, we show how to automatically create labels for our task, and further boost training capabilities by pseudo-labeling largescale unlabeled data
- WYS 2 shows strong performance against competitive baselines and state-of-the-art video representations; it successfully improves the state-of-the-art in alignability prediction and instructional video summarization; and it even generalizes to audio-only inputs, an attractive low-power application
- We additionally introduce a new benchmark that will be released to the community
