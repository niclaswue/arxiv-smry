---
title: "Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations"
date: 2022-12-20T16:33:09.000Z
author: "Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi, Chandra Bhagavatula"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10409v1.webp" # image path/url
    alt: "Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10409)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10409).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/reinforced-clarification-question-generation).

# Abstract
- Context is vital for commonsense moral reasoning
- "Lying to a friend" is wrong if it is meant to deceive them, but may be morally okay if it is intended to protect them
- Such nuanced but salient contextual information can potentially flip the moral judgment of an action
- Thus, we present ClarifyDelphi, an interactive system that elicits missing contexts of a moral situation by generating clarification questions such as "Why did you lie to your friend?"
- Our approach is inspired by the observation that questions whose potential answers lead to diverging moral judgments are the most informative
- We learn to generate questions using Reinforcement Learning, by maximizing the divergence between moral judgements of hypothetical answers to a question.
- Human evaluation shows that our system generates more relevant, informative and defeasible questions compared to other question generation baselines.

# Paper Content

## Introduction
- Thinking about different contexts to come up with a moral judgement.
- The situation "offering a cup of coffee" generally receives a positive moral judgement.
- Eliciting more salient context for various moral situations is non trivial.

### Hypothetical Answer Simulation
- Given a situation and a generated question, the CLARIFYDELPHI system is able to determine which of the answers is a weakener and which a strengthener.
- The CLARIFYDELPHI system is trained via reinforcement learning and the reward simulates a set of possible (defeasible) answers to the questions.
- The CLARIFYDELPHI system is able to generate questions that lead to maximally divergent answers in terms of moral judgements.
- Our contributions are as follows: we introduce the task of clarification question generation for social and moral situations, and define a new type of question-relevance. We are additionally releasing a dataset of about 30k crowdsourced clarification questions, a question-enriched defeasible inference dataset and the trained models with their code.

## Problem Setup
- Given a situation, we aim to generate questions that are the most relevant in terms of being able to uncover the most consequential context with respect to making a social or moral judgement.
- We define consequential questions to be questions whose possible answers could function as either weakeners or strengtheners of the default judgement.
- The task is to predict a question q given a base-situation s.
- The base situation has a default moral judgement j.
- For every input tuple of (s i , q i , j i ) there is a hypothetical set of weakening answers A W and a hypothetical set of strengthening answers A S .
- Adding the additional information obtained from any q i and a i to the base situation s i results in an updated situation s ui , with an updated moral judgement j ui .

## Approach
- A pipeline consisting of various components
- The components are wrapped up in an RL algorithm
- Section 3.2 describes the policy and Section 3.4 describes the reward

### Collecting a Dataset of Clarification Questions
- The dataset consists of crowdsourced questions, enriched with questions generated by GPT-3 (Brown et al., 2020).
- We call the dataset δ-CLARIFY.
- A Crowdsourced Dataset of Clarification Questions
- We prompt GPT-3 (textdavinci-003) (Brown et al., 2020) to generate these questions in a 5-shot manner, conditioned on the situation and the answer.
- Dataset Analysis
- Fig. 2 shows that the most frequent WH-word used in the crowdsourced questions is what, followed by how and why.
- Polar (yes/no) questions appear less frequently, likely due to the instructions Turkers received about avoiding these unless no other suitable question comes to mind.
- For the situation "tipping people decently" in Tab. 1, all five Turkers chose to start their questions differently.
- Nevertheless, three out of these five questions ask in one way or the other about the service quality.
- For the other situation in Tab. 1 4/5 Turkers asked for a specification "What was the comment?" and 1 Turker asked about the missing agent role.

### Supervised Question Generation
- A question generation system that outputs a question conditioned on a situation.
- We fine-tune T5-large (Raffel et al., 2020b) on our clarification questions dataset δ-CLARIFY.
- The answergeneration model is trained on the δ-SOCIAL defeasible inference dataset which we enriched with questions.
- Input/output of the model looks as follows: Input <Judgement> + <Situation>, TYPE: <Weakener/Strengthener>, QUESTION: <Question> Output <Answer>

### Question Selection
- As a reward for generating a question, we aim to quantify how well the generated questions are able to elicit consequential answers.
- For this purpose we query Delphi (Jiang et al., 2022) for feedback, using situations updated with answers.
- To create an updated situation that sounds natural and can be used to query Delphi, the situation s, question q i and answer (both a w and a s separately) have to be fused together into s ui .
- For example: Situation changing behavior when drinking Question How do you change your behavior? Answer you become rude
- Fusing these three would result in the following updated situation: "changing behavior when drinking by becoming rude".
- Delphi is then queried with the updated situation s ui to produce a judgement.
- Besides the text output, Delphi also provides a probability distribution over three classes: bad, ok, good.
- Since Delphi is based on a Seq2Seq model (namely T5), the classification labels are encoded in special tokens.
- The probability scores of the three categorical classes are thus the probabilities of the tokens representing each of the three classes, normalized by their sum.
- In order to assess whether the simulated weakener and strengthener answers to the generated questions lead to varying judgement, we calculate the Jensen-Shannon divergence between the Delphi probability distributions j w and j s obtained from two updated situations originating from answers to the same question: JSD(P jw ||P js ).

### PPO

## Baselines
- We compare the RL approach with four other baselines.
- To assess what additional improvements the RL model provides, we report performance of the supervised question generation model on its own.
- We decode using nucleus sampling with top-p= 0.6.
- The other two baselines are based on a pipeline approach where, as the first step, a diverse set of questions is generated for a given situation and then, as the second step, the best question is selected according to a score.
- In order to generate a diverse set of questions we fine-tune T5 on δ-CLARIFY, conditioned on a slightly modified input compared to the model from Section 3.2.
- Input <Situation>.
- QUESTION: <wh-word>
- By also conditioning on the first wh-word of the question it is possible to generate different questions.
- During inference we generate questions for 14 different question starts.
- 4 e propose two approaches to scoring and ranking these questions.
- Discriminator
- We train a discriminator classifier which labels these questions as either relevant or irrelevant to a given situation.
- We then choose the question that has been assigned the relevant label with the highest probability.
- The discriminator is a binary classifier based on DeBERTa (He et al., 2020).
- The positive examples are situations and their respective 5 questions written by annotators.
- The negative question examples are sampled from other situations in the same cluster, where each negative question has a ROUGE score ≥ 0.5 with one of the gold question.
- This type of sampling ensures that the relevant and irrelevant questions are similar enough in order for the model not to overfit on token matching between situation and question.
- Divergence Ranking
- We run the hypothetical answer simulation with feedback from Delphi for each question in the set.
- This process is the same as the reward function of the RL approach, except that the JS-divergence score is used to rank the questions instead of being used as a reward for question generation.
- We call this baseline pipeline.

### Human Evaluation
- Automatic evaluation of questions and their usefulness for clarifying moral situations is tricky
- While we do have gold reference questions, we have shown that humans will produce diverse questions for the same situation
- Just because a question does not appear in the reference set does not necessarily indicate that it is not a consequential question
- We therefore perform human evaluation of the models' outputs on Amazon Mechanical Turk on the 500 test set instances from δ-CLARIFY
- Given a situation and a question Turkers are asked to rate the question along three different attributes: Grammaticality, Relevance, and Informativeness
- We aim to evaluate the defeasibility of the questions, e.g. how well the generated questions can elicit weakener or strengthener answers

### Results of Human Evaluation
- We first run the grammaticality, relevance and informativeness evaluation.
- All questions which are given the lowest rating (e.g. irrelevant and/or uninformative) by at least two annotators are excluded from the second evaluation.
- We found that it does not make sense to ask about defeasibility for questions which already are irrelevant or uninformative: A question that asks about information already present in the base situation will not be able to elicit additional weakening or strengthening context.
- Figure 5 shows that CLARIFYDELPHI has the biggest percentage of relevant and informative questions in the test set, compared to the baselines.
- Since all models are based upon the same transformer model fine-tuned on the same data (with slight variations in the input/output setup), it makes that the differences in grammaticality among the models are minimal, with the lowest average score being 0.98 and the highest 0.99 (on a scale from 0 to 1, with 1 being grammatical).
- The evaluation in general shows that a big majority of the generated questions, from all models, are relevant and informative, with the lowest performing model (discriminator) still producing 448/500 questions that are passed on to the next evaluation round.
- The CLARIFYDELPHI questions also outperform the baselines in terms of defeasibility, as seen in Figure . 2: annotators can more often think of a strengthener answer and/or a weakener answer to our questions.

### How much supervision does the policy require?
- Uses reinforcement learning to generate questions
- Uses a supervised policy that has been fine-tuned on question generation
- Measures informativeness and QA-metric and finds that more training data leads to more informative questions
- Scores are averaged every 1000 steps, between 1000 and 6000

### Analysis
- The answer generation model generally succeeds at generating diverse weakener and strengthener answers to the same question: for only about 0.05% of questions per 1000 PPO epochs the model generates the same answer for both weakener and strengthener.
- This type of answer generation could be looked at as question-guided defeasible update generation.
- Rudinger et al. (2020) introduced the task of Generative Defeasible Inference where, for δ-SOCIAL, they aim to generate an update given a situation, a moral judgement and the update type (e.g. weakener/strengthener).
- In our answer generation approach we condition on the same input together with a generated question. This intermediate question generation step functions as a type of macro planning which has been shown to be effective for NLG (Puduppully et al., 2022;Narayan et al., 2022).
- We evaluate our approach on the same test set using the same evaluation metrics as Rudinger et al. (2020).
- Table 3 shows that by first predicting the question and then the updates as its answers, we improve upon generating defeasible updates for δ-SOCIAL.
- Questions are generated from the base situation and the pronouns that are present in it.
- 60% of the situations containing 'something', for example, elicit what-questions from our model.
- It is also interesting to see that often when a situation has a missing or implicit semantic argument, such as being anxious sometimes, CLARIFYDELPHI inquires about it: "What are you anxious about?"

## Interactive Judgements
- Simulation only requires a situation as input
- Clarification questions can then be used to elicit additional context, in the form of answers, through interaction
- Fig. 7 illustrates two examples of such an interaction between a user, Delphi as the moral reasoning system and CLARIFYDELPHI
- We limit the interaction to three turns, with two clarification questions
- After each turn the situation is being updated with more context and Delphi produces a new decision
- This way one can observe how the context affects the model's predictions over multiple turns

## Related Work
- Clarification questions have been studied for multiple domains
- White et al. (2021) propose an approach for identifying images with questions
- Rao and Daumé III (2018) aim to clarify StackExchange posts and Majumder et al. (2021) worked on Amazon product descriptions
- These types of tasks have a single true answer to the clarification questions: for a given image, for example, we know whether the answer to the question 'Are there people?' is yes or no.
- To the best of our knowledge we are the first to work on clarification questions in this domain.
- There are not many datasets for clarification question generation available and the existing datasets are highly domain specific.
- Kumar and Black (2020), for example, created a dataset from stackexchange posts and Aliannejadi et al. (2019) crowdsourced clarification questions for TREC Web data.
- In this work we crowdsource a high-quality clarification question dataset for social and moral situations, comprising of more than 30,000 questions.
- What makes a good clarification question can be defined differently. Most works define a good clarification question through their possible answer(s), in terms of their general utility or informativeness.

## Conclusion
- introduced CLARIFYDELPHI, a clarification question generator that outperforms other baselines
- used a reinforcement learning approach to optimize for maximally divergent answers in terms of defeasibility
- believe that the questions can be useful for adding more context to situations through interaction
- generated results for strengthener/weakener update generation on the δ-SOCIAL test set
