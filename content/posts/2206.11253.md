---
title: "Towards Robust Blind Face Restoration with Codebook Lookup Transformer"
date: 2022-06-22T17:58:01.000Z
author: "Shangchen Zhou, Kelvin C. K. Chan, Chongyi Li, Chen Change Loy"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2206-11253v2.webp" # image path/url
    alt: "Towards Robust Blind Face Restoration with Codebook Lookup Transformer" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.11253)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.11253).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with).

# Abstract
- Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to improve the mapping from degraded inputs to desired outputs
- In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named CodeFormer, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded.
- To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, CodeFormer outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation.

# Paper Content

## Introduction
- Face images captured in the wild often suffer from various degradation, such as compression, blur, and noise.
- Restoring such images is highly ill-posed as the information loss induced by the degradation leads to infinite plausible high-quality (HQ) outputs given a low-quality (LQ) input.
- The ill-posedness is further elevated in blind restoration, where the specific degradation is unknown.
- Despite the progress made with the emergence of deep learning, learning a LQ-HQ mapping without additional guidance in the huge image space is still intractable, leading to the suboptimal restoration quality of earlier approaches.
- To improve the output quality, auxiliary information that 1) reduces the uncertainty of LQ-HQ mapping and 2) complements high-quality details is indispensable.
- Various priors have been used to mitigate the ill-posedness of this problem, including geometric priors [5,6,31,45], reference priors [24][25][26], and generative priors [2,38,44].
- Although improved textures and details are observed, these approaches often suffer from high sensitivity to degradation or limited prior expressiveness.
- These priors provide insufficient guidance for face restoration, thus their networks essentially resort to the information of LQ input images that are usually highly corrupted.
- As a result, the LQ-HQ mapping uncertainty still exists, and the output quality is deteriorated by the degradation of the input images.
- Most recently, based on generative prior, some methods project the degraded faces into a continuous infinite space via iterative latent optimization [27] or direct latent encoding [29].
- Despite great realness of outputs, it is difficult to find the accurate latent vectors in case of severe degradation, resulting in low-fidelity results (Fig. 1(d)).
- To enhance the fidelity, skip connections between encoder and decoder are usually required in this kind of methods [38,44,2]
- d-e) Results of existing methods with continuous prior (PULSE [27] and GFP-GAN [38]).
- (f-g) Results of discrete prior (Nearest Neighbor [11,35] and CodeFormer).
- Different from the aforementioned approaches, this work casts blind face restoration as a code prediction task in a small finite proxy space of the learned discrete codebook prior, which shows superior robustness to degradation as well as rich expressiveness.
- The codebook is learned by selfreconstruction of HQ faces using a vector-quantized autoencoder, which along with decoder stores the rich HQ details for face restoration.
- In contrast to continuous generative priors [11,38,44], the combinations of codebook items form a discrete prior space with only finite cardinality.
- Through mapping the LQ images to a much smaller proxy space (e.g., 1024 codes), the uncertainty of the LQ-HQ mapping is significantly attenuated, promoting robustness against the diverse degradation, as compared in Figs. 1(d-g).
- Besides, the codebook space possess greater expressiveness, which perceptually approximates the image space, as shown in Fig. 1(h).
- This nature allows the network to reduce the reliance on inputs and even be free of skip connections.
- Though the discrete representation based on a codebook has been deployed for image generation [4,11,35], the accurate code composition for image restoration remains a non-trivial challenge.
- The existing works look up codebook via nearest-neighbor (NN) feature matching, which is less feasible for image restoration since the intrinsic textures of LQ inputs are usually corrupted.
- The information loss and diverse degradation in LQ images inevitably distort the feature distribution, prohibiting accurate feature matching.
- As depicted in Fig. 1(b) (right), even after fine-tuning the encoder on LQ images, the LQ features cannot cluster well to the exact code but...

## Related Work
- Blind face restoration: Some methods introduce facial landmarks [6], face parsing maps [5,31,42], facial component heatmaps [45], or 3D shapes [16,28,49] in their designs.
- Reference-based approaches [9,[24][25][26] have been proposed to circumvent the above limitations. These methods generally require the references to possess same identity as the input degraded face.
- DFDNet [24] pre-constructs dictionaries composed of high-quality facial component features. However, the component-specific dictionary features are still insufficient for high-quality face restoration, especially for the regions out of the dictionary scope (e.g., skin, hair).
- To alleviate this issue, recent VQGAN-based methods [40,47] explore a learned HQ dictionary, which contains more generic and rich details face restoration.
- Recently, the generative facial priors from pre-trained generators, e.g., StyleGAN2 [21], have been widely explored for blind face restoration. It is utilized via different strategies of iterative latent optimization for effective GAN inversion [12,27] or direct latent encoding of degraded faces [29]. However, preserving high fidelity of the restored faces is challenging when one projects the degraded faces into the continuous infinite latent space.
- To relieve this issue, GLEAN [2,3], GPEN [44], and GFPGAN [38] embed the generative prior into encoder-decoder network structures, with additional structural information from input images as guidance.
- Despite the improvement of fidelity, these methods highly rely on inputs through the skip connections, which could introduce artifacts to results when inputs are severely corrupted.
- Dictionary Learning. Sparse representation with learned dictionaries has demonstrated its superiority in image restoration tasks, such as super-resolution [13,33,34,41] and denoising [10].
- However, these methods usually require an iterative optimization to learn the dictionaries and sparse coding, suffering from high computational cost.
- Despite the inefficiency, their high-level insight into exploring a HQ dictionary has inspired reference-based restoration networks, e.g., LUT [18] and selfreference [48], as well as synthesis methods [11,35].
- Jo and Kim [18] construct a look-up table (LUT) by transferring the network output values to a LUT, so that only a simple value retrieval is needed during inference.
- VQVAE [35] is first to introduce a highly compressed codebook learned by a vector-quantized autoencoder model. VQGAN [11] further adopts the adversarial loss and perceptual loss to enhance perceptual quality at a high compression rate, significantly reducing the codebook size without sacrificing its expressiveness.
- Unlike the large hand-crafted dictionary [18,24], the learnable codebook automatically learns optimal elements for HQ image reconstruction, providing superior efficiency and expressiveness as well as circumventing the laborious dictionary design.
- Inspired by the codebook learning, this paper investigates a discrete proxy space for blind face restoration. Different from recent VQGAN-based approaches [40,47], we exploit the discrete codebook prior by predicting code sequences via global modeling, and we secure prior effectiveness by fixing the encoder. Such designs allow our method to take full advantage of the codebook so that it does not depend on the feature fusion with LQ cues, significantly enhancing the robustness of face restoration.

## Methodology
- Exploiting a discrete representation space that reduces the uncertainty of restoration mapping and complements high-quality details for the degraded inputs.
- Incorporating the idea of vector quantization and pre-training a quantized autoencoder to obtain a discrete codebook and the corresponding decoder.
- Employing a Transformer for accurate prediction of code combination from the low-quality inputs.
- Introducing a controllable feature transformation module to exploit a flexible trade-off between restoration quality and fidelity.

### Codebook Learning (Stage I)
- To reduce uncertainty of the LQ-HQ mapping and complement high-quality details for restoration, we first pre-train the quantized autoencoder to learn a context-rich codebook
- The HQ face image I h ∈ R H×W ×3 is first embeded as a compressed feature Z h ∈ R m×n×d by an encoder E H . Following VQVAE [35] and VQGAN [11], we replace each "pixel" in Z h with the nearest item in the learnable codebook
- The decoder D H then reconstructs the high-quality face image I rec given Z c . The m • n code token sequence s forms a new latent discrete representation that specifies the respective code index of the learned codebook, i.e., Z Training Objectives
- To train the quantized autoencoder with a codebook, we adopt three imagelevel reconstruction losses: L1 loss L 1 , perceptual loss [19,46] L per , and adversarial loss [11] L adv
- Since, image-level losses are underconstrained when updating the codebook items, we also adopt the intermediate code-level loss [11,35] L f eat code to reduce the distance between codebook C and input feature embeddings Z h : where sg(•) stands for the stop-gradient operator and β = 0.25 is a weight trade-off for the update rates of the encoder and codebook
- The quantization operation in Eq. ( 1) is non-differentiable, we adopt straight-through gradient estimator [11,35] to copy the gradients from the decoder to the encoder
- The complete objective of codebook prior learning L codebook is: where λ adv is set to 0.8 in our experiments
- Codebook Settings. Our encoder E H and decoder D H consist of 12 residual blocks and 5 resize layers for downsampling and upsampling, respectively. Hence we obtain a large compression ratio of r = H/n = W/m = 32, which leads to a great robustness against degradation and a tractable computational cost for our global modeling in Stage II.

### Codebook Lookup Transformer Learning (Stage II)
- Due to corruptions of textures in LQ faces, the nearest-neighbour (NN) matching in Eq. (1) usually fails in finding accurate codes for face restoration.
- As depicted in Fig. 1(b), LQ features with diverse degradation could deviate from the correct code and be grouped into nearby clusters, resulting in undesirable restoration results, as shown in Fig. 1(f).
- To mitigate the problem, we employ a Transformer to model global interrelations for better code prediction.
- Built upon the learned autoencoder presented in Sec. 3.1, as shown in Fig. 2(b), we insert a Transformer [37] module containing nine self-attention blocks following the encoder.
- We fix the codebook C and decoder D H and finetune the encoder E H for restoration.
- The finetuned encoder is denoted as E L .
- To obtain the LQ features , and then feed them to the Transformer module.
- The s-th self-attention block of Transformer computes as the following: where X 0 = Z v l .
- The query Q, key K, and value V are obtained from X s through linear layers.
- We add a sinusoidal positional embedding P ∈ R (m•n)×d [1,7] on the queries Q and the keys K to increase the expressiveness of modeling sequential representation.
- Following the self-attention blocks, a Linear layer is adopted to project features to the dimension of (m • n) × N .
- Overall, taking the encoding feature Z v l as an input, the Transformer predicts the m in form of the probability of the N code items.
- The predicted code sequence then retrieves the m • n respective code items from the learned codebook, forming the quantized feature Ẑc ∈ R m×n×d that produces a high-quality face image through the fixed decoder D H .
- Thanks to our large compression ratio (i.e., 32), our Transformer is effective and efficient in modeling global correlations of LQ face images.

### Controllable Feature Transformation (Stage III)
- The Stage II model has obtained a great face restoration model
- The controllable feature transformation (CFT) module is used to control information flow from LQ encoder E L to decoder D H
- The loss for this stage is the sum of above losses weighted with their original weight factors
- The w is set to 1 during training of this stage, which then allows network to achieve continuous transitions of results by adjusting w in [0, 1] during inference

## Experiments

### Datasets
- The training dataset is the FFHQ dataset which contains 70,000 high-quality images.
- The testing datasets are the CelebA-Test, WebPhoto-Test, and WIDER-Test which contain 3,000, 407, and 970 images, respectively.

### Experimental Settings and Metrics
- Settings: We represent a face image of 512 × 512 × 3 as a 16 × 16 code sequence.
- For all stages of training, we use the Adam [23] optimizer with a batch size of 16.
- We set the learning rate to 8×10 −5 for stages I and II, and adopt a smaller learning rate of 2×10 −5 for stage III.
- The three stages are trained with 1.5M, 200K, and 20K iterations, respectively.
- Our method is implemented with the PyTorch framework and trained using four NVIDIA Tesla V100 GPUs.
- Metrics: For the evaluation on CelebA-Test with ground truth, we adopt PSNR, SSIM, and LPIPS [46] as metrics.
- We also evaluate the identity preservation using the cosine similarity of   features from ArcFace network [8], denoted as IDS.
- For the evaluation on real-world datasets without ground truth, we employ the widely-used non-reference perceptual metrics: FID [15] and MUSIQ (KonIQ) [22].

### Comparisons with State-of-the-Art Methods
- Our CodeFormer achieves comparable perceptual quality of FID score with the compared methods on the real-world testing datasets with mild and medium degradation, and the best score on the testing dataset with heavy degradation.
- Although PULSE [27] also obtains good perceptual MUSIQ score, it cannot preserve the identity of input images, as the identity score of IDS and visual results respectively suggested in Table 1 and Fig. 4.
- From the visual comparisons in Fig. 4, it is observed that our method shows exceptional robustness to the real heavy degradation and produces most visually pleasing results.
- Notably, CodeFormer successfully preserves the identity and produces natural results with rich details.
- Effectiveness of Codebook Space. We first investigate the effectiveness of the codebook space. As shown in Exp. (a) of Table 3, removing the codebook (i.e., directly feeding the encoder features Z l to the decoder) results in worse LPIPS and IDS scores. The results suggest that the discrete space of codebook is the key to ensure the robustness and effectiveness of our model.
- In comparison to the pure CNN-based method, i.e., Exp. (c), our Transformer-based solution produces better-fidelity results in terms of LPIPS and IDS scores, as well as higher accuracy of code prediction under all degradation degrees, as shown in Fig. 6.
- Besides, the superiority of CodeFormer is also demonstrated in visual comparisons shown in Fig. 5 and Fig. 9.
- Importance of Fixed Decoder. Unlike the large dictionary (∼3.2G) in DFDNet [24], which aims to store a massive quantity of facial details, we deliberately adopt a compact codebook C ∈ R N ×d with N =1024 and d=256 that only keeps the essential codes for face restoration, which then activates the detailed cues stored in the pre-trained decoder. Thus, the codebook must be used alongside the decoder to fully unleash its potential.
- To vindicate our design, we conduct two studies: 1) fixing both codebook and decoder, i.e., Exp. (g), and 2) fixing codebook but fine-tuning decoder, i.e., Exp. (e).
