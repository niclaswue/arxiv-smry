---
title: "Towards Robust Blind Face Restoration with Codebook Lookup Transformer"
date: 2022-06-22T17:58:01.000Z
author: "Shangchen Zhou, Kelvin C. K. Chan, Chongyi Li, Chen Change Loy"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2206-11253v2.webp" # image path/url
    alt: "Towards Robust Blind Face Restoration with Codebook Lookup Transformer" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.11253)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.11253).


# Abstract
- Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to improve the mapping from degraded inputs to desired outputs
- In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named CodeFormer, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded.
- To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, CodeFormer outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic and real-world datasets verify the effectiveness of our method.

# Paper Content

## Introduction
- Face images captured in the wild often suffer from various degradation, such as compression, blur, and noise.
- Restoring such images is highly ill-posed as the information loss induced by the degradation leads to infinite plausible high-quality (HQ) outputs given a low-quality (LQ) input.
- The ill-posedness is further elevated in blind restoration, where the specific degradation is unknown.
- Despite the progress made with the emergence of deep learning, learning a LQ-HQ mapping without additional guidance in the huge image space is still intractable, leading to the suboptimal restoration quality of earlier approaches.
- To improve the output quality, auxiliary information that 1) reduces the uncertainty of LQ-HQ mapping and 2) complements high-quality details is indispensable.
- Various priors have been used to mitigate the ill-posedness of this problem, including geometric priors [5,6,31,45], reference priors [24][25][26], and generative priors [2,38,44].
- Although improved textures and details are observed, these approaches often suffer from high sensitivity to degradation or limited prior expressiveness.
- These priors provide insufficient guidance for face restoration, thus their networks essentially resort to the information of LQ input images that are usually highly corrupted.
- As a result, the LQ-HQ mapping uncertainty still exists, and the output quality is deteriorated by the degradation of the input images.
- Most recently, based on generative prior, some methods project the degraded faces into a continuous infinite space via iterative latent optimization [27] or direct latent encoding [29].
- Despite great realness of outputs, it is difficult to find the accurate latent vectors in case of severe degradation, resulting in low-fidelity results (Fig. 1(d)).
- To enhance the fidelity, skip connections between encoder and decoder are usually required in this kind of methods [38,44,2]
- d-e) Results of existing methods with continuous prior (PULSE [27] and GFP-GAN [38]).
- (f-g) Results of discrete prior (Nearest Neighbor [11,35] and CodeFormer).
- Different from the aforementioned approaches, this work casts blind face restoration as a code prediction task in a small finite proxy space of the learned discrete codebook prior, which shows superior robustness to degradation as well as rich expressiveness.
- The codebook is learned by selfreconstruction of HQ faces using a vector-quantized autoencoder, which along with decoder stores the rich HQ details for face restoration.
- In contrast to continuous generative priors [11,38,44], the combinations of codebook items form a discrete prior space with only finite cardinality.
- Through mapping the LQ images to a much smaller proxy space (e.g., 1024 codes), the uncertainty of the LQ-HQ mapping is significantly attenuated, promoting robustness against the diverse degradation, as compared in Figs. 1(d-g).
- Besides, the codebook space possess greater expressiveness, which perceptually approximates the image space, as shown in Fig. 1(h).
- This nature allows the network to reduce the reliance on inputs and even be free of skip connections.
- Though the discrete representation based on a codebook has been deployed for image generation [4,11,35], the accurate code composition for image restoration remains a non-trivial challenge.
- The existing works look up codebook via nearest-neighbor (NN) feature matching, which is less feasible for image restoration since the intrinsic textures of LQ inputs are usually corrupted.
- The information loss and diverse degradation in LQ images inevitably distort the feature distribution, prohibiting accurate feature matching.
- As depicted in Fig. 1(b) (right), even after fine-tuning the encoder on LQ images, the LQ features cannot cluster well to the exact code but spread into other nearby code clusters, thus the nearest-neighbor matching is unreliable in such cases.
- Tailored for restoration, we propose a Transformer-based code prediction network, named Code-Former, to exploit global compositions and long-range dependencies of LQ faces for better code prediction.
- Specifically, taking the LQ features as input, the Transformer module predicts the code token sequence which is treated as the discrete representation of the face images in the codebook space. Thanks to the global modeling for remedying the local information loss in LQ images, the proposed CodeFormer shows robustness to heavy degradation and keeps overall coherence.
- Comparing the results presented in Figs. 1(f-g), the proposed CodeFormer is able to recover more details than the nearest-neighbor matching, such as the glasses,...

## Related Work
- Sparse representation with learned dictionaries has demonstrated its superiority in image restoration tasks, such as super-resolution [13,33,34,41] and denoising [10].
- However, these methods usually require an iterative optimization to learn the dictionaries and sparse coding, suffering from high computational cost.
- Despite the inefficiency, their high-level insight into exploring a HQ dictionary has inspired reference-based restoration networks, e.g., LUT [18] and selfreference [48], as well as synthesis methods [11,35].
- Jo and Kim [18] construct a look-up table (LUT) by transferring the network output values to a LUT, so that only a simple value retrieval is needed during inference.
- However, storing HQ textures in the image domain usually requires a huge LUT, limiting its practicality.
- VQVAE [35] is first to introduce a highly compressed codebook learned by a vector-quantized autoencoder model.
- VQGAN [11] further adopts the adversarial loss and perceptual loss to enhance perceptual quality at a high compression rate, significantly reducing the codebook size without sacrificing its expressiveness.
- Unlike the large hand-crafted dictionary [18,24], the learnable codebook automatically learns optimal elements for HQ image reconstruction, providing superior efficiency and expressiveness as well as circumventing the laborious dictionary design.
- Inspired by the codebook learning, this paper investigates a discrete proxy space for blind face restoration.
- Different from recent VQGAN-based approaches [40,47], we exploit the discrete codebook prior by predicting code sequences via global modeling, and we secure prior effectiveness by fixing the encoder. Such designs allow our method to take full advantage of the codebook so that it does not depend on the feature fusion with LQ cues, significantly enhancing the robustness of face restoration.

## Methodology
- Exploiting a discrete representation space that reduces the uncertainty of restoration mapping and complements high-quality details for the degraded inputs.
- Incorporating the idea of vector quantization and pre-training a quantized autoencoder through self-reconstruction to obtain a discrete codebook and the corresponding decoder.
- Using a prior from the codebook combination and decoder for face restoration.
- Employing a Transformer for accurate prediction of code combination from the low-quality inputs.
- Introducing a controllable feature transformation module to exploit a flexible trade-off between restoration quality and fidelity.

### Codebook Learning (Stage I)
- To reduce uncertainty of the LQ-HQ mapping and complement high-quality details for restoration, we first pre-train the quantized autoencoder to learn a context-rich codebook
- The HQ face image I h is first embeded as a compressed feature Z h by an encoder E H
- Following VQVAE [35] and VQGAN [11], we replace each "pixel" in Z h with the nearest item in the learnable codebook
- The decoder D H then reconstructs the high-quality face image I rec given Z c
- The m • n code token sequence s forms a new latent discrete representation that specifies the respective code index of the learned codebook
- To train the quantized autoencoder with a codebook, we adopt three imagelevel reconstruction losses: L1 loss L 1 , perceptual loss [19,46] L per , and adversarial loss [11] L adv
- Since the quantization operation in Eq. ( 1) is non-differentiable, we adopt straight-through gradient estimator [11,35] to copy the gradients from the decoder to the encoder
- The complete objective of codebook prior learning L codebook is: where λ adv is set to 0.8 in our experiments

### Codebook Lookup Transformer Learning (Stage II)
- Due to corruptions of textures in LQ faces, the nearest-neighbour (NN) matching in Eq. (1) usually fails in finding accurate codes for face restoration.
- As depicted in Fig. 1(b), LQ features with diverse degradation could deviate from the correct code and be grouped into nearby clusters, resulting in undesirable restoration results, as shown in Fig. 1(f).
- To mitigate the problem, we employ a Transformer to model global interrelations for better code prediction.
- Built upon the learned autoencoder presented in Sec. 3.1, as shown in Fig. 2(b), we insert a Transformer [37] module containing nine self-attention blocks following the encoder.
- We fix the codebook C and decoder D H and finetune the encoder E H for restoration.
- The finetuned encoder is denoted as E L .
- To obtain the LQ features , and then feed them to the Transformer module.
- The s-th self-attention block of Transformer computes as the following: where X 0 = Z v l .
- The query Q, key K, and value V are obtained from X s through linear layers.
- We add a sinusoidal positional embedding P ∈ R (m•n)×d [1,7] on the queries Q and the keys K to increase the expressiveness of modeling sequential representation.
- Following the self-attention blocks, a Linear layer is adopted to project features to the dimension of (m • n) × N .
- Overall, taking the encoding feature Z v l as an input, the Transformer predicts the m in form of the probability of the N code items.
- The predicted code sequence then retrieves the m • n respective code items from the learned codebook, forming the quantized feature Ẑc ∈ R m×n×d that produces a high-quality face image through the fixed decoder D H .
- Thanks to our large compression ratio (i.e., 32), our Transformer is effective and efficient in modeling global correlations of LQ face images.
- Training Objectives.
- We train Transformer module T as well as finetune the encoder E L for restoration, while the codebook C and decoder D H are kept fixed.
- Instead of employing reconstruction loss and adversarial loss in the image-level, only code-level losses are required in this stage: 1) crossentropy loss L token code for code token prediction supervision, and 2) L2 loss L f eat code to force the LQ feature Z l to approach the quantized feature Z c from codebook, which eases the difficulty of token prediction learning: where the ground truth of latent code s is obtained from the pre-trained autoencoder in Stage I (Sec. 3.1), thus the quantized feature Z c is then retrieved from codebook according to the s.
- The final objective of Transformer learning is: where λ token is set to 0.5 in our method.

### Controllable Feature Transformation (Stage III)
- Stage II has obtained a great face restoration model
- The controllable feature transformation (CFT) module to control information flow from LQ encoder E L to decoder D H is proposed
- The LQ features F e are used to slightly tune the decoder features F d through spatial feature transformation [39] with the affine parameters of α and β
- An adjustable coefficient w ∈ [0, 1] is then used to control the relative importance of the inputs
- For inference, unless otherwise stated, w = 0.5 is set

## Experiments

### Datasets
- The training dataset is the FFHQ dataset which contains 70,000 high-quality images.
- To form training pairs, we synthesize LQ images from the HQ counterparts with the following degradation model: where the HQ image is first convolved with a Gaussian kernel, followed by a downsampling of scale.
- After that, additive Gaussian noise is added to the images, and then JPEG compression with quality factor is applied.
- Finally, the LQ image is resized back to 512×512.
- We randomly sample σ, r, δ, and q from [1,15], [1,30], [0, 20], and [30,90], respectively.
- We evaluate our method on a synthetic dataset CelebA-Test and three real-world datasets: LFW-Test, WebPhoto-Test, and our proposed WIDER-Test.
- CelebA-Test contains 3,000 images selected from the CelebA-HQ dataset, where LQ images are synthesized under the same degradation range as our training settings.
- The three real-world datasets respectively contain three different degrees of degradation, i.e., mild (LFW-Test), medium (WebPhoto-Test), and heavy (WIDER-Test).
- LFW-Test consists of the first image of each person in LFW dataset, containing 1,711 images.
- WebPhoto-Test [38] consists of 407 low-quality faces collected from the Internet.
- Our WIDER-Test comprises 970 severely degraded face images from the WIDER Face dataset, providing a more challenging dataset to evaluate the generalizability and robustness of blind face restoration methods.

### Experimental Settings and Metrics
- Settings: We represent a face image of 512 × 512 × 3 as a 16 × 16 code sequence.
- For all stages of training, we use the Adam [23] optimizer with a batch size of 16.
- We set the learning rate to 8×10 −5 for stages I and II, and adopt a smaller learning rate of 2×10 −5 for stage III.
- The three stages are trained with 1.5M, 200K, and 20K iterations, respectively.
- Our method is implemented with the PyTorch framework and trained using four NVIDIA Tesla V100 GPUs.
- Metrics: For the evaluation on CelebA-Test with ground truth, we adopt PSNR, SSIM, and LPIPS [46] as metrics.
- We also evaluate the identity preservation using the cosine similarity of   features from ArcFace network [8], denoted as IDS.
- For the evaluation on real-world datasets without ground truth, we employ the widely-used non-reference perceptual metrics: FID [15] and MUSIQ (KonIQ) [22].

### Comparisons with State-of-the-Art Methods
- Our proposed CodeFormer outperforms state-of-the-art methods on the CelebA-Test, both qualitatively and quantitatively
- CodeFormer preserves the identity well even when inputs are highly degraded

### Ablation Studies
- Effectiveness of codebook space
- Adopting code prediction for codebook lookup is more effective than NN feature matching
- Local nature of convolution operation of CNNs restricts its modeling capability for long code sequence prediction
- In comparison to the pure CNN-based method, i.e., Exp. (c), our Transformer-based solution produces better-fidelity results in terms of LPIPS and IDS scores
- Besides, the superiority of CodeFormer is also demonstrated in visual comparisons shown in Fig. 5 and Fig. 9

### Running time
- The proposed CodeFormer has a similar running time as PSFRGAN and GPEN that can infer one image within 0.1s.
- Our method achieves the best performance in terms of LPIPS on the Celeb-Test dataset.

### Extensions
- Face color enhancement
- Face inpainting
- Performance on real-world old photos
- Masking
- Codebook lookup

### Limitation
- The autoencoder has a codebook that can affect the performance of the method
- CodeFormer exhibits great robustness in most cases, but when it comes to side faces, it offers limited superiority to other methods and also cannot produce good results
- This is expected because there are only few side faces in the FFHQ training dataset

## Conclusion
- Our discrete codebook prior learns a small but expressive codebook space that significantly reduces the uncertainty of restoration mapping.
- We turn face restoration to code token prediction, significantly reducing the uncertainty of restoration mapping and easing the learning of restoration network.
- To remedy the local loss, we explore global composition and dependency from degraded faces via an expressive Transformer module for better code prediction.
- Benefiting from these designs, our method shows great expressiveness and strong robustness against heavy degradation.
- To enhance the adaptiveness of our method for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality.
- Experimental results suggest the superiority and effectiveness of our method.
