---
title: "Visual Abductive Reasoning"
date: 2022-03-26T10:17:03.000Z
author: "Chen Liang, Wenguan Wang, Tianfei Zhou, Yi Yang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2203-14040v1.webp" # image path/url
    alt: "Visual Abductive Reasoning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2203.14040)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2203.14040).


# Abstract
- Abductive reasoning is used to try and find the most likely explanation for partial observations
- In this paper, a new task and dataset called Visual Abductive Reasoning (VAR) is proposed
- AI systems are required to not only describe what is observed, but also infer the hypothesis that can best explain the visual premise
- Based on the large-scale VAR dataset, a strong baseline model called Reasoner is developed
- First, contextualized directional position embedding is adopted in the encoder to capture the causal structure of the observations
- Multiple decoders are cascaded to generate and progressively refine the premise and hypothesis sentences
- Prediction scores of the sentences are used to guide cross-sentence information flow in the cascaded reasoning procedure

# Paper Content

## Introduction
- Charles Sanders Peirce (1839 -1914)
- Abductive reasoning [51] was coined by Charles Sanders Peirce, the founder of American pragmatism, around 1865.
- Abductive reasoning is invariably employed in our everyday life; the generated hypothesis (H) is expected to best explain what happens before, after, or during the observation (O).
- In this article, we propose Visual Abductive Reasoning (VAR), a novel task and dataset for investigating the abductive reasoning ability of AI systems in daily visual situations.
- VAR requires the AI systems to describe the incomplete observation (i.e., visual premise) and write down the hypothesis that can best explain the premise.
- This allows to thoroughly evaluate the entire abduction procedure, as accurate understanding of the premise is the basis of abductive reasoning.
- Moreover, this can hasten the development of this new field, by comparing and embracing ideas for a relevant, well-established, yet different task -dense video captioning (DVC) [30].
- In contrast to DVC that focuses only on describing the observation, VAR yields a new visual-linguistic reasoning paradigm -inference beyond observation.
- Three characteristics make VAR uniquely challenging: i) VAR needs imagination to find hypotheses outside the observation.
- ii) VAR seeks to discover the plausible causal structure among the observed events.
- iii) VAR is more related to the kind of human reasoning in daily situations where the information at hand is often incomplete [26] and absolutely certain conclusions cannot be reached [5,27].
- Our dataset is collected to address the characteristics of the VAR task (cf. §3). It contains 9K examples from 3,718 videos.
- Each example consists of several chronologicallyordered events, most of which are logically related.
- For each event, abduction oriented description is written by people, and its role of premise or explanation is also annotated.
- When presenting each example to the AI system, the explanation event is masked and premise events are visible.
- The AI system is required to understand the partial, noisy observations (i.e., premise events) and construct the most plausible explanatory hypothesis -accurately describing both the observable premise events and the masked explanation event.
- The examples are on average 37.8s long, with 4.17 events, and harvested from diversely event-rich sources, i.e., YouTube Lifestyle videos, movies and TV shows.
- To lay a solid foundation for future efforts, a new model, named REASONER (causal-and-cascaded reasoning Transformer), is proposed (cf. §4).
- Specifically, REASONER is building upon a Transformer encoder-decoder architecture.
- In the encoder of REASONER, a contextualized directional position embedding strategy is adopted to capture causal dependencies among the premise events. Hence the context of the premise events can be gathered in a causality-aware manner, enabling REASONER to learn discriminative representations for the premise and explanatory hypothesis.
- Then REASONER cascades a set of decoders for premise/hypothesis sentence production and refinement.
- For each generated sentence, the associated prediction score is viewed as the confidence and embedded into the next decoder as a signal for inspiring more information to flow from highconfident sentences to the low-confident ones.
- This leads to a confidence-guided multi-step reasoning strategy, boosting the reasoning power of REASONER eventually.

## Related Work
- Dense video captioning is to comprehensively describe all the events in an untrimmed video through a multi-sentence paragraph
- Context-Aware Text Generation is related to some context-aware text generation tasks in the NLP literature
- Counterfactual story revision requires generating a new ending, given a story context altered by a counterfactual condition
- Our work draws inspiration from abductive text generation, which investigates abductive reasoning via a natural language inference task: write an appropriate reason that could explain observations described by narrative text
- Unlike these language tasks addressing inter-sentential relationship understanding only, our VAR task requires abduction and narrative for a sequence of partially observable visual events
- Our VAR task setting is more general; it is not limited to the strict form of abductive reasoning in [5] Visual Future/State Prediction.

## Methodology
- The AI system is only presented with a partially observable version of the video, where the explanation event is obtained by setting all the pixel values of the explanation event as zero.
- The AI system is required to not only describe the premise, but also reason about the most likely explanation for the premise, i.e., generate N sentences that describe the content of the N events in the video, while conditioning on only Ṽ.
- A contextualized directional position embedding strategy is adopted to capture causal relations residing in the input video Ṽ, leading to a Causality-aware encoder.
- To accommodate ii), a confidence-guided multistep reasoning strategy is developed, i.e., utilize the prediction scores of sentences to guide cross-sentence information flow, yielding a cascaded-reasoning decoder.

### Causality-Aware Encoder
- The Causality-aware encoder is to leverage the context from the past and/or future observable events {E n } =h to reinforce their own representations as well as posit a meaningful representation for the most likely explanatory hypothesis, i.e., the masked explanation event E h .
- The attention operation is the core of Transformer: where the output Y ∈ R N ×d is with the same length N and embedding dimension d as the input X ∈ R N ×d , and Causality-Aware Encoder Cascade -Reasoning Decoder
- a boy throws a frisbee out and his dog is running after it S K, E1 S K, Eh S K, EN
- the dog caught the frisbee back the boy gets the frisbee  W q,k,v ∈ R d×d project the input into query, key, and value matrices, respectively.
- As the attention computation is invariant with respect to reordering of the inputs, explicit position encoding is widely adopted, in two typical ways: i) Absolute position encoding [65]: each position n is assigned an embedding, i.e., U n =F Abs (n) ∈ R 1×d , and the position embeddings are directly added to the input, i.e., X ← X+U . F Abs (•) can be a linear projection [12], a sinusoidal function [65], etc.
- ii) Relative position encoding [58]: the position embeddings are constructed considering the pairwise relationships between positions, i.e., U nm = F Rel (n, m) ∈ R.
- Contextualized Directional Position Embedding.
- Since the VAR task is essentially aware of the plausible chains of cause-effect, the relative ordering of the input events matters.
- We continue in the vein of relative position encoding [58,73] and adopt a contextualized directional position embedding strategy, i.e., U nm = F Rel (n, m, X n ) ∈ R: where R ∈ R (2N −1)×d is a learnable matrix, and (•, •) is a directional indexing function, i.e., (n, m) = (m, n).
- The directional projection F Rel is conditioned on the visual context, i.e., X n , since the causal dependency between events is typically related to specific content, e.g., when we see people are laughing, we tend to look back only a short time into the past to figure out the reason; when we see a man falls off his horse, we worry about whether he gets hurt and the impact on his future life.

### Cascaded-Reasoning Decoder
- The cascaded-reasoning decoder first generates a descriptive sentence for each event/hypothesis individually
- The decoder is a multi-modal, masked Transformer decoder that is first adopted for initial description generation
- During training, it is computed over the groundtruth description, i.e., ŜEn , and masked attention is adopted to prevent the leakage of future words
- The decoder iteratively refines the descriptions by gathering context from more reliable sentences
- The decoder uses a confidence-guided decoding scheme to improve the descriptions

### Training Objective
- Given the groundtruth sentences ŜEn corresponding to the N events E n of video, REASONER is trained by minimizing the negative log-likelihood over the outputs of the cascaded-reasoning decoder D k .
- As the teacher forcing scheme [72] is used for training, H n in Eq. 5 and 7 is embedded over one-hot encoded groundturth words ŵEn l .
- We further adopt a hypothesis reconstruction based optimization criterion, to provide the encoder with more explicit supervision signals for explanatory hypothesis reasoning: where Ṽh and Vh are embeddings for the explanatory hypothesis obtained from the masked and original videos, i.e., Ṽ and V, respectively, and F Proj is a projection head, based on a small multi-layer perceptron.

### Implementation Details
- Detailed network architecture: The encoder (4.1) of REASONER is implemented as two Transformer encoder blocks, and each decoder module (4.2), i.e., D k , is implemented as two Transformer masked decoder blocks. They have d = 768 hidden size and 12 attention heads.
- Data preprocessing: For each video event, action/appearance features are pre-extracted using ActivityNet [6] pretrained ResNet200 [15]/BN-Inception [20], as in [32,71,82].
- Training/Inference: For the first decoder D 0 , we adopt scheduled sampling [4] to make the later decoders fully trained. The coefficient between the main and auxiliary training objectives is set as 0.2. During inference, the final descriptive sentences are generated from the last decoder D K , using deterministic decoding, i.e., greedy search.

## Experiments
- We provide benchmarking results on our VAR dataset.
- To verify the efficacy of our core model designs, we conduct a set of diagnostic studies.
- Finally, for comprehensive evaluation, we test our REASONER on the classic, dense video captioning (DVC) task.

### Performance on VAR Task
- We benchmark five top-leading DVC models on VAR to reveal the abductive reasoning ability in existing techniques.
- They include three Transformer-based [9,32,82] and two RNN-based [71,74] models, which are trained on train set of our VAR dataset with pre-provided event segments using their original training protocols.
- Evaluation Metric. Five well-known automated metrics, i.e., BLEU@4 [47], CIDEr [66], METEOR [3], ROUGE-L [36], and BERTScore [79], are used for evaluation.
- Quantitative Result. Table 2 summarizes the benchmarking results on the test set of our VAR dataset. For detailed analysis, we report the performance over the observable premise events and invisible explanation events separately. Moreover, to probe the upper bound of model performance, we evaluate human performance by asking ten volunteers to perform VAR. Specifically, we randomly sample 500 examples from unique videos in VAR test.
- The volunteers are only provided with partially observable videos and requested to write down the corresponding descriptions and hypotheses. The human-written descriptions and hypotheses are evaluated by the automatic metrics, and evaluation scores are shown in the first row of Table 2.
- Several essential conclusions can be drawn from Table 2: i) Humans are good at VAR; although human-written hypotheses for explanation scored lower than the descriptions for the visual premise, they are still very plausible in absolute terms.
- All traditional DVC models [9,32,71,74,82] struggle with VAR that humans excel at. Their generated hypotheses are usually untrusted, and far worse than their created premise narratives. This suggests that existing video-based language generation models are not good at reasoning beyond observation.
- Our REASONER outperforms other AI models [9,32,71,74,82], in both explanatory hypothesis reasoning and premise description, demonstrating the effectiveness of our whole model design. Compared to other AI models, REASONER also yields a relatively smaller performance drop, from premise description to hypothesis reasoning. This suggests that REASONER can make better use of the context of observed events to infer the explanatory hy-pothesis.
- Although our REASONER shows more promising results, there still remains a significant gap from human performance, that is waiting for more sophisticated abductive reasoning models to conquer.
- For comprehensive performance assessment, we further carry out a subjective evaluation, based on pairwise model comparison. Specifically, we randomly sample 500 examples from unique videos in VAR test. Three volunteers are presented the outputs of a pair of systems (i.e., REASONER vs PDVC [71] or human) on the sampled examples, and requested to do a comparison about which one is better, or "equally good" or "equally bad". The human preference results are collected in Table 3, and again the statistics for premise events and explanation events are presented separately.
- The human subjective judgments are generally accordant with the trends reflected by Table 2. Specifically, the human pairwise comparison results confirm the superiority of REASONER over PDVC, the second-best model in Table 2: REASONER receives 34.2 and 29.9 percent preference votes on the premise description and explanatory hypothesis, respectively. However, human-written hypotheses and descriptions are much more favorable than our results, showing again VAR is a very challenging task.
- A test video example in VAR dataset is shown in Fig. 13. It contains the explanatory hypotheses and premise descriptions from our REASONER and other competitors [32,71,82] as well as groundtruth sentences. We can find that our REASONER is able to discover and correctly describe the cause-effect chain, and hence generate a plausible hypothesis, i.e., making a small splash, that well explains the observed events, i.e., standing on the podium. In contrast, other competitors typically produce unsatisfactory results, especially for the explanatory hypothesis.

### Diagnostic Experiment
- The efficacy of core model components is studied
- Contextualized directional position embedding is found to be significantly better than the two alternatives
- Confidence embedding is found to hinder performance

### Performance on DVC Task
- Dataset: ActivityNet Captions
- For completeness, REASONER is trained on the ActivityNet Captions dataset
- REASONER outperforms state-of-the-art DVC models over all the metrics, e.g., +2.81 performance gain in CIDEr
- This proves the strong reasoning ability of REASONER and emphasizes the value of our VAR task in promoting innovations of powerful video-language models

## Conclusion
- We introduce VAR - a novel task that investigates the abductive reasoning ability of machine intelligence in the visual world.
- We establish REASONER, a new Transformer based visual-language model, which captures the context from visual premise in a causality-aware manner, and generates premise descriptions and hypothesis sentences in a confidence-guided, step-bystep fashion.
- REASONER shows promising results on both VAR and dense video captioning tasks.
- We also observe a remaining large headroom for AI systems in VAR, which is expected to encourage exciting avenues in the future.

### A.2. Detailed Annotation Process
- The annotation process includes a quick pass for filtering out videos without cause-effect relations and a second pass for annotating the event type, i.e., the premise and explanation.
- In this phase, the entire video with initialized events and descriptions are all shown to the experts.
- We use the original event annotation provided in [30,33,34] to initialize event boundaries, while they might contain noise annotations. We thus request human experts to i) edit event boundaries when the initial separation can not well fit the description; ii) delete duplicate events when they are overlapped; iii) add additional events when they find a missing part in the cause-effect chain.
- And the annotation interface is shown in Fig. 9.
- After that, experts are requested to further annotate the event type based on the events he selected in the previous step, as shown in Fig. 11.
- Human experts are only shown with premise visual events while the hypothesis event is hidden.
- Annotation interfaces for annotating the premise and hypothesis are shown in Fig. 10 and Fig. 12 respectively.
- Experts might vote to delete an example if they find that a plausible explanation can not be inferred from the premise.
- And the remaining examples are re-annotated with abductive reasoning oriented descriptions.
- Notably, a video might contain multiple examples (candidate cause-effect chains), and we manually control the example distribution to make sure the same video will not be shown to the same expert twice.

## B. Additional Details of Baselines
- We benchmark five top-leading DVC models on VAR
- In this section, we detail the implementation and training protocol of these baseline methods
- MFT is an LSTM-based method that consists of a selection LSTM for relevant event filtering and a captioning LSTM for coherent sentence generation
- We adapt it to VAR task by recurrently passing the given events into MFT and the selection LSTM is transformed into a visually coherent maintaining module
- PDVC employs an LSTM-based captioning decoder, while it is conditioned on a deformable soft attention aggregated visual event
- And the visual events are embedded with a Transformer-based encoder that could also capture bidirectional causal dependencies within it
- VTrans is a fully attentional model that originates from the vanilla Transformer proposed in [65]
- We follow the implementation in [32], which serves as a baseline that only considers a single event and independently generates a single sentence describing the given event
- Thus causal structure can not be formulated in this method
- Trans-XL is originally proposed for modeling unlimited longer-term dependencies with a segment-level recurrent strategy
- Following the implementation in [32], gradients can flow through recurrent steps instead of being stopped
- This enables stronger long-term modeling
- MART is also built on a fully Transformerbased encoder-decoder architecture, that maintains a summarized memory module to model dependencies among events
- Similar to Trans-XL, the unidirectional causal structure is preserved in the memory
- Whereas, experimental results show that it suffers more on our VAR potentially due to the content drift brought by masked visual hypotheses
- Ftihng the implementation provided by [32], all of these baselines are trained with given events from our VAR train and evaluated on VAR test under the same setting of REASONER

## C. Additional Experimental Results
- REASONER is better than other methods at inferring the hypothesis in the linguistic modality only
- The visual-based abductive reasoning is indispensable in the VAR task

## D. Additional Qualitative Visualization
- Fig. 13-14: Shows qualitative examples from VAR test following Fig. 7 in the main paper.
- Generated sentences from competitors [32,71,82] along with our REA-SONER are presented in Fig. 13.
- In contrast to the competitors, both adequate descriptions on premises and plausible inferences for hypotheses are observed for our proposed method, REASONER, which demonstrates a superior abductive reasoning ability for capturing causal structures among visual events.
- Some failure cases and gold human-written explanations are shown in Fig. 14.
- Even though REASONER shows impressive performance on inferring with abduction, VAR task is still a mostly unsolved technical problem. And there remains a large headroom for future works to conquer.

## E. Limitation and Reproducibility
- During annotation process, we observe a bias against women and minorities
- This bias is due to the highly biased nature of movie and web sourced video data
- VAR, derived from these data, inevitably run into the same problem
- We suggest that models trained on VAR dataset should be cautiously examined before being deployed onto real-world applications
- And we will devote further efforts to mitigating the issue in our later works

### E.2. Details of BERTScore Evaluation
- The BERTScore algorithm uses pre-trained embeddings from BERT-based models
- The BERTScore algorithm is evaluated using a hash code version of the score
- The BERTScore algorithm is released at https://github.com/leonnnop/VAR

### F.2. Concerns on Personal Data Collection
- VAR is annotated by human experts
- The annotation and evaluation will be used for academic research and individual consents are reached with signed agreements
- The annotation will not leak any personal information about the experts

### F.3. Potential Societal Impact
- AI systems are endowed with human intelligence
- Abductive reasoning is inference to the most likely explanation for an incomplete set of observations
- An illustrative example of our VAR dataset
- The distribution of frequently used words in VAR descriptions
- The distribution of premise events
