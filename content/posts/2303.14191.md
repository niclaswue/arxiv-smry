---
title: "Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning"
date: 2023-03-24T17:59:58.000Z
author: "Xiaoyang Wu, Xin Wen, Xihui Liu, Hengshuang Zhao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-14191v1.webp" # image path/url
    alt: "Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.14191)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.14191).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/masked-scene-contrast-a-scalable-framework).

# Abstract
- PointContrast conducts unsupervised 3D representation learning via contrastive learning over raw RGB-D frames.
- Two stumbling blocks prevent large-scale unsupervised learning in 3D: inefficiency of matching RGB-D frames and mode collapse phenomenon.
- Proposed efficient and effective contrastive learning framework generates contrastive views directly on scene-level point clouds.
- Reconstructive learning on contrastive learning framework with contrastive cross masks to reconstruct point color and surfel normal.

# Paper Content

## Introduction
- Unsupervised visual representation learning uses unlabeled data to learn visual representations
- Learned representations are beneficial for downstream tasks like segmentation and detection
- PointContrast relies on raw RGB-D frames with restricted views, leading to low efficiency and inferior versatility
- Our approach operates on scene-level views with contrastive learning and masked point modeling, leading to high efficiency and superior generality
- Unsupervised visual representation learning is underexplored in 3D
- Our algorithm accelerates pre-training procedure and achieves better performance on downstream tasks

## Related work
- 2D image contrastive learning uses instance discrimination and contrastive learning to learn transferable visual representations
- Large batch size and data augmentation pipeline are important for better performance
- 2D image reconstructive learning uses masked image modeling to reconstruct RGB values, discrete tokens, or features of masked pixels
- 3D scene understanding can be categorized into projection-based, voxel-based, and point-based methods
- 3D representation learning is still not mature and most works train from scratch on target data
- Frame matching extracts raw RGB-D frames and camera positions from raw data and projects 2D frames into 3D space

## Pilot study
- Redundant frame encoding
- Low learning efficiency
- Dependency on raw RGB-D frames
- Mode collapse remains unsolved
- Difficulty of unsupervised pretext task needs to be raised

## Approach
- Introduced optimized contrastive learning design in Section 4.1 to make pre-training process more efficient
- Solved mode collapse problem with reconstructive learning design in Section 4.2
- Described optimization target in Section 4.3
- Built Masked Scene Contrast (MSC) framework, visual illustration in Figure 3

### Contrastive learning
- View generation: Generate query view and key view of point cloud with data augmentations
- Feature extraction: Encode point cloud features with U-Net style backbone
- Point matching: Calculate correspondence mapping between query and key view
- Loss computation: Compute contrastive learning loss on representation of two views and correspondence mapping
- Data augmentation: Photometric, spatial, and sampling augmentations
- View mixing: Mix up query views before feature extraction
- Contrastive target: Apply InfoNCE loss to matched points

### Reconstructive learning
- Mode collapse is a stumbling block for large-scale representation
- Solution is to scale up the difficulty of the unsupervised pre-training task
- Masked point modeling is proposed to be integrated into the contrastive learning framework
- Contrastive cross mask is used to enable additional construction learning
- Reconstruction targets are built for point cloud texture and surfel normal vectors

### Loss function
- Combines 3 targets to make unsupervised task more scalable
- Overall loss function is a weighted sum of 3 equations with weight parameters λ c and λ n

## Experiments
- Conducted extensive experimental evaluations to validate framework
- Utilized ScanNet point cloud in pre-training pipeline
- Explored large-scale pre-training across multiple datasets

### Main properties
- Main designs and properties of MSC outlined in Table 1
- View generation pipeline introduced to enable efficient pre-training
- Strategy directly utilizes scene-level point cloud to reduce storage requirements and allow more efficient use of training data
- Increasing number of positive pairs leads to consistent improvements
- Combination of spatial and photometric augmentation leads to best performance
- Randomly mixing query views while leaving key views unchanged yields best performance
- Cross mask strategy ensures fewer shortcuts to the task and enables consistent downstream improvements
- Masking ratio of 30-40% works best with design

### Results comparison
- Merged multiple datasets to extend pre-training assets
- Improved semantic segmentation results on ScanNet and ScanNet200
- Improved instance segmentation results on ScanNet and ScanNet200
- Data efficiency better than previous methods

## Conclusion and discussion
