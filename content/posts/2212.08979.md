---
title: "Language model acceptability judgements are not always robust to context"
date: 2022-12-18T00:11:06.000Z
author: "Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08979v1.webp" # image path/url
    alt: "Language model acceptability judgements are not always robust to context" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08979)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08979).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/language-model-acceptability-judgements-are).

# Abstract
- Most targeted syntactic evaluation datasets ask models to make these judgements with just a single context-free sentence as input.
- This does not match language models' training regime, in which input sentences are always highly contextualized by the surrounding corpus.
- This mismatch raises an important question: how robust are models' syntactic judgements in different contexts?
- In this paper, we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality.
- We find that model judgements are generally robust when placed in randomly sampled linguistic contexts. However, they are substantially unstable for contexts containing syntactic structures matching those in the critical test content.
- Among all tested models (GPT-2 and five variants of OPT), we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures. This effect is amplified by the length of the context, except for unrelated inputs.
- We show that these changes in model performance are not explainable by simple features matching the context and the test inputs, such as lexical overlap and dependency overlap. This sensitivity to highly specific syntactic features of the context can only be explained by the models' implicit in-context learning abilities.

# Paper Content

## Introduction
- Neural large language models (LLMs) have made unprecedented progress in the development of language
- The minimal-pair paradigm (MPP) is a popular method for evaluating models' knowledge of linguistic phenomena
- Under the MPP, models are presented with datasets containing pairs of minimally differing text sequences-usually differing in word order or in a few tokens-one of which is deemed by humans to be acceptable and the other unacceptable
- Studies that employ MPP datasets generally compare the probability of two stand-alone text sequences without any explicit linguistic context (or, the probability of two words that are part of some stand-alone sentence). However, this is not a naturalistic or realistic approach
- The syntactic priming literature investigates the effect of linguistic contexts to some extent, but mostly in a constrained setting with only one or a small number of context sentences
- The interaction of context with minimal pair accuracies remains underexplored for multi-sentence contexts, despite the fact that multi-sentence inputs are more likely for many NLP tasks-especially with the rise of prompting and in-context learning
- We explore model calibrations when subjected to prefixed stimuli using perplexity margins, to explain the changes in accuracy with different types of prefixes

## Background
- Sequence length matters, mismatching sequence lengths between training and testing can affect model performance
- Priming can elicit learning behavior in LLMs
- Higher-level features for unsupervised tasks can be extracted by LLMs given enough priming examples

## Approach
- We follow standard practice in MPP, where we evaluate the preference (P) of a language model M towards acceptable sentence (x) over its unacceptable counterpart (x ), with respect to log likelihood, and compute the value over the full evaluation dataset D.
- D typically consists of several test suites, each of which instantiates a particular linguistic phenomenon.
- We denote the particular test suite under evaluation as the target suite: S ⊂ D.
- Each target suite consists of k pairs of acceptable and unacceptable sentences, (x, x ) k i=0 ∈ S, and may have multiple conditions.
- Within each target suite, we compute the acceptability judgements on one or more experimental conditions, comparing a given LM's log-likelihood preference P for the acceptable and unacceptable sentence in each condition.
- The accuracy score (A) over a test pair from a single condition is defined as: (1)
- Depending on the dataset, it can have either one or multiple conditions evaluated for each test item.
- To simulate increasing length of input, we prepend a prefix sequence c i to both x and x , and compute the preferences over the concatenated sequence, P([c i , x i ]) and P([c i , x i ]), where c can be of arbitrarily large length.
- Datasets. We use the standard targeted syntactic evaluation datasets of BLiMP (Warstadt et al., 2020) and SyntaxGym (Hu et al., 2020).
- BLiMP is a large-scale MPP dataset consisting of 67 different subsets of 1000 English sentence pairs each. Each BLiMP subset targets a separate linguistic paradigm that belongs to 12 different linguistic phenomena-for instance, subject-verb agreement, argument structure, etc.
- For each minimal pair of sentences (x, x ) k i=0 in BLiMP, models are expected to rate the log-likelihood of the acceptable sentence x above the log-likelihood of the unacceptable sentence x .
- SyntaxGym is a syntactic evaluation benchmark designed with more stringent evaluation criteria.
- For 34 different linguistic phenomena, the Syn-taxGym benchmark defines test items with two to four different conditions, consisting of minimal structural variations on the same sentence which render the sentence either grammatical or ungrammatical.
- Model log-likelihoods are measured at a critical region within each sentence, rather than across the whole sentence, and models are expected to produce log-likelihoods that satisfy multiple inequalities across all conditions.
- SyntaxGym is smaller than BLiMP (with about 20 items per phenomenon on average) and all of the examples are hand-written.
- We adapt 23 of the 34 test paradigms in SyntaxGym whose structure was compatible with the prefixing analyses of this paper.
- These two datasets offer complementary value to the analyses in this paper: BLiMP's large scale allows us to make general conclusions about the average effect of prefix interventions, while SyntaxGym's stringent evaluation allows us to verify that the effects are sustained under more rigorous experimental conditions.
- We compute the log-likelihood on the given input using the minicons library (Misra, 2022), which is based on huggingface (Wolf et al., 2020).
- For each dataset D, we first compute the baseline acceptability accuracy according to Equation 3.
- Next, we aim to re-evaluate the acceptability accuracy by steadily increasing the token length of the input.
- Following prior work on priming ( §2), we analyze how prepending the test examples with additional context affects a given model's acceptability judgements.
- To increase the token length while maintaining the MPP formulation, we introduce a context c by prepending the same sequence to each target...

## Main Results
- Prefixes with longer grammatical content are judged as more acceptable
- Prefixes with ungrammatical content are judged as less acceptable
- Prefixes from the same domain are judged as more acceptable than those from a different domain
- Prefixes with ungrammatical content have a drastic impact on the model's accuracy when prefixed with longer input
- Prefixes from the same domain have a less drastic impact on the model's accuracy when prefixed with longer input
- Predicting the model's acceptability on BLiMP is not significant for prefixes from Wikipedia

## Prefix Similarity Analysis
- We have observed that length effects on acceptability judgements are conditional on the similarity of the prefix to the test sentence.
- Does some specific kind of similarity (e.g., syntactic or lexical similarity) explain this phenomenon?
- Perhaps the prefix is syntactically priming the model for the target sentence (Sinclair et al., 2022), in which case we would expect the syntactic similarity of the sentences to correlate with accuracy when using grammatical prefixes.
- Another possibility is that a more spurious feature-such as lexical overlap-is responsible (Misra et al., 2020;Kassner and Schütze, 2020).
- To test this, we can correlate syntactic similarity and lexical overlap with accuracies on each example.
- To measure lexical overlap, we use F 1 scores to measure how many tokens4 in the prefix and test sentences are shared.
- To approximate syntactic overlap, we can compute the F 1 score over dependency labels in two sentences, rather than across tokens.
- If multiple prefix sentences are present, we can take the mean similarity with the target sentence across prefixes.
- Then, we compute the point-biserial correlation5 (ρ p ) between the similarity metric and accuracy on a given example, averaging similarities across prefix sentences.
- We compute the correlation separately for each model size and each prefixing strategy.
- Note that we only use grammatical prefixes; thus, we expect positive correlations if priming explains the length effects.
- We find very low and non-significant correlations with dependency overlap and token overlap (ρ p < 0.05, p > 0.1) regardless of prefixing strategy or model size.
- This could be evidence that the model is more sensitive to the length of the prefixes than any notion of syntactic or lexical similarity on this task.
- However, this instance-level analysis could be confounded by the mixture of various phenomena in the prefixes.
- The model could be sensitive to sentences from certain phenomena more than others, or the varying lengths of sentences from each phenomenon.
- To more specifically measure whether priming can explain our findings, we focused on BLiMP and prefixed sentences from one phenomenon at a time with a given test phenomenon; in other words, we sample out-of-domain prefixes, but controlling which phenomenon we sample from.
- Using this approach, we can capture how structurally similar each BLiMP phenomenon is with each other BLiMP phenomenon, and how this correlates with accuracies.
- We describe the out-of-domain single-phenomenon prefixing strategy and present similarity metrics in Appendix A.
- When correlating phenomenon-level similarities with accuracies, we find that correlations here are a bit stronger than when we mix out-of-domain prefixes (ρ s = 0.11 for dependency overlap, and ρ s = 0.18 for token overlap, p < 0.001 for both).
- The magnitude of the correlations is low, but they are still significant.
- Thus, there is some relationship between the similarity of the prefix and test sentence with accuracy, but the relationship is weak.

## Discussion
- On average, language models' acceptability judgements are highly sensitive to the domain and acceptability of the input in their contexts
- This has implications for interpreting results from MPP benchmark datasets: single-sentence or otherwise short inputs may not be representative of models' true abilities
- Indeed, shorter inputs may not be what pre-trained language models expect, given that their pre-training procedures often entail packing many sentences into a single training example
- This agrees with prior work that finds performance improvements from reformatting train and test inputs in a way that more closely resembles the pre-training setup (Hupkes et al., 2020;Newman et al., 2020;Varis and Bojar, 2021;Chada and Natarajan, 2021)
- Crucially, however, performance is only sensitive to length given in-domain examples
- Our results also demonstrate a notable capacity of LMs to show behavior that is consistent with the acceptability of their prefix
- That is, while models showed marked improvements on acceptability tasks when prefixed by acceptable sentences from the same domain, they also (more substantially) showed the opposite behavior-preferring unacceptable sentences-when prefixed by sentences that were unacceptable in the same way
- This twoway consistency adds more credence to the general observation that models tend to be sensitive to abstract features (such as acceptability) emergent in their context, than recent work that only explores this behavior in one direction (Lampinen, 2022;Sinclair et al., 2022)
- More broadly, this adds to the literature on prompt sensitivity in pre-trained language models. Prompt tuning work has found that LMs are sensitive to individual prompts (Kojima et al.), and that the ordering of in-context examples (Lu et al., 2022) greatly affects model performance. Smaller LMs are also sensitive to the choice of prompt and output verbalizer (Schick and Schütze, 2021a;Gao et al., 2021), and we indeed find sensitivity to prefixes in our study for a variety of model sizes and prefixing strategies.
- To our knowledge, however, our study is the first to implicate input length as a factor contributing to linguistic performance.

## Conclusion
- In general, the margins are larger for grammatical prefixes than for ungrammatical prefixes.
- The margins are also larger for longer prefixes.
- The margins are also larger for more accurate models.
- The margins are also larger for models that are more confident.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the training data.
- The margins are also larger for models that are more similar to the training context.
- The margins are also larger for models that are more similar to the test sentence.
- The margins are also larger for models that are more similar to the test context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the training data.
- The margins are also larger for models that are more similar to the training context.
- The margins are also larger for models that are more similar to the test sentence.
- The margins are also larger for models that are more similar to the test context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are more similar to the target sentence.
- The margins are also larger for models that are more similar to the target context.
- The margins are also larger for models that are more similar to the input.
- The margins are also larger for models that are...
