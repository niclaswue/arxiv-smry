---
title: "The effectiveness of MAE pre-pretraining for billion-scale pretraining"
date: 2023-03-23T17:56:12.000Z
author: "Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal and 7 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-13496v1.webp" # image path/url
    alt: "The effectiveness of MAE pre-pretraining for billion-scale pretraining" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.13496)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.13496).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/the-effectiveness-of-mae-pre-pretraining-for).

# Abstract
- This paper looks at the standard pretrain-then-finetune paradigm used in computer vision.
- An additional pre-pretraining stage is introduced that uses the self-supervised MAE technique to initialize the model.
- Pre-pretraining improves model convergence and downstream transfer performance across a range of model and dataset sizes.
- New state-of-the-art results are achieved on 10 different visual recognition tasks.

# Paper Content

## Introduction
- Pretrain-then-finetune paradigm enables high performance visual recognition models
- Pretraining consists of training a model using a pretraining task on large scale data
- Initial stage of pre-pretraining before standard pretraining task can improve vision models
- Combines two common pretraining tasks: weakly supervised and self-supervised
- Large-scale empirical study to measure effectiveness of pre-pretraining on 10 different visual recognition tasks
- Pre-pretraining improves model convergence and downstream performance
- Combines benefits of self-supervised and weakly-supervised learning
- Sets new state-of-the-art results on image classification, 1-shot image classification, and zero-shot transfer

## Related work
- Supervised pretraining of transferrable representations on large labeled datasets has been a powerful approach in computer vision
- Self-supervised pretraining is an alternative to learn representations without relying on large well-labeled datasets
- Weakly supervised pretraining is a middle-ground between supervised and self-supervised pretraining
- Weakly supervised pretraining relies on large quantity of "free" annotation available on the internet
- This work explores WSP in conjunction with self-supervised approaches

## Setup
- Goal is to study effectiveness of self-supervised pre-pretraining for representation learning
- Leverage Masked AutoEncoding (MAE) as self-supervised pre-pretraining approach
- Study shows MAE scales with size of pretraining dataset and model size
- Combining MAE with weak supervision improves large scale vision models
- Combination leads to faster convergence and is a simple, scalable way to learn visual representations at scale

## Experiments
- Evaluated and analyzed large scale MAE pre-training using Instagram data
- Described datasets used for pretraining and evaluation
- Analyzed pretraining design decisions
- Evaluated downstream transfer of learned representation

### Datasets and training details
- Used Instagram-3B (IG-3B) dataset with 28K classes and 3B images
- Resampled to 5B images
- Obtained labels using hashtags and WordNet synsets
- Evaluated MAE→WSP on image classification, object detection, segmentation, video classification, and zero-shot transfer
- ImageNet-1k (IN1k) and iNaturalist-18 (iNat18) used for image classification
- COCO, LVIS used for object detection and segmentation
- Kinetics-400 (K400) and Something Something-v2 (SSv2) used for video classification
- IN1k and Food-101 (F-101) used for zero-shot transfer
- INv2, IN-ReaL, and ObjectNet used to evaluate robustness
- MAE model trained on IG-3B without labels
- Supervised pretraining used cross-entropy loss
- LiT approach used for zero-shot understanding

### Scaling mae pretraining to large data
- MAE pretraining on IG-3B provides consistent gains over IN1k for all vision tasks
- MAE scales with the size of the pretraining dataset and benefits from using billions of images from IG-3B
- MAE→WSP is a simple yet promising strategy to improve performance while requiring no extra data or tuning
- MAE pre-pretraining gives consistent gains over the WSP baseline across all model sizes
- Pre-pretraining improves results over the standard pretraining and provides large gains with fewer WSP pretraining epochs
- Pre-pretraining leads to faster convergence
- MAE pre-pretraining works with datasets of different scales and distributions

### Transfer evaluation
- Comparing with state-of-the-art research on image and video classification, detection and segmentation, low-shot image classification, zero shot transfer, and robustness analysis
- MAE→WSP gets the best performance for ViT-H and ViT-L sized models on ImageNet-1k
- MAE→WSP is significantly more robust and generalizes better on additional test sets
- MAE→WSP sets a new state-of-the-art result on iNaturalist-18
- MAE→WSP is competitive with state-of-the-art methods on video classification
- MAE→WSP has impressive low-shot performance on IN1k and iNat18
- MAE→WSP has the highest reported performance with just one labeled example per class on IN1k
- MAE→WSP outperforms MAE IN1k pretraining on detection AP
- MAE→WSP performs better than WSP on all tasks

## Conclusion
- Introduced pre-pretraining as an initial stage in the standard pretrain-then-finetune paradigm
- Uses MAE and does not need additional supervision
- Improves downstream performance on multiple recognition tasks
- Improves model convergence and is more efficient than standard weakly-supervised pretraining
- Scalable technique that matters even at web-scale
- Benefits of using pre-pretraining hold across varying model sizes and different pretraining data distributions
- Combines self-supervised and weakly-supervised learning
- Model initialization plays a significant role in final performance and training dynamics
