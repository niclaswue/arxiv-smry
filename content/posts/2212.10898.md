---
title: "Training language models for deeper understanding improves brain alignment"
date: 2022-12-21T10:15:19.000Z
author: "Khai Loong Aw, Mariya Toneva"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10898v1.webp" # image path/url
    alt: "Training language models for deeper understanding improves brain alignment" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10898)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10898).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/training-language-models-for-deeper).

# Abstract
- Building systems that achieve a deeper understanding of language is one of the central goals of natural language processing
- Recent works have begun to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts
- However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task
- This work investigates this further by turning to the one language processing system that truly understands complex language: the human brain
- We show that training language models for deeper narrative understanding results in richer representations that have improved alignment to human brain activity
- Further, we find that the improvements in brain alignment are larger for character names than for other discourse features, which indicates that these models are learning important narrative elements

# Paper Content

## INTRODUCTION
- Language models that have been trained to predict the next word over millions of text documents have led to large improvements on a range of benchmarks in natural language processing (NLP).
- However, researchers have shown that NLP models may be relying on shallow heuristics to perform these tasks rather than learning a deeper language understanding (McCoy et al., 2019;Min et al., 2020;Linzen, 2020).
- To build systems with deeper language understanding, recent work proposed to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts (Kryscinski et al., 2021;Sang et al., 2022;Kočiskỳ et al., 2018).
- Does this approach truly lead to deeper understanding of language? We investigate this by turning to the one language processing system that truly understands complex language: the human brain.
- Prior work has used human brain recordings to interpret representations of pretrained language models (LMs) (Søgaard, 2016;Toneva & Wehbe, 2019;Abdou et al., 2021).
- They evaluate how well representations obtained from a language model can predict representations sampled from the human brain during language comprehension via a brain imaging device, such as functional magnetic resonance imaging (fMRI).
- If this prediction performance, also known as brain alignment, is determined to be significant via a statistical test, then the LM and specific brain location or timepoint are thought to significantly align in their representations of language.
- Using these methods, researchers have shown that pretrained language models significantly predict large parts of the brain regions that are thought to underlie language comprehension (Wehbe et al., 2014b;Jain & Huth, 2018;Toneva & Wehbe, 2019;Caucheteux & King, 2020;Schrimpf et al., 2021;Goldstein et al., 2022).
- In this work, we draw insights from the human brain to study whether language models are truly learning deeper language understanding. We analyze the effect of training LMs for deeper language understanding on their alignment with fMRI recordings of human subjects reading a book chapter.
- We specifically investigate 4 pretrained language models (i.e. "base models") and 4 corresponding models which are obtained by training the base models on the BookSum dataset (Kryscinski et al., 2021) to improve the base language model's narrative understanding (i.e. "booksum models").
- The BookSum dataset was selected because it is a summarization dataset that requires understanding complex interactions across long narratives, and the 4 models were selected because their architectures were designed to integrate information across long contexts.
- We evaluate the alignment of the base and booksum models with fMRI recordings of 8 participants reading a chapter of a popular book word-by-word, made publicly available by Wehbe et al. (2014a).
- This dataset was chosen because it is one of the largest datasets of participants processing a narrative story (5176 words which corresponds to approximately 1300 samples of fMRI recordings per participant).
- Our main contributions are as follows: In Section 4, we show that training language models for deeper narrative understanding improves alignment to human brain activity. Also, when increasing the number of words fed to the models, up to 500 words, brain alignment increases. Lastly, for each model, we identify the layers where these improvements in brain alignment occur.

## RELATED WORK ON BRAINS AND LANGUAGE
- Toneva et al. (2020) present an approach to disentangle supra-word meaning from lexical meaning in language models and show that the supra-word meaning is predictive of fMRI recordings in two language regions (anterior and posterior temporal lobes).
- Caucheteux et al. (2021) and Reddy & Wehbe (2021) aim to disentangle alignment due to syntactic and semantic processing.
- Toneva et al. (2022) examine whether representations obtained from a language model align with different language processing regions in different ways. Additionally, researchers have suggested that one contributor to the alignment is the language model's ability to predict the next word, with a positive relationship between lower perplexity and higher brain alignment across NLP models (Schrimpf et al. , 2021;Goldstein et al., 2022).
- On the other hand, more recent work shows that no simple relationship exists, and language modeling loss is not a good predictor of brain alignment (Pasquiou et al., 2022).
- Merlin & Toneva (2022) introduced perturbations to disentangle the contribution of next word prediction and semantic knowledge towards brain alignment.
- Our work contributes to this research area in two ways: (1) We disentangle the contributions of LM ability from deep language understanding towards brain-NLP alignment. (2) We show that deep language understanding is another factor that contributes to brain-NLP alignment.

## METHODS

## BRAIN REPRESENTATIONS
- We use a publicly available brain dataset consisting of fMRI recordings of 8 participants reading chapter 9 of the book Harry Potter and the Sorceror's Stone (Rowling et al., 1998).
- The 5176 words in the chapter are presented to participants at a fixed interval of 0.5 seconds, one word at a time.
- The chapter was divided into four runs of approximately equal length, and participants were allowed a short break at the end of each run.
- We use this specific dataset for two main reasons: 1) the story is situated within a rich narrative world with characters, emotions and relationships, and 2) it is one of the largest publicly available datasets in terms of samples per participant which improves the reliability of the brain alignment estimate.

## NLP REPRESENTATIONS

## ALIGNING BRAIN AND NLP REPRESENTATIONS.
- We use a general alignment approach that has been previously used in several works
- This approach learns a function to predict the fMRI recordings at every voxel of each participant i using the NLP representations that correspond to the same text read by the participant
- We parameterize this function as a linear function, regularized using the ridge penalty
- We train the function in a cross-validated way and test its performance on the data that was heldout during training
- We select the ridge parameter via nested cross-validation
- Since the fMRI data was collected in 4 runs of approximately equal length, we use 4-fold cross validation where each fold corresponds to holding out one run of fMRI data for testing
- We also remove 10TRs of fMRI data and the corresponding NLP representations from the ends of each test run to avoid possible train-test overlap
- Other details about the alignment approach are provided in Appendix C

## TRAINING NLP MODELS FOR DEEP NARRATIVE UNDERSTANDING
- Do NLP models trained for deeper understanding of long narratives have improved brain alignment over those trained for language modeling (LM)?
- To investigate this, we evaluate the 20v20 brain alignment scores of 4 booksum models (deeper understanding) against 4 base models (LM), as described in Section 3.
- We use all 8 NLP models, layers 1 to 12, sequence lengths 1 to 1000, and all 8 human subjects.
- In Figure 1, we present average results for all 4 pairs of models, as well as results for a representative set of layers for one of the models.
- Other results are in Appendix F and G.
- Booksum models are significantly better aligned to brain representations.
- We suggest that this is due to deeper understanding of text, because of two key facts. (1) The booksum models were trained on the BookSum dataset, which is designed with challenging tasks to learn deeper narrative understanding. It requires understanding complex interactions across long-range contexts and generating summaries that capture the most critical information. (2) Furthermore, these models have architectures that were designed to integrate information across long context lengths. This allowed them to develop deeper understanding when trained on BookSum.
- Could the improved brain alignment of booksum models be due to other confounding variables and not deeper language understanding? In this paragraph, we describe how we tried to eliminate possible confounding variables.
- Both base and booksum models were trained with similar data: large narrative datasets. Hence, improved brain alignment is not simply because booksum models were trained on data from a more similar domain to the brain alignment dataset (Harry Potter).
- We also verified this by comparing booksum models against base models trained on BookSum with a language modeling objective in Appendix L.
- In Section 5, we show that training NLP models for deep understanding improves brain alignment, but does not improve LM ability. Hence, improved brain alignment is not because booksum models developed better language modeling ability through BookSum-training.
- BART trained to summarize CNN news articles (BART-cnn) is not substantially higher than BART-base. Hence, the improved brain alignment is not simply due to the summarization training objective.
- Therefore, we argue that improved brain alignment for booksum models is likely not due to these confounding variables.

## RELATIONSHIP BETWEEN BRAIN ALIGNMENT, LANGUAGE MODELING AND DEEP UNDERSTANDING
- Language models developed better brain alignment by training on BookSum
- This improved brain alignment is not due to better language modeling ability
- For BigBird and LED, the booksum model achieves significantly higher brain alignment but significantly worse LM performance than the base model
- For BART, the booksum model achieves greater brain alignment but does not significantly differ in LM performance
- For LongT5, the booksum model has both higher brain and LM performance
- Hence, while all 4 booksum models have significantly better brain alignment, the majority of them do not have a better LM performance
- This shows that the improved brain alignment for at least 3 of the 4 models is due to deeper understanding and not their ability to predict the next word.
- Across all brain language ROIs, brain alignment is significantly greater for booksum models.

## CONCLUSIONS AND FUTURE WORK
- LM models achieve poorer representations for Characters and other discourse features, compared to deeper understanding models.
- Existing training methods for narrative understanding are a first step towards developing language models with deep language understanding.
