---
title: "Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"
date: 2022-12-20T18:59:23.000Z
author: "Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, Nick Haber"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10561v1.webp" # image path/url
    alt: "Parsel: A Unified Natural Language Framework for Algorithmic Reasoning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10561)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10561).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/parsel-a-unified-natural-language-framework).

# Abstract
- LLMs struggle with hierarchical multi-step reasoning
- We introduce Parsel, a framework that enables automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language
- Parsel can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning
- We demonstrate Parsel's capabilities by using it to generate complex programs that cannot currently be automatically implemented from one description and backtranslating Python programs in the APPS dataset

# Paper Content

## Introduction
- Many computer scientists believe that it is easier to verify a solution than to write one.
- For code large language models like Codex (Chen et al., 2021), generating solutions, rather than testing them, is often the bottleneck.
- As we try to generate longer programs, the chance of any sample yielding a complete, working solution dramatically decreases.
- We propose Parsel, a compiler framework to implement and extend a high-level algorithmic design for interacting with an external environment (e.g. a programming language interpreter, a set of parameterized robot actions, or a formal theorem-proving environment).
- We formalize the high-level algorithmic design as a hierarchical set of function descriptions and constraints on their implementations (e.g. unit tests) in natural language, translating to a fixed underlying grammar.
- Given such a design, a Parsel compiler considers a minimal combinatorial set of function implementations based on the function descriptions and identifies one implementation per function description such that all constraints are satisfied.
- In effect, Parsel aims to perform algorithmic problem-solving by mirroring a common pattern in human reasoning -making an abstract plan to the level that it can be solved automatically.

## Specifying Programs in Parsel
- We develop a simple language underlying Parsel
- The language has scoping rules and has nuances per target language
- The language is designed considering programmers, code LLMs, and students

### Descriptions
- A function description is represented as a function name followed by comma-separated input arguments in parentheses, and optionally what the function returns, then a colon and text describing the function to be implemented.
- For example, count_living_neighbors(grid, i, j): count the number of living neighbors of the cell at the index (i, j).
- A function generated from a description can call either the functions defined directly below the description in the function graph (indicated with indentation) or references directly below the description.

### Constraints
- A constraint is represented as a function's input values comma-separated, optionally followed by an arrow and the expected output of the function.
- Constraints are provided at the same indentation level as the preceding description.
- For example, after the definition of count living neighbors, at the same indentation, one can write This indicates that the function count living neighbors should return 1 when called with the arguments [[1, 0], [0, 1]], 0, 0 and 2 when called with Notably, to apply complex constraints on functions, one can describe and constrain higher-order functions.
- Note that constraints can only be applied following descriptions, and not to references.
- What it means to satisfy constraints to validate a program varies from language to language: in Python, one can check that a program passes certain assert statements by evaluating them; however, in a language like Lean, where the ability to run a program without any sorry lines shows that a proof holds, one would instead represent the formal proof statement as the specified constraint (that is, that you are actually proving what you set out to prove).
- For languages where correctness can be checked without any unit tests, their functions can be treated as also having implicit constraints.

### References
- A reference is simply the name of a function defined in the current scope (see Subsection 2.4 for details)
- A reference allows and encourages (via prompt) the parent function to use the referenced function
- This allows for recursive function definitions and functions called by multiple functions
- Note a reference without a description in scope is not valid

### Scoping
- The scope of a function is defined by the indentation of the code surrounding it.
- This means that the scope of a function includes the functions that can be used as references for it.

### Variations Due to Target Language Requirements
- Every language has a different evaluation function
- Every language will likely require a different prompt for the language model

## Implementing Programs in Parsel
- Parsel parses a string into a list of tokens
- Each token is a word or a number
- The first token in the list is the start of a line
- The next token is the character at the current position in the string
- The next token is the end of the line
- If the token is a number, it is interpreted as a position in the string
- If the token is a word, it is interpreted as the first letter of the word
- Parsel compiles the list of tokens into a Parsel program
- The start of the program is the first token in the list
- The end of the program is the last token in the list
- The program is terminated when the end of the string is reached
- Parsel parses a string into a list of tokens
- Each token is a word or a number
- The first token in the list is the start of a line
- The next token is the character at the current position in the string
- If the token is a number, it is interpreted as a position in the string
- If the token is a word, it is interpreted as the first letter of the word
- Parsel compiles the list of tokens into a Parsel program
- The start of the program is the first token in the list
- The end of the program is the last token in the list
- The program is terminated when the end of the string is reached

### Constrained Implementation
- Every approach relies on a code LLM to identify an implementation that satisfies the constraints
- To generate examples, we use a code language model to prompt a description and function signature
- In Python, we use this to mean passing assert statements

### Leaves and Merges
- Functions are implemented by querying the code language model using the description text as a docstring and the description's function name and arguments for the signature
- If a function is a leaf node of its call graph, it is implemented by querying the language model
- If a function is not a leaf node, it is implemented by referencing the implementations of its direct children
- In this case, the descriptions and function signatures of the children are aggregated (for Python, we generate a prompt as if the child functions are imported and use their descriptions as comments)
- We then query the language model similarly to the leaf case, but with the function name and arguments of the parent function
- Crucially, this allows us to easily interchange child implementations
- parse_reference(line): add reference as a child of current node if reference is an ancestor or a direct child of an ancestor
- parse_constraint(line): add the constraint to the current node's constraints.
- get_dependency_graph(function_graph) -> dependency_graph: taking the function graph, create a copy where all nodes without asserts → also depend on their parents unless the target language implicitly tests all functions.
- identify_strongly_connected_components(dependency_graph): return SCCs of the dependency graph and the edges between the SCCs.
- compile_scc(scc, scc_graph): find an implementation string solving a given SCC, starting with SCC dependencies, then generating → possible implementations of SCC functions, then finding an implementation combination satisfying the functions' constraints
- compile_children(scc, scc_graph): compile any SCCs this SCC depends on and add them to the implementation string.
- compile_scc generate_implementations(scc, n, children_implementation_str): for each function in the SCC, prompt the language model to generate n → implementations of each function starting with the implementation string of the SCC's children.
- solve_constraints(scc, fn_implementations): taking the provided constraints of each function in the scc, evaluate a shuffled list of → the direct product of implementations with the constraints until one passes all of them
- direct_product_implementations(fn_implementations): return the direct product of the list of lists of fn_implementations
- generate_constraints(fn_node): translate each of the constraints into an evaluation string idiomatic to the target language and add → these to the list of combined implementations
- eval_str(scc, implementation_str): evaluate an implementation including constraints by running it in a target-language executor
- on_fail(scc, scc_graph): raise an error highlighting the scc which could not be compiled

### Sequential Case
- Parsel programs can be implemented with post-order traversal
- In practice, many programs have more complex structures and constraints

### Strongly Connected Components of Functions
- One can imagine a strict version of Parsel where we require all functions to have accompanying constraints.
- However, even with this requirement, as long as recursion is possible, it is sometimes necessary to implement functions jointlywith cyclic dependencies, none can be tested alone.
- Further, there are many contexts in which one may only have access to constraints for some set of functions.
- In particular, to support automatic "elaboration" where a language model may automatically construct the Parsel subfunctions for a given function, it is necessary to allow for functions to be defined without constraints.
- This raises a key question: how?
- One solution would be to consider all possible implementations of all functions defined in the Parsel program and then to iterate through combinations until a valid implementation is found.
- However, this is clearly intractable for large programs.
- Instead, we propose a more efficient solution, inspired by (Cocke & Kennedy, 1977): for cases of recursion, we reference the function call graph and identify all of its strongly connected components (e.g. Fig 2).
- In other words, these are the sets of functions where their definitions depend on one another.
- For these cases, we apply the above-described technique of considering all possible sets of their implementations until one satisfies all of their constraints.
- Notably, we make the simplifying assumption that any statefulness across functions does not interfere with dependent asserts.
- We evaluate each element of a shuffled list corresponding to the function implementation sets arising from the direct product of all the sets of function implementations in the strongly connected component.
- In other words, for possible implementations of functions f, g, h which form a strongly connected component of a call graph, and I(f ) corresponds to the language-model-generated implementations of f , we consider uniformly random samples without replacement from I(f ) × I(g) × I(h).
- We use multiprocessing with a user-specified timeout and encourage the use of a sandboxed evaluation environment in order to allow for many fast solutions to be tested alongside slower solutions.

### Caching
- We cache responses from the language model with respect to the prompt and language model decoding parameters.
- This reduces the number of queries necessary.
- We do this to keep the programs generated mostly stable.

### Headers
- We support program headers, allowing global contexts.
- This is indicated by a line containing an optional string of special characters (e.g. "#*#*#") separating the body and the text.

### Automatic Function Infilling
- Sometimes, a function generated by a language model may call a function that is not yet implemented.
- In this case, we can (optionally) attempt to automatically generate and implement it based on its usage.
- The function is then incorporated into the call graph as a unit-test-less child of the function which calls it.
- To avoid infinite recursion and inefficient use of language model quota, we limit the number of times that this process can be applied to a function.

### Automatic Decomposition
- As indicated by a rapidly growing number of papers (Brohan et al., 2022;Huang et al., 2022), the task of decomposing a task into steps in natural language is one that language models are surprisingly capable of.
- Thus, we treat the ability to automatically decompose a Parsel function as a key feature of Parsel.
- This is an optional flag that prompts a language model to generate Parsel code corresponding to any additional subfunctions necessary when Parsel fails to implement a function.
- These proposed subfunctions are then added as child nodes to the decomposed function node.
- However, an additional consequence is that Parsel can thus be used to recursively decompose tasks into steps, by repeatedly identifying descriptions that cannot be directly implemented and attempting to decompose them.

## Experiments

### APPS Dataset
- We anticipate that there are many programs that LLMs can implement by first generating Parsel code.
- But, as Parsel is a new framework, while language models can sometimes generate Parsel programs with few-shot prompts, it is not a syntax they have previously encountered.
- Thus, we may want to use existing code in other languages to construct datasets of Parsel programs from other languages.
- This requires us to first extract the call graph from the code, generate descriptions for each of the functions, and then generate Parsel programs from the graph.
- This call graph representation is broadly convenient, so it is useful to have a bidirectional method to produce a graph from Parsel code and to produce Parsel code from the graph.
- The APPS dataset is a collection of coding challenges (in long-form text) with solutions in Python (Hendrycks et al., 2021).
- We filter the dataset to problems with starter code (providing the name of the evaluated function) and unit tests (provided as input-output pairs).
- For those tasks, we select solutions that define and call at least three functions, with at least one over 4 lines long and none over 15 lines.
- As a proof of concept, we show 10 Parsel solutions which we could automatically generate from the APPS solutions.
- We generated the descriptions by prompting Codex to explain each function and its inputs and outputs.
- From this, we use backtranslation to attempt to implement these solutions in Python.
- We then verify that they are correct by applying the original unit tests as constraints on the root function.
- As mentioned in Section 1, the Parsel code is substantially shorter in terms of lines of code.
- We also explored whether a code language model, given examples of Parsel code, a problem statement, and that problem's asserts, could generate and implement new and complex solutions to APPS problems.
- We include one Parsel example in Figure 4 (with the resulting Python in the appendix), which returns the current leader from a chess board as either "Draw", "White", or "Black".

### Case Studies
- The function we include is exactly 58 lines of code, including 17 lines of comments.
- The corresponding Python code is exactly 58 lines of code, including 17 lines of comments.
- It is not necessary to provide any of the function descriptions besides the top one.
- The model is able to understand that game_of_life_inversion_iteration can be broken down into invert_array and game_of_life_iteration.

### Theorem Proving in Lean
- We can generate proofs in formal theorem-proving languages such as Lean
- The ability to run Lean on proofs with no errors/warnings indicates the proof is correct
- Each function in a Lean Parsel proof has an "implicit constraint."

### Robotic Planning in VirtualHome
- We use Parsel to generate a python program that can generate action plans in natural language similar to ones used in (Huang et al., 2022).
- In each specified constraint, the produced natural language action plan is translated to formal VirtualHome instructions with minimal regex matching and tested executability.
- If the instructions can be successfully executed, they are considered valid -however, one could describe object-relational constraints on the state of the world after instructions execution.

## Related Works

### Step by Step Problem Solving with LMs
- Large body of work shows that step-by-step reasoning benefits LLM performance
- Step-by-step reasoning can be improved with further guidance and tool use
- One encouraging prior work is Acquaviva et al. (2022)

### Program Synthesis
- Program synthesis is the long-standing challenge of generating programs from high-level specifications
- Recently, library learning has shown a way to make progress: even complex programs can be short in terms of the right high-level library
- In turn, this library can be progressively induced from solutions to simpler synthesis problems
- This idea is embodied in DreamCoder (Ellis et al., 2021;Bowers et al., 2022)

### LMs for Formal Environment Multi-step Planning
- Several existing works can be expressed in Parsel.
- In both cases, the generated language corresponds directly to pre-implemented low-level robotic game_of_life_inversion_iteration(array_at_time_t): Takes a board and returns the next iteration of → the game of life, but with all values flipped game_of_life_iteration(array_at_time_t) -> array_at_time_t_plus_1: Takes a board with active and → inactive cells as a list of lists and returns the next iteration of the game of life
- In Parsel, the ability to express high-level tasks in a language that is easy to read and write is a major advantage.
- Jiang et al. (2022) proposed a framework to generate formal proofs in formal theorem-proving languages from informal proofs by first generating an intermediate natural language proof sketch. This could be expressed in Parsel by generating each sketch step as a function and then using formal verification for each lemma as the Parsel validation step.

### Testing Code Language Model Outputs
- Related works have explored the capacity of assert statements to constrain the generation space of LLMs for code on individual functions.
- In particular, Merrill et al. (2021) proves essential constraints on what can be learned from assertions alone, and more crucially, what cannot.

## Implications
- Parsel is a natural language compiler framework that bridges the gap between natural language and programming language by allowing programmers to write high-level algorithmic designs in natural language and automatically compiling them into valid code
- This has potential benefits for programmers, students, and code language models
- For Programmers, 6.1.1. CURRENT LIMITATIONS First, programming generation language models like Codex continue to be constrained primarily to individual functions, rarely exceeding a few dozen lines in practice (Chen et al., 2021;Tabachnyk & Nikolov, 2022). This is still a dramatic shift from foundational earlier works, which focused on the association between one line of natural language pseudocode with one line of code (Kulal et al., 2019) or a line of text to a StackOverflow snippet (Yin et al., 2018). Yet, these models perform worse the more unusual the desired functions are, and recent research suggests that people using these language models are more likely to introduce buggy code (Perry et al., 2022), although this is not yet conclusive (Sandoval et al., 2022).
- Parsel allows code language models to stay closer to natural language when generating code, which corresponds more closely to their primary source of training data. Moreover, it allows complex but standard methods to be described concisely, requiring fewer tokens to generate.
- One exciting additional benefit is the potential to generate solutions recursively: if the Parsel compiler is unable to find a solution for a set of functions, it should be possible to prompt the model to define new helper functions.
- In fact, we find that often the model attempts to reference undefined auxiliary functions when defining complex functions (e.g. "count living neighbors(grid, i, j)" in Conway's game of life), and as a result support an optional argument where the model can attempt to resolve NameErrors automatically by attempting to implement functions.

### For Students
- Traditional programming languages are less robust to slight variations in wording
- Traditional programming languages require many tokens for syntactic details and in some cases, may take many lines to express what can be expressed far more simply in language
- LLMs have shown remarkable algorithmic generalization ability

## Limitations
- Code language models are limited in their ability to generate correct code for functions with complex dependencies or without constraints
- The current implementation of Parsel may struggle to generate correct code for these types of functions
- Code language models are also limited in their ability to generate code for languages that were underrepresented in their training data
- However, some large language models can adapt and learn new languages in context, allowing them to generate code in languages not in their training data
- The current Parsel implementation has shown promising results in generating correct code for a variety of functions and languages

## Conclusion and Future Work
- Parsel provides a language for robust code generation without the need to evaluate the underlying code
- Parsel allows the teaching of algorithmic reasoning with less emphasis on syntax and more emphasis on problem-solving, similarly to a mathematics curriculum
- In the future, we also hope to integrate automatic unit test generation
- One method would be to identify edge cases and check whether the set of functions that successfully solve all existing tests disagree on any new tests. This could permit automatic decomposition without exponential growth in implementation combinations.
- We aim to support more general reward functions for function implementations where multiple may be valid but we rank implementations based on a desired feature. These "soft" constraints may also allow new Parsel uses, e.g. planning stories in natural language
