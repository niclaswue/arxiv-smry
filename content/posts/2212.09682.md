---
title: "Multilingual Sequence-to-Sequence Models for Hebrew NLP"
date: 2022-12-19T18:10:23.000Z
author: "Matan Eyal, Hila Noga, Roee Aharoni, Idan Szpektor, Reut Tsarfaty"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09682v1.webp" # image path/url
    alt: "Multilingual Sequence-to-Sequence Models for Hebrew NLP" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09682)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09682).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multilingual-sequence-to-sequence-models-for).

# Abstract
- Large language models (LMs) with increased model size and large quantities of pretraining data are responsible for recent progress in NLP
- Pretrained Hebrew LMs focused on encoder-only models are under-parameterized and under-trained
- Sequence-to-sequence generative architectures are more suitable for LLMs in the case of morphologically rich languages (MRLs) such as Hebrew
- Casting tasks in the Hebrew NLP pipeline as text-to-text tasks can leverage powerful multilingual, pretrained sequence-to-sequence models as mT5, eliminating the need for a specialized, morpheme-based, separately fine-tuned decoder

# Paper Content

## Introduction
- Large pretrained language models have been successful in a variety of tasks
- Hebrew language models have been proposed, but they are under-parameterized and have limited training data
- A new large multilingual sequence-to-sequence model, mT5, can be used to adapt tasks to its text-only paradigm

## Modeling
- We use mT5 (Xue et al., 2021), a multilingual generative text-to-text version of T5 (Raffel et al., 2020), trained simultaneously on 101 languages.
- We evaluate mT5 on all its available sizes -Small, Base, Large, XL and XXL ranging from 300M to 13B parameters.
- Subsequently, we propose casting all Hebrew NLP tasks for which evaluation benchmarks exist as text-to-text tasks: The input text is fed into the model and targets are produced in a generative manner.

## Experiments
- Goal: To assess the performance of a sequence-to-sequence large language model, specifically mT5, that was trained on a large quantity of multilingual data, compared to existing Hebrew language models, in order to showcase the efficacy of larger, well trained and thoughtfully designed models on Hebrew tasks.
- The goal of the study was to compare the performance of mT5, a sequence-to-sequence large language model, against YAP, the only available Hebrew language model, on a variety of tasks.
- For evaluation, the best-performing checkpoint from the development set was used.
- Results were compared against YAP, which is the only available Hebrew language model.
- mT5 performed better than YAP on all tasks, with the exception of the lemmatization task.

### Tasks
- We assembled an evaluation suite of Hebrew benchmarks composed of the following tasks: QA (Keren and Levy, 2021), NER (Bareket and Tsarfaty, 2021;Mordecai and Elhadad, 2005), Sentiment Analysis (Amram et al., 2018), and the morpho-syntactic tasks of segmentation, POS tagging and lemmatization from Sade et al. (2018).
- Keren and Levy (2021) introduced ParaShoot, an Hebrew Question-Answering dataset which was created using the format and crowdsourcing methodology of SQuAD (Rajpurkar et al., 2016).
- We report token-level F1 and Exact Match scores as no morpheme boundaries are available.
- The input is constructed by concatenating the context and question, with the output being the answer.
- We conducted manual evaluation of different mT5 models on this dataset to evaluate the impact of model sizes, see details in B. Bareket and Tsarfaty (2021).
- NEMO, an NER annotation for the Hebrew UD corpus (Sade et al., 2018) was created.
- As entities in Hebrew can correspond to only a subset of a word's morphemes (See "habayit halavan" example in Sec. 1), the authors proposed two dataset versions: token-level, where entities correspond to white-space boundaries, similarly to BMC (Mordecai and Elhadad, 2005), and morpheme-level, with morpheme-based boundaries.
- The authors additionally revised the common NER evaluation procedure by comparing predicted and target entities on the surface form, boundaries, and entity types, but not positions.
- Thus, we train the sequence-to-sequence model to simply generate all of the sentence entities and tags one after the other in the input text.
- Correspondingly with previous work, we report F1 scores for Amram et al. (2018), a sentiment analysis dataset curated by annotating Facebook user comments with positive/negative/neutral tags.

## Results
- Our findings demonstrate a marked improvement over previously published results on existing Hebrew NLP benchmarks
- mT5 produces the biggest performance boost for the Question-Answering task of ParaShoot, with mT5-base already surpassing baseline models and mT5-XXL outperforming AlephBERT by 27.9 F1 points
- For NER, mT5 produces better results than evaluated baselines on both datasets
- The largest performance boost comes in the morpheme-level NEMO version where mT5 learns to segment and label entities in an end-to-end fashion
- For sentiment analysis, mT5 outperforms the baseline models by a small fraction, however, manual analysis we performed on mT5's best performing model show that 34% of its errors are annotation errors and for further 30% our annotators we not able to decide what is the correct label
- We con-clude that we should work towards a cleaner, more challenging sentiment analysis dataset in Hebrew

## Related work

## Conclusions
- All Hebrew LMs to date are encoder-only models, which could not directly generate morpheme sequences, and thus necessitate a specially-tuned decoder.
- In this work we propose to take advantage of mT5, a publicly available multilingual large language model that was trained on a considerable amount of multilingual and Hebrew data.
- Additionally the generative approach of text-to-text modeling is more aligned with the morphological challenges inherent in Hebrew and by that dispense the need for specially tuned decoders.
- We fine-tuned and evaluated mT5 on a set of Hebrew downstream tasks and report that mT5 outperforms all previous baselines.
- Subsequently, we propose that multilingual sequence-to-sequence models should be used for Hebrew and other MRLs as an alternative for their smaller encoder-only counterparts.
- mT5, comparing to previous Hebrew LMs, is bigger, pretrained on more multiligual data, and learning to segment and tag in an end-to-end manner.
- While it was beyond the scope of this paper to pretrain new LMs and study which factors contributed to the improved performance, identifying these factors will be useful for determining the most effective approach for future work.
- While larger mT5 models perform better than available LMs, they require more powerful hardware accelerators and take longer to train and infer.
- Future research should aim to achieve similar levels of performance with smaller mT5 models.
- Additionally, the inclusion of data from 101 languages in the training of mT5 may have negatively impacted its performance on Hebrew, as some of the data may not have been relevant or beneficial to this particular language.
- Future work will need to address this issue by training a monolingual Hebrew LM in order to further improve performance for Hebrew.
