---
title: "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts"
date: 2022-12-20T18:50:00.000Z
author: "Skyler Hallinan, Alisa Liu, Yejin Choi, Maarten Sap"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10543v1.webp" # image path/url
    alt: "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10543)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10543).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/detoxifying-text-with-marco-controllable).

# Abstract
- Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning
- However, subtle toxicity remains challenging to tackle
- We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs)
- MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace
- We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo's rewrites are preferred 2.1 $\times$ more in human evaluation
- Its applicability to instances of subtle toxicity is especially promising.

# Paper Content

## Introduction
- Toxic language can cause online and offline harms
- Toxic language is challenging for NLP systems to detect and account for
- Text detoxification, i.e., rewriting text to be less toxic while preserving non-toxic meaning, provides a promising solution

## Background: Text Detoxification
- Text detoxification is a form of stylistic rewriting that aims to produce a non-toxic rewrite given a toxic input sentence.
- This task is challenging, as it requires both detoxification and preservation of non-toxic meaning, in contrast to controllable text generation, which aims to simply generate any non-toxic continuation for a prompt.
- Due to a lack of supervision with parallel data, an often effective approach to stylistic rewriting relies on unsupervised masking-and-reconstructing approaches.
- In this paradigm, source style-specific tokens/spans in the input text are detected and masked, then filled in with tokens/spans from the target-style using a masked language model.
- Other work has framed detoxification as a translation or paraphrasing task, using a classifier to steer away from toxic content.
- Our method for unsupervised controlled revision is based on denoising autoencoder LMs (AE-LMs), which are trained to mask and reconstruct sequences of text.
- Our setup consists of a base pretrained AE-LM G, an expert AE-LM G + finetuned on data with desirable attributes, and an anti-expert AE-LM G − finetuned on data with undesirable attributes.
- We use BART-base (Lewis et al., 2020) as our base autoencoder. We finetune the expert and antiexpert using 1M non-toxic and 100K overtly toxic comments from the Jigsaw corpus (Do, 2019), as done in Liu et al. (2021) and Dale et al. (2021).
- BART can infill multiple or no tokens even if only one token is masked, allowing for more flexible mask infilling.

### Contextual Masking

### Contextual Replacing
- After masking potentially toxic locations, MARCO replaces them with more benign tokens
- The KL divergence is defined as they are indeed toxic
- To autoregressively produce a rewrite g given the original and masked sentences w and w m , we transform the DEXPERTS (Liu et al., 2021) framework
- We obtain the next-token unnormalized logprobabilities (i.e., logits) z i , z + i , and z − i from the base and expert AE-LMs G, G + , and G − , respectively, conditioned on the previously generated tokens g <i , the original sequence w, and the masked variant w m .
- We then ensemble those logits into a modified next-token probability distribution for X i a random variable over the vocabulary V, which represents the next token at index i given the previous generation, g <i : 4
- In our method, the expert and anti-expert use the masked sequence w m as their input, while the base model uses the unmasked w.

## Detoxification Experiments & Results
- The computer science paper is about rewriting sentences from three toxicity datasets.
- The computer science paper uses both automatic and human evaluations to measure MARCO's performance at detoxifying text.

### Datasets

### Baselines
- MARCO is a new approach to automated toxic text recognition that outperforms the two baseline approaches from Dale et al. (2021).
- ParaGeDi uses a class-conditioned language model (using control codes for toxic and non-toxic styles) on top of a paraphrasing language model to steer generated text towards a specific attribute.
- CondBERT follows a pointwise editing setup, first identifying tokens to mask in the input, then using a mask-filling model to replace them.

### Evaluation Setup
- We perform automatic and human evaluations
- Original because ain't nobody got time to pick all that cotton.
- CondBERT because ain't nobody got time to pick all that cotton.
- ParaGeDi because nobody has time to pick up all the cotton.
- MARCO: because ain't nobody got time to pick all that up.
- Table 2: Different rewriting methods on a toxic example from SBF (containing a racist slavery reference to cotton picking).
- MARCO detects and masks "cotton" as a toxicity indicator, which baselines fail to rewrite.
- We assess the quality of the models' rewrites with automatic metrics used in previous work (Liu et al., 2021;Ma et al., 2020).
- We report the average toxicity score of rewrites using the PerspectiveAPI.
- Additionally, we measure fluency of rewrites by computing their perplexity with an external LM (GPT-2 XL; Radford et al., 2019), and meaning similarity between the input and the rewrite using BERTScore (Zhang et al., 2019).
- See Appendix B.3 for further details.
- We conduct a head-to-head human evaluation (Kiritchenko and Mohammad, 2017) of the toxicity of the rewrites using Amazon Mechanical Turk.
- For each dataset's validation and test sets, we sample 75 prompts each, then compare each pair of MARCO, ParaGeDi and CondBERT's generations against each other and ask which one is less toxic.
- We use three workers per rewrite pair.
- We use Cohen's kappa to measure inter-annotator agreement for each head-to-head evaluation.

### Results
- Automatic metrics show that MARCO is better at detoxification than baselines across all datasets and splits by 10.3% on average.
- Human evaluations corroborate this, as MARCO is on average rated as less toxic than CondBERT 2.2 times more often than vice versa across datasets and splits, and 1.9 times more often vs. ParaGeDi.
- BERTScore does not measure meaning preservation of only non-toxic content; removing toxic meaning by definition requires trade-offs between fluency, style accuracy, and meaning preservation.
- Compared to DynaHate, MARCO's margin of winning is even larger on MAgr and SBF, which contain more subtle toxicity.

## Conclusion
- MARCO is a novel method for text detoxification that uses auto-encoder language model experts in a mask and reconstruct process.
- Our method outperforms strong baselines in automatic and human evaluations, showing strong ability to detoxify even subtle biases.
- MARCO's success demonstrates the effectiveness of controllable generation mixed with text rewriting methods for controllable revision, and highlights the usefulness of using LMs for capturing toxicity.
- Despite the promising performance of MARCO at detoxifying text, there are several limitations, ethical considerations, and broader impacts of our approach, which we list below.
- First, in this work, we seek to detoxify sentences. However, toxicity itself is a subjective and sensitive concept with large potential downstream impacts caused by annotator and subsequent model biases.
- We somewhat mitigate this variation by selecting human evaluators that scored highly on a toxicity qualification task (see Appendix D), in line with a prescriptive paradigm of toxicity annotation.
- Future work could investigate the effect of demographics on preference for different rewriting algorithms, e.g., in a more descriptive paradigm.
- In addition, achieving meaningful semantic preservation in detoxification is challenging. Specifically, it is difficult to disentangle the toxic and non-toxic meanings from the input, making it challenging to generate detoxified rewrites with high preservation of only the non-toxic content; this may risk minimizing marginalized groups' speech.
- Partially, this could be due to a lack of context incorporation (social, conversational, preceding sentences); future work should consider adapting detoxification methods in context.
- MARCO also requires finetuning two pretrained LMs, which is not computationally insignificant.
- Future work could explore using smaller LMs to control a larger model (Liu et al., 2021), or even more lightweight approaches.
- Additionally, we acknowledge that in the evaluation, we expose Turkers to toxic content, which might harm individuals, especially those with identities that the offensive content applies to.
- However, we pay a fair wage (US$8/h) and our work is approved by our institution's ethics review board (IRB).
- See Appendix D for further details.
- Another major ethical implication of our work is that, following previous work, we use the Perspective API to automatically assess toxicity, a classifier which contains documented biases.
- Future research could consider different, more holistic views of toxicity and biases.
- Finally, although our application in this paper is detoxification, we acknowledge that MARCO could be applied for the opposite purpose, ie., generation of toxic text from non-toxic text; this is a malicious application which we condemn.
