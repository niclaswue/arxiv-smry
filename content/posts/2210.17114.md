---
title: "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM"
date: 2022-10-31T07:42:52.000Z
author: "Shira Guskin, Moshe Wasserblat, Chang Wang, Haihao Shen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-17114v2.webp" # image path/url
    alt: "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.17114)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.17114).


# Abstract
- Limited computational budgets often prevent transformers from being used in production
- A knowledge distillation approach addresses the computational efficiency by self-distilling BERT into a smaller transformer representation having fewer layers and smaller internal embedding
- However, the performance of these models drops as we reduce the number of layers, notably in advanced NLP tasks such as span question answering
- In addition, a separate model must be trained for each inference scenario with its distinct computational budget
- Dynamic-TinyBERT tackles both limitations by partially implementing the Length Adaptive Transformer (LAT) technique onto TinyBERT, achieving x3 speedup over BERT-base with minimal accuracy loss
- QuaLA-MiniLM is trained only once, dynamically fits any inference scenario, and achieves an accuracy-efficiency trade-off superior to any other efficient approaches per any computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with <1% accuracy loss).

# Paper Content

## Introduction
- transformers such as BERT, RoBERTa, and GPT-3 are inefficient and require massive computational resources and large amounts of data as basic requirements for training and deploying
- knowledge distillation, in which the knowledge of a large model defined as the teacher is transferred into a smaller more efficient model defined as the student, is an effective method for improving efficiency
- TinyBERT and MiniLM are two methods that achieve superior accuracy-speed-size tradeoffs, both introducing a novel distillation method specially designed for transformers
- LAT reduces the sequence length at each layer, finally bringing back the dropped tokens in the last layer to allow for a wide range of NLP tasks despite the dropping of tokens
- Dynamic-TinyBERT utilizes TinyBERT distillation and some LAT techniques (Drop-and-Restore inference and evolutionary-search) to train an efficient model that can be used for a wide range of NLP tasks with optimal performance per any computational budget

## Method
- QuaLA-MiniLM is generated by applying the following optimization techniques on top of each other: MiniLM distillation, Length Adaptive Transformer, and Quantization.
- Figure 1 demonstrates the training pipeline.
- Table 1: Inference performance on the SQuAD1.1 evaluation dataset.
- For all the length-adaptive (LA) models we show the performance both of running the model without token-dropping, and of running the model in a token-dropping configuration according to the optimal length configuration found to meet our accuracy constraint.
- MiniLM using multi-head self-attention distillation on a general-domain's data; second, fine-tuning it to a specific downstream task using standard supervised fine-tuning on the task dataset.

### LAT (Length Adaptive Transformer)
- After fine-tuning a transformer to a specific task, the model is trained with Drop-and-Restore and LayerDrop in a process called
- In Drop-and-Restore, tokens are dropped at a random rate at each layer, and are brought back in the last hidden layer to enable a wider range of tasks and to make the model robust to the choice of length configuration at inference time.
- A length configuration is a sequence of retention parameters (l 1 , • • • l L ) each of which corresponds to the number of word vectors that are kept at each layer.
- Drop-and-Restore training is done using inplace distillation and sandwich rule methods, as follows: the full model without LengthDrop is fine-tuned for the downstream task as usual by minimizing the supervised loss function, while simultaneously, randomly-sampled sub-models with length reduction (sandwiches) learn to mimic the predictions of the full model using knowledge distillation.
- As proposed by LAT, we run a multi-objective evolutionary search [3,14] to optimize the accuracy-efficiency trade-off per each computational budget without additional training.
- The algorithm finds the optimal length configurations per possible computational constraints by generating an initial population of length configurations, and evolving the population at each iteration by mutation and crossover to consist only of configurations that lie on a newly updated accuracy-efficiency Pareto frontier. This process repeats for many iterations until best tradeoff is found.

### Quantization
- We use the traditional post-training quantization [8,15] to quantize the model.
- Quantization for neural network is the process of approximating used floating-point numbers r by low bit width numbers q by the following mapping: r=S(q-Z) where S, Z represents the scale-factor and the zero-point values.
- Post-training quantization requires calibration of samples from representative datasets and collection of tensor statistics such as min and max values to determine the scale-factor and zero point values.
- Typically, we expect an INT8 8-bit model to gain more instruction throughput over the FP32 model (e.g., x4 for Intel DL Boost) and to gain about x4 lower memory bandwidth over the FP32 model, and therefore deliver higher inference efficiency.

## Experiments Setup

## Results

### Accuracy-efficiency trade-off
- Figure 2 shows the significant boost to inference performance achieved by our approach.
- Both LA-MiniLM and QuaLA-MiniLM outperforms BERT-base, Dynamic-TinyBERT, TinyBERT, and DistilBERT models in terms of both accuracy and efficiency, with up to x14 reduction of FLOPs and less than 1% accuracy drop vs. BERT-base.
- QuaLA-MiniLM exhibits lower accuracy than LA-MiniLM with the same FLOPs count (since compression of weights from 32-bit to 8-bit does not affect the number of floating/int operations), but runs x2 faster due to its x4 instruction throughput gain (see the next section for a speedup gain analysis).

### Inference speedup
- The QuaLA-MiniLM model is x5 smaller than BERT-base, x3 smaller than TinyBERT, and runs about x8.8 faster than BERT-base, x5 than TinyBERT and x2 than quantized-TinyBERT
- LA-MiniLM configured with the optimal length configuration achieves x1.5 inference speedup over the LA-MiniLM model without token-dropping

## Conclusions and future work
- QuaLA-MiniLM is a sequence length reduction technique that enhances MiniLM inference performance
- QuaLA-MiniLM can be configured to have an optimal accuracy-efficiency tradeoff per a specific computational budget
- QuaLA-MiniLM was found to be more accurate and efficient than previous work on BERT compression on the SQuAD1.1 benchmark dataset
