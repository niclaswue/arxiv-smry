---
title: "Brauer's Group Equivariant Neural Networks"
date: 2022-12-16T18:08:51.000Z
author: "Edward Pearce-Crump"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08630v1.webp" # image path/url
    alt: "Brauer's Group Equivariant Neural Networks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08630)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08630).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/brauer-s-group-equivariant-neural-networks).

# Abstract
- We provide a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$
- In particular, we find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$ when the group is $O(n)$ or $SO(n)$, and in the symplectic basis of $\mathbb{R}^{n}$ when the group is $Sp(n)$
- The neural networks that we characterise are simple to implement since our method circumvents the typical requirement when building group equivariant neural networks of having to decompose the tensor power spaces of $\mathbb{R}^{n}$ into irreducible representations

# Paper Content

## Introduction
- Finding neural network architectures that are equivariant to a symmetry group has been an active area of research
- Unlike with multilayer perceptrons, however, the requirement for the overall network to be equivariant to the symmetry group typically restricts the form of the neural network itself
- Moreover, since these networks often exhibit parameter sharing within each layer, there are typically far fewer parameters appearing in these networks than in multilayer perceptrons
- This often results in simpler, more interpretable models that generalise better to unseen data
- These symmetry groups typically appear in problems coming from physics, where the data that arises from a physical process often comes with a certain type of symmetry that is baked into the data itself
- Consequently, it is important to be able to construct neural networks that can learn efficiently from such data
- The typical approach for constructing equivariant neural networks for many symmetry groups has involved decomposing tensor product representations of the symmetry group in question into irreducible representations
- For example, neural networks that are equivariant to the special orthogonal group SO(3) [21], the special Euclidean group SE (3) [31], and the proper orthochronous Lorentz group SO + (1, 3) [7] all used irreducible decompositions and the resulting change of basis transformations into Fourier space to implement these neural networks
- However, for most groups, finding this irreducible decomposition is a hard problem, and, even if such a decomposition can be found, the neural networks that are constructed are often inefficient since forward and backward Fourier transforms are typically required to perform the calculations, which come with a high computational cost
- In this paper, we take an entirely different approach, one which results in a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of R n for the following three symmetry groups: O(n), the orthogonal group; SO(n), the special orthogonal group; and Sp(n), the symplectic group
- In doing so, we avoid having to calculate any irreducible decompositions for the tensor power spaces, and therefore avoid having to perform any Fourier transforms to change the basis of the spaces
- Our approach is motivated by a mathematical concept that, to the best of our knowledge, has not appeared in any of the machine learning literature to date, other than in our recent paper [26]
- The main contributions of this paper are as follows:
- 1. We are the first to show how the combinatorics underlying the Brauer and Brauer-Grood vector spaces, adapted from the Schur-Weyl dualities established by Brauer [8], provides the theoretical background for constructing group equivariant neural networks for the orthogonal, special orthogonal, and symplectic groups when the layers are some tensor power of R n
- 2. We find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of R n when the group is O(n) or SO(n), and in the symplectic basis of R n when the group is Sp(n)
- 3. The neural networks that we characterise are simple to implement since we avoid having to consider the decomposition of the tensor power spaces of R n into irreducible representations for each of the three groups in question
- 4. We generalise our diagrammatical approach to show how to construct neural networks that are equivariant to local symmetries
- 5. We suggest that Schur-Weyl duality is a powerful mathematical concept that could be used to characterise other group equivariant neural networks beyond those considered in this paper

## Preliminaries
- We choose our field of scalars to be R throughout.
- Tensor products are also taken over R, unless otherwise stated.
- Also, we let [n] represent the set {1, . . . , n}.
- Recall that a representation of a group G is a choice of vector space V over R and a group homomorphism.
- We choose to focus on finite-dimensional vector spaces V that are some tensor power of R n in this paper.
- We often abuse our terminology by calling V a representation of G, even though the representation is technically the homomorphism ρ.
- When the homomorphism ρ needs to be emphasised alongside its vector space V , we will use the notation (V, ρ).

## Group Equivariant Neural Networks
- Group equivariant neural networks are constructed by alternately composing linear and non-linear G-equivariant maps between representations of a group.
- We define G-equivariance below: Definition 3.1. Suppose that (V, ρ V ) and (W, ρ W ) are two representations of a group G. The set of all linear G-equivariant maps between V and W is denoted by Hom G (V, W ).
- When V = W , we write this set as End G (V ).
- It can be shown that Hom G (V, W ) is a vector space over R, and that End G (V ) is an algebra over R.
- A special case of G-equivariance is G-invariance: Definition 3.2. The map φ given in Definition 3.1 is said to be G-invariant if ρ W is defined to be the 1-dimensional trivial representation of G.
- As a result, W = R.
- We can now define the type of neural network that is the focus of this paper: Definition 3.3. An L-layer G-equivariant neural network f NN is a...

## Adding Features and Biases
- The assumption that the feature dimension for all of the layers appearing in the neural network was one was simplified for the results seen in Section 6.
- All of the results seen in Section 6 can be adapted for the case where the feature dimension of the layers is greater than 1.
- To include bias terms in the layer functions of a G-equivariant neural network, it is harder but can be done.
- To redefine each layer function appearing in Definition 3.3 as a composition, where φ l and σ l are as before, but now φ l and σ l are still G-equivariant for all g ∈ G and c ∈ (R n ) ⊗l .
- To find the matrix form of c, all we need to do is to find a spanning set for Hom G (R, (R n ) ⊗l ). But this is simply a matter of applying Theorem 6.5 for G = O(n), Theorem 6.6 for G = Sp(n), with n = 2m, and Theorem 6.7 for G = SO(n), setting k = 0.

## Equivariance to Local Symmetries
- We can extend our results to looking at linear layer functions that are equivariant to a direct product of groups; that is, we can construct neural networks that are equivariant to local symmetries.
- Specifically, we wish to find a spanning set for where is the external tensor product.
- The Hom-space given in (8.1) is isomorphic to Hom G(nr) ((R nr ) ⊗kr , (R nr ) ⊗lr )
- Consequently, we can construct a surjection of vector spaces p r=1 Θ lr kr,nr : where As a result, the spanning set can be found by placing each possible basis diagram for each of the appropriate vector spaces side by side and taking the image of each under its appropriate map, and then calculating the Kronecker product of the resulting matrices to obtain the spanning set for (8.1).

## Related Work
- Brauer developed the combinatorial representation theory of the Brauer algebra for the purpose of understanding the centraliser algebras of the groups O(n), SO(n) and Sp(n) acting on End((R n ) ⊗k ).
- Brown published two papers [9], [10] in the 1950s on the Brauer algebra, written as B k k (n) in our notation, showing that it is semisimple if and only if n ≥ k − 1.
- Weyl [33] had previously shown that the Brauer algebra was semisimple if and only if n ≥ 2k.
- After Brown's papers, the Brauer algebra was largely forgotten about until 1985 when Hanlon and Wales [19] provided an isomorphism between two versions of the Brauer algebra -these versions share a common basis but have different algebra products defined on them.
- Grood [16] investigated the representation theory of what we have called the Brauer-Grood algebra, written as D k k (n) in our notation, as Brauer's paper only contained half a page on the algebra and nothing more had been published on it since its publication.
- Lehrer and Zhang [22] studied the kernel of the surjection of algebras given in Theorems 6.5 and 6.6 when l = k, showing that, in each case, it is a two-sided ideal generated by a single element of the Brauer algebra.
- With regard to the machine learning literature, Maron et al.'s 2018 paper [24] is the nearest to ours in terms of how it motivated our research idea. They looked to characterise all of the learnable, linear, equivariant layer functions when the layers are some tensor power of R n for the symmetric group. However, as we show in our paper [26], their characterisation is not entirely correct, since they conclude, incorrectly, that the dimension of the layer functions is independent of n.
- In 2021, Finzi et al. [14] were the first to recognise Maron et al.'s error, noting this in the appendix of their paper. However, they were only able to calculate the correct dimensions for a few values of k and n as their approach uses a numerical algorithm which runs out of memory on higher values of k and n.
- In our paper, we provide an analytic solution for these dimensions, for any k and n, fully correcting Maron et al.. Finzi et al.'s numerical algorithm also enabled them to find a basis for the learnable, layer, equivariant functions for the groups that are the focus of our study, namely O(n), Sp(n) and SO(n), but only for small values of n, k and l, since their algorithm once again ran out of memory on higher values.
- Whilst we have not found a basis in all cases, we have provided a spanning set and an analytic solution for all values of n, k and l, which will make it possible to implement group equivariant neural networks for any such values of n, k and l for the three groups in question.

## Conclusion
