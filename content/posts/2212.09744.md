---
title: "DSI++: Updating Transformer Memory with New Documents"
date: 2022-12-19T18:59:34.000Z
author: "Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jinfeng Rao, Marc Najork, Emma Strubell, Donald Metzler"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09744v1.webp" # image path/url
    alt: "DSI++: Updating Transformer Memory with New Documents" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09744)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09744).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/dsi-updating-transformer-memory-with-new).

# Abstract

# Paper Content

## INTRODUCTION

## PROBLEM SETUP
- We focus on a setup where we receive an initial corpus of documents, and user queries corresponding to a subset of them, R = {< q, j >, ∀j ∈ YD }, where D ⊂ D.
- DSI paradigm involves two tasks: (i) memorization task where the goal is to learn an indexer f θ : X → Y, a text-to-text model like T5 (Raffel et al., 2020) parameterized by θ ∈ R, that takes document tokens (x ∈ X) as input and maps it to a document identifier (docid) j ∈ Y, and (ii) retrieval task where the goal is to use the same indexer f θ to directly map a user query q to a relevant docid j ∈ Y.
- Following Tay et al. (2022), two different prompts are used to differentiate between these tasks. Tay et al. (2022) discuss several variants for representing docids -unstructured atomic and structured string docids, where each document is assigned a unique token and tokenized string, respectively.
- Under the unified text-to-text format, both of the above tasks are cast as generation tasks, i.e., decoding one unique token (unstructured atomic) or decoding a tokenized string sequentially, one token at a time (naively/ semantically structured).
- In the dynamic corpus scenario, we simulate the arrival of new documents by updating the initial corpus Dwith a sequence of batches D, D, ..., with Darriving first, followed by D, and so on.
- In DSI++, we have access to the new batch of documents D, but we do not have any queries related to these documents. Goal: Learn a DSI++ system that incrementally indexes D, D, • • • in f θ while being able to answer queries related to previously as well as additionally indexed documents.

## BENCHMARK FOR DSI++.
- To enable research on DSI++, we introduce two benchmarks constructed from the Natural Questions (NQ; Kwiatkowski et al. (2019)) and MS MARCO (Nguyen et al., 2016) datasets.
- The NQ dataset consists of Wikipedia articles and corresponding natural language questions. Similar to Tay et al. (2022), we consider Wikipedia articles for memorization and the retrieval task as identifying the Wikipedia article that answers the given question.
- We use the original NQ train split to construct train(80%)/ validation(20%) splits and use NQ validation as a test split for our setup.
- We randomly sample 50K unique articles to constitute the initial D 0 corpus. Next, we construct five corpora (D 1 , • • • , D 5 ), each containing 10K unique articles, to sequentially add them to the DSI model. Corresponding to articles in each of these corpora, we filter queries from original NQ train/ validation splits to construct
- We use R 0 to train the DSI model for the retrieval task, and use R test i to evaluate on previously and newly indexed articles.
- The full MS MARCO dataset has approx. 500K passage-query training pairs and 6, 980 validation pairs. Similar to the benchmark created from the NQ dataset, we randomly sample 50K unique passages to constitute the initial D 0 corpus, and five more corpora, each with 10K passages.

## EVALUATION METRICS
- Indexing accuracy and Hits@k are defined as the proportion of documents We use atomic docids by default and denote (N)/(S) for naively/ semantically structured docids. ↑ indicates higher is better, ↓ indicates lower is better.
- For DSI evaluation, we report indexing accuracy for memorization task and Hits@k (k ∈ {1, 10}) metric for retrieval task.
- Indexing accuracy and Hits@k are defined as the proportion of documents We use atomic docids by default and denote (N)/(S) for naively/ semantically structured docids. ↑ indicates higher is better, ↓ indicates lower is better.
- We observe that by increasing the model scale, the average A n and learning LA n performance improves. However, forgetting F n is severe across all model scales. Moreover, we observe that naively structured docids, T5-Base(N), underperform unstructured atomic docids, T5-Base, across all metrics -indexing accuracy, Hits@1, (see Figure 5 in Appendix A.2 for Hits@10 results).
- Imbuing the docid space with semantic (S) structure alleviates the forgetting compared to an arbitrary/ naive (N) structure.
- correctly memorized and the proportion of correct documents ranked in the top k predictions, respectively.
- Now, we formally define metrics to summarize the model performance as we incrementally index new documents.
- Let P n,o denote the performance (e.g., indexing accuracy) on corpus D o after training on corpus D n . Following prior works (Lopez-Paz & Ranzato, 2017;Riemer et al., 2019), we compute the average performance (A n ), forgetting (F n ) and learning performance (LA n ) metrics after indexing the corpus D n .
- The term F n (also known as backward transfer) refers to the effect of indexing the corpus D n on the performance of all previously indexed documents D o , where 0 ≤ o < n. LA n (or forward transfer) measures the model's ability to learn when presented with a new corpus D n , and is defined as the average performance over the new corpora D 1 , • • • , D n .
- When the D th n corpus is incrementally indexed, A n , F n , and LA n are defined as follows:
- Forgetting (or negative backward transfer). From Figure 2, we see that T5-Base model with atomic docid representation (blue line plots), undergo significant forgetting and this trend holds across all DSI evaluation metrics -indexing accuracy, Hits@1, and Hits@10 (see 5 in Appendix A.2).
- For the originally indexed D 0 corpus, indexing accuracy and Hits@1 drop by approx. 25 and 20 points, respectively. Further, as we continue indexing the sequence of D 1 , • • • , D 5 corpora, we see that forgetting becomes even more severe.
- For continual indexing the D 5 corpus, F 5 (forgetting) for indexing accuracy increases to 75. These results provide evidence to answer (Q1) & (Q2) that the DSI model undergoes severe forgetting under continual indexing of new documents.
- Forward transfer. To answer (Q3), we visualize the learning performance (LA n ) for all DSI metrics for sequential indexing. From Figure 2, we see LA n increases for indexing accuracy, suggesting that the DSI model is plastic enough to index new documents. However, from Figure 2, we see a declining trend for Hits@1. Due to the continuous indexing updates, the underlying DSI model drifts and becomes less effective for the retrieval task. These findings hint at an approach that replays indexing and retrieval tasks during continual learning (hence our proposed method in §4).

## Docid representations.
- For studying, we consider unstructured atomic, naively structured, and semantically structured docid representations.
- From Figure 2, we see that T5-Base underperforms T5-Base by a significant margin.
- For example, average performance A 0 for Hits@1 metric is approx. 30 and 39 for naive and atomic docids, respectively.
- Furthermore, as the naively structured approach treats unstructured docids as tokenizable strings as opposed to dedicated unique tokens in the case of atomic docids, they are relatively more prone to interference from new docids.
- Imbuing semantic structure to the naive docid space helps to reduce forgetting however still underperforms unstructured atomic docids.
- Model scale. As atomic docids are superior to naive docids, we only consider atomic docids for answering (Q5).
- From Figure 2, we observe that larger models outperform their smaller counterparts in terms of the average performance A n and the learning performance LA n (T5-XL > T5-Large > T5-Base).
- However, empirically we report that forgetting F n is severe across all model scales, without any clear best performer, and therefore, we focus on T5-Base for the rest of our experimentation.

## IMPLICIT FORGETTING DURING MEMORIZATION: SAM

## EXPLICIT FORGETTING: GENERATIVE MEMORY
- DSI paradigm consists of two tasks -memorization and retrieval.
- In the previous section, we showcase that SAM alleviates implicit forgetting by stably memorizing documents.
- In this section, we focus on the forgetting phenomenon that arises from the continual indexing of new documents, specifically in the context of the retrieval task.
- Through our systematic study (in §2.4), we show that irrespective of the model scale and docid representations, DSI models undergo severe forgetting.
- Moreover, we observe that the learning performance LA n keeps declining for the retrieval task (see Figures 2 and 5 for Hits@1 and Hits@10, respectively). This observation suggests that as we continuously update the DSI model with the indexing objective, the model starts to forget the retrieval task.
- In DSI, both memorization and retrieval tasks return docid for the given input, so by setup, we can assume access to the contents of the previous documents and continue indexing both old and new documents with the hope to reduce forgetting of the retrieval task.
- However, in Figure 4, we see that the model still undergoes forgetting (more discussion in §5.3).
- Episodic memory. According to the Complementary Learning Systems (CLS) (McClelland et al., 1995) theory, humans rely on an episodic memory -a module that stores past experiences to rehearse/ revisit them and thereby retain previously learned knowledge.
- Based on this motivation, memory-based approaches (Sodhani et al., 2022) for continual learning use a subset of previous task data to regularize the future task learning while minimizing forgetting.
- Experience Replay (ER) (Chaudhry et al., 2019b) is one such approach that samples previous task data (from episodic memory) to co-train with the current task. ER outperforms other sophisticated memory-based approaches like Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017), Averaged GEM (Chaudhry et al., 2019a), even with a sparse replay.
- Based upon this line of work, one approach for DSI++ is to retain ground-truth queries for the retrieval task in an episodic memory and use them to co-train with incremental indexing task.
- However, in DSI++, we do not have access to ground-truth queries for an incoming batch of new documents.
- Even if one retains queries for the initial D 0 corpus, we show in Table 1 that such a method suffers from forward transfer to newly indexed documents.
- Generative memory. Recent years have seen significant progress in the capabilities of the generative language models (Raffel et al., 2020;Brown et al., 2020).
- Motivated by the success of these models and in-applicability of the episodic memory for DSI++, we pose a questions -instead of retaining the ground-truth queries, can we learn a parametric model to generate such queries given a document?
- Concretely, we propose to train a query generator model to sample queries for previously seen documents, and supplement them during incremental indexing.
- Since we use the generator model to sample queries for sparse experience replay, hence our proposed method -generative memory.
- Moreover, generative memory is also used to generate pseudo-queries for incoming batch of new documents, thus, enabling continual semi-supervised learning of the retrieval task.

## EXPERIMENTATION

## IMPLEMENTATION DETAILS
- We utilize the pre-trained T5-Base (Raffel et al., 2020) model to initialize all models and randomly initialize the additional parameters for atomic docid tokens.
- While indexing D 0 corpus, we train all the models for a maximum of 1M steps with a warmup of 100K steps.
- For the rest of hyper-parameters, we follow Tay et al. (2022) -set a learning rate to 0.001, batch size to 128, and input sequence length to 32.
- We evaluate models after every 5K steps and retain the checkpoint yielding the best performance.
- For the initial training with D 0 corpus, we co-train on indexing and retrieval tasks, therefore we use the average of all DSI metrics (indexing accuracy, Hits@1, and Hits@10) for model selection.
- For the continual learning experiments, we have access to only indexing accuracy for all involved corpora and so we use it for model selection.

## METHODS
- The proposed generative memory-based approach is compared to two other methods: continuous indexing and continual indexing with all seen documents.
- The proposed generative memory-based approach is compared to continuous indexing with the objective of fine-tuning the indexing on the incoming corpus of documents.
- The proposed generative memory-based approach is compared to continuous indexing with the objective of fine-tuning the indexing on the updated corpus.
- The proposed generative memory-based approach is compared to sampling documents from old and new corpora in equal proportion.

## Added

## Method
- Eval corpus = D0 Eval corpus = D1 corpus (Catastrophic forgetting) (Forward transfer) Index acc. Hits@1 Hits@10 Index acc. Hits@1 Hits@10 Natural Questions (NQ) -|D0| = 50K, |D1| = 10K Table 1: Comparing performance on incremental indexing of D 1 corpus across different methods -cl(D 1 ): continue fine-tuning with indexing task on D 1 , cl(U 1 ): continue fine-tuning on the updated corpus U 1 , cl(U 1 )+epsmem(D): continual indexing of U 1 along with ER of queries for D, cl(U 1 )+genmem(D): continual indexing of U 1 along with ER of pseudo-queries for D. We observe that continual indexing on the updated corpus cl(U 1 ) reduces forgetting compared to just indexing new corpus cl(D 1 ) in both NQ and MS MARCO datasets.
- Next, ER with either D 0 or D 1 hurts forward transfer or forgetting. Our proposed approach of augmenting pseudo-queries for all documents along with continual indexing, cl(U 1 )+genmem(U 1 ), alleviates forgetting of D 0 corpus and improves forward transfer to D 1 corpus.
- We also show that our proposed solution reduces forgetting of D 0 (= 8M ) passages while incrementally indexing in a large corpus setting, MS MARCO (full) containing 8.9M passages.

## RESULTS
- In this section, we revisit some of the questions, (Q1)-(Q3), raised in our case study (see §2.4) to investigate effectiveness of our proposed generative memory-based approach.
- Towards answer these questions, in Table 1, ↑ indicates higher is better, ↓ indicates lower is better.
- We observe that continual indexing of old and new documents cl(U n ) helps to alleviate forgetting of older documents when evaluated on retrieval tasks. However, average Hits@10 (A n ) still undergo 23 points drop after sequential updates.
- Generative memory enables sparse replaying of pseudo-queries for old documents and continual semi-supervised learning with new documents.
- We observe that augmenting generative memory during continual indexing not only reduces the forgetting (F n ) but also improves average Hits@10 (A n ) by +21.1% over considered baselines (see Figure 6 for Hits@1 results and Figure 7 for MS MARCO results in the Appendix A.2).

## Does generative memory alleviate forgetting of old documents?
- 35.9 for the model after training on D . We see that continually indexing both D and D corpora (cl(U ) -28.9), significantly reduce forgetting the retrieval task (Hits@1) over just indexing the new corpora D (cl(D ) -19.2).
- episodic memory (cl(U )+epsmem(D ) -22.9), and generative memory (cl(U )+genmem(D ) -26.0) reduce forgetting compared to cl(D ) when we replay (pseudo-)queries corresponding to D corpus.
- generative memory outperforms episodic memory, without retaining original queries. Although from Table 1, we see generative memory, cl(U ), underperforms cl(U ), from Figures 4 and 6, we see that generative memory, cl(U ), outperforms cl(U ) both in terms of average performance A and forgetting F over five sequential updates.
- without any fine-tuning on the retrieval task for D , incremental learning with indexing objective shows impressive forward transfer (or zero-shot gains, cl(D ) -31.7 and cl(U ) -34.0).
- ER with generative memory outperforms supervised baseline (cl(U )+genmem(D ) -57.7).
- replaying queries corresponding to either D or D hurt forward transfer to D (cl(U )+genmem(D ) -8.6) or amplify forgetting of D (cl(U )+genmem(D ) -7.0), respectively. These results suggest that the memory module should include (pseudo-)queries corresponding to both old and new documents.
- From Figure 4, we see that continual indexing method cl(U ) has a downward trend for LA (Hits@10), therefore, eventually forgetting the retrieval task.
- ER with generative memory is relatively constant, providing evidence against forgetting.

## Does generative memory generalize to different datasets?
- In Table 1, for the MS MARCO dataset, we report Hits@1 to be 78.2 after training on D 0 passages.
- We see that continually indexing both D 0 and D 1 corpora (cl(U 1 ) -76.5 and cl(U 1 )+genmem(U 1 ) -73.7), significantly reduce forgetting the retrieval task (Hits@1) over just indexing the new corpora D 1 (cl(D 1 ) -68.0).
- Next, we look at the retrieval task performance (Hits@1) for D 1 after incrementally indexing D 1 . We see that without any fine-tuning on the retrieval task for D 1 , incremental learning with indexing objective shows impressive forward transfer (cl(D 1 ) -36.1 and cl(U 1 ) -35.3).
- Moreover, ER with generative memory, cl(U 1 )+genmem(U 1 ) -80.6, performs far superior to just incremental indexing objective. Similar to the results with the NQ dataset, we show that ER with generative memory, cl(U n )+genmem(U n ), improves the overall performance for the retrieval task, reducing forgetting of previously indexed documents and enables forward transfer to new documents compared to continual indexing of all documents, cl(U n ).
- We show that our results hold across two datasets -Natural Questions and MS MARCO, thus, showcasing the generalizability of our approach.
- Investigating the effectiveness of the generative memory with the scale of a corpus. We conduct experiments with a full MS MARCO passage retrieval dataset containing approx. 8.9M passages. We construct two corpora -D 0 = 8M and D 1 = 841, 823 passages. We train the DSI model (T5-Base with unstructured atomic docid representations) using D 0 passages and incremental add D 1 passages.
- In Table 1, we report our results for MS MARCO (full). We see that continual fine-tuning with the indexing task on D 1 , cl(D 1 ), completely forgets the retrieval task for D 0 passages (Hits@1 goes to 0.1 from 16.3). However, the generative memory-based approach significantly reduces forgetting (Hits@1 of 7.3). Moreover, generative memory enables continual semi-supervised learning by augmenting pseudo-queries for D 1 passages and thereby improving forward transfer (Hits@1 of 31.6 as compared to 18.2 for cl(D 1 )).
- We convincingly demonstrate that our proposed solution also reduces forgetting in a large corpus setting.

## RELATED WORK
- We review relevant prior works along two dimensions -application setups related to DSI++ and continual learning methods to alleviate forgetting and enable forward transfer.
- Language models (LMs) as knowledge bases (KBs).
- Petroni et al. (2019) shows that pre-trained BERT (Devlin et al., 2019) models capture relational knowledge comparable to that of the KBs constructed using off-the-shelf techniques.
- Concretely, these models can be used to extract factual knowledge about relations between entities by providing a prompt to predict missing words in a cloze-style template (e.g., "New Delhi is the capital of ").
- In the same vein, Roberts et al. (2020) demonstrates that pre-trained T5 (Raffel et al., 2020) models can be employed to answer open-domain questions without access to any external knowledge or context.
- However, unlike structured KBs, it is non-trivial to update knowledge stored implicitly in the weights of these models.
- Therefore, Zhu et al. (2020) introduces an experimentation setup where the task is to update facts stored within the pre-trained models and proposes a constrained optimization method, similar to Elastic Weight Consolidation (Kirkpatrick et al., 2017), to alleviate catastrophic forgetting.
- With similar motivation, (Dhingra et al., 2022) introduces a diagnostic dataset to probe LMs for facts that change over time and suggests jointly modeling text with its timestamp for improved memorization of seen facts.
- Recent works have been investigating efficient ways to localize and edit facts stored with the LMs (AlKhamissi et al., 2022) using finetuning (Zhu et al., 2020;Dhingra et al., 2022), hyper-networks (De Cao et al., 2021;Mitchell et al., 2022), and direct editing (Meng et al., 2022).
- Although a crucial line of work around updating facts in the pre-trained LMs, using prompting as our probing mechanism only provides a lower bound estimate of the knowledge contained in these models (Jiang et al., 2020).
- On the other hand, we explicitly focus on the memorization task in DSI++. This task helps us to answer questions related to catastrophic forgetting more convincingly rather than bounded by the mechanism of how we probe these models.
- Our work outperforms episodic memory.
- Lastly, in the context of pseudo-query generation, neural models are prone to hallucinate additional content not supported by the input documents, and future works can study methods to filter out noisy pseudo-queries (Mehta et al., 2022) during incremental indexing.

## CONCLUSION
- DSI++ presents a novel direction for exploration of DSI models to solve one of the critical requirements for them to be usable in most production setups where new documents are added to the corpus continuously.
- Though DSI models are prone to catastrophic forgetting while we index new documents, we have shown through extensive experiments that our two proposed solutions to optimize for the flatter loss basins using SAM and using generative memory alleviate forgetting to a significant extent.
- With this work, we lay down foundations for further research in this space, so that the strategies here do not just benefit DSI, but the continual learning community in general.
- Training large models is expensive, and has a detrimental impact on the environment (Strubell et al., 2019).
- Continual learning on top of existing models is cheaper and better compared to re-training from scratch since it requires a much smaller number of steps.
- With DSI++, we aim to reduce the need to re-train DSI models from scratch whenever a new set of documents is added to the corpus thereby making it cheaper and better for the environment.
