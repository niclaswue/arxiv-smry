---
title: "Deep Differentiable Logic Gate Networks"
date: 2022-10-15T12:50:04.000Z
author: "Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-08277v1.webp" # image path/url
    alt: "Deep Differentiable Logic Gate Networks" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.08277)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.08277).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/deep-differentiable-logic-gate-networks).

# Abstract
- Recently, research has increasingly focused on developing efficient neural network architectures
- In this work, we explore logic gate networks for machine learning tasks by learning combinations of logic gates. These networks comprise logic gates such as "AND" and "XOR", which allow for very fast execution.
- The difficulty in learning logic gate networks is that they are conventionally non-differentiable and therefore do not allow training with gradient descent.
- Thus, to allow for effective training, we propose differentiable logic gate networks, an architecture that combines real-valued logics and a continuously parameterized relaxation of the network. The resulting discretized logic gate networks achieve fast inference speeds, e.g., beyond a million images of MNIST per second on a single CPU core.

# Paper Content

## Introduction
- Neural networks are a type of machine learning algorithm that are composed of a large number of interconnected neurons.
- Neural networks are used to learn patterns in data.
- Neural networks are composed of a large number of interconnected neurons, which makes them difficult to train and optimize.
- One approach for training neural networks is to use gradient descent, which is a method that allows the network to learn how to improve its performance.
- In this work, we propose an approach for gradient-based training of logic gate networks, aka. arithmetic / algebraic circuits.
- Logic gate networks are based on binary logic gates, such as "and" and "xor".
- For training logic gate networks, we continuously relax them to differentiable logic gate networks, which allows efficiently training them with gradient descent.
- After training, the resulting network is discretized to a (hard) logic gate network by choosing the logic gate with the highest probability.
- As the (hard) logic gate network comprises logic gates only, it can be executed very fast.
- Additionally, as the logic gates are binary, every neuron / logic gate has only 2 inputs, and the networks are extremely sparse.
- Our method accelerates inference speed (in comparison to fully connected ReLU neural networks) by around two orders of magnitude.

## Related Work
- Differentiable logics are well-known in the fields of fuzzy logics and probabilistic metric spaces
- Chatterjee [14] explored "memorization", a method for memorizing binary classification data sets with a network of binary lookup tables
- In his experiments, he considers the binary classification task of distinguishing the combined classes '0'-'4' from the combined classes '5'-'9' of MNIST and achieves a test accuracy of 90%.
- Brudermueller et al. [15] propose a method where they train a neural network on a classification task and then translate it, first into random forests, and then into networks of AND-Inverter logic gates, i.e., networks based only on "and" and "not" logical gates. They evaluate their approach on the "gastrointestinal bleeding" and "veterans aging cohort study" data sets and argue for the verifiability and interpretability of small logical networks in patient care and clinical decision-making.
- Continuous relaxations A popular approach for making discrete structures differentiable is continuous relaxation
- Zantedeschi et al. [30] propose to learn decision trees by quadratically relaxing the decision trees from mixed-integer programs that learn the discrete parameters of the tree (input traversal and node pruning).
- Logic gate-based trees are conceptually vastly different from decision trees: decision trees rely on splitting decisions instead of logical operations, and the tree structure of decision trees and logic gate-based trees are in the opposite directions

## Logic Gate Networks
- Logic gate networks are networks similar to neural networks where each neuron is represented by a binary logic gate
- Given a binary vector as input, pairs of Boolean values are selected, binary logic gates are applied to them, and their output is then used as input for layers further downstream
- Logic gate networks do not use weights
- Instead, they are parameterized via the choice of logic gate at each neuron
- In contrast to fully connected neural networks, binary logic gate networks are sparse because each neuron has only 2 instead of n inputs, where n is the number of neurons per layer
- In logic gate networks, we do not need activation functions as they are intrinsically non-linear

## Differentiable Logic Gate Networks
- Differentiable logic gate networks are not differentiable, so no gradient descent-based training is possible
- To make binary logic networks differentiable, we leverage the following relaxation: instead of hard binary activations / values a ∈ {0, 1}, we relax all values to probabilistic activations a ∈ [0, 1]
- We replace the logic gates by computing the expected value of the activation given probabilities of independent inputs a 1 and a 2 . For example, the probability that two independent events with probabilities a 1 and a 2 both occur is a 1 • a 2 . These operators correspond to the probabilistic T-norm and T-conorm; we report the full set of relaxations corresponding to the probabilistic interpretation in Table 1
- We parameterize each neuron with 16 floats (i.e., w ∈ R 16 ), which, by softmax, map to the probability simplex (i.e., a categorical probability distribution such that all entries sum up to 1 and it has only non-negative values)
- During training, we evaluate for each neuron all 16 relaxed binary logic gates and use the categorical probability distribution to compute their weighted average. Thus, we define the activation a of a differentiable logic gate neuron as

### Training Considerations
- We randomly initialize the connections and the parameterization of each neuron.
- For the initial parameterization of each neuron, we draw elements of w independently from a standard normal distribution.
- In all reported experiments, we use the same number of neurons in each layer (except for the input) and between 4 and 8 layers, which we call straight network.
- We train all models with the Adam optimizer [33] at a constant learning rate of 0.01.
- After training, during inference, we discretize the probability distributions by only taking their mode (i.e., their most likely value), and thus the network can be computed with Boolean values, which makes inference very fast.
- In practice, we observe that most neurons converge to one logic gate operation; therefore, the discretization step introduces only a small error, e.g., for MNIST, the gap is smaller than 0.1%.
- For regression learning, let us assume that we need to predict a k-dimensional output vector.
- Here, τ and β play the role of an affine transformation to transform the range of possible predictions from 0 to n/k to an application specific and more suitable range.
- Here, the optional bias β is important, e.g., if we want to predict values outside the range of [0, n/k/τ ].
- In some cases, it is desirable to cover the entire range of real numbers, which may be achieved using a logit transform logit(x) = σ −1 (x) = log x 1−x in combination with τ = n/k, β = 0.
- During differentiable training, we sum up the probabilities of the outputs in each group instead of counting the 1s, and we can train the model, e.g., using an MSE loss.

### Remarks
- Uses larger data types (e.g. int64) for boolean vectorization, which improves speed on current hardware
- Memory considerations: does not need to store connections as they can be reproduced from a single seed, memory footprint of logic gate networks is drastically reduced in comparison to neural networks, binary neural networks, and sparse neural networks
- Pruning the model can improve speed by reducing the number of operators, but requires storing connections

## Current Limitations and Opportunities
- Differentiable logic gate networks have a higher training cost than comparable conventional neural networks
- Asymptotically, differentiable logic gate networks are cheaper to train
- Edge computing and embedded machine learning are applications where the training cost is negligible in comparison to the inference cost
- We show the performance of our method, a regular neural network, and a few of the original learning methods that have been benchmarked. Our method performs better than logistic regression on all three data sets.

### Adult and Breast Cancer
- The Adult Census data set is used for a second set of experiments
- The Breast Cancer data set is used for a second set of experiments
- The method performs very similar to neural networks and logistic regression on the Adult data set
- The method achieves the best performance while still being the fastest model

### MNIST
- Our model is objectively more than 10× cheaper to evaluate.
- Our model is 12× faster than the model by Umuroglu et al. [44] on their specialized FPGA hardware, even though our model only achieves a 7% utilization of the GPU.
- When compared to the smallest sparse neural network, our model requires substantially fewer operations than each of the of baselines.

### CIFAR-10
- Our method outperforms neural networks in the first setting (color-channel resolution of 4) by a small margin, while requiring less than 0.1% of the memory footprint and (with a larger model) by a large margin while requiring less than 1% of the memory footprint.
- In comparison to the best fully-connected neural network baselines, which are trained with various tricks such as student-teacher learning and retain the full color-channel resolution, our model does not achieve the same performance, while it is also much smaller and has access to fewer data.
- With 1 million parameters, the student-teacher model [52] has a footprint that is about 64% larger than the footprint of our largest model (large×4), and achieves an accuracy which is only 3.7% better than ours.
- Also, note that two out of three ResNet32 based sparsification methods achieve only around 37% accuracy.

### Distribution of Logic Gates
- Histograms of operators present in each layer of a trained model show that the constant 0/1 "operator" is rarely used
- In the first layer, 'and', 'nand', 'or', and 'nor' are most common
- In the second and third layers, there are more 'A', 'B', '¬A', and '¬B's, which can be seen as a residual / direct connection
- In the last layer, the most frequent operations are 'xor' and 'xnor', which can create conditional dependencies of activations of the previous layers

## Conclusion
- Logic gate networks are more efficient than state-of-the-art neural networks
- The training time for a logic gate network is faster than for a state-of-the-art neural network
- The accuracy of a logic gate network is orders of magnitude more efficient than a state-of-the-art neural network
- The accuracy of a logic gate network is similar to the accuracy of a regular neural network
- Logic gate networks can be trained efficiently up to around 8-10 layers
