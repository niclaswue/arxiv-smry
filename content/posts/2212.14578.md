---
title: "MAUVE Scores for Generative Models: Theory and Practice"
date: 2022-12-30T07:37:40.000Z
author: "Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Zaid Harchaoui"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-14578v1.webp" # image path/url
    alt: "MAUVE Scores for Generative Models: Theory and Practice" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.14578)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.14578).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/mauve-scores-for-generative-models-theory-and).

# Abstract
- Generative AI has matured to a point where large-scale models can generate text that seems indistinguishable from human-written text and remarkably photorealistic images.
- Automatically measuring how close the distribution of generated data is to the target real data distribution is a key step in diagnosing existing models and developing better models.
- We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling.
- We explore four approaches to statistically estimate these scores: vector quantization, non-parametric estimation, classifier-based estimation, and parametric Gaussian approximations. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of $f$-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts.
- We conclude the paper by demonstrating its applications to other AI domains and discussing practical recommendations.

# Paper Content

## Introduction
- Large-scale generative artificial intelligence models can produce human-like text and realistic images
- Automatic measures can dramatically reduce the cost of evaluation, in turn making it easier to rapidly develop models
- One approach to evaluation is to compare a generative model's distribution Q with the target distribution P of the real data that it aims to model
- Quantifying these errors in a principled, computationally tractable manner is challenging when faced with real-world text or image distributions
- We present a family of comparison measures between pairs of probability distributions, such as those encountered in the generative modeling of text and images
- Building upon the notion of divergence frontiers proposed by Djolonga et al. (2020), our measures are statistical summaries of fdivergence frontiers, which capture the two types of errors
- We explore four methods for estimating these divergence frontiers and their scalar summaries
- We provide statistical bounds for two of these estimation methods-vector quantization and nearest-neighbor estimation-as well as theoretical guidance on choosing the level of vector quantization
- In the spirit of popular metrics in natural language processing such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), we call these measures MAUVE scores

### Contributions
- We make the following contributions in this work. Statistical Summaries of Divergence Frontiers (Section 3).
- Our goal is to provide a scalar summary of the discrepancy between a generative model Q and the target distribution P that it aims to model. To do so, following (Djolonga et al., 2020), we consider two types of costs: (I) the mass of Q that has low probability under P , and (II) the mass of P that has low probability under Q.
- We formalize these costs using a divergence frontier, where R λ = λP +(1−λ)Q, and D f is an f -divergence such as the Kullback-Leibler (KL) divergence.
- This extends the frontiers of (Djolonga et al., 2020) to general f -divergences.
- We shall show in Section 3 that the nice properties of the divergence frontiers also extend their variants based on f -divergences.
- We propose three scalar statistical summaries of divergence frontiers. The first summary measures the area under a transformed divergence frontier: exp(−x), exp(−y) : (x, y) ∈ F f (P, Q) ∪ {(1, 0), (0, 1)} . Here, exp(•) monotonically transforms the frontier to account for unbounded divergences.
- Second, we consider an integral summary that sweeps over the coordinates on the divergence frontier and accumulates their costs: Finally, the third summary simply uses costs from the mid-point of the frontier, i.e., the coordinates corresponding to λ = 1/2:
- At their core, all three summaries are based on f -divergences. Thus, all three benefit from our estimation algorithms and error bounds for f -divergences, which we discuss next.
- We give algorithms for computing the summaries MAUVE f , FI f , and Mid f on real-world distributions of text or images. This requires computing f -divergences between the target distribution P and the model distribution Q, which is challenging due to the lack of direct access to P and Q, and the large support of each distribution.
- To address these challenges, we propose four methods for estimating divergence frontiers from i.i.d. samples using embeddings of the data (e.g., from a large language model for text data):
- 1. Quantization: we jointly quantize the distributions P and Q in some embedding space to form two multinomial distributions, then estimate the divergence frontier between the two multinomial distributions.
- 2. Nearest-neighbor : we use the nearest neighbors (in some embedding space) of each sample to estimate the likelihood ratio P (x)/Q(x), which we use to estimate the required f -divergences.
- 3. Classifier : we train a classifier to identify whether each sample belongs to the target or model distribution. We use the classifier to estimate the likelihood ratio and, in turn, the required f -divergences.
- 4. Parametric: we assume that embedded samples from the target and model distributions are distributed according to multivariate Gaussians. We estimate the parameters of each Gaussian and estimate the divergence frontier by numerical integration.
- We develop new error bounds for the first quantization approach. The total estimation error of the divergence frontier consists of two parts: (i) the statistical error in estimating the frontier from samples, and (ii) the quantization error that arises from passing from the original distributions to their quantized versions.
- For the statistical error, Theorem 10 gives an error bound that allows for long tails and countable support of the distribution P ....

## Background and Setup
- We discuss the basics of open-ended text generation
- We set up the problem of comparing multiple generative models

### Language Modeling and Open-Ended Text Generation
- Neural autoregressive language models are the backbone of prevailing approaches to text generation.
- Contemporary neural language models, i.e., language models parameterized by a neural network, are based on the transformer architecture (Vaswani et al., 2017) summarized in Figure 1 (left).
- The usual training objective for neural language modeling is via supervised multi-class classification of the next token.
- We assume that there is an underlying distribution P (• | x 1:t ) for the next token x t+1 humans would write in continuation to a prefix x 1:t .
- The training procedure aims to minimize the Kullback-Liebler (KL) divergence between the distributions P (• | x 1:t ) and P (• | x 1:t ) assigned by humans and the language model respectively over the next token x t+1 in continuation to a context x 1:t ∼ P t coming from human-written text: where T is the maximum sequence length.
- Since neither the distribution P t over prefixes of length t nor the distribution P (• | x 1:t ) over the next token is known in practice, plug-in estimates of both are employed in practice.
- Autoregressive models also yield an estimate of the joint probability P (x) of a sequence.
- The open-ended text generation task asks us to output text xs+1:|x| in continuation of a given context x 1:s .
- In contrast to directed text generation tasks such as translation, summarization, and question-answering, the task here is open-ended in that the context size s | x| is typically small and does not really constrain the output space.
- Unlike directed text generation tasks such as translation, summarization, and question-answering, the goal here is to generate text that is coherent, fluent, creative, and engaging.
- Since these criteria are hard to make mathematically precise, we instead consider the surrogate goal of generating text which is humanlike, such that generated text samples can pass for samples from the distribution P over human written text sequences.
- We model a text generation system as a probability distribution Q(• | x 1:s ) such that its generated text xs+1:|x| is an i.i.d. sample from Q.
- Given a neural autoregressive language model P , we can generate open-ended text in a serial, left-to-right fashion, by sampling xs+1 ∼ P (•|x 1:s ), xs+2 ∼ P (•|x 1:s , xs+1 ), etc.
- This is also known as ancestral sampling, and the induced distribution Q over sequences is where we assume that the prefix x 1:s ∼ P s is drawn from the human distribution.
- Decoding Algorithms. Assuming the language model learning has succeeded, we have that P (• | x 1:t ) ≈ P (• | x 1:t ) for prefixes x 1:t ∼ P t drawn from the distribution of human-written text, in the sense that the objective of (1) is bounded above by some ε > 0.
- However, for x1:t drawn from a distribution Q t which is different from the human distribution P t , the model's next-token distribution P (• | x1:t ) can be quite different from P (• | x1:t ) of humans.
- In the iterative process of ancestral sampling, the gap between P ( x1:t ) and Q samp ( x1:t ) keep increasing as the generation length t grows larger, so that Q samp is quite far from P .
- This leads to decoding algorithms which produce samples where Q(• | x 1:t ) is a reshaping of...

### Comparing Generative Models
- The usual approach to evaluating a text generation model is to compare the output of the model to human-written text for the same prompt (Papineni et al., 2002;Lin, 2004, etc.).
- This paradigm, however, breaks down for open-ended generation since there can be multiple correct outputs.
- We frame the problem as comparing two distributions.
- Let Q ∈ P(X ) denote the model distribution over some data space X such as text sequences or images and let P ∈ P(X ) denote the target real data distribution.
- For text distributions, Q depends on the underlying language model P as well as the decoding algorithm.
- The goal of open-ended text generation is to generate text that is human-like and the goal of image generation is to generate photorealistic images.
- Both these goals can be framed as finding a model distribution Q that is as close to P as possible in some metric.
- Therefore, we cast the evaluation of the generative model as measuring the gap between the model distribution Q and the target distribution P .

### Information Divergences
- Definition 1: Let f : (0, ∞) → R + be a convex function with f (1) = 0.
- The f -divergence between P and Q is defined as with the convention f (0
- Since f is convex and nonnegative with f (1) = 0, we have that f is non-increasing on (0, 1] and non-decreasing on [1, ∞).
- The conjugate generator to f is the function where again we define f * (0) = lim t→0 + f * (t).
- Since f * can be constructed by the perspective transform of f , it is also convex.
- We can verify that f * (1) = 0 and f * (t) ≥ 0 for all t ∈ (0, ∞), so it defines another divergence D f * .
- We call this the conjugate divergence to D f since The divergence D f is symmetric if and only if f = f * , and we write it as D f (P, Q) to emphasize the symmetry.
- Example 2: We give a few examples of f -divergences.
- (a) KL divergence: It is an f -divergence generated by f KL (t) = t log t − t + 1.
- (b) Interpolated KL divergence: For λ ∈ (0, 1), the interpolated KL divergence is given by
- It is an f -divergence whose generator can be obtained from the upcoming Property 5.
- (c) Jensen-Shannon divergence: The Jensen-Shannon Divergence is defined as
- More generally, we have the λ-skew Jensen-Shannon Divergence (Nielsen and Bhatia, 2013), which is defined for λ ∈ (0, 1) as
- (d) Interpolated χ 2 divergence: Similar to the interpolated KL divergence, we can define the interpolated χ 2 divergence D χ 2 ,λ and the corresponding generator f χ 2 ,λ for λ ∈ (0, 1) as
- The usual χ 2 divergence is obtained in the limit λ → 0.

### Tradeoff Curves to Evaluate Generative Models
- Consider a generative model Q ∈ P(X ) which attempts to model the target distribution P ∈ P(X ).
- It has been argued in (Sajjadi et al., 2018;Kynkäänniemi et al., 2019) that one must consider two types of costs to evaluate Q with respect to P : (a) a type I cost incurred from generating poorquality data, which is the mass of Q that has low or zero probability mass under P , and (b) a type II cost incurred from a failure to capture the diversity of the real data, which is the mass of P that Q does not adequately capture.
- Suppose P and Q are uniform distributions on their supports, and R is uniform on the union of their supports. Then, the type I cost is the mass of Supp(Q) \ Supp(P ), or equivalently, the mass of Supp(R) \ Supp(P ).
- We measure this using the surrogate KL(Q R), which is large if there exists an atom x such that Q(x) is large but R(x) is small. Likewise, the type II cost is measured by KL(P R).
- When P and Q are not constrained to be uniform, it is not clear what the measure R should be. Djolonga et al. (2020) propose to vary R over all possible probability measures and consider the Pareto frontier of the multi-objective optimization min R KL(P R), KL(Q R) .
- This leads to a curve called the divergence frontier, illustrated in Figure 2), and is reminiscent of the precision-recall curve in binary classification. See (Cortes and Mohri, 2005;Clémençon and Vayatis, 2009;Clémençon and Vayatis, 2010;Flach, 2012) and references therein on trade-off curves in machine learning.
- It was shown in (Djolonga et al., 2020, Props. 1 and 2) that the divergence frontier F(P, Q) of probability measures P and Q is carved out by mixtures R λ = λP + (1 − λ)Q for λ ∈ (0, 1).
- We present an elementary proof for completeness.
- Property 3. Consider two distributions P, Q with finite support. Then, the Pareto frontier for the pair of objectives KL(P •), KL(Q •) is given by where In other words, there does not exist any distribution R such that KL(P |R) < KL(P |R λ ) and KL(Q|R) < KL(Q|R λ ) simultaneously for any λ ∈ (0, 1).
- Proof. The convexity of KL(P •), KL(Q •) allows us to compute the Pareto frontier F(P, Q) exactly by minimizing linear combinations of the objectives.
- Concretely, we have from (Miettinen, 2012, Thms. 3.4.5 & 3.5.4) that where Simple algebra gives us the identity The first two terms of the right-hand side are independent of R and the last term is minimized at In this work, we consider a more general family of f -divergence frontiers.
- Definition 4. The f -divergence frontier F f (P, Q) for two distributions P, Q ∈ P(X ) and a divergence generator function f satisfying f (0) < ∞ and f * (0) = ∞ is defined as...

### Scalar Summaries of Divergence Frontiers
- Area under the curve (Flach, 2012)
- Divergence frontiers can be unbounded (e.g. as λ → 1, we have KL(Q R λ ) → KL(Q P ), which can be unbounded)
- For the KL divergence, R λ is the minimizer of the linearized objective λ KL(P R)+ (1 − λ)KL(Q R)
- For the Le Cam divergence, L λ is the λ-skew Jensen-Shannon Divergence
- The linearized cost L f,λ depends on the choice of λ

### Properties of Divergence Frontier Summaries
- Kernel Density Estimation: We use a kernel density estimator to estimate the empirical distributions Pn,k and Qm,k . We then estimate the divergence frontier by the maximum likelihood estimator F f ( Pn,k , Qm,k ), from which the corresponding summaries MAUVE, FI, and Mid can be estimated. This approach is more accurate than vector quantization when the target distributions are not i.i.d. but have some non-zero kernel density.
- Support Vector Machines: We use a support vector machine to learn a function that maps the empirical distributions Pn,k and Qm,k into a lower dimensional space. We then estimate the divergence frontier by the maximum likelihood estimator F f ( Pn,k , Qm,k ), from which the corresponding summaries MAUVE, FI, and Mid can be estimated. This approach is more accurate than kernel density estimation when the target distributions are not i.i.d. but have some non-zero kernel density.
- Random Forest: We use a random forest to learn a function that maps the empirical distributions Pn,k and Qm,k into a lower dimensional space. We then estimate the divergence frontier by the maximum likelihood estimator F f ( Pn,k , Qm,k ), from which the corresponding summaries MAUVE, FI, and Mid can be estimated. This approach is more accurate than support vector machines when the target distributions are not i.i.d. but have some non-zero kernel density.
- The area summary MAUVE(P, Q) satisfies the following: Figure 3: Relationship between the area summary MAUVE f and the mid-point summary Mid f .
- The integral summary FI of the f -divergence frontier defined by a convex generator f satisfies the following properties: (a) FI f is an f -divergence generated by the convex function where f λ is as defined in (4).
- The mid-point summary Mid f of the f -divergence frontier satisfies the following properties: (a) Mid f is an f -divergence generated by the convex function f 1/2 as defined in (4).
- The four estimation methods considered are: Vector Quantization, Kernel Density Estimation, Support Vector Machines, and Random Forest.

### Estimation via Vector Quantization
- Given a k-partition S = {S 1 , . . . , S k } of the space X , we define the quantization of P over S as P S = P (S 1 ), . . . , P (S k ) .
- The quantization approach to estimating the divergence frontier consists of two approximations: approximating the intractable divergence frontier F f (P, Q) with the lower-dimensional counterpart F f (P S , Q S ), and estimating this frontier F f (P S , Q S ) with its plug-in estimator F f ( PS,n , QS,m ), where PS,n = l=1 is the empirical distribution of P S , and QS,m is the corresponding empirical distribution of Q S .
- In practice, the best quantization schemes are data-dependent, such as k-means clustering or latticetype vector quantization of dense representations of images or text; we will discuss this in more detail in Section 4.1.2.
- When the two distributions P and Q have long tails, the empirical estimators PS,n and QS,m can be of poor quality due to the missing mass phenomenon (Good, 1953), i.e., some probability masses do not appear in the finite sample. This is illustrated in Figure 5.
- A widely used technique to address such a challenge is the add-constant smoothing (see, e.g., Krichevsky and Trofimov, 1981). It adds a small constant b to the counts of each bin and normalizes these pseudo-counts to form a distribution.
- Precisely, the add-b estimator of P S is defined as Nearest neighbor estimation of f -divergences typically require continuous distributions on a Euclidean space with densities satisfying certain regularity conditions.
- To this end, we consider estimation on a noisy version of the problem. First, we pass from a discrete data space X to an Euclidean embedding space by taking embeddings from a model ϕ : X → R d . While the pushforward distributions ϕ P and ϕ Q are now supported on R d , they are not guaranteed to have a density w.r.t. the Lebesgue measure.
- To overcome this, we consider smooth these pushforward distributions by convolving them with a Gaussian N (0, σ 2 I d ) to get distributions P = ϕ P N (0, σ 2 I d ) and Q = ϕ Q N (0, σ 2 I d ). Sampling from the convolved distribution is trivial: u i = ϕ(x i ) + ξ i and u j = ϕ(x j ) + ξ j are a valid samples from P and Q respectively for x i ∼ P and x j ∼ Q with independent Gaussian noise ξ i , ξ j ∼ N (0, σ 2 I d ).
- We analyze the corresponding version of ( 22) that is constructed using the 2 distance between the noisy vectors u i , u j . We show that this procedure always underestimates the f -divergence.
- Property 15. For any divergence generator f , we have
- Further, if the data space X is discrete and the embedding model is injective, i.e., ϕ(x) = ϕ(x ) for all distinct x, x ∈ X , then the last inequality hold with equality.

### Estimation via Nearest Neighbors
- We now turn to estimation of the divergence frontier and its summaries by counting the nearest neighbors of each sample.
- We consider near neighbors from the 2 -distance in an embedding space.
- Given an embedding model ϕ : X → R d , we define a metric ρ on the data space X as
- Let N k (x) denote the set of k-nearest neighbors (under the metric ρ) of x from the set X ∪ X where X = {x i } n i=1 are samples from P and X = {x j } m j=1 are samples from Q.
- Following (Noshad et al., 2017), we estimate the f -divergence D f (P Q) with the estimator The intuition behind the estimator is that we expect |N k (x j ) ∩ X| ∝ P (x j ) and |N k (x j ) ∩ X | ∝ Q(x j ), so their ratio (with appropriate normalization) can be considered an estimate of the likelihood ratio r(x)
- We could also similarly define a kernel density estimator of the divergence frontier.
- Given a kernel κ : R d → R + normalized such that R d κ(z)dz = 1, the kernel density estimate of the density of a distribution R using i.i.d. samples U = {u 1 , . . . , u n } is given by where h is a bandwidth parameter.
- A typical choice of kernel is the Gaussian kernel κ(z) = (2π) −d/2 exp(− z 2 2 /2).
- Similar to the nearest neighbor approach, we define the kernel density estimator in the embedding space of a model ϕ : X → R d .
- We approximate D f (P Q) that of the kernel density estimator using samples X ∼ P n and X ∼ Q m as D f (g ϕ(X) g ϕ(X ) ), which is in turn estimated using its plug-in estimate The expectation over Q is approximated by a sample average over X .
- The numerator of the term inside f (•) is simply the kernel density estimate (26) of P at x j using all n samples from X, while the denominator is the corresponding estimate for Q using the other m − 1 samples X \ {x j }.
- The rest of the estimation procedure is identical to Algorithm 2.

### Estimation via Classification
- Estimating the likelihood ratio r(x) := P (x)/Q(x) with a probabilistic classifier such as logistic regression
- Concretely, define the class prior as P(y = +1) = n/(n + m) and P(y = −1) = m/(n + m) and the class-conditional distribution by P(x|y = +1) = P (x) and P(x|y = −1) = Q(x)
- By the Bayes rule, the likelihood ratio can equivalently be written as
- Given a probabilistic classifier that outputs an estimate η(x) for P(y = 1|x), we can estimate the likelihood ratio as
- We then estimate the f -divergence D f (P Q) using the Monte Carlo estimate
- To train a classifier, we split X = X 1 ∪ X 2 and X = X 1 ∪ X 2 , train a probabilistic classifier such as a logistic regression model to separate X 1 from X 2 (train set) and evaluate the likelihood ratios on X 1 and X 2 (validation set) to estimate the f -divergence

### Estimation via Parametric Approximations
- Finally, we discuss how to estimate the -divergence with parametric approximation. Given an embedding model : 1 is an i.i.d. sample from , we then approximate by a multivariate Gaussian distribution with mean and covariance matrix given by where φ(• ; µ, Σ) is the probability density function of the multivariate normal distribution (µ, Σ). To evaluate the integration in (30), we can use the Monte Carlo approach-(i) generate i.i.d. samples , and (ii) approximate ( 30) by the empirical average
- Given an embedding model ϕ : i=1 is an i.i.d. sample from ϕ P , we then approximate ϕ P by a multivariate Gaussian distribution with mean and covariance matrix given by where φ(• ; µ, Σ) is the probability density function of the multivariate normal distribution N (µ, Σ). To evaluate the integration in (30), we can use the Monte Carlo approach.

## Related Work
- Information divergence based scores are being used to evaluate generative models
- There is research on statistical trade-off curves, information divergence based scores for texts and images, and theoretical results on the statistical estimation of information divergences in mathematical statistics and information theory.

### Divergence Frontiers for Generative Models
- Sajjadi et al. (2018) proposed to account for the two types of errors of generative modeling using trade-off curves in the spirit of operation characteristics and precision-recall curves for binary classification and statistical detection
- Kynkäänniemi et al. (2019) proposed to account for the two types of errors of generative modeling using trade-off curves in the spirit of operation characteristics and precision-recall curves for binary classification and statistical detection
- Djolonga et al. (2020) proposed information divergence frontiers based on Rényi divergences thereby encompassing both (Sajjadi et al. 2018) and (Kynkäänniemi et al. 2019)
- The authors of (Djolonga et al., 2020) show how to compute the divergence frontiers in special cases such as exponential families. Their exploration of statistical estimation via vector quantization leads to two observations. First, a small quantization size can lead to a bias of optimism, where the D f (P S Q S ) ≤ D f (P Q) and this gap can be large when |S| is small. Second, the statistical error from small sample sizes can lead to pessimistic estimates of the divergences.
- However, (Djolonga et al., 2020) do not provide statistical bounds for vector quantization nor do they analyze statistical properties of divergence frontiers defined using f -divergences.
- Also, these papers do not consider applications to open-ended text generation.
- We extend the above line of work, presenting a general framework for estimating divergence frontiers and their statistical summaries for generative models.
- Theoretically, we provide quantitative upper bounds for both the statistical error and quantization error. Specifically, we show that the statistical error is bounded by Õ( k/n).
- Our bounds also demonstrate the interest of using smoothed distribution statistical estimators to account for the missing mass problem.
- We explore other estimation procedures based on nonparametric nearest-neighbor and kernel density estimation, classifier-based estimation, and parametric Gaussian approximations.
- We also perform a thorough empirical evaluation and operationalize these scores for large-scale text and image models.
- Finally, based on our observations, we discuss practical recommendations, to facilitate the application to applied AI domains.

### Divergence Measures for Text
- Reference-based metrics evaluate generated text with respect to a (small set of) reference text sample(s), rather than comparing full sequence distributions.
- Statistics-based metrics compare the model distribution Q with respect to the human distribution P on the basis of some statistic T (P ) and T (Q).
- Property-specific statistics such as the amount of repetition (Holtzman et al., 2020;Welleck et al., 2020b), verifiability (Massarelli et al., 2020), or termination (Welleck et al., 2020a) are orthogonal to MAUVE, which provides a summary of the overall gap between P and Q rather than focusing on an individual property.
- Other statistic is the generation perplexity (Fan et al., 2018;Holtzman et al., 2020), which compares the perplexity of the model text x ∼ Q with that of human text x ∼ P under an external model R. We find in Section 7 that generation perplexity fails to correctly capture the effect of the decoding algorithm and the text length.
- Language modeling metrics calculate how (un)likely human text x ∼ P is under the model distribution Q, for instance, using the probability Q(x).
- Non-Automatic Metrics. HUSE (Hashimoto et al., 2019) aims to combine human judgments of Type I errors with Type II errors measured using perplexity under Q.

### Divergence Measures for Images
- Evaluation of generative models is an active area of research
- The Inception Score (Salimans et al., 2016) is based on large-scale supervised classification tasks
- The Fréchet Inception Distance (Heusel et al., 2017;Semeniuta et al., 2018) and its unbiased counterpart, the Kernel Inception Distance (Bińkowski et al., 2018) are both used for evaluating generative models
- However, unlike divergence frontier methods, do not take into account trade-offs between different kinds of errors between the learned and the reference distribution
- We find in Section 7.2 that the Fréchet distance adopted to the text setting fails to capture the dependence on the text length, while our proposed approach can

### Statistical Estimation of Information Divergences
- KL divergences have been studied in both fixed and large alphabet regimes
- The minimax quadratic risk of the naïve plug-in estimator is infinite
- The main challenge arises from the missing mass phenomenon
- This challenge can be addressed by applying add-constant smoothing
- Our results also utilize add-constant smoothing without the need for the boundedness assumption
- There exists a rich literature on statistical estimation of f -divergences using other methods
- Nonparametric estimation of f -divergences via nearest-neighbor and kernel density estimation was studied in (Póczos et al., 2011;Moon and Hero III, 2014;Kandasamy et al., 2015;Noshad et al., 2017), to name a few
- The variational expression for f -divergences was leveraged for optimizationbased estimation in (Nguyen et al., 2010;Sreekumar and Goldfeld, 2022)

## Experiments: Setup
- We consider open-ended text generation tasks
- The open-endedness of the task is reflected in the relative lengths of the prompt and the generation
- The prompt is often quite short (35 to 50 tokens), while the generation is 10× to 30× longer (approximately 500 to 1000 tokens)

### Task Domains and Models
- Generate articles from the publicly available analogue of the Webtext dataset using pre-trained GPT-2 models
- At generation time, use as prompts the first 35 tokens of each of the 5000 articles from the Webtext test set, keeping the maximum generation length to 1024 tokens (which corresponds, on average, to around 750 words)
- For comparison with human text, use the corresponding human-written continuations from the test set (up to a maximum length of 1024 tokens)
- Under this task, the goal is to generate the body of a news article, given the title and metadata (publication domain, date, author names)
- We use a left-to-right transformer language model, Grover (Zellers et al., 2019), which is similar to GPT-2 but tailored to generating news by conditioning on the metadata of the article as well
- Our generations rely on pre-trained Grover architectures of various sizes
- The generation prompt comprises the headline and metadata of 5000 randomly chosen articles from the April2019 set of the RealNews dataset (Zellers et al., 2019), and the maximum article length was 1024 tokens
- We reuse the publicly available Grover generations for our evaluation

### Decoding Algorithms
- Greedy decoding selects the most likely next token x t+1 = arg max x∈V P (x | x 1:t ) and is representative of a broader class of approximate likelihood maximization decoding algorithms.
- Ancestral sampling samples directly from the language model's per-step distributions as x t+1 ∼ P ( • | x 1:t ), and generates unbiased samples from the model distribution.
- Nucleus sampling (Holtzman et al., 2020) samples from top-p truncated per-step distributions, x t+1 ∼ Qnuc,p ( • | x 1:t ) as defined in Equation ( 2).
- Greedy decoding attempts to find text that approximately maximizes its likelihood under the model. While such algorithms are highly successful for directed text generation tasks such as translation, they produce highly degenerate repetitive text in the open-ended setting.
- While ancestral sampling produces unbiased samples from the model distribution, it also has been found to generate degenerate text (Holtzman et al., 2020), ostensibly because the model is imperfect, especially in the low-probability tail of the next-token distribution.
- Nucleus sampling attempts to fix this by truncating the tail and is representative of the broader class of truncated sampling methods that are now widely considered state-of-the-art.
- We vary the nucleus parameter p ∈ {0.9, 0.92, 0.95, 0.99} for web text generation and story continuation, and p ∈ {0.9, 0.92, 0.94, 0.96, 0.98} for news generation.
- In addition, we also consider the following decoding algorithms: is the Shannon entropy. This is a set that covers at least τ -fraction of the probability mass but also has log probabilities that are as close to the conditional entropy as possible. The samples are obtained by sampling from this truncated distribution as where Z is a normalizing constant.
- Adversarial perplexity sampling is designed to generate low-quality text that nevertheless matches the perplexity of human text. Adversarial perplexity sampling proceeds in two phases: (1) we generate the first 15% of tokens in a sequence uniformly at random from the vocabulary, and (2) we generate the remaining tokens greedily to make the running perplexity of the generated sequence as close as possible to the perplexity of human text.

### Baseline Metrics
- Generation Perplexity (Gen. PPL.): We compute the perplexity of the generated text under the GPT-2 large model.
- Zipf Coefficient: we report the slope of the best-fit line on log-log plot of a rank versus unigram frequency plot.
- Repetition Frequency (Rep.): The fraction of generations which devolved into repetitions.
- Distinct-n: The fraction of distinct n-grams from all possible n-grams across all generations.
- Self-BLEU: Self-BLEU is calculated by computing the BLEU score of each generation against all other generations as references.
- Discriminator Accuracy: We train a binary classifier to classify text as human or not.
- For the story continuation task, we train a classification head on a frozen GPT-2 large model using the logistic loss.
- We use 25% of the data as a test set and the rest for training; a regularization parameter is selected with 5-fold cross-validation.
- For the news dataset, we follow the protocol of (Zellers et al., 2019), i.e., a Grover large model finetuned with a binary classification head.

### Human Judgements and Evaluation of Automatic Metrics
- The metric should yield judgments that correlate highly with human judgments
- We evaluate how the quality judgments of the proposed measures correlate with human quality judgments
- In our study, a quality judgment means choosing a particular (model, decoder) setting based on the resultant generations
- Evaluation Protocol:
- Since our goal is to measure the gap between a model text distribution Q and a human text distribution P , we employ a pairwise setup for human evaluations
- At each round, an annotator receives a context and continuations from two different (model, decoder) settings, and selects the continuation they found more (a) human-like, (b) interesting, and (c) sensible on a 5-point Likert scale
- Our interface for collecting annotations is shown in Figure 18 of Appendix D
- We collect these annotations for web text generation with 8 different (model, decoder) settings plus a ninth setting for human-written continuations
- Each setting is a GPT-2 model size paired with either ancestral or nucleus sampling
- Given the known difficulties with human evaluation of longer texts (Ippolito et al., 2020), we use a maximum completion length of 256 tokens
- We obtain 90 preference ratings for each pair of settings, coming from a total of 214 crowd-workers from the Amazon Mechanical Turk platform
- The evaluators were paid USD 0.40 per evaluation based on an estimated wage of USD 16 per hour
- Pairwise Scores to a Ranking:
- We convert these pairwise preferences to a ranking by fitting a Bradley-Terry model (Marden, 1995), a parametric model used to predict the outcome of a headto-head comparison
- In order to account for the standard deviation of the metric, we define the worst-case Spearman rank correlation between a 1 ± s 1 , . . . , a n ± s n with h = (h 1 , . . . h n ) as where ρ(a, h) denotes the Spearman rank correlation between a and h
- The end result is a correlation score in [−1, 1], with higher values meaning that quality judgments using the automatic metric correlate with quality judgments made by human evaluators up to one standard deviation from the randomness of sampling

### Hyperparameters
- By default, we summarize the divergence frontier with MAUVE KL computed using k-means vector quantization (Algorithm 1)
- Following the discussion in Section 4.1, we use the Krichevsky-Trofimov (add-1/2) smoothing.
