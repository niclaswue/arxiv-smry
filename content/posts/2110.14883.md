---
title: "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"
date: 2021-10-28T04:45:55.000Z
author: "Shenggui Li, Jiarui Fang, Zhengda Bian, Hongxin Liu, Yuliang Liu, Haichen Huang, Boxiang Wang, Yang You"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2110-14883v2.webp" # image path/url
    alt: "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2110.14883)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2110.14883).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/colossal-ai-a-unified-deep-learning-system).

# Abstract
- Transformer models have pushed the deep learning model scale to billions of parameters.
- Due to the limited memory resource of a single GPU, the best practice for choosing the optimal parallel strategy is still lacking.
- The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments.
- It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer.
- Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.

# Paper Content

## I. INTRODUCTION

## II. BACKGROUND
- Transformer architecture has led to deep learning models having improved performance in computer vision and natural language processing
- BERT and Vision Transformer

## A. Data Parallelism
- Data parallelism is the most common form of parallelism
- Data parallelism makes it easy to train and deploy a model
- Data parallelism has sub-linear scaling with the number of GPUs

## B. Model Parallelism
- In data parallel training, one prominent feature is that each GPU holds a copy of the whole model weights.
- However, this makes it tough to fulfill the memory requirement of largescale models, especially when using stateful optimizers such as Adam.
- The GPU memory becomes the bottleneck when the scale of the model increases.
- Thus, it is necessary to shard the model parameters so that each device only needs to hold part of the model weights.
- As a result, model parallelism was proposed to tackle this problem.
- There are generally three subcategories of model parallelism: tensor parallelism, pipeline parallelism, and sequence parallelism.
- Pipeline parallelism is to splits the model by layers and each device holds a portion of the layers.
- 1) Tensor Parallelism Tensor parallelism shards the tensor over an array of devices and requires a distributed matrix-matrix multiplication algorithm for arithmetic computation as shown in Figure 3b.
- Megatron-LM [12] proposed 1D tensor parallelism which splits the linear layer in the row or column dimensions for the Transformer architecture [19].
- Taking the multi-layer perceptron (MLP) module of the Transformer layer as an example, we can see the MLP module as a matrix multiplication of Y = W 2 W 1 X as shown in Figure 4, where X is the hidden states, W 1 and W 2 are the linear layer weights, and Y is the layer output.
- In a typical Transformer model, the hidden states is projected to higher dimensions by W 1 and then projected back to the original dimension by W 2 . We can split the matrix W 1 in the column dimension and matrix W 2 in the row dimension. This will lead to a partial result of Y on each device.
- An allreduce operation can be applied on the partial result to obtain the correct final result of the matrix multiplication.
- The Self-Attention module in the Transformer architecture is computed in a similar distributed fashion.
- In this way, each device will hold 1/N of the MLP parameters when training on N devices. This allows the model size to scale with the number of devices and goes beyond the memory capacity of a single device even though extra communication is required between devices to ensure arithmetic correctness.
- Besides 1D tensor parallelism, more advanced tensor parallelism are introduced for large-scale model training, namely 2D, 2.5D, and 3D tensor parallelism [20]- [22] as shown in Figure 6.
- These methods applied traditional distributed matrix multiplication for deep learning training and have advantages in memory and communication efficiency.
- 2D tensor parallelism [20] relies on the SUMMA and Cannon matrix multiplication algorithm [23]- [25] and splits the input data, model weights and layer outputs along two different dimensions.
- Given N devices, the tensor is split into N chunks and each chunk is owned...

## âˆš
- N rows as shown in Figure 6a.
- For example, a tensor of shape [P, Q] will be partitioned into a chunk tensor of shape This methods leads to zero memory redundancy as the inputs and outputs are no longer duplicated, leading to much lower memory consumption compared to that of 1D tensor parallelism.
- To ensure arithmetic correctness, the partial input and weights have communicated among devices via collective communication such as broadcast.
- Even though 2D tensor parallelism has extra communication overhead for model weights while 1D tensor parallelism does not, 2D tensor parallelism is still more efficient as the amount of data transferred in lower due to finer partitioning scheme when 2.5D Tesnor Paralleism [21] was inspired by 2.5D matrix multiplication algorithm [26] and proposed to further parallelize 2D tensor parallelism.
- It adds the optional depth dimension of the matrix for parallelization.
- When depth = 1, it is close to 2D tensor parallelism. When depth > 1, it partitions the matrix 3 times and adds one more degree of parallelization.
- Given N devices, the tensor is split in a way such as N = S 2 * D where S is the size of one side of the square and D is the depth of the cube as shown in Figure 6b. S can be calculated if D is given by the user.
- When scaled to a large number of devices, 2.5D tensor parallelism will be more efficient than 2D tensor parallelism as it further reduces the communication volume.
- 3D tensor parallelism [22] was proposed based on the 3D matrix multiplication algorithm [27] and integrated into Colossal-AI as well.
- 3D tensor parallelism splits a tensor in a cubic manner as shown in Figure 6d.
- As not every tensor has 3 dimensions, we choose to partition the first and last dimension only where the first dimension will be partitioned twice.
- For example, a tensor of shape [P, Q] will be partitioned into a chunk tensor of shape This method achieves the optimal O(N 1/3 ) communication overhead on N devices, while both computation and memory usage are evenly distributed through optimized load balancing of parameters as well as activations.
- As the advanced tensor parallelism methods require different network topology, the user needs to choose the method based on the number of GPUs.
- 1D method can work with any number of GPUs while 2D, 2.5D and 3D methods require the n 2 , a * n 2 , and n 3 GPUs respectively, where a and n are positive integers.
- The user can fall back to 1D tensor parallelism when the number of GPUs does not fulfill the requirement.
- These advanced tensor parallelism methods provide lower communication volume when scaling to a larger number of devices [23], [24], [26], [27].
- Meanwhile, these methods has better compatibility with pipeline parallelism as it removes the communication overhead between pipeline stages which exist in 1D tensor parallelism.
- In 1D tensor parallelism proposed by Megatron-LM, the layer activation is duplicated. Therefore, in order to save cross-node communication bandwidth, the tensor is split into chunks before passing to the next pipeline stage and the chunks are gathered with intra-node communication after reaching the target pipeline stage as shown in Figure 7.
- In contrast, in advanced tensor parallelism methods, the tensor to pass to the next stage is already a sub-chunk of the whole logical tensor. Thus, the split-gather operation is not needed and no extra communication cost is incurred.
- Pipeline Parallelism Besides tensor parallelism, another sharding scheme is to...

## C. Zero Redundancy Data Parallelism
- The methods above mainly tackle the memory challenge brought by the amount of model parameters.
- However, model parameters only form a part of the memory consumption during training.
- Among the the model data, optimizer states can take up a more significant proportion of memory consumption.
- When using stateful optimizers such as Adam [18], the optimizer states (i.e. momentum and variance) can occupy three times larger memory space compared to that occupied by the model parameters [18], [33].
- In data parallel training on N devices, there are N copies of model parameters, gradients and optimizer states.
- To eliminate such redundancy, Zero Redundancy Optimizer was proposed in DeepSpeed [16] to partition these redundant model data over different devices.
- Zero Redundancy Optimizer has three stages, which adds optimizer states, gradients, and model parameters to the partitioning scheme stage by stage.
- Taking the last stage as an example, each device only holds a 1/N of model data and updates only the partition of parameters owned by the corresponding device.
- As all the model data are partitioned, collective communication is required in the forward and backward passes.
- In the forward pass, all-gather is required to gather the partitioned model weights to compute the intermediate activation and the model output.
- During the backward, the model weights are gather again to compute the gradients.
- The partitioned gradients are then reduced to their respective devices.
- As each device only holds a partition of gradients and parameters, it will only update the partitioned parameters instead of the full model parameters.
- As this method is an enhancement of the data parallel training, it remains orthogonal to model parallelism and can be integrated with model parallelism to support larger models.

## D. Heterogeneous Training
- The CPU memory has larger capacity than the GPU memory in general
- By utilizing high-performance heterogeneous storage devices and constantly swapping tensors between different hardware devices, it became possible to train a model with billions of parameters on a single GPU
- Zero-offload moves the tensors from GPU to CPU or NVMe disks when not in use to make room for larger models

## III. DESIGN
- Hardware specifications have become diverse across supercomputing centers and research labs
- This makes it difficult to have a universal method which yields consistently satisfactory acceleration to deep learning training
- Megatron-LM and DeepSpeed are used to address this issue
- In contrast, Colossal-AI has an arsenal of acceleration techniques which are constructed in a modular way
- This makes it more versatile and able to achieve better performance in a wider range of training settings

## A. Multi-dimensional model parallelism
- Model parallelism is an important technique in distributed training,
- Colossal-AI provides an array of model parallelism methods to cater to the needs of distributed training,
- One major feature of the Transformer model is that its architecture heavily replies on the linear layer,
- As tensor parallelism is mainly applied to matrix-matrix multiplication, it is highly suitable for the acceleration of Transformer model training.
- In Colossal-AI, all existing tensor parallelism methods are supported so that the user can choose one method based on their training requirements and the number of GPUs.
- Among all tensor parallelism methods, one prominent advantage of the advanced tensor parallelism, namely 2D, 2.5D and 3D tensor parallelism, is that it has lower communication cost compared to 1D tensor parallelism.

## B. Enhanced Sharding and Offloading
- Zero redundancy training eliminates memory redundancy by partitioning model parameters, gradients and optimizer states
- Offloading utilizes the CPU memory and NVMe disks to enlarge the storage space available for largescale models
- However, the unnecessarily complex implementation makes DeepSpeed less efficient and extensible
- For zero redundancy data parallel training, we have defined a unified sharded tensor interface
- The tensor partitioning can be supported by customizable sharding strategies
- This allows the model data and optimizer data to be sharded in a common way with loose coupling
- The training workflow can be easily modified with userdefined life-cycle hooks as well
- In this way, we can not only implement DeepSpeed's zero redundancy mechanism, but also make space for more optimization at different granularity

## IV. IMPLEMENTATION
- Colossal-AI is a deep learning system implemented with PyTorch
- The overall architecture of Colossal-AI is shown in Figure 1
- The fundamental component of the Colossal-AI is the parallel context manager
- Based on the parallel context, the distributed operators will automatically switch to the corresponding parallel mode
- For example, if the parallel context specifies to use 1D tensor parallelism, the computation flow of the linear layer will be modified to include row-splitting and column-splitting matrix multiplication like Figure 4 during runtime
- In this way, it is easy to build a tensor-parallel model easily as the complex computation and communication workflow is hidden from the user and separated from the model building process
- The user can use these operators like native PyTorch operators to construct their model which will execute the parallel computation graph during runtime
- One layer above lies the various acceleration tools
- The basic tools include activation checkpointing, mixed precision training and so on
- More advanced ones will be optimized sharding and offloading for large-scale model training
- Lastly, the execution engine takes control of the training process and the trainer provides a higher-level APIs to start training in lines. These two components provide great extensibility for user customization as they can define their own training schedule and hooks at the operator or trainer level

## A. Experiment Setup
- To holistically evaluate the system performance of Colossal-AI, we have conducted various experiments on different hardware
- The system specification is listed in Table II
- Due to resource constraints, we only tested a portion of the prominent features on each system
- We used Megatron-LM and DeepSpeed as our baselines for experiments
- As Megatron-LM tensor parallelism has been implemented in Colossal-AI, it will be annotated as 1D tensor parallelism in the experiment results

## 1) Convergence
- We have conducted experiments with Vision Transformer (ViT) on the ImageNet-1k dataset to verify the arithmetic correctness and numerical stability of multi-dimensional tensor parallelism on System III.
- The ViT model is configured to have 12 Transformer layers with hidden size of 384 and 6 attention heads.
- We used Jax initialization and AdamW optimizer with 0.003 learning rate and 0.3 weight decay.
- The input image is of shape 224 and the patch size is 16.
- The global batch size is 4k and the model is trained for 250 epochs.
- The baseline is model training with Data Parallelism as each device maintains the complete computation graph.
- As shown in Figure 11, the testing accuracy curves of Multi-Dimensional tensor parallelism align with that of the baseline, demonstrating that the Multi-Dimensional tensor parallelism does not affect model convergence.

## 2) Memory Efficiency
- As 2D, 2.5D, 3D tensor parallelisms partition the input data, layer weight, and output activation while 1D tensor parallelism only partitions the layer weight, the former is expected to have lower memory consumption.
- As a result, the first three methods allow the GPUs to accommodate larger models.
- To demonstrate the memory-efficiency, we have conducted two range tests which scale by batch size and hidden size on System I.
- In this range test, we created a model which consists of two linear layers.
- We run 1D, 2D and 2.5D experiments on 4 GPUs and 1D, 2.5D (depth=2), and 3D experiments on 8 GPUs.
- We measure the max allocated CUDA memory during the forward and backward pass, and the results are shown in Figure 12.
- The memory consumption of 1D tensor parallelism is much higher than those of 2D, 2.5D, and 3D tensor parallelism.
- With batch size equal to 512 and 8 GPUs, the memory consumption of 2.5D and 3D is 44% and 65% lower than that of 1D tensor parallelism respectively in Figure 12b.
- With hidden size of 16384 and 8 GPUs, the memory performance of 2.5D and 3D tensor parallelism is 62% and 74.2% better than that of 1D tensor parallelism respectively in Figure 12d.
- Therefore, more advanced tensor parallelism is a better option when scaling to super large-scale models.
- 3) Hardware Compatibility
- To further explore the performance of the various tensor parallelism methods, we have conducted experiments on System I and II to investigate the impact of GPU interconnect on the performance of tensor parallelism.
- System I and System II were selected for experiments as the former have fully connected NVLink between any pair of GPUs as shown in Figure 5a while the latter only has NVLink between 4 pairs of adjacent GPUs as shown in Figure 5b.
- The communication bandwidth of System I is consistently high regardless of whether it is measured for a pair of GPUs or a group of GPUs as shown in Figure 13.
- However, the communication bandwidth drops significantly from 184 GB/s to 15 GB/s when the communication occurs among non-adjacent GPUs as only adjacent GPUs have high-performance NVLink.
- The GPU topology of System II is therefore not friendly to 1D tensor parallelism which relies on all-reduce operations across all the GPUs via PCIe.
- Instead, the 2D and 2.5D only have communication between a pair of GPUs instead of across all GPUs.
- This allows part of the communication to still utilize the high NVLink bandwidth between adjacent GPUs.
- We trained ViT on the ImageNet-1k dataset with different configurations for 4 GPUs and 8 GPUs on both System I and II.
- On 4 GPUs, the ViT model has 64 Transformer layers with hidden size of 3072 and 48 attention heads.
- On 8 GPUs, the hidden size and the number of attention heads are adjusted to 4096 and 64 respectively as there is more memory available.
- The model is trained with different batch size settings until the out-of-memory problem occurs.
- As such, we present the best throughput for each tensor parallelism method.
- In Figure 14a, the throughput of 2D, 2.5D, and 3D tensor parallelism cannot compete with 1D tensor parallelism on both 4 GPUs and 8 GPUs.
- This is expected for two reasons. The first reason is that 1D tensor parallelism can utilize the high communication bandwidth with all GPUs involved on System I.
- The second reason is that 2D, 2.5D, and 3D tensor parallelism have more communication volume with a small number of processors and will only surpass 1D tensor parallelism when the number of processors increases.
- However, when the experiment is switched to System II in Figure 14b, 1D tensor parallelism will encounter bottleneck due to the low communication bandwidth in collective communication across 4 and 8 GPUs.
- Meanwhile, 2D and 2.5D can deliver a throughput which is 40% higher than that of 1D tensor parallelism with 4 GPUs.
- With 8 GPUs, 2.5D tensor parallelism can still outperform 1D tensor parallelism by 20.6%.
- 3D tensor parallelism still delivers lower performance than 1D tensor parallelism due to the low scaling.

## 4) Throughput Comparison
- The model is set to have 24 layers with hidden size of 2048 and 32 attention heads for 4 and 8 GPUs.
- From 16 GPUs onwards, the model is set to have 32 layers with hidden size of 4096 and 64 attention heads.
- The results for 4 to 64 GPUs are shown in Table III.
- It can be observed that as the number of GPUs increases, the speedup of advanced tensor parallelism over 1D tensor parallelism increases up to 2.76.
- This can be attributed to the lower communication volume of advanced tensor parallelism methods when scaling to more processors.

## C. Sequence Parallelism
- Sequence Parallelism is faster than 1D tensor parallelism in terms of memory efficiency and training throughput
- The maximum batch size of Sequence Parallelism is 4.44 times larger than that of the 1D tensor parallelism
- The maximum sequence length of Sequence Parallelism is 1.18 times larger than that of 1D tensor parallelism

## D. Sharding and Offloading
- Both Colossal-AI and DeepSpeed offload the optimizer data to the CPU memory
- With the extra optimization discussed in Section III-B, Colossal-AI is 2.33 times faster than DeepSpeed on 8 GPUs
- For middlesize models with a large amount of activations such as BERT-Base, sequence parallelism can deliver 1.43 times higher training throughput than 1D tensor parallelism
