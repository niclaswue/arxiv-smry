---
title: "Multi-scale Multi-band DenseNets for Audio Source Separation"
date: 2017-06-29T05:56:06.000Z
author: "Naoya Takahashi, Yuki Mitsufuji"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1706-09588v1.webp" # image path/url
    alt: "Multi-scale Multi-band DenseNets for Audio Source Separation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1706.09588)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1706.09588).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio).

# Abstract
- Deep neural networks are used to obtain instrumental spectra from a mixture
- A novel network architecture is proposed that extends the recently developed densely connected convolutional network (DenseNet)
- The proposed approach outperforms state-of-the-art results on SiSEC 2016 competition by a large margin in terms of signal-to-distortion ratio

# Paper Content

## INTRODUCTION
- Audio source separation has attracted considerable attention in the last decade
- Various approaches have been introduced so far such as local Gaussian modeling, non-negative factorization, kernel additive modeling, and combinations of those approaches
- Recently, deep neural networks (DNNs) based source separation methods has shown significant improvement in separation performance over earlier methods
- In [10,11], a standard feed-forward fully connected network (FNN) was used to obtain the source spectra
- As an input for the FNN, multiple frames (typically up to about 20 frames) were concatenated to take advantage of temporal contexts
- To model longer contexts, long short term memory (LSTM) was used in [12]
- Despite its good performance, the LSTM usually requires a relatively long training time, making it difficult to re-train the network to adapt to different domains or to explore the best architecture
- Another well-known architecture, Convolutional Neural Network (CNN) [13] has been very successful in image domain and also widely used in a variety of audio and video tasks
- As the convolution layers are stacked, a receptive field of the deeper layer covers a larger area of the input field, enabling the deep CNN architecture to take long contexts, as LSTM does. However, considerable depth is required to cover long contexts, making the network training difficult and leading to performance degradation
- Recent works, such as ResNets [18] and Highway Networks [19], address this problem by bypassing signals from one layer to the next via identity connections; this enable to successfully train the networks with more than 100 layers
- Most recently, a novel CNN architecture called densely connected convolutional networks (DenseNet) has shown excellent performance on image recognition task
- The idea of DenseNet is to use concatenation of output feature maps of preceding layers as the input to succeeding layers. Unlike ResNet, this iterative connection enables the network to learn explicit cross-layer interactions and reuses features computed in preceding layers, which yields efficient use of parameters
- This property suits the audio source separation problem very well because the goal of audio source separation is to estimate the instrumental spectrograms buried in interference sounds and the estimated source spectrograms could be brushed up more easily by referring the mixture or previous layer outputs. However, DenseNet is inherently memory demanding because the number of inter-layer connections grows quadratically with depth. Even for image recognition tasks involving relatively low resolution images (for instance 32 × 32 input and 10 or 100 output, as in CIFAR [21] and SVHN [22]), the authors used pooling layers to overcome the explosion of the number of feature maps.
- In audio source separation, both the input and output dimension would be far larger (e.g. 1024 frequency bins × 128 frames) in order to utilize sufficiently long contexts with high frequency resolution
- To address this problem, we propose a fully convolutional multi-scale DenseNet equipping dense blocks with multiple resolutions
- The input of lower resolution dense blocks is created by iteratively down-sampling the outputs from the preceding dense blocks. The low resolution dense blocks are then up-sampled to recover a higher resolution, and the result are fed to higher resolution dense blocks together with the output from the preceding dense blocks having same resolution, as shown in Fig. 2
- The lower resolution blocks capture the entire context while the higher resolution blocks recover details of the time-frequency structure in the spectrogram. This architecture enables the network to model both long contexts and fine-grained structures efficiently within a practical model size while maintaining the advantages of DenseNet.

## MULTI-SCALE MULTI-BAND DENSENET
- DenseNet architecture
- Up-scaling blocks and inter block skip connections
- Multi-band DenseNet architecture

## DenseNet
- In a standard feed forward network, the output of the lth layer is computed as x l = H l (x l−1 ), where the network input is denoted as x0 and H l (•) is a non-linear transformation which can be a composite function of operations such as Batch Normalization (BN) [24], rectified linear units (ReLU) [25], pooling, or convolution.
- In order to mitigate difficulties of training very deep models, ResNet [18] employs a skip connection which adds an identity mapping of the input to the non-linear transformation: The skip connection allows the network to propagate the gradient directly to the preceding layers, making the training of deep architectures easier.
- DenseNet [20] further improves the information flow between layers by replacing the simple addition of the output of a single preceding layer with a concatenation of all preceding layers: where [. . .] denotes the concatenation operation. Such dense connectivity enables all layers not only to receive the gradient directly but also to reuse features computed in preceding layers. This avoids the re-calculation of similar features in different layers, making the network highly parameter efficient.
- In the next section, We discuss how to apply these ideas to audio source separation.

## Multi-Scale DenseNet with block skip connection and transposed convolution
- Dense blocks and down-sampling layers comprise the downsampling path of the proposed multi-scale DenseNet.
- Downsampled feature maps enable the dense block network to model longer contexts and wider frequency range dependency while alleviating computational expense.
- In order to recover the original resolution from lower resolution feature maps, we introduce an upsampling layer defined as a transposed convolution whose filter size is same as the pooling size.
- We again alternate up-sampling layers and dense blocks to successively recover the higher resolution feature maps.
- In order to allow forward and backward signal flow without passing though lower resolution blocks, we also introduce inter-block skip connection which directly connect two dense blocks of the same scale.
- With this connection, dense blocks in the downsampling path are enabled to receive supervision and send the extracted features without compressing and decompressing them.

## Multi-band MDenseNet
- The kernels of the convolution layer are shared across the entire input field.
- In audio, different patterns occur in different frequency bands, though a certain amount of translation of patterns exists.
- Therefore, limiting the band that share the kernels is more suitable for efficiently capturing local patterns.
- Indeed, limited kernel sharing has been shown to be effective in speech recognition.
- We split the input into multiple bands and apply multiscale DenseNet to each band.
- However, simply splitting frequency band and modeling each band individually may hinder the ability to model the entire structure of spectrogram.
- Hence, we build in parallel an MDenseNet for the full band input and concatenate its output with outputs from multiple sub-band MDenseNets.

## Architecture details
- The proposed network architectures for audio source separation are described in Table 1
- One advantage of MMDenseNet is that we can design suitable architectures for each band individually
- In this work, we split the frequency into two bands in the middle and design a relatively larger model for the lower frequency band

## EXPERIMENTS

## Setup
- The DSD100 dataset is used to evaluate the proposed method
- The proposed method is able to separate songs into the four source instruments, or simply into the vocals and accompaniment track
- The method is able to estimate target spectrogram Si(t, f ) by minimizing the square error between the network output Ŝi(t, f ) and Si(t, f ), where f is the frequency bin index, t is the frame index and i ∈ I = {bass, drums, others, vocals} is the index of instruments
- The method was compared with other state-of-the-art approaches and BLEND showed the best performance

## Architecture validation
- The proposed multi-sale dense block enables the network to model the signal on different scales, i.e. the global context in the downscaled blocks and local fine-grained structure in the high resolution blocks.
- To validate if dense blocks at each scale actually contribute to recovering the target spectrogram, we computed the map-wise l2norm of filter weights of dense blocks in up-sampling path (dense 5, 6 and 7 in Table 1).
- The input of dense blocks in the up-sampling path is the concatenation of the output of the preceding up-sampling layer from down scaled block, and the skip connection from the dense block in down-sampling path, as in Fig. 2.
- By comparing the averaged l2-norm of the filter weights corresponding to the up- sampling path and the skip connection path, we can conjecture the contribution of dense blocks in different scale.
- Fig. 5 shows that the l2-norms of these two path are roughly the same, indicating that every dense block at different scale indeed contributes reasonably.

## Model efficiency
- The proposed architecture encourages feature reuse within and between dense blocks, leading to a compact and efficient model.
- To verify this, the number of parameters and the model training times are compared in Table 3. The number of parameters of the proposed architectures are significantly less than the baseline methods. MDenseNet achieved comparable performance to the sate-of-theart BLEND approach with only 1.5% of the parameters, and MM-DenseNet largely outperformed BLEND with only 3.6% of the parameters. This demonstrates the compactness and efficiency of the model, which is preferable for deployment.
- The training time is also significantly less than for the BLSTM and BLEND methods, making it easier to tune the hyper-parameters.

## CONCLUSION
- DenseNet is extended to tackle the audio source separation problem
- The proposed architectures have dense blocks at multiple scales connected though down-sampling and up-sampling layers, which enable the network to efficiently model both finegrained local structure and global structure
- Furthermore, a multi-band DenseNet is proposed to enable kernels in convolution layer to learn more effectively
- Experimental results on the SiSEC 2016 DSD100 dataset shows that our approach outperforms the state-of-the-art by a large margin, while reducing the model size and training time significantly
