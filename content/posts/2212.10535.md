---
title: "A Survey of Deep Learning for Mathematical Reasoning"
date: 2022-12-20T18:46:16.000Z
author: "Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10535v1.webp" # image path/url
    alt: "A Survey of Deep Learning for Mathematical Reasoning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10535)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10535).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-survey-of-deep-learning-for-mathematical).

# Abstract
- Mathematical reasoning is a fundamental aspect of human intelligence
- Mathematical reasoning is applicable in various fields, including science, engineering, finance, and everyday life
- The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing.
- For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances.
- On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning.
- In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.

# Paper Content

## Tasks and Datasets
- Deep learning methods are commonly used to study mathematical reasoning.
- A summary of the commonly used datasets in this field can be found in Table 2.

### Math Word Problem Solving
- Researchers have been interested in developing algorithms to solve math word problems automatically for decades
- A math word problem is a brief narrative that involves characters, entities, and quantities
- The mathematical relationship of an MWP can be modeled with a set of equations
- Most MWP datasets provide annotated equations as a rationale for the solution
- To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead

### Theorem Proving
- The problem of automating theorem proving is a long-standing challenge in AI
- Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers
- Example ITPs include Lean (Moura et al., 2015), Isabelle (Paulson, 1994), Coq (Barras et al., 1999), and Metamath (Megill and Wheeler, 2019)
- To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating "proof steps" until it is reduced to known facts
- Data sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries
- For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP
- For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs
- For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2022) provides an interactive REPL
- The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions
- Other resources provide proxy environments or tasks, such as INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization
- Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition
- Early applications of deep learning for formal theorem proving focus on selecting relevant premises

### Geometry Problem Solving
- Automated geometry problem solving (GPS) is a long-standing AI task in mathematical reasoning research
- Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram
- As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable
- GPS is a challenging task for deep learning methods due to the complex skills it requires
- It involves the ability to parse multimodal information, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning
- Some early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017)
- However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods
- In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs
- More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.

### Math Question Answering
- Numerical reasoning is a core ability within human intelligence and plays an important role in many NLP tasks
- Aside from theorem proving and gradelevel math word problem solving, there is a wide range of question answering (QA) benchmarks that center around mathematical reasoning
- In this work, we refer to these tasks as math question answering (MathQA)
- A large number of datasets have been presented recently. For example, QuaRel (Tafjord et al., 2019) is a dataset of diverse story questions that involve 19 different types of quantities. McTaco (Zhou et al., 2019) studies temporal commonsense problems, while Fermi (Kalyan et al., 2021) studies Fermi problems whose answers can only be approximately estimated.
- Recent studies have shown that state-of-the-art mathematical reasoning systems might suffer from brittleness in reasoning, in that the models rely on spurious signals and plug-and-chug calculations in the specific dataset to achieve "satisfactory" performance
- To address this issue, new benchmarks are proposed from various aspects. The Mathematics dataset (Saxton et al., 2020) consists of many different types of mathematics problems, covering arithmetic, algebra, probability, and calculus. The dataset allows for measuring the algebraic generalization ability of a model. Similarly, MATH (Hendrycks et al., 2021) consists of challenging competition mathematics to measure the problemsolving ability of models in complex scenarios.
- Some work incorporates tabular contexts in the question inputs. For example, FinQA (Chen et al., 2021c), TAT-QA (Zhu et al., 2021), and MultiHiertt (Zhao et al., 2022) collect questions that require both table understanding and numeric reasoning to answer. Others, instead, present large-scale unified benchmarks for mathematical reasoning. NumGLUE (Mishra et al., 2022b) is a multi-task benchmark with the goal of evaluating the performance of models on eight different tasks. Mishra et al. 2022a push this direction further and presents Lila, which consists of 23 mathematical reasoning tasks, spanning a wide range of mathematics topics, linguistic complexity, question formats, and background knowledge requirements.

### Other Quantitative Problems
- AI can reason with numbers
- Various benchmarks have been developed to evaluate AI's abilities
- FigureQA (Kahou et al., 2017), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a) are released to investigate models' abilities
- NumerSense (Lin et al., 2020) examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge
- EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework

### Graph-based Networks for Math
- Seq2Seq approaches generate mathematical expressions and don't rely on hand-crafted features
- Mathematical expressions can be transformed into a tree-based structure, e.g., an abstract syntax tree (AST) and a graphbased structure, which describes structured information in the expressions
- However, this important information is not explicitly modeled by Seq2Seq methods
- To solve this issue, graph-based neural networks are developed to explicitly model the structure in expressions

### Attention-based Networks for Math
- The attention mechanism has been successfully applied to natural language processing and computer vision problems
- Recently, researchers have been exploring its usefulness in mathematical reasoning tasks
- For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from longdistance dependency information learned by selfattention
- Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving and theorem proving

### Other Neural Networks for Math
- Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks.
- For geometry problem solving, a Graph Neural Network (GNN) is employed.
- WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019).
- MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving.
- However, though pre-trained language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning.
- First, pre-trained language models are not specifically trained on mathematical data.
- Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks.
- Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022;Chen et al., 2021c;Zhu et al., 2021) or diagrams (Lu et al., 2021a;Chen et al., 2022a;Lu et al., 2021b).
- To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.
- Lastly, though pre-trained language models can encode substantial amounts of linguistic information, it may be difficult for models to learn numerical representation or high-level reasoning skills just from the language modeling objective.

### Self-Supervised Learning for Math
- Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data.
- Table 4 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning.
- Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2018;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020).
- A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy.
- The study also mentions an interesting thresholding effect: "all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters".
- There is generally two types of pre-training corpus for mathematical language models. (i) Curated datasets from openly accessible sources. For example, Hendrycks et al. (2021) present the first large-scale mathematics pre-training dataset with step-by-step solutions in natural language and L A T E X, called the Auxiliary Mathematics Problems and Solutions (AMPS). AMPS consists of Khan Academy and Mathematica data. Minerva (Lewkowycz et al., 2022) collects a high-quality dataset containing scientific and mathematical data, which contains 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server. Thor (Jiang et al., 2022a) pre-trains a language model on the GitHub + arXiv subsets of The Pile (Gao et al., 2020).
- (ii) Synthetic datasets based on templates or interaction with engines. Recent work (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pretraining can actually provide substantial gains.
- Representative work includes TaPEx (Liu et al., 2022b), which obtains a pre-training corpus by automatically synthesizing executable SQL queries and their execution outputs. LISA (Jiang et al., 2021) extracts lemmas and theorems by interacting with the Isabelle standard library and the Archive of Formal Proofs. GenBERT (Geva et al., 2020) generats numerical and textual pre-training datasets based on manually crafted and extracted templates.
- Pre-training tasks. General pre-training language models have two typical self-supervised learning tasks: (i) Masked Language Modeling (MLM), where it randomly masks a portion of words in each sequence to predict the outcome; (ii) Causal Language Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens.
- Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical/scientific corpora for downstream tasks (Polu and Sutskever, 2020;Jiang et al., 2021;Hendrycks et al., 2021;Han et al., 2022;Lewkowycz et al., 2022;Jiang et al., 2022a).

### Task-specific Fine-tuning for Math
- Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task
- Existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as Math Word Problems (Kim et al., 2020;Shen et al., 2021;Yu et al., 2021b;Lu et al., 2021b;Cobbe et al., 2021;Li et al., 2022b;Jie et al., 2022;Ni et al., 2022;Mishra et al., 2022a;Welleck et al., 2022b), MathQA over financial tabular data (Zhao et al., 2022;Chen et al., 2021c;Zhu et al., 2021), Geometry (Lu et al., 2021a;Chen et al., 2022a;Cao and Xiao, 2022), Linear Algebra (Charton, 2021), and informal theorem proving (Welleck et al., 2022a).

## In-context Learning for Mathematical Reasoning

### In-context Example Selection
- Early chain-of-thought work randomly or heuristically selects in-context examples.
- However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples.
- To address the limitation, recent work has investigated various methods to optimize the incontext examples selection process (Rubin et al., 2022;Zhang et al., 2022b;Lu et al., 2022b;Yu et al., 2022;Fu et al., 2022).
- For example, Rubin et al. (2022) attempt to address this issue by retrieving semantically similar examples. However, this approach has been shown to work poorly on mathematical reasoning problems (Zhang et al., 2022b), and it is sometimes hard to measure the similarity if structured information (e.g., tables) is contained (Lu et al., 2022b).
- In addition, Fu et al. (2022) propose complexity-based prompting, which chooses examples with complex reasoning chains, i.e., chains with more reasoning steps, as the prompt.
- Lu et al. (2022b) propose a method for selecting in-context examples via reinforcement learning (RL). Specifically, an agent learns to find optimal in-context examples from a candidate pool, with the goal of maximizing the prediction rewards on given training examples when interacting with the GPT-3 environment.
- In addition, Zhang et al. (2022b) find diversifying demonstration questions could also improve model performance. They propose a two-step approach to construct in-context demonstrations: first, partitioning questions of a given dataset into a few clusters; second, selecting a representative question from each cluster and generating its reasoning chain using a zero-shot chain-of-thought with simple heuristics.

### High-quality Reasoning Chains
- Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt.
- However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary Table 6: In-context learning with large language models for mathematical reasoning.
- For GPT-3, all papers use the text-davinci-002 version; for Codex, all papers use the code-davinci-002.
- RL is short for reinforcement learning.
- To address this limitation, recent studies mainly focus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2022;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2022;Li et al., 2022a).
- Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.
- In least-to-most prompting (Zhou et al., 2022), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.
- Similarly, Khot et al. (2022) leverage diverse decomposition structures and use different prompts to answer each sub-question. Apart from these multi-step reasoning methods, Chen et al. (2022b); Gao et al. (2022) propose programof-thoughts (PoT), an alternative solution that uses large language models to express the reasoning process as a program.
- The computation is then relegated to an external computer, which executes the generated programs to derive the answer.
- Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al. (2022);Li et al., 2022a).
- Selfconsistency (Wang et al., 2022) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.
- In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through "self teaching", as a complementary solution to produce a higher degree of diversity.

## Discussion

### Analysis of Benchmarks
- Most existing benchmarks for mathematical reasoning have targeted the textual-only modality.
- However, visual elements can provide a rich source of quantitative information, making multi-modal datasets beneficial for reasoning over quantitative relations in natural images (Lu et al., 2022a), abstract diagrams (Lu et al., 2021b), figures (Kahou et al., 2017), and charts (Kafle et al., 2018).
- Tables, which are commonly found in daily documents and contain hierarchically structured information, have also been the focus of tasks that require quantitative reasoning over textual and tabular context (Chen et al., 2021c;Zhu et al., 2021;Zhao et al., 2022;Lu et al., 2022b).
- In addition, recent datasets have been developed for mathematical reasoning grounded on conversations (Sun et al., 2019;Zhang et al., 2021;Chen et al., 2022c), as well as reports (Chen et al., 2022c).
- Low-resource setting. Despite the creation of various datasets, mathematical reasoning in lowresource settings remains largely under-explored.
- Pioneering research has developed mathematical reasoning benchmarks for financial (Chen et al., 2021c;Zhu et al., 2021;Zhao et al., 2022) and scientific domains (Lu et al., 2022a). Additionally, there have been attempts to build non-English datasets for Chinese (Wang et al., 2017;Qin et al., 2020;Yu et al., 2021a) and Arabic (Alghamdi et al., 2022) for mathematical reasoning.
- To imitate the reasoning process of a human, a more recent trend is to annotate solutions in natural language (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b;Hendrycks et al., 2021;Lu et al., 2022a).

### Analysis of Deep Learning Methods
- Deep learning methods for numeracy are not robust and susceptible to adversarial attacks
- The standard practice for deep learning techniques is to treat numbers in the same way as words, which results in less frequent numbers being collapsed into an "UNK" token
- Two numbers on the same or close number line could have surface forms with no shared common tokens
- Few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task, but the question remains as to whether these methods are sufficiently advanced to tackle more complex problems

### Generalization and Robustness
- Neural models commonly display generalization and robustness failures on reasoning tasks
- For example, above we discussed difficulties in generalizing to larger numbers (Table 7) or remaining robust to nearby problems (Table 8), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)).
- One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.
- Another aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution?
- Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022).
- On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection.
- Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.

### Trustworthy Reasoning
- Language models can generate ungrounded answers that users must choose either to blindly accept or to verify themselves
- Even with recent prompting strategies that provide rationales before making predictions, language models still often hallucinate statements, produce flawed reasoning, and output wrong answers
- Methods that enable more trustworthy reasoning are urgently needed

### Learning from Feedback
- The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient
- Existing work about online learning (LeCun et al., 1998;Gimpel et al., 2010;Liang and Klein, 2009) and incorporating humans in the loop (Li et al., 2016;Wu et al., 2022a) is also related to this research direction.

### Multi-modal Mathematical Reasoning
- There is growing interest in multi-modal mathematical reasoning
- Currently available datasets in this domain tend to be small, generated from templates, or focus on specific topics
- One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems

## Conclusion
- In this paper, we present a comprehensive survey of deep learning for mathematical reasoning.
- We review the various tasks and datasets that have been used, and discuss the various approaches that have been taken, including early neural networks, later pre-trained language models, and recent large language models.
- We also identify several gaps in the existing datasets and methods, including limited focus on low-resource settings, insufficient numeracy representations, and inconsistent reasoning abilities.
- Finally, we outline directions for future research and highlight the potential for further exploration in this field.
- Our goal with this paper is to provide a comprehensive and useful resource for readers interested in the development of deep learning for mathematical reasoning.
