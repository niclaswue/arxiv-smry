---
title: "Debiasing NLP Models Without Demographic Information"
date: 2022-12-20T18:59:42.000Z
author: "Hadas Orgad, Yonatan Belinkov"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10563v1.webp" # image path/url
    alt: "Debiasing NLP Models Without Demographic Information" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10563)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10563).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/debiasing-nlp-models-without-demographic).

# Abstract
- Models trained from real-world data tend to imitate and amplify social biases.
- Although there are many methods suggested to mitigate biases, they require a
- In this work, we propose a debiasing method that operates without any prior
- Results on racial and gender bias demonstrate that it is possible to mitigate

# Paper Content

## Introduction
- Neural natural language processing models can suffer from social biases, such as performance disparities between genders or races
- Various debiasing methods have been proposed, with varying degrees of success
- A disadvantage of these methods is that they require knowledge of the biases one wishes to mitigate
- In this paper, we propose a new debiasing method that does not require demographic attribute
- The method we use relies on Debiased Focal Loss (DFL)
- The problem of social bias can be seen as a private case of robustness issues and can be viewed as a shortcut learning
- Based on this perspective, we propose to use DFL for social debiasing
- In contrast to the original implementation of DFL that explicitly defined biases to mitigate, we design the bias detection mechanism in a general manner such that the model's biases are learned and mitigated dynamically during training
- Our approach is demonstrated in Figure 1a
- Our methods are shown to be effective in bias mitigation, with the no-demographic method sometimes succeeding in cases where other methods that use demographic information fail

## Methodology

### Problem Formulation
- We consider general multiclass-classification problems.
- The dataset D = {x i , y i , z i } N i=1 consists of triples of input x i ∈ X , label y i ∈ Y, and a protected attribute z i ∈ Z which corresponds to a demographic group, such as gender.
- z might be latent, meaning that it cannot be accessed during training or validation stages.
- Our goal is to learn a mapping f M : X → R |Y | , such that f M , which we call the main model, is robust to differences in demographics as induced by z i ∈ Z.
- The robustness of a model is measured using a variety of fairness metrics.

### Fairness Metrics
- A fairness metric is a mapping from a model's predictions and the protected attributes to a numerical measure of bias
- The practical fairness metrics measured in this work are described in section Section 3.2

### Debiased Focal Loss for Social Bias
- The model f M can be written as a composition of two functions: g, the feature extractor, and h, the classifier, such that f M = h • g.
- In our case, g is a transformer language model such as BERT (Devlin et al., 2018), and f is a linear classification layer.
- Loss term. We use the DFL (Karimi Mahabadi et al., 2020) formulation, to re-weight examples that contain bias.
- To determine the re-weighting coefficients, we need a separate model that acts as a bias detector, f B .
- Karimi Mahabadi et al. (2022) explicitly defined the biases they are aiming to mitigate, and trained a biased model on the same task as the main model by feeding it with biased features.
- On the contrary, we feed the same features to the biased model and the main model, but the task of the biased model is different than the main task: Without demographics, the biased model only predicts whether the main model will succeed on each example.
- When demographics are incorporated, the biased model aims to predict the demographic annotation of the example.
- The loss on the parameters of the main model and the biased model, θ M and θ B respectively, is defined as:
- For a single instance (x, y, s), where σ(u) j = e u j / Y k=1 e u k is the softmax function, and γ is a hyper-parameter that controls the strength of the re-weighting.
- γ = 0 restores the vanilla negative log likelihood loss.

### Debiasing Without Demographic Annotations
- As in the sentiment classification task, debiasing  without demographic attributes tends to be less effective than debiasing using demographic attributes.
- Nevertheless, it is still successful in mitigating bias on some of the fairness metrics, while maintaining a small reduction in performance.
- Once again, the control model results are statistically indistinguishable from those of the baseline.

### Debiasing With Demographic Annotations
- The bias detector is a function that takes the features extracted from a model and classifies it for its demographic attributes.
- The bias detector is formulated as f B : g(X) → R |Z| .
- The bias detector is designed to align with the observation that this measure correlates with gender bias measures.
- The bias detector is designed to down-weight examples where it is hard to detect the demographic information and up-weight examples where it is easy to detect the demographic information.

## Experiments

### Tasks and Models
- We experiment with two classification tasks and bias types: Occupation Prediction and Gender.
- The dataset presented by De-Arteaga et al. (2019), which contains 400K biographies scraped from the internet.
- The task is predicting one's occupation based on a subset of their biography -without the first sentence, which states the occupation.
- The protected attribute is gender, and each instance is assigned binary genders based on the pronouns in the text, which indicate the individual self-identified gender.
- Sentiment Analysis and Race.
- We follow the setting of Elazar and Goldberg (2018), who used a twitter dataset collected by Blodgett et al. (2016).
- A subset of 100k examples is used.
- As a proxy for the writer's racial identity, each tweet is associated with a prediction about whether it is written in African American English or Standard American English based on the geo-location of the author.

### Metrics
- Measures the difference in performance between two demographic groups
- Measures the difference in performance between two groups of data
- Measures the difference in performance between two groups of labels
- Measures the difference in performance between two groups of data with different labels
- Measures the difference in performance between two groups of labels with different data
- Measures the difference in performance between two groups of data and labels
- Measures the difference in performance between two groups of labels and data
- Measures the difference in performance between two groups of labels and data with different labels

### Training and Evaluating
- Experiment with a BERT (Devlin et al., 2018) and DeBERTa V1 (He et al., 2020) based architectures
- Feature extractor is fed into a linear classifier
- Fine-tunes the feature extractor with the linear layer on the downstream task
- Adds another hyper parameter of temperature (t) to the softmax function of the biased model
- Increases the smoothness of the softmax result, resulting in a less radical regularization
- In each experiment, grid search γ ∈ {1, 2, 4, 8, 16} and t ∈ {1, 2, 4, 8}
- Since we are interested in balancing the importance of task accuracy and fairness, we adapt the 'distnace to optimum' (DTO) introduced by Han et al. (2021a)
- If no demographic information is available, we could require demographics for the validation and test set, which are typically much smaller than the train set, and therefore require much less effort to annotate.

### Baselines and Competitive Systems
- The same model architecture, optimized to solve the downstream task without any debiasing mechanism.
- A post-hoc method for removing linear information from the neural representation of a trained model, by repeatedly training a linear classifier to predict a property (in our case, gender or race) from the neural representation, and then projecting the neural representations to the null space of the linear classifier.
- The goal of this method is also to linearly remove information from the neural representations of a trained model by utilizing a different strategy based on a linear minimax game.
- DFL trained with demographic annotations, as described in Section 2.5.
- DFL trained without demographic annotations, but tuned for hyperparameters using a validation set with demographic annotations. We present it only when it achieved different results than DFL-no demog.

## Results
- Accuracy is measured by the F-measure and is found to be good for both training and testing
- Fairness metrics are measured by the AUC and are found to be good for both training and testing
- The F-measure is found to be better than the AUC for both accuracy and fairness
- The F-measure is also found to be more accurate than the AUC for a representative subset of fairness metrics

### Sentiment classification
- The vanilla fine-tuning baseline yields the best accuracy, but also the worst bias (highest fairness metrics), on both BERT and DeBERTa.
- Debiasing With Demographic Annotations. We first focus on DFL trained with a biased model that predicts demographic attributes, race in this case. DFL leads to a statistically significant reduction of bias compared to the fine-tuned baseline in all metrics, with a minor drop in accuracy (2-3% absolute).
- method to reduce bias in this setting while maintaining high performacne on the downstream task.
- Debiasing Without Demographic Annotations. Next we examine the effectiveness of DFL when it has no access to demographic attributes, instead using a success detector as a proxy for a biased model.
- Remarkably, we observe a statistically significant reduction of bias compared to the fine-tuned model in BERT.
- In DeBERTa, choosing γ, t based on the reduction in accuracy results does not results in bias reduction.
- Using demographic information for the hyper-parameter choice only resulted in significant bias reduction.
- Reduction in accuracy is minimal, as before.
- Comparing DFL with and without attributes, the model trained with attributes leads to much lower fairness metrics, both in BERT and DeBERTa.

### Occupation prediction
- Next, the paper discusses how to reduce gender bias in classification models.
- The paper finds that the DFL loss (Equation (1)) is more effective at reducing bias than other methods.
- Accuracy decreases as bias decreases, so choosing a model based on accuracy is reasonable.

### Effect of Debiasing on Internal Model Representations
- Debiasing with DFL decreases the amount of demographic information encoded in the neural representations of the model
- This debiasing process is successful in reducing bias metrics associated with these demographics

## Previous Work
- The literature suggests a variety of ways to debias a NLP model from social biases
- All these methods require that we define the bias we want to address
- Focal loss was proposed as a method for addressing class imbalances by down-weighting the loss associated with wellclassified examples
- Debiased Focal Loss (DFL) is a variant of Focal loss proposed to improve NLU models on outof-distribution data, by down-weighting examples that are well-classified by a separate, biased model, which only receives biased features as input.

## Discussion
- We demonstrated reduction of racial and gender biases without any prior knowledge of those biases in the data
- In light of these results, we can potentially apply this method to any dataset, making bias reduction a much more feasible endeavor
- Weakness of our method was that sometimes it was not possible to debias without an annotated validation set, though most of the time the simple rule of selecting the model that had the most accuracy reduction resulted in most debiasing
- While we were able to mitigate some biases in the data without demographic attributes, the method was still less effective than other methods that used demographic attributes
- The reason may have been that it is less focused and may also mitigate other biases in the data that we have not identified
- A variation of the algorithm in which only examples where the main model was correct are considered bias might lead to a more focused debiasing
- These directions will be addressed in future research.

## A Bias Metrics
- DTO is measured as the L2-distance from a utopia point.
- Accuracy, fairness are the candidate points, where fairness is determined by averaging the various fairness metrics measured in this study.

### F.2 Full probing results
- Figure 1: Our proposed debiasing methods.
- Figure 2: Sentiment classification: effect of γ across different softmax temperatures (t).
- Figure 3: Occupation prediction: effect of γ across different softmax temperatures (t).
- Figure 4: Effect of γ on compression.
