---
title: "Image-to-Markup Generation with Coarse-to-Fine Attention"
date: 2016-09-16T08:14:50.000Z
author: "Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander M. Rush"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/1609-04938v2.webp" # image path/url
    alt: "Image-to-Markup Generation with Coarse-to-Fine Attention" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/1609.04938)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/1609.04938).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/image-to-markup-generation-with-coarse-to).

# Abstract
- We present a neural encoder-decoder model to convert images into presentational markup
- Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a
- To reduce the inference complexity associated with the attention-based

# Paper Content

## Introduction
- Optical character recognition is used to recognize text from an image
- There has been research interest in converting images into structured language or markup that defines both the text itself and its presentational semantics
- The primary target for this research is OCR for mathematical expressions
- There are a variety of systems that have been developed to address this task
- One approach is to use a specialized character segmentation with grammars of the underlying mathematical layout language
- Another approach is to use a data-driven model that is adaptable to a wide range of datasets
- In this work, we explore the use of attention-based imageto-text models for the problem of generating structured markup from an image
- Our model incorporates a multi-layer convolutional network over the image with an attention-based recurrent neural network decoder
- To adapt this model to the OCR problem and capture the document's layout, we also incorporate a new source encoder layer in the form of a multi-row recurrent model

## Problem: Image-to-Markup Generation
- The image-to-markup problem is converting a rendered source image to target presentational markup.
- The source, x, consists of an image.
- The target, y, consists of a sequence of tokens y 1 , y 2 , • • • , y T where T is the length of the output, and each y is a token in the markup language.
- The rendering is defined by a possibly unknown, many-to-one, compile function, compile.
- In practice this function may be quite complicated, e.g. a browser, or ill-specified, e.g. the LaTeX language.
- The supervised task is to learn to approximately invert the compile function using supervised examples of its behavior.
- We assume that we are given instances (x, y), with possibly differing dimensions and that, compile(y) ≈ x, for all training pairs (x, y) (assuming possible noise).
- At test time, the system is given a raw input x rendered from ground-truth y.
- It generates a hypothesis ŷ that can then be rendered by the black-box function x = compile(ŷ).
- Evaluation is done between x and x, i.e. the aim is to produce similar rendered images while ŷ may or may not be similar to the ground-truth markup y.

## Model
- The model uses a full grid encoder over the input image, so that it can support non left-to-right order in the generated markup.
- The encoder is adapted from the encoder of Xu et al. (2015) developed for image captioning. Notably, though, our model also includes a row encoder which helps the performance of the system.
- The model first extracts image features using a convolutional neural network (CNN) and arranges the features in a grid. Each row is then encoded using a recurrent neural network (RNN). These encoded features are then used by an RNN decoder with a visual attention mechanism.
- The decoder implements a conditional language model over the vocabulary, and the whole model is trained to maximize the likelihood of the observed markup.
- The full structure is illustrated in Figure 2.

## Attention in Markup Generation
- The accuracy of the model is dependent on being able to track the next current position of the image for generating markup, which is conveyed through an attentive context vector.
- In practice, the attention distribution is parameterized as part of the model.
- We consider three forms of attention: standard, hierarchical, and coarse-to-fine.
- Standard Attention In standard attention (Bahdanau et al., 2014), we use a neural network to approximate the attention distribution p(z t ): where a(•) is a neural network to produce unnormalized attention weights.
- Hierarchical Attention When producing a target symbol from an image, we can infer the rough region where it is likely to appear from the last generated symbol with high probability. In addition to the fine grid, we therefore also impose a grid over the image, such that each cell belongs to a larger region.
- When producing the markup, we first attend to the coarse grid to get the relevant coarse cell(s), and then attend to the inside fine cells to get the context vector, a method known as hierarchical attention.
- For this problem, define V as a coarse grid of size H × W , which we construct by running additional convolution and pooling layers and row encoders on top of Ṽ.
- We also introduce a latent attention variable z t that indicates the parent level cell of the attended cell, and write p(z t ) as p(z t ) = , where we first generate a coarse-level cell z t followed by a fine-level cell z t only from within it.
- We parameterize p(z t ) and p(z t |z t ) as part of the model.
- For p(z t ), we employ a standard attention mechanism over V to approximate the probability in time O(H W ).
- For the conditional p(z t |z t ), we also employ a standard attention mechanism to get as before, except that we only consider the fine-level cells within coarse-level cell z t .
- Note that computing p(z t |z t ) takes time O( H H W W ). However to compute the p(z t ) even with this hierarchical attention, still requires O(HW ) as in standard attention.
- Coarse-to-Fine Attention Ideally we could consider a reduced set of possible coarse cells in hierarchical attention to reduce time complexity.
- Borrowing the name coarse-tofine inference (Raphael, 2001) we experiment with methods to construct a coarse attention p(z t ) with a sparse support to reduce the number of fine attention cells we consider.
- We use two different approaches for training this sparse coarse distribution.
- For the first approach we use sparsemax attention (Martins & Astudillo, 2016) where instead of using a softmax for p(z t ) at the coarse-level, we substitute a Euclidean projection onto the simplex.
- The sparsemax function is defined as, sparsemax(p) = argmin q∈∆ K−1 q−p 2 , where ∆ K−1 is the probability simplex and K denotes the number of classes.
- For the second approach we use "hard" attention for z t , an approach which has been shown to work in several image tasks (Xu et al., 2015;Mnih et al., 2014;Ba et al., 2015).
- We aim to maximize the total expected reward E z t [ T t=1 r t ], or equivalently minimize the negative expected reward as our loss.
- For parameters θ that precede the nondifferentiable z t in the stochastic computation graph, we backpropagate a gradient of the form r t • ∂ log p(z t ;θ) ∂θ . This gives us an unbiased estimate of the loss function gradient...

## Dataset Construction
- The IM2LATEX-100K dataset provides 103,556 different LaTeX math equations along with rendered pictures.
- We extract formulas by parsing LaTeX sources of papers from tasks I and II of the 2003 KDD cup (Gehrke et al., 2003), which contain over 60,000 papers.
- We extract formulas from the LaTeX sources with regular expressions, and only keep matches whose number of characters fall in the range from 40 to 1024 to avoid single symbols or text sentences.
- With these settings we extract over 800,000 different formulas, out of which around 100,000 are rendered in a vanilla LaTeX environment.
- Rendering is done with pdflatex
- Tokenization Training the model requires settling on a token set. One option is to use a purely character-based model. While this method requires fewer assumptions, character-based models would be significantly more memory intensive than word-based models due to longer target sequences.
- Therefore original markup is simply split into minimal meaningful LaTeX tokens, e.g. for observed characters, symbols such as \sigma, modifier characters such as ˆ, functions, accents, environments, brackets and other miscellaneous commands.
- Finally we note that naturally occurring LaTeX contains many different expressions that produce identical output. We therefore experiment with an optional normalization step to eliminate spurious ambiguity (prior to training).
- For normalization, we wrote a LaTeX parser5 to convert the markup to an abstract syntax tree. We then apply a set of safe normalizing tree transformation to eliminate common spurious ambiguity, such as fixing the order of sub-superscripts and transforming matrices to arrays.
- Surprisingly we find this additional step gives only a small accuracy gain, and is not necessary for strong results.

## Experiments
- The proposed model, IM2TEX, is compared to classical OCR baselines, neural models, and model ablations on the image-to-LaTeX task.
- InftyReader, an implementation of the INFTY system of (Suzuki et al., 2003), is compared to the proposed model.
- The model with standard attention has 9.48 million parameters, and the models with hierarchical or coarse-to-fine attention have 15.85 million parameters due to the additional convolution layers and row encoders.
- The model is trained end-to-end to maximize the likelihood of the training data.

## Results
- The main experimental results, shown at the top of Table 2, compare different systems on the image-to-markup task.
- The INFTY system is able to do quite well in terms of text accuracy, but performs poorly on exact match image metrics.
- The poor results of the neural CTC system validate our expectation that the strict left-to-right order assumption is unsuitable in this case.
- Our reimplementation of im-
- We next compare the different hierarchical and coarse-tofine extensions to the system. We first note that the use of the coarse-only system leads to a large drop in accuracy, indicating that fine attention is crucial to performance.
- On the other hand, the high performance of hierarchical indicates that two layers of soft-attention do not hurt the performance of the model.
- Table 4 shows the average number of cells being attended to at both the coarse and fine layers by each of the models. Both the hard REINFORCE system and sparsemax reduce lookups at a small cost in accuracy. Hard is the most aggressive, selecting a single coarse cell. Sparsemax achieves higher accuracy, at the cost of selecting multiple coarse cells. Depending on the application, these are both reasonable alternatives to reduce the number of lookups in standard attention.
- Our final experiments look at the CROHME 2013 and 2014 datasets, which were designed as a stroke recognition task, but are the closest existing dataset to our task. For this dataset we first train with our synthetic handwriting dataset and then fine-tune on the CROHME training set. We find our models achieve comparable performance to all best systems excepting MyScript, a commercial system with access to additional in-domain data.
- Note that our synthetic dataset does not contain variation in baselines, font sizes, or other noise, which are common in real data. We expect increased performance from the system when trained with well-engineered data.
- For these datasets we also use the hierarchical and coarse-to-fine models, and find that they are similarly effective. Interestingly, contrary to the full data for some problems hard performs better than sparsemax.
- Analysis To better understand the contribution of each part of the standard IM2TEX model, we run ablation experiments removing different features from the model, which are shown in Table 3. The simplest model is a basic (nonconditional) NGRAM LM on LaTeX which achieves a perplexity of around 8. Simply switching to an LSTM-LM reduces the value to 5, likely due to its ability to count parentheses and nesting-levels. These values are quite low, indicating strong regularity just in the LaTeX alone. Adding back the image data with a CNN further reduces the perplexity down to 1.18. Adding the encoder LSTM adds a small gain to 1.12, but makes a large difference in final accuracy. Adding the positional embeddings (trainable initial states for each row) provides a tiny gain.
- Hard attention leads to a small increase in perplexity. We also consider the effect of training data on performance. Figure 4 shows accuracy of the system with different training set size using standard attention. As with many neural systems, the model is quite data hungry. In order for the model to reach ≥ 50% accuracy, at least 16k training examples are needed. Finally Figure 5 illustrates several common errors. Qualitatively the system is quite accurate on difficult LaTeX constructs. Typically the structure of the expression is preserved with one or two symbol recognition errors. We find that the most common presentation-affecting errors come from font or sizing issues, such as using small parentheses instead of large ones, using standard math font instead of escaping or using mathcal.

## Conclusion
- We have presented a visual attention-based model for OCR of presentational markup.
- We also introduce a new dataset IM2LATEX-100K that provides a test-bed for this task.
- In order to reduce the attention complexity, we propose a coarse-to-fine attention layer, which selects a region by using a coarse view of the image, and use the fine-grained cells within.
- These contributions provide a new view on the task of structured text OCR, and show data-driven models can be effective without any knowledge of the language.
- The coarse-to-fine attention mechanism is general and directly applicable to other domains, including applying the proposed coarse-to-fine attention layer to other tasks such as document summarization, or combining the proposed model with neural inference machines such as memory networks.
