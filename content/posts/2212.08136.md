---
title: "Efficient Long Sequence Modeling via State Space Augmented Transformer"
date: 2022-12-15T20:51:27.000Z
author: "Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-08136v1.webp" # image path/url
    alt: "Efficient Long Sequence Modeling via State Space Augmented Transformer" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.08136)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.08136).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/efficient-long-sequence-modeling-via-state).

# Abstract
- Transformer models have achieved superior performance in various natural language processing tasks
- However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences
- There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information
- In parallel to Transformers, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information
- We propose SPADE, short for $\underline{\textbf{S}}$tate
- Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the

# Paper Content

## Introduction
- Transformers models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019;He et al., 2021).
- Full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019).
- Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases.
- In approximation methods, we approximate the full attention using fast algorithms with linear complexity.
- For example, we can approximate and speedup the computation of the attention score matrix (i.e., softmax(QK / âˆš d) in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021).
- However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue.
- To incorporate structural biases to the Transformer model, partial attention methods are pro-arXiv:2212.08136v1 [cs.CL] 15 Dec 2022 posed.
- Such methods can be further categorized into sparse attention and clustering methods.
- In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns.
- In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed.
- However, the introduced structural biases restrict the models' ability to capture global information.
- For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information.
- Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation.
- These models can be seen as linear recurrent neural networks with specifically designed fixed weights.
- Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information.
- This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens.
- We propose SPADE, short for State sPace AugmenteD TransformEr.
- The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information.
- In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods.
- We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines. Finally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method.

## Background

### Attention Mechanism
- The attention mechanism can simultaneously compute the alignment between any pair of input tokens.
- The attention score matrix captures the alignment between the input tokens.

### State Space Models
- Continuous time latent space model
- Eq. 2 can be used to model long sequences
- HiPPO matrices are proposed to initialize A
- S4 model is developed to efficiently compute Eq. 4

## Method
- We first conduct experiments to demonstrate that SSMs do not model local information well.
- Then, we present SPADE, which effectively combines global and local information by augmenting SSMs into the Transformer architecture.

### Attention vs. State Space Models
- SSMs perform poorly on language modeling tasks
- To demonstrate such an observation, we compare S4 with Transformer with full attention and Transformer with local (window and chunk) attention
- In local attention, each token can only attend to its neighboring tokens
- We conduct experiments on token-level language modeling
- In this setting, local information is more important than global information
- We see that both Transformer with full attention and Transformer with local attention (e.g., window and chunk) outperforms S4
- Notice that replacing full attention with local attention does not significantly hurt model performance, indicating that local information is more important in this setting
- SPADE is flexible to accommodate different approaches, such as window attention and chunk attention

## Experiments
- The experiments were done using PyTorch and Fairseq.
- Details about the hyperparameters for the models are in the appendix.

### Long Range Arena

### Language Modeling
- We evaluate our model by conducting language modeling experiments on Wikitext-103.
- The dataset contains English-language Wikipedia arti- Transformer (Vaswani et al., 2017)
- 18.8 19.5 SPADE (softmax-window)
- significant performance improvement and outperform all the baselines.
- For example, the vanilla window attention has a 19.7 perplexity on the test set, and by integrating a SSM into SPADE, we achieve a 1.2 perplexity gain.
- We remark that we do not need to train the S4 in the bottom layer of SPADE to achieve the performance in Table 2. That is, we initialize the parameters in S4 using Eq. 5, and the parameters are frozen during training.

## Language Model Pre-Training
- We implement model pre-training using Fairseq
- We implement model fine-tuning using MT-DNN

### Pre-Training Details
- Pre-train an encoder-decoder variant of SPADE
- The model architecture is the same as T5 base (Raffel et al., 2020), except that we use postlayernorm instead of pre-layernorm to improve model performance
- The embedding dimension is 768, the hidden dimension of the FFN is 3072, the number of attention heads is 12, and both the encoder and the decoder have 12 layers
- We add a S4 module to the bottom layer of SPADE, and the parameters of the S4 are fixed after initialization
- We use the softmax-window attention as the local information extractor

### Natural Language Understanding
- Fine-tune pre-trained models on General Language Understanding Evaluation (GLUE) benchmark
- Includes two single-sentence classification tasks: CoLA (Warstadt et al., 2019) is a linguistic acceptability task; and SST-2 (Socher et al., 2013) is a binary classification task that classifies movie reviews to positive or negative
- Also contains three similarity and paraphrase tasks: STS-B (Cer et al., 2017) is a text similarity task; MRPC (Dolan and Brockett, 2005) is a paraphrase detection task; and QQP is a duplication detection task
- Includes natural language inference tasks: MNLI (Williams et al., 2018); QNLI (Rajpurkar et al., 2016); RTE (Dagan   et al., 2006;Bar-Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009)

### Natural Language Generation
- We fine-tune the pre-trained models on several abstractive summarization datasets
- The sources and statistics are summarized in Table 4
- We use ROUGE-2 scores as the evaluation metric
- We compare SPADE with LongT5 (Guo et al., 2022), which is a T5 variant that is tailored for long sequences
- Note that LongT5 uses PEGASUSstyle (Zhang et al., 2020) Principle Sentences Generation as its pre-training objective, such that LongT5 is tailored for natural language generation tasks (e.g., summarization)
- In contrast, SPADE is a more general model in that it also has superior performance on natural language understanding tasks
- We remark that LongT5 uses C4 (Raffel et al., 2020) as its pre-training dataset, which is much larger and diverse than the dataset we adopted
- Experimental results are summarized in Table 5
- From the results, we see that our model significantly outperforms LongT5
- Note that SPADE base++ have about 290M parameters, while LongT5 large contains about 770M parameters and LongT5 xl contains about 3B parameters

### Efficiency Comparison
- SPADE is more efficient than other models
- SPADE is faster than Transformer
- SPADE uses less GPU memory

### Location and Number of Global Layers
- In SPADE, the bottom layer is equipped with a SSM and serves as the global layer, while the rest are local layers
- From the results, we see that model performance decreases as we use more global layers
- This is because the SSM in the bottom layer captures and filters out global information, such that subsequent SSMs only introduce noise to the intermediate representations

### Different Configurations
- We examine how performance changes when we change the sequence length and window size.
- From Figure 6 (left), we see that when we increase the sequence length from 512 to 3k, performance of Transformer with full attention increases. However, when we further increase the sequence length to 4k, model performance drastically drops. This is because in long sequences, the signal-tonoise ratio is low, such that the full attention may easily fit to the noise.
- From Figure 6 (right), we see that performance of Transformer with window attention increases when we increase the window size. Moreover, model performance is better with shorter sequences for the same window size. Such findings indicate that performance of window attention depends on the proportion of information within its perception.
- From Table 6, we see that for the same sequence length, performance of SPADE increases when we increase the window size. Also, we see that performance of SPADE marginally decreases when we increase the sequence length from 4k to 6k.

### Pre-Trained Language Models
- Pre-trained language models (Devlin et al., 2019;Liu et al., 2019b;Raffel et al., 2020;Brown et al., 2020;He et al., 2021) have achieved state-of-the-art performance on various natural language processing tasks.
- However, most of these models are not suitable for long sequences. For example, BERT (Devlin et al., 2019) uses a fixed-length positional embedding, such that it cannot handle sequences with length more than 512.
- In contrast, LongT5 (Guo et al., 2022) facilitates training on long sequences by leveraging relative positional embedding (Shaw et al., 2018) and efficient attention methods.

## Conclusion
- SPADE is a multi-layer Transformer model that targets long sequence modeling
- The bottom layer is a global layer and the rest are local layers
- In the global layer, we use a SSM to augment coarse global information, which are subsequently refined by the following local layers
- We instantiate the local layers with off-the-shelf efficient attention methods, such as window attention
- The proposed model has linear time and space computationally complexity, facilitating it to handle long sequences
- We conduct extensive experiments on the Long Range Arena (LRA) benchmark and language modeling datasets to demonstrate the effectiveness and efficiency of SPADE
- We also pre-train encoder-decoder models to demonstrate the scalability of SPADE
- In all the experiments, SPADE exhibits superior performance and outperforms the baselines
