---
title: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"
date: 2023-01-09T18:59:50.000Z
author: "Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, Zehuan Yuan"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-03580v2.webp" # image path/url
    alt: "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.03580)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.03580).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/designing-bert-for-convolutional-networks).

# Abstract
- Convolution operation cannot handle irregular, random-masked input images
- BERT pre-training is inconsistent with convnet's hierarchical structure
- We develop a hierarchical decoder to reconstruct images from multi-scale encoded features

# Paper Content

## INTRODUCTION
- The pretrain-finetune paradigm in natural language processing (NLP), popularized by BERT (Devlin et al., 2018;Dong et al., 2019;Clark et al., 2020) and GPT (Radford et al., 2019;Brown et al., 2020), is remarkably effective and thus long envied by our vision community.
- A bold move that increases the mask ratio to a staggering level (60~75%) is largely credited with this success (He et al., 2021;Xie et al., 2021).
- And the field of visual self-supervised learning on ViTs (Dosovitskiy et al., 2020;Liu et al., 2021) has now shifted from contrastive learning (Grill et al., 2020;Chen et al., 2021;Caron et al., 2021) to BERT-style masked modeling or a fusion of them (Zhou et al., 2021).
- Nonetheless, extending the success of BERT pre-training from transformers to convolutional networks (convnets) is a wonderful, but unrealized vision.
- The pioneering work (Pathak et al., 2016;Zhang et al., 2017) preceded BERT but performed much worse than supervised pre-training.
- There have been efforts over the past year trying to port BERT to convnets, yet eventually compromise on proposing a non-convolutional model (Gao et al., 2022) or non-masked modeling (Fang et al., 2022).
- One may therefore ask: what exactly is preventing the application of BERT to convnets?
- We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014;He et al., 2021).
- Typical NLP models like recurrent networks or transformers process text as a variable-length sequence of words (well-defined semantic units), while convnets have to recognize objects of different sizes from raw pixels (like "units" at different scales).
- This huge disparity rises two challenges: (i) Removing the information of masked "words" is difficult for convnets.
- For ViTs, an input image is divided into several non-overlapping patches. Simply dropping masked patches or replacing them with mask tokens can remove the information. This ease relies on transformer being able to handle irregular (variable-length) and (a) illustrates MAE (He et al., 2021) that has no such side effect thanks to the transformer's ability to process variable-length input.
- We propose (c) to adapt convnets to irregular masked input without a distribution shift.
- non-overlapping patches, thus cannot be achieved on convnets as they not only operate on regular grids, but also perform sliding window with overlapping.
- One may zero-out all masked pixels and feed this "mosaic" into a convnet. This, however, leads to a severe data distribution shift (as in figure 1) and other issues (discussed later in section 3.1 and figure 3), thus cannot be an ideal solution.
- (ii) Single-scale algorithm cannot learn multi-scale (hierarchical) features.
- Multi-scale structure has long been a gold standard in computer vision, which allows visual processing systems like SIFT descriptors (Lowe, 1999;Bay et al., 2006) and pyramid networks (He et al., 2015;Lin et al., 2017) to cope with changes in object scale.
- In contrast, masked modeling from NLP originally works in a single-scale manner. Applying it directly on convnets will miss the advantage of model hierarchy.
- In this work, we clear the hurdles above and make BERT...

## RELATED WORK

## HIERARCHICAL VISUAL PROCESSING SYSTEMS
- Hierarchical structure is acknowledged as a gold standard for visual representation systems
- Many fundamental handcrafted feature descriptors extract multi-scale visual representations via scale-space extremum on feature pyramid
- The crux behind this hierarchical design is to extract scale-invariant (or equivariant) features, thus, allows the system to cope with varying object sizes (scales)
- Widely used in visual tasks (Felzenszwalb et al., 2008;Yang et al., 2009), these descriptors also motivate the design principles of convolutional networks (He et al., 2016;Tan & Le, 2019;Liu et al., 2022)

## RECENT PROGRESS ON VISUAL SELF-SUPERVISED LEARNING
- Recently, the contrastive learning formulates self-supervise learning as an instance classification task
- Efforts have been made (Grill et al., 2020;Caron et al., 2020;Chen & He, 2021) to overcome the core issue of mode collapse
- More advanced methods are developed since then (Tian et al., 2020;Zbontar et al., 2021;Chen et al., 2021;Caron et al., 2021), and this line of work had dominated the area of visual unsupervised learning until masked generative pre-training along with the vision transformer architecture came into view.
- Masked image modeling, inspired by the recent success of masked language modeling in natural language processing (NLP) (Devlin et al., 2018;Liu et al., 2019) as targets due to their rich semantics.
- Gao et al. (2022) designs a transformer with a heavier patchifier to perform masked modeling. So far, enormous studies have successfully verified the efficacy of these algorithms on vision transformers (Zhou et al., 2021;Chen et al., 2022;Fang et al., 2022). However, on the other hand, their methodology is almost the same as that in NLP (Devlin et al., 2018;Liu et al., 2019), and is therefore difficult to be used for hierarchical convolutional models -on convnets, contrastive learning still remains state-of-the-art.

## SPARSE CONVOLUTION FOR VISUAL REPRESENTATION
- Convolution is widely used in 2D computer vision
- When facing with 3D point clouds, this operator quickly becomes unaffordable due to the cubic increasing number of grids (voxels)
- Considering point clouds are highly sparse and irregular, one can skip all empty voxels for speed
- This motivates the sparse convolution (sparseconv) (Liu et al., 2015)
- Minkowski Engine (Choy et al., 2019) is one of the most common sparseconv frameworks
- Some prior arts (Verelst & Tuytelaars, 2020) also tried to introduce sparseconv for faster 2D visual understanding
- And in this work, we have observed the similarity between 3D point clouds and 2D masked images in BERT-style pre-training
- We thus use sparseconv, for the first time, with the purpose of "facilitating the adaptation of convnet to BERT masked modeling", rather than of "speeding up the computation of convolution".

## APPROACH
- Illustrated in figure 2, our SparK framework aims to pre-train a convolutional network encoder via hierarchical masked image modeling
- We are going to detail SparK by introducing a sparse masking strategy (section 3.1), a hierarchical encoderdecoder architecture (section 3.2), and the optimization target of SparK pre-training (section 3.3)
- After pre-training, only the encoder is used for downstream tasks.

## SPARSELY GATHERING UNMASKED PATCHES
- The patch-wise masking strategy widely used in masked image modeling
- An image is divided into several non-overlapping square patches, each of which will then be masked independently with a given probability called mask ratio
- Previous transformer-based masked modeling can easily eliminate the information by directly removing masked patches or replacing them with a mask token
- To overcome the problems, we propose to sparsely gather all unmasked patches into a sparse image, and then use sparse convolutions to encode it

## HIERARCHICAL ENCODING AND DECODING
- The encoder will generate a set of feature maps with different resolutions
- These feature maps will be used to decode
- The encoder will use a hierarchical encoding

## Raw Mask
- After Ordinary Conv., the decoder follows the design of UNet (Ronneberger et al., 2015).
- We use a relatively light decoder that contains three successive blocks {B 3 , B 2 , B 1 } with upsampling layers.
- Before reconstructing a dense image, it is necessary to fill in all the empty positions on sparse feature maps. This is called "densifying".
- Taking the smallest sparse feature S 4 as example, all empty positions (inactive sites) on S 4 are filled with a mask embedding [M 4 ] to get a dense feature S 4 .
- A projection layer φ 4 is applied then, in case encoder and decoder have different network widths: Note that four different mask embeddings [M 4~1 ] and projection layers φ 4~1 are required: they belong to different scales, and may have different network widths.
- The final output of decoder is D 1 .

## OPTIMIZATION TARGET AND TRANSFERRING TO DOWNSTREAM
- To reconstruct an image from D, a head module h is needed, which should include two more upsampling layers to reach the original resolution of input H×W.
- As for the reconstruction target, we choose per-patch normalized pixels as targets with an L-loss, and calculate errors only on masked positions.
- These designs have been proven to facilitate models to learn more informative features in He et al. (2021), and are also verified by the ablation study later in section 4.5.
- After pre-training, we discard the decoder and only use the encoder for downstream tasks.
- When fine-tuning, the pre-trained sparse encoder can be directly generalized to dense images without any tuning, due to the fact that dense input is a special case of the sparse, where every position is active.

## EMPIRICAL RESULTS

## IMPLEMENTATION DETAILS
- Components: SparK can use any convolutional network as the encoder, without any special design of the backbone architecture.
- Positional embeddings are not used since convnet already encodes the spatial information.
- We pre-train all models with 1.28 million unlabeled images from ImageNet-1K (Deng et al., 2009) training set.
- We use the same mask patch size (32) and ratio (60%) as in SimMIM (Xie et al., 2021).
- We train with a LAMB optimizer (You et al., 2019), a batch size of 4096, and a cosine-annealing learning rate with peak value = 0.0002 × batchsize/256.

## IMAGENET EVALUATION
- SparK is the best performing self-supervised transformer on Ima-geNet with the pure convolutional model ConvNeXt.
- SparK has the advantage of encoding efficiency, especially compared to contrastive learning that encodes two or more images in a forward pass.
- For instance, DINO and iBOT by default (Caron et al., 2021;Zhou et al., 2021) are not as efficient as SparK.
- SparK is better than self-supervised vision transformers on all three downstream tasks: classification (Cls.), object detection (Det.), and instance segmentation (Seg.).
- SparK is better than self-supervised convnets on all three downstream tasks.
- Adding absolute positional embeddings (row 5) is practically useless for learning convolutional representations.
- Calculating loss values only on masked patches gives higher accuracy (row 6).

## VISUALIZATION
- The model is able to make different but plausible predictions on masked regions
- The model can almost reconstruct the round shape of red fruits from the very small portion of exposed edges

## Raw Input

## Masked Input
- Prediction is the act of estimating what will happen in the future
- Input is the information that is used to make the prediction
- The goal of prediction is to make accurate predictions
- Input can come from a variety of sources, including data, observations, and experiments
- Input can be used to improve the accuracy of predictions
- Input can also be used to improve the understanding of the system being predicted
- Input can be used to improve the design of the system
- Input can also be used to improve the performance of the system

## CONCLUSION
- Transformers have a sparse nature that makes them well suited for masked modeling
- Masked modeling has been successful on transformers, but it doesn't work well on convnets
- Sparse masked modeling can be adapted to irregular masked input without a distribution shift
- Pre-training a hierarchical encoder can overcome the "mask pattern vanishing" issue
