---
title: "Towards Reasoning in Large Language Models: A Survey"
date: 2022-12-20T16:29:03.000Z
author: "Jie Huang, Kevin Chen-Chuan Chang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10403v1.webp" # image path/url
    alt: "Towards Reasoning in Large Language Models: A Survey" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10403)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10403).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/towards-reasoning-in-large-language-models-a).

# Abstract
- Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking.
- In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large.
- However, it is not yet clear to what extent LLMs are capable of reasoning.
- This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions.

# Paper Content

## Introduction
- Reasoning is a cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments.
- The study of reasoning is important in fields like psychology (Wason and Johnson-Laird, 1972), philosophy (Passmore, 1961), and computer science (Huth and Ryan, 2004), as it helps individuals make decisions, solve problems, and think critically.
- Recently, large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022;Shoeybi et al., 2019, inter alia) have made significant advancements in natural language processing and related fields.
- It has been shown that these models exhibit emergent behaviors, including the ability to "reason", when they are large enough (Wei et al., 2022a).
- For example, by providing the models with "chain of thoughts", i.e., reasoning exemplars, or a simple prompt "Let's think step by step", these models are able to answer questions with explicit reasoning steps (Wei et al., 2022b;Kojima et al., 2022), e.g., "all whales are mammals, all mammals have kidneys; therefore, all whales have kidneys."
- This has sparked considerable interest in the community since reasoning ability is a hallmark of human intelligence that is frequently considered missed in current artificial intelligence systems (Marcus, 2020;Russin et al., 2020;Mitchell, 2021;Bommasani et al., 2021).
- However, despite the strong performance of LLMs on certain reasoning tasks, it remains unclear whether LLMs are actually reasoning and to what extent they are capable of reasoning.
- For example, Kojima et al. (2022) claim that "LLMs are decent zero-shot reasoners (p. 1)", while Valmeekam et al. (2022) conclude that "LLMs are still far from achieving acceptable performance on common planning/reasoning tasks which pose no issues for humans to do (p. This limitation is also stated by Wei et al. (2022b): "we qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually reasoning (p. 9)."
- Dence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).
- Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.
- Although "reasoning" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.
- To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:
- Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.
- Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.
- Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain....

## Towards Reasoning in Large Language Models
- Reasoning ability is seen as a weakness in language models
- Recent research suggests that reasoning ability may emerge in language models at a certain scale
- In this paper, we focus on techniques applicable to improving or eliciting "reasoning" in large-scale models

### Fully Supervised Finetuning
- There is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets.
- For example, Rajani et al. (2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019).
- Talmor et al. (2020) train RoBERTa (Liu et al., 2019)

### Prompting & In-Context Learning
- Large language models such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) have demonstrated remarkable few-shot performance across a variety of tasks through in-context learning.
- However, research has shown that these models still fall short when it comes to tasks that require multiple steps of reasoning to solve (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022).
- One approach for doing this is chain of thought prompting, proposed by Wei et al. (2022b). This approach involves providing a few examples of "chain of though" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs (Figure 1).
- Specifically, in CoT prompting, input, output demonstrations are replaced with input, chain of thought, output triples, e.g., "[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? [chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. [output] The answer is 11."
- In this way, given a target question, the model learns to generate explicit rationale before producing the final answer.
- Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.
- There are several variants of chain of thought prompting that have been proposed in the literature.
- For example, Kojima et al. (2022) introduce Zeroshot-CoT, in which LLMs are simply prompted with the phrase "Let's think step by step" after the input, in order to elicit reasoning without the need for few-shot demonstrations.
- Wang et al. (2022a) propose to iteratively prompt chain of thought. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English).
- Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar.
- Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors.
- The original version of chain of thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation.
- Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs. This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.
- A summary of raltionale engineering is illustrated in Figure 1.

### Hybrid Method
- While "prompting" techniques can help elicit or better utilize reasoning in large language models to solve reasoning tasks, they do not actually improve the reasoning capabilities of the LLMs themselves
- In contrast, the "hybrid approach" aims to simultaneously improve the reasoning capabilities of LLMs and make better use of these models in order to solve complex problems
- This approach involves both enhancing the reasoning capabilities of the LLMs and using techniques such as prompting to effectively utilize these capabilities
- One approach to improving the reasoning capabilities of LLMs is to pretrain or finetune the models on datasets that include "reasoning". finetuning and scratchpad prompting results in a significant improvement in LLMs' ability to generalize to longer problems
- Instead of finetuning LLMs on pre-built datasets that include reasoning, there are studies that have explored the idea of using LLMs to self-improve their reasoning abilities through a process known as bootstrapping
- One example of this is the Self-Taught Reasoner (STaR) introduced by Zelikman et al. ( 2022), in which a LLM is trained and refined on its own output iteratively.

## Measuring Reasoning in Large Language Models
- There has been a focus in the literature on using downstream performance on reasoning tasks as the primary measurement for a model's "reasoning" ability.
- However, there has been relatively little work on directly analyzing the rationales generated by these models.
- We summarize methods and benchmarks for evaluating reasoning abilities of LLMs in this section.

### Downstream Task Performance
- LLMs can be evaluated on their arithmetic reasoning abilities
- There are various existing benchmarks that can be used for this purpose
- Arithmetic reasoning involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems
- Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015)
- In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs
- BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills

### Formal Analysis on Reasoning
- LLMs have demonstrated impressive performance on various reasoning tasks
- However, the extent to which their predictions are based on true reasoning or simple heuristics is not always clear
- This is because most existing evaluations focus on their accuracy on downstream tasks, rather than directly assessing their reasoning steps
- While some error analysis has been conducted on the generated rationales of LLMs (Wei et al., 2022b;Kojima et al., 2022, inter alia), this analysis has often been limited in depth
- There have been some efforts to develop metrics and benchmarks that enable a more formal analysis of reasoning in LLMs
- Golovneva et al. (2022) design ROSCOE, a set of interpretable, detailed step-by-step evaluation metrics covering various perspectives including semantic alignment, logical inference, semantic similarity, and language coherence
- Saparov and He (2022) create a synthetic dataset called PrOntoQA that is generated from real or fictional ontologies
- Each example in the dataset has a unique proof, which can be converted to simple sentences and back again, allowing for a formal analysis of each reasoning step
- Han et al. (2022) introduce a dataset called FOLIO to test the first-order logic reasoning capabilities of LLMs
- FOLIO contains first-order logic reasoning problems that require models to determine the correctness of conclusions given a set of premises

## Findings and Implications
- Reasoning ability emerges only in large language models
- Chain of thought prompts improve the performance of LLMs
- LLMs exhibit human-like content effects on reasoning
- Future research should focus on more challenging tasks and applications

## Conclusion
- In this paper, we have provided a detailed and upto-date review of the current state of knowledge on reasoning in large language models.
- We have discussed techniques for improving and eliciting reasoning in LLMs, methods and benchmarks for evaluating reasoning abilities, and the findings and implications of previous studies in this topic. While LLMs have made significant progress in natural language processing and related fields, it remains unclear to what extent they are capable of true reasoning or whether they are simply using memorized patterns and heuristics to solve problems. Further research is needed to fully understand the reasoning abilities of LLMs, improve LLMs' reasoning capabilities, and determine their potential for use in a variety of applications.
- There are also efforts to distill reasoning from large LLMs into smaller models, such as the work byLi et al. (2022a);Shridhar et al. (2022);Magister et al. (2022). Finally, we refer the reader toDohan  et al. ( 2022 ) for a more in-depth discussion of the current state of the field.
