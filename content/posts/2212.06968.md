---
title: "Particle-Based Score Estimation for State Space Model Learning in Autonomous Driving"
date: 2022-12-14T01:21:05.000Z
author: "Angad Singh, Omar Makhlouf, Maximilian Igl, Joao Messias, Arnaud Doucet, Shimon Whiteson"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-06968v1.webp" # image path/url
    alt: "Particle-Based Score Estimation for State Space Model Learning in Autonomous Driving" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.06968)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.06968).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/particle-based-score-estimation-for-state).

# Abstract
- Multi-object state estimation is a fundamental problem for robotic
- Particle filtering can perform such inference given approximate transition and observation models
- However, these models are often unknown a priori, yielding a difficult parameter estimation problem
- In this work, we consider learning maximum-likelihood parameters using particle methods
- Recent methods addressing this problem typically differentiate through time in a particle filter, which requires workarounds to the non-differentiable resampling step, that yield biased or high variance gradient estimates. By contrast, we exploit Fisher's identity to obtain a particle-based approximation of the score function (the gradient of the log likelihood) that yields a low variance estimate while only requiring stepwise differentiation through the transition and observation models. We apply our method to real data collected from autonomous vehicles (AVs) and show that it learns better models than existing techniques and is more stable in training, yielding an effective smoother for tracking the trajectories of vehicles around an AV.

# Paper Content

## Introduction
- Multi-object state estimation is a fundamental problem in settings where a robot must interact with other moving objects
- Typically, other objects' relevant state features are not directly observable
- Inferring relevant state features from a stream of observations is difficult
- Modern perception systems often involve multiple stages and combine information from multiple sensors, making observation models practically impossible to specify by hand
- Collecting observations from a robotic system is relatively easy and cheap
- We are interested, therefore, in algorithms that can leverage such observations to learn transition and observation models in a self-supervised fashion
- Learned transition and observation models can also be independently useful for other applications

## Related Work
- Particle filters are widely used for state estimation in non-linear non-Gaussian SSMs where no closed form solution is available
- The original bootstrap particle filter samples at each time step using the transition density particles that are then reweighted according to their conditional likelihood, which measures their "fitness" w.r.t. to the available observation
- Compared to many newer methods, such as the auxiliary particle filter, the bootstrap particle filter only requires sampling from the transition density, not its evaluation at arbitrary values
- In most practical applications, the SSM has unknown parameters that must be estimated together with the latent state posterior
- Simply extending the latent space to include the unknown parameters suffers from insufficient parameter space exploration
- While particle filters can estimate consistently the likelihood for fixed model parameters, a core challenge is that the such estimated likelihood function is discontinuous in the model parameters due to the resampling step, hence complicating its optimization; see e.g. [6, Figure 1] for an illustration
- Instead, the score vector can be computed using Fisher's identity [8]
- However, as shown in [8], performance degrades quickly for longer sequences if a standard particle filter is used, due to the path degeneracy problem: repeated resampling of particles and their ancestors will leave few or even just one remaining ancestor path for earlier timesteps, resulting in unbiased, but very high variance estimates
- Methods for overcoming this limitation exist [8,16,17], but with requirements making them unsuitable in this work
- Poyiadjis et al. [8] store gradients separately for each particle, making this approach infeasible for all but the smallest neural networks
- Ścibior and Wood [17] propose an improved implementation with lower memory requirements by smartly using automatic differentiation
- However, their approach still requires storing a computation graph whose size scales with O(N 2 ) as the transition density for each particle pair must be evaluated during the forward pass
- Both previous methods' computational complexity also scales quadratically with the number of particles, N , which is problematic for costly gradient backpropagation through large neural networks
- Lastly, Olsson and Westerborn [16] require evaluation of the transition density for arbitrary values, which our compositional transition model does not allow
- Instead, in this work, we show that fixedlag smoothing [18,19] is a viable alternative to compute the score function of large neural network models in the context of extended object tracking
- There is extensive literature on combining particle filters with learning complex models such as neural networks [2,3,4,5,6,20,21,22,23,24]
- In contrast to our work, they make use of a learned, data-dependent proposal distribution. However, for parameter estimation, they rely on differentiation of an estimated lower bound (ELBO). Due to the non-differentiable resampling step, this gradient estimation has either extremely high variance or is biased if the high variance terms are simply dropped, as in [2,3,4]
- As we show in Section 5, this degrades performance noticeably

### Particle Filtering
- Particle methods provide non-parametric and consistent approximations of these quantities
- They rely on the combination of importance sampling and resampling steps of a set of N weighted particles
- We focus on the bootstrap particle filter, shown in Algorithm 1, which samples particles according to the transition density
- Let k ∼ Cat(α 1 , ..., α N ) denote the categori- 3.
- At any time t, this algorithm produces particle approximations of p θ (x 0:t |y 0:t ) and t (θ) = log p θ (y 0:t ), where δ α is the Dirac delta distribution centred at α.
- Step 2 resamples, discarding particles with small weights while replicating those with large weights before evolving according to the transition density.
- This focuses computational effort on the "promising" regions of the state space.
- Unfortunately, resampling involves sampling N discrete random variables at each time step and as such produces estimates of the log likelihood that are not differentiable w.r.t. θ as illustrated in [6, Figure 1].
- While the resulting estimates are consistent as N → ∞ for any fixed time t [28], this does not guarantee good practical performance.
- Fortunately, under regularity conditions the approximation error for the estimate pθ (x t |y 0:t ) and more generally pθ (x t−L+1:t |y 0:t ) for a fixed lag L ≥ 1 as well as log p θ (y 0:t )/t does not increase with t for fixed N .
- However, this is not the case for the joint smoothing approximation because successive resampling means that pθ (x 0:L |y 0:t ) is eventually approximated by a single unique particle for large enough t, a phenomenon known as path degeneracy; see e.g. [12,Section 4.3].

### Particle Score Approximation

### Score Estimation with Deterministic, Differentiable, Injective Motion Models
- The transition density function, f θ (x t |x t−1 ), is the composition of a policy, π θ (a t |x t−1 ), which characterises the action distribution conditioned on the state, and a potentially complex but deterministic, differentiable, and injective motion model, τ : R nx × R na → R nx where n a < n x .
- For many applications, however, the transition density function, f θ (x t |x t−1 ), is the composition of a policy, π θ (a t |x t−1 ), which characterises the action distribution conditioned on the state, and a potentially complex but deterministic, differentiable, and injective motion model, τ : R nx × R na → R nx where n a < n x .
- Under such a composition, the transition density function on the induced manifold is thus obtained by marginalising out the latent action variable, i.e., It is easy to sample from this density but it is intractable analytically if the motion model is only available through a complex simulator or if it is not invertible.
- This precludes the use of sophisticated proposal distributions within the particle filter.
- Additionally, even if it were known, one cannot use the O(N 2 ) smoothing type algorithms developed in [8,16] as the density is concentrated on a low-dimensional manifold [29].
- This setting is common in mobile robotics, in which controllers factor into policies that select actions and motion models that determine the next state.
- Indeed, this is precisely the case in our application (see Section 5).
- The standard result from differential geometry [30,31], the transition density (9) satisfies It follows directly that Indeed for the marginals pθ x t−1:t |y 0:min{t+L,T } , we can store the actions corresponding to transitions x t−1 → x t during filtering, and it follows that for the class of SSMs described above, the score estimate reduces to: where we use Lemma 4.1 to replace the gradient of the transition log density with the gradient of the policy log density in (8).

## Experiments
- Marginal Log Likelihood (MLL): The marginal log likelihood T (θ) given by filtering observations y 0:T using the learned models. This measures the quality of the learned policy.

## A Observation Model
- For the problem setting described in Section 5, our observation model is concerned with the distribution of 2D points around the peripheries of observed road users.
- Such models are reviewed in [25].
- Let the i th point from an observation y t be y i t .
- We assumed independence across different points, i.e. g θ (y In order to conveniently sample y i t or to measure its likelihood, we represented each point via a triplet, e i t , α i t e i t , β i t e it , where e i t ∈ {0, 1, 2, 3} is a categorical variable encoding the edge of the road user's bounding box, α i t e i t is the corresponding parallel offset, and β i t e it is the corresponding perpendicular offset (see Figure 3b).
- Such a triplet uniquely determines y i t , and we used following generative model to sample such points:- if e i t = 0, α i t ∼ Uniform(0, w) and , where l and w are the length and the width of the road user's bounding box respectively, and φ 0:11 θ (x t ) are the outputs from a neural network with weights θ.
- While the mapping e i t , α i t e i t , β i t e it → y i t is unique, the reverse mapping has four representations depending on the choice of e i t , i.e., e i t is a latent variable.
- The likelihood of a point, g θ y i t |x t , was therefore obtained by:-
- We use a feed-forward neural network with 4 hidden layers, each with 16 Tanh units, which outputs 12 parameters, φ 0:11 θ (x t ).
- We provide the network with 5 input features -range, bearing, relative bearing, length, and width -of the road-user's bounding box as measured from the observing AV's viewpoint (see Figure 3a).
- For each of the experiments in Section 5 (with synthetic and real data), we used the same observation model design. Moreover, we trained an observation model using supervision from a dataset of manually labelled trajectories, x0:T , by maximising the AOTLL. This model was then used to generate the synthetic data and also as a baseline for the experiments with real data.

## B Transition Model
- The class of SSMs that we are concerned with involves a transition function, f θ (x t |x t−1 ), that factorises into a policy which produces actions, π θ (a t |x t−1 ), and a deterministic, differentiable, and injective motion model, such that x t = τ (x t−1 , a t ).
- In this section, we provide more details on the motion model and on the policies used for the experiments described in Section 5.
- All experiments used a state space that consists of the 2D Pose (x, y, θ) of an observed road user, in addition to its instantaneous linear speed v and curvature κ. Moreover, the action space used consists of linear acceleration a and pinch p, i.e. the instantaneous rate of change of curvature.
- This choice of actions, i.e. acceleration and pinch, naturally maps to the controls exercised by road users (vehicles users in particular), i.e. to gas and rate of change of steering respectively.
- The calculations below compute the next state, (x(t), y(t), θ(t), v(t), κ(t)), from the previous state (x 0 , y 0 , θ 0 , v 0 , κ 0 ) under the influence of constant actions (a, p) for t ∈ [0, ∆t].
- Since θ(t) = v(t)κ(t), we have ( 16) Finally, using ẋ(t) = v(t) cos θ(t), and ẏ(t) = v(t) sin θ(t), we have ( and To make these integrals analytically tractable, we drop the cubic term in θ t , i.e. ap t 3 3 , and use the integral2 (a + bs) cos(c + ds + es 2 ) ds with appropriate coefficients to compute x(t) and y(t).
- This approximation is justified as for the experiments described in Section 5, we use a small ∆t of ≈ 0.33 seconds.

## C Training Setup
- The training setup for the experiments described in Section 5
- The set of hyperparameters that were used for each of the experiments
- All experiments were repeated 10 times to obtain the median and interquartile ranges that are shown in Figures 2, 5, and 7, and in Table 1
- Smaller learning rates yield similar results, but require proportionately longer training time, while larger learning rates cause instability

## D Sampled Observations from Learned Observation Model
- Figure 10 shows sampled observations from the observation model learned on real data.
- The qualitative similarity between the real and sampled observations indicates the efficacy of the method.

## E Training on Shorter Sequences
- DPF-SGR performed poorly on 50 step sequences, but performed much better on shorter 25 step sequences
- In the case of real data, DPF-SGR and other baselines still fail to learn good models, while PF-SEFI performs almost as well
- At 15 steps, the baselines do actually perform much better, though the final performance for all methods is worse than PF-SEFI on 60 steps

## F Performance on the Filtering Task
- ADE and AYE are unchanged between the smoothing and filtering distributions
- The patterns are unchanged between the offline and online task of state estimation

## G Training with Higher Dimensional Observations
- In Section 5, they report experiments with 32 and 64 2D points
- In Table 5, they summarize the empirical findings of these experiments
- The learned models match the performance of the true models as measured by MLL

## H Effect of a Noisy AV State on Learning
- In Section 5, we assume that the AV state is known precisely. In practice, we expect there to be some minimal errors in state estimation.
- To test our sensitivity to the same, we ran additional experiments with the 25 steps synthetic dataset by injecting Gaussian noise (with a standard deviation of 0.5m in x and y, and 0.05rad in θ) in the AV's 2D pose. We retrained our models using PF-SEFI in the  presence of such noise, and observe no change in MLL at convergence over the held out test data (see Figure 12), nor in training stability.
- Observations from the real data. (b) Observations from the synthetic data.
