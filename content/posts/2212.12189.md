---
title: "Stop using the elbow criterion for k-means and how to choose the number of clusters instead"
date: 2022-12-23T08:15:17.000Z
author: "Erich Schubert"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12189v1.webp" # image path/url
    alt: "Stop using the elbow criterion for k-means and how to choose the number of clusters instead" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12189)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12189).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/stop-using-the-elbow-criterion-for-k-means).

# Abstract
- The "elbow method" is a heuristic that often leads to poor conclusions
- There are better alternatives that have been known in literature for a long time
- We want to call attention to these alternatives and encourage educators to discuss the problems of the method - if introducing it in class at all - and teach alternatives instead

# Paper Content

## INTRODUCTION
- Cluster analysis aims to identify subgroups in the data that have high similarity within the group, while they also differ from the remainder of the data set.
- There is no single "best" definition of a cluster, and each data set and use case may call for different properties to be desirable, which in turn leads to different algorithms to find the "best" solution.
- This leads to a large number of clustering methods being developed over the last decades, based on concepts such as finding a hierarchical structure (akin to phylogenetic trees), quantization and compression, parametric modeling, or identifying dense areas.
- Despite the many different concepts of clusters and the wide variety of clustering algorithms available, one method currently is the most used and most taught clustering method: k-means clustering.
- One of the main reasons may be the simplicity of the standard algorithm: assigning each point to the nearest center, then recomputing all the cluster centers until nothing changes -this algorithm can be easily described in a single sentence.
- At the same time, this algorithm runs very fast, and it will always produce a result with exactly k clusters, giving a (false) suggestion of success.
- A key problem then with applying this method to data is often the need to choose the number of clusters k, although users should first consider whether k-means is even the right choice for their problem at all, and pay more attention to data preprocessing, too.

## K-MEANS CLUSTERING
- Formally, k-means clustering is a least-squares optimization problem.
- We can best view it as a data quantization technique, where we want to approximate the data set of N objects in a continuous, d-dimensional vector space R d using k centers.
- The quantization error for a data set X and a set C of centers then is called inertia, the within-cluster sum of squares (WCSS), or the sum of squared errors (SSE):
- While it is easy to optimize this for a single cluster center by taking the arithmetic average in each dimension, i.e., the data set centroid, the problem is NP-hard for multiple clusters and higher dimensionality [22; 1].
- Several methods to optimize this objective exist, but because of the hardness, most heuristics will only find a local fixpoint.
- Because of the very common least-squares objective, the standard algorithm has likely been invented several times independently, as discussed in the overview of Bock [4].
- The standard heuristic for k-means is an alternating optimization, which first assigns each point to the nearest current cluster center, then updates each cluster center position with the centroid of the points assigned to it.
- If we keep assignments unchanged whenever distances are identical, the algorithm will eventually not find any changes and stop (because both steps may never worsen the objective function, and there exists only a finite number of possible cluster assignments).
- The standard algorithm has a complexity of O(N kdi), where i is the number of iterations (which theoretically could be very high, but usually is small in practice).
- This makes it one of the fastest clustering methods we have available, compared to O(N 3 + N 2 d) for the standard algorithm for hierarchical clustering, or O(N 2 d) for DBSCAN without index acceleration.
- Many improvements have been proposed that avoid repeated computations in the standard algorithm, nevertheless, this very basic form has become quite popular again with the rise of parallel processing and GPUs: it is embarrassingly parallel, and hence very easy to implement both in clusters as well as GPUs.

## THE ELBOW CRITERION
- The elbow plot is a chart plotting the approximation error SSE on the y-axis over a range of values for k on the x-axis.
- The elbow criterion is the concept of diminishing returns: as we increase  the number of clusters, the approximation error decreases.
- There are several problems associated with the elbow plot, that statisticians know too well from the scree plot.

## Elbow Detection
- Several attempts to formalize the notion of an "elbow" can be found in software and literature.
- We present only an excerpt in the following, largely to illustrate how heuristic and visual the machine learning community currently approaches this problem, instead of improving theory.
- Sugar et al. [38] propose a "jump method", finding the max- where Y is a power parameter suggested to be half the dimensionality.
- Salvador et al. [30] propose the L-method, which fits linear functions to the points before and after the break; choosing the breaking point where this piecewise-linear approximation fits best.
- But as discussed above, we will often see an exponential curve, and such a linear approximation often does not fit this curve at all.
- To improve this, the authors also suggest an iterative approach where they truncate the plot to the first 2 • k values if k is the best solution found.
- Satopää et al. [31] in their Kneedle algorithm want to measure the curvature.
- For this they fit a smoothing spline to the data, normalize it to 0 to 1, and compute the difference to the diagonal.
- The last maximum before a parameterizable stopping threshold is chosen.
- Zhang et al. [42] note that the standard curvature definition is not independent of rescaling the data, and propose to choose the maximum of a modified curvature: The pyclustering library [25] defines an elbow length: where x0, x1, y0, y1 denote the minima and maxima of the graph; intended to measure the length when approximating the curve with the elbow point.
- There is no literature given for this approach, the given reference to Thorndike [39] does not entail this equation, which again appears to depend very much on the scaling of the plot.
- Shi et al. [37] note that "experienced analysts cannot clearly identify the elbow point", and suggest applying a min-max scaling to the range 0 to 10 instead, then computing angles between triples of adjacent values.
- The performance of this approach depends much on this weighting factor.

## Detection performance
- The quantity measured, the sum of squared devi-ations, is a squared value.
- How meaningful are "angles", "distances", "elbows", and "slopes" on a graph that compares k to SSE, two quantities of different scales?
- If we scale the entire data set by a factor of α, the SSE will change by α 2 , and the "optimum" found by most of the geometric methods changes, while it is clear that it should not.
- Normalizing to the observed minimum and maximum values (often even starting with k = 2, not k = 1) seems inappropriate.
- In particular, even without running the algorithm, we know that for k = N we will be able to get an approximation error of 0, so we likely should always consider N to be the maximum x coordinate, and 0 to be the minimum y coordinate.
- A meaningful normalization should preserve 0.
- Even on random data we obtain a descending curve, and hence we should try to remove this expected behavior from our measure.

## Expected behavior of SSE
- The sum of squared errors closely resembles the variance of the data set.
- Because our cluster centers are derived from the data, we should be using a form of sample variance.
- Simply diving the SSE by N will be a biased estimate, and we postulate that SSE /(N − k) is a more suitable estimate in this context.
- But since usually k N , this will not make much of a difference yet.
- Instead of working with the squared quantity, we then may want to apply the square root instead, i.e., use SSE /(N − k) to have the intuition of a standard deviation from the nearest center.
- Still, the plot obtained this way will look similar to what we started with, and because the square root is a monotone function on the outside, it will not affect the ordering of results -it only serves to make the quantity more interpretable, because ideally, the domain expert should judge whether this is sufficiently small.
- As a baseline "expected" behavior, we will for simplicity assume the input data to be uniformly distributed in a single dimension, but with the variance of the input data set.
- The variance of a uniform interval of length b is Var([0; b]) = ).
- Because of this observation, we propose to use the naïve estimate SSE1 /k as normalization factor.
- But Krzanowski and Lai [20] suggest that SSE /k 2 d may be more appropriate than our naïve estimate.
- We should further include the N − k factor discussed above.
- If there is more than one good parameter k (e.g., because there are substructures in the data), we may also want to compare the solution with the best found so far, e.g., using:
- We can now generate a standard deviation reduction plot, comparing the observed with the estimated values:
- Note that because SSE k will tend to 0 as we increase k, eventually this will become unstable for too large k.
- Depending on our objective, either the smallest value or the last value below 1 (or below a suitable threshold such as 0.99) can be chosen as "best" k.
- The normal distribution never scores below 1, and the uniform distribution remains very close to 1; hence these data sets can be recognized as unclustered.
- For the many blobs data set, the largest reduction is obtained for k = 8, and the last reduction is at k = 25, the number of generated clusters in this data set.
- To better understand why both of these solutions are of interest, Figure 3 shows that when going from k = 7 to k = 8 we observe structural changes that substantially improved the clustering result (such as separating the far cluster in the bottom, but also improving the clustering in the center), whereas the last improvement from k = 24 to k = 25 only affected three overlapping blobs in the center, that previously were split into two and now correctly into three clusters.
- For k = 8 we have the best structural improvement, but for k = 25 we get the finest clustering; both may have their use cases.
- The solution k = 21 is preferred by many distance-based measures, it is worth noting that well-separated blobs are separated, but touching blobs are still joined at this k.

## Variance-based criteria
- Variance Ratio Criterion (VRC)
- Uses the fact that BGSS = TSS − WGSS = SSE1 − SSE k
- Many more methods discussed in the 70s and 80s literature overlooked by many machine learning scientists of today
- Superior because it takes correlations into account

## Distance-based criteria
- The Dunn index compares the diameter of clusters to the cluster separation.
- The Davies-Bouldin-Index compares the distance to the nearest other cluster with the radius of the two clusters.
- One of the most used distance-based criteria is the average silhouette width measure.

## Information-theoretic criteria
- The principle of minimum description length suggests that a k-means solution is better if the data can be encoded more compactly
- X-means integrates this with k-means in an algorithm that dynamically increases the number of clusters as long as a cluster quality criterion improves
- G-means uses Anderson-Darling tests instead to decide when to accept a new cluster, and when to stop increasing k

## Simulation-based criteria
- Tibshirani et al. propose the gap statistic, which estimates a baseline SSE k obtained by clustering uniform random data sets.
- The gap statistic failed to recognize the uniform data as unclustered.
- For the more challenging data sets, the estimated number of clusters was unstable with the default sample sizes.
- As we are not convinced that the "novel" approach we constructed above is clearly superior to VRC, BIC, or the Gap statistic, we suggest that you simply use one of these approaches to choose k, and rather pay attention to the way you preprocess your data for k-means.

## THE TRUE CHALLENGES OF K-MEANS
- The difficulty of choosing k is easily noticed by the user,
- but nevertheless remains much more difficult to obtain meaningful and useful results from k-means than commonly anticipated.
- If we study the foundations of k-means, and the relationship to Gaussian mixture modeling, we can observe that k-means assumes errors to be invariant across the entire data space, whereas in full Gaussian mixture modeling, the deviation from a cluster in certain directions weight less than in other, and depend on the individual clusters.
- We can consider kmeans as a limit case of Gaussian mixture modeling, where we perform (i) all clusters have an identical, spherical shape, and (ii) we make hard cluster assignments, for example by making the cluster standard deviations tend to zero.
- In k-means we somewhat assume that all clusters have the same spherical shape.
- This is simply a consequence of the sum of squared errors (Eq. 1) not including any weights depending on the cluster or axis.
- This is a reasonable simplification if we assume that our data set was generated from k pure signals (corresponding to the cluster centers) plus i.i.d. Gaussian noise.
- In many cases the results suffer from at least one of these additional problems as well.

## CONCLUSION
- The elbow method is a popular way to cluster data.
- The elbow method is unreliable because it is based on the assumption that the data is uniform.
- There is no "optimal" solution to cluster analysis, but it is an exploratory approach that may yield multiple interesting solutions.
- Pham et al. propose a scoring function for k ≥ 2 based on SSE k /(α k SSE k−1 ).
- If k-means cannot be expected to work well on a data set, the "optimum" k may not be the best option.
