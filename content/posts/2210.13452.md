---
title: "MetaFormer Baselines for Vision"
date: 2022-10-24T17:59:57.000Z
author: "Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, Xinchao Wang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2210-13452v2_d2qY5PEiA.jpg" # image path/url
    alt: "MetaFormer Baselines for Vision" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.13452)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.13452).


# Abstract
- MetaFormer, the abstracted architecture of Transformer, is found to play a significant role in achieving competitive performance.
- In this paper, we further explore the capacity of MetaFormer, again, without focusing on token mixer design: we introduce several baseline models under MetaFormer using the most basic or common mixers, and summarize our observations as follows: (1) MetaFormer ensures solid lower bound of performance. By merely adopting identity mapping as the token mixer, the MetaFormer model, termed IdentityFormer, achieves >80% accuracy on ImageNet-1K. (2) MetaFormer works well with arbitrary token mixers. When specifying the token mixer as even a random matrix to mix tokens, the resulting model RandFormer yields an accuracy of >81%, outperforming IdentityFormer. Rest assured of MetaFormer's results when new token mixers are adopted.
- MetaFormer effortlessly offers state-of-the-art results. With just conventional token mixers dated back five years ago, the models instantiated from MetaFormer already beat state of the art.

# Paper Content

## Related work
- Transformer has become a popular backbone for various tasks in NLP
- In computer vision, iGPT and ViT introduce pure Transformer for self-supervised learning and supervised learning
- The success of Transformers had long been attributed to the attention module, and thus many research endeavors have been focused on improving attention-based token mixers
- However, it is shown in MLP-Mixer and FNet that, by replacing attention in Transformer with spatial MLP and Fourier transform, the resulting models still deliver competitive results
- Along this line, [83] abstracts the Transformer into a general architecture termed MetaFormer, and meanwhile proposes the hypothesis that, it is the MetaFormer that really plays a critical role in achieving promising performance
- To this end, [83] specifies the token mixer to be as embarrassingly simple as pooling, and observes that the resultant model PoolFormer surpasses the well-tuned ResNet/ViT/MLP-like baselines

## Conclusion
- The abstracted architecture of Transformer
- The solid lower bound of MetaFormer
- The universality of MetaFormer to token mixers
- The new activation, StarReLU, achieves better performance and reduces FLOPs
- MetaFormer finds its even boarder domain of vision applications in the future work
