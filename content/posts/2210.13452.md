---
title: "MetaFormer Baselines for Vision"
date: 2022-10-24T17:59:57.000Z
author: "Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, Xinchao Wang"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2210-13452v2.webp" # image path/url
    alt: "MetaFormer Baselines for Vision" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2210.13452)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2210.13452).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/metaformer-baselines-for-vision).

# Abstract
- MetaFormer plays a significant role in achieving competitive performance
- MetaFormer easily offers state-of-the-art results
- ConvFormer outperforms ConvNeXt
- CAFormer sets a new record on ImageNet-1K

# Paper Content

## Recap the concept of MetaFormer
- The concept MetaFormer is a general architecture instead of a specific model, which is abstracted from Transformer by not specifying token mixer.
- Specifically, the input is first embedded as a sequence of features (or called tokens) [73,17]: Then the token sequence X ∈ R N ×C with length N and channel dimension C is fed into repeated MetaFormer blocks, one of which can be expressed as where Norm 1 (•) and Norm 2 (•) are normalizations [31,1]; TokenMixer(•) means token mixer mainly for propagating information among tokens; σ(•) denotes activation function; W 1 and W 2 are learnable parameters in channel MLP.
- By specifying token mixers as concrete modules, MetaFormer is then instantiated into specific models.

## Techniques to improve MetaFormer
- The paper introduces a new activation function, StarReLU, which is more efficient than the other commonly used activation functions.
- The paper also introduces two modifications to the MetaFormer algorithm, which improve its performance.

## StarReLU
- ReLU is the activation function that is used in vanilla Transformer
- GPT uses GELU as the activation function by default
- GELU can be approximated as, where Φ(•) is the Cumulative Distribution Function for Gaussian Distribution (CDFGD)
- Although revealing better performance than ReLU, GELU approximately brings 14 FLOPs, much larger then ReLU's 1 FLOP of cost
- To make the activation adaptable to different situations, like different models or initialization, scale and bias can be set to be learnable
- We uniformly re-write the activation as where s ∈ R and b ∈ R are scalars of scale and bias respectively, which are shared for all channels and can be set to be constant or learnable to attain different StarReLU variants
- StarReLU only costs 4 FLOPs (or 3 FLOPs with only s or b), much fewer than GELU's 14 FLOPs but achieving better performance as shown in Section 3.4.

## Other modifications
- BranchScale: scaling all branches
- LayerScale: multiplying layer output by a learnable vector
- Norm(•): normalization
- F(•): token mixer or channel MLP module
- λ l : learnable LayerScale parameters initialized by a small value like 1e-5
- ⊙: element multiplication

## IdentityFormer and RandFormer
- Following [83], we would like to instantiate token mixer as basic operators, to further probe the capacity of MetaFormer.
- The first one we considered is the identity mapping, Identity mapping does not conduct any token mixing, so actually, it can not be regarded as token mixer.
- For convenience, we still treat it as one type of token mixer to compare with other ones.
- Another basic token mixer we utilize is global random mixing, where X ∈ R N ×C is the input with sequence length N and channel dimension C, and W R ∈ R N ×N is a matrix that are frozen after random initialization.
- This token mixer will bring extra frozen parameters and computation cost quadratic to the token number, so it is not suitable for large token length.
- To build the overall framework, we simply follow the 4-stage model [35,24] configurations of PoolFormer [83] to build models of different sizes.
- Specifically, we specify token mixer as identity mapping in all four stages and name the derived model IdentityFormer.
- To build RandFormer, considering that the token mixer of random mixing will bring much extra frozen parameters and computation cost for long token length, we thus remain identity mapping in the first two stages but set global random mixing as token mixer in the last two stages.
- To compare IdentityFormer/RandFormer with PoolFormer [83] fairly, we also apply the techniques mentioned above to PoolFormer and name the new model PoolFormerV2.

## ConvFormer and CAFormer
- Used basic token mixers to probe the lower bound of performance and model universality in terms of token mixers
- Without designing novel token mixers, we just specify the token mixer as commonly-used operators to probe the model potential for achieving state-of-the-art performance
- The first token mixer we choose is depthwise separable convolution [9,48]
- Specifically, we follow the inverted separable convolution module in MobileNetV2 [60], where Conv pw1 (•) and Conv pw2 (•) are pointwise convolutions, Conv dw (•) is the depthwise convolution, and σ(•) means the non-linear activation function
- In practice, we set the kernel size as 7 following [46] and the expansion ratio as 2
- We instantiate the MetaFormer as ConvFormer by specifying the token mixers as the above separable convolutions
- ConvFormer also adopts 4-stage framework [24,74,45] and the model configurations of different sizes are shown in the Table 2
- Besides convolutions, another common token mixer is vanilla self-attention [73] used in Transformer
- This global operator is expected to have better ability to capture long-range dependency
- However, since the computational complexity of self-attention is quadratic to the number of tokens, it will be cumbersome to adopt vanilla self-attention in the first two stages that have many tokens
- As a Model Params (M) MACs (G) Top-1 (%) RSB-ResNet-18 [24,76] 11 comparison, convolution is a local operator with computational complexity linear to token length. Inspired by [4,17,12,83], we adopt 4-stage framework and specify token mixer as convolutions in the first two stages and attention in the last two stages to build CAFormer, as shown in Figure 2. See Table 2 for model configurations of different sizes.

## Setup
- ImageNet-1K is a widely-used dataset in computer vision
- ImageNet-1K is used to benchmark these baseline models
- The models are trained for 300 epochs at 224 2 resolution
- Data augmentation and regularization techniques include RandAugment, Mixup, CutMix, Random Erasing, weight decay, Label Smoothing, and Stochastic Depth
- We do not use repeated augmentation and LayerScale

## Results of Models with basic token mixers
- IdentityFormer outperforms RSB-ResNet-18/34 [24,76]
- IdentityFormer-S12/S24 outperforms RSB-ResNet-18/34 [24,76]
- PoolFormerV2 [83] with basic token mixer of pooling, neither of IdentityFormer nor RandFormer can match its performance.

## Results of models with commonly-used token mixers
- We build ConvFormer by specifying the token mixer in MetaFormer as separable convolutions [9,48] used in MobileNetV2 [60].
- Meanwhile, CAFormer is built with token mixers of separable convolutions in the bottom two stages and vanilla self-attention in the top two stages.
- The results of models trained on ImageNet-1K are shown in Table 4. ConvFormer actually can be regarded as pure CNN-based model without any attention mechanism [28,77,73,17]
- Besides ConvFormer, CAFormer achieves more remarkable performance. Although CAFormer is just built by equipping token mixers of separable convolutions [9,48,60] in bottom stages and vanilla self-attention [73] in top stages, it already consistently outperforms other models in different sizes, as clearly shown in Figure 3.
- Remarkably, to the best of our knowledge, CAFormer sets new record on ImageNet-1K with top-1 accuracy of 85.5% at 224 2 resolution under normal supervised setting (without external data or distillation models).
- When pre-trained on ImageNet-21K, the performance of ConvFormer-B36 and CAFormer-B36 surges to 87.0% and 87.4%, with 2.2% and 1.9% accuracy improvement compared with the results of ImageNet-1K training only.
- Both models keep superior to Swin-L [45]/ConvNeXt-L [46], showing the promising scaling capacity with a larger pre-training dataset.
- For example, ConvFormer-B36 outperforms ConvNeXt-L by 0.2% with 49% fewer parameters and 34% fewer MACs.
- Just equipped with "old-fashioned" token mixers, ConvFormer and CAFormer instantiated from MetaFormer already can achieve remarkable performance, especially CAFormer sets a new record on ImageNet-1K.
- These results demonstrate MetaFormer can offer high potential for achieving state-of-the-art performance.
- When advanced token mixers or training strategies are introduced, we will not be surprised to see the performance of MetaFormer-like models setting new records.
