---
title: "EVA-02: A Visual Representation for Neon Genesis"
date: 2023-03-20T17:59:59.000Z
author: "Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang and 1 others"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2303-11331v2.webp" # image path/url
    alt: "EVA-02: A Visual Representation for Neon Genesis" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2303.11331)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2303.11331).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/eva-02-a-visual-representation-for-neon).

# Abstract
- EVA-02 is a Transformer-based visual representation pre-trained to reconstruct language-aligned vision features.
- EVA-02 has an updated plain Transformer architecture and extensive pre-training from a giant CLIP vision encoder.
- EVA-02 outperforms prior state-of-the-art approaches across various vision tasks with fewer parameters and compute budgets.
- EVA-02 with 304M parameters achieves 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.

# Paper Content

## Introduction
- Recent research advancements have led to an increase in interest in scaling up vision and vision-language representations.
- Increasing the number of parameters, data, and compute budgets is believed to result in improved performance.
- Training, tuning, and evaluating very large vision models requires significant computational resources.
- Evaluating the effects of modeling advancements is difficult due to the need for significant infrastructure and web-scale private training data.
- EVA-02 is a series of robustly optimized plain Vision Transformers with moderate model sizes.
- EVA-02 is able to achieve superior performance compared to prior state-of-the-art approaches with much larger model sizes.
- EVA-02 requires fewer compute budgets and resources to investigate.
- EVA-02 democratizes access to state-of-the-art vision models.

## Approach
- EVA-02 is a Transformer-based visual representation that is designed to have strong performance with moderate model sizes.
- EVA-02 consists of two parts: architectural improvements and a MIM pre-training strategy.
- EVA-02 uses FFNs for feature transformation without downsampling layers or multi-stage design.
- EVA-02 is compatible with masked modeling, which is a simple, strong, and scalable pretraining approach.
- Pre-trained plain ViT can be adapted to challenging vision tasks with high-resolution inputs and multi-scale representations.

### Architecture
- Plain ViT has been evolving since 2020
- Significant architectural advances in language models have not been explored in visual representation learning
- Pilot experiments studied architectural modifications
- SwiGLU FFN works well with xavier normal weight initialization
- Sub-LN slightly improves performance
- 2D RoPE can improve performance
- TrV model configuration aligns with current leading language models and achieves a favorable accuracy

### Pre-training strategy
- Chosen to use features from a giant CLIP vision encoder with one billion parameters as the target representation for MIM pretext task
- Assumption that larger CLIP will provide more robust and transferable target representations for MIM
- Impact of target representations produced by different-sized CLIPs studied in Table 3
- Accuracy degenerates when using EVA-CLIP target with base-sized plain ViT
- Architectural modifications from TrV compensate for this
- Pre-training schedule extended to 1600 epochs
- EVA-02 variants provided with different model depths, widths and #heads
- Pre-training objective is to regress masked-out image-text aligned vision features
- Pre-training data from IN-21K, CC12M, CC3M, COCO, ADE20K, Object365 and OpenImages
- Hyper-parameters generally follow BEiT series
- Optimizer is Adam with decoupled weight decay
- Pre-training code based on open-sourced EVA implementation
- Wallclock pre-training time is âˆ¼10% shorter than official BEiT series implementations

## Experiments and evaluation
- Evaluated EVA-02 on representative vision tasks and benchmarks
- Tested base-sized (86M) and large-sized (304M) pre-trained representations
- Results show EVA-02 outperforms larger counterparts and achieves state-of-the-art performance with minimal additional fine-tuning

### Image classification
- Evaluated EVA-02 on IN-1K
- Evaluated robustness and generalization capability of EVA-02 using IN-1K validation set variants
- Optional intermediate fine-tuning on IN-21K
- Pre-trained representations are robust enough to be fine-tuned using various numerical precisions and optimizers
- EVA-02 outperforms several state-of-the-art models on IN-1K val set
- EVA-02 has excellent robustness and generalization ability

### Contrastive language-image pre-training and zero-shot evaluation
- CLIP model is a representation learning engine for recognition and generation tasks
- EVA-02-CLIP is initialized following settings in [44]
- Table 11 shows zero-shot video classification performance
- EVA-02-CLIP-L model has 1/2 of the model size and 1/5 image-text pairs
- EVA-02-CLIP outperforms competitors in zero-shot image and text retrieval
- Retrieval tasks depend more on the capacity and capability of the language encoder

### Object detection and segmentation
- EVA-02 is evaluated on object detection and instance segmentation tasks
- EVA-02 is compared to existing state-of-the-art methods
- Three different transfer learning settings are evaluated
- Sanity check: EVA-02 outperforms ViTDet
- System comparison without additional detection data: EVA-02 outperforms same- and larger-sized counterparts
- System comparison with additional intermediate detection finetuning: EVA-02 is competitive with state-of-the-art approaches
- Semantic segmentation performance of EVA-02 is evaluated on two benchmarks
- EVA-02 creates new state-of-the-art results with large-sized models on both benchmarks

### Summary of all evaluations
- EVA-02 representations have good transfer learning ability for a variety of computer vision tasks.
- EVA-02-CLIP is important for its potential for zero-shot transferability and its alignment with natural language.

## Related work
- Previous advancements in representation learning do not necessarily come with new ideas or approaches.
- GPT series, RoBERTa, DeiT, RSB, and ConvNeXt all evaluate different aspects of language and image representation learning.
- Knowledge distillation can make large-scale image classification models affordable.
- This paper evaluates MIM visual representation learning to bridge the gap between large-scale visual representations and models that are affordable and accessible.

## Discussion and conclusion
- Aim to contribute to research on visual and vision-language representation learning
- Evaluate existing MIM pre-training with CLIP vision features
- Outperform larger state-of-the-art specialized models
- Compact and expressive CLIP representations
- Alternate training of MIM and CLIP representations can improve both
- Pre-training with Merged-38M images has little impact on performance
- EVA-02-L outperforms inductive biases with tiny and small models
- EVA-02-CLIP improves zero-shot performance, sample efficiency, and training speed
- EVA-02-CLIP zero-shot image classification performance on 27 datasets
- EVA-02-CLIP zero-shot retrieval performance
- Object detection and instance segmentation results of EVA-02-B and -L
