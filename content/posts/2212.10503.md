---
title: "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training"
date: 2022-12-20T18:17:28.000Z
author: "Kelly Marchisio, Patrick Lewis, Yihong Chen, Mikel Artetxe"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10503v1.webp" # image path/url
    alt: "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10503)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10503).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/mini-model-adaptation-efficiently-extending).

# Abstract
- Prior work has shown that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen.
- In this work, we propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters.
- New language-specific embeddings can then be efficiently trained over the mini-model, and plugged into the aligned large model for rapid cross-lingual transfer.
- We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model and build a mini-model by extracting and freezing a few layers and learning a small number of parameters on top.
- Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches the performance of the standard approach using up to 2.4x less compute.

# Paper Content

## Introduction

### Mini-Model Adaptation
- Our proposed approach follows a similar four-step training paradigm.
- In Step 1, we learn two aligned models in Step 2: the primary model and a shallow mini-model.
- For Step 1, we explore the following two alternatives depending on whether we start from an existing L src model, or we are training one from scratch: MINIJOINT (left) or BASELARGE (right).
- In MINIJOINT, we pretrain a dualhead 12-layer L src transformer from scratch, attaching a secondary head to an intermediary N th layer.
- The model is trained to minimize the average MLM loss over the two heads.
- As such, the whole model receives gradient updates from the primary head, and the bottom layers also get updates from the secondary head.
- Having done that, we extract the bottom N layers and the secondary head to create the mini-model for Step 2.
- Unless otherwise indicated, we use N = 4.
- In BASELARGE, we start with a regular 12-layer MLM in L src (same as BASELARGE), and build an aligned mini-model in Step 1b.
- To that end, we first copy the bottom N layers into a new, shallower model, along with the embeddings and the MLM head.
- However, this does not work out of the box, as we must bridge the gap between the output of the N bottom layers and the input of the MLM head, which goes through 12 − N additional layers in the original model.
- To that end, we add 2 randomly-initialized layers between the N bottom layers and the MLM head, and train them with the MLM objective in L src , while keeping the rest of the parameters frozen.
- Because the new layers are unfrozen, they update themselves to "complete" the MLM-bridging representations from the bottom layers' output to the MLM head's input, and resulting in a mini-model with N + 2 layers that is fully functional and aligned with the primary model.

## Experimental Settings
- Uses English as the source language (L src ), and experiments with 14 other languages as the target (L trg ).
- Uses CC-100 (Conneau et al., 2020) as the training corpus, which is a filtered version of CommonCrawl.
- Uses the RoBERTa BASE architecture throughout as implemented in fairseq (Ott et al., 2019), which uses a model dimension of 768 and 12 attention heads.
- As discussed in §2 , compares 4 systems in our experiments: 2 variants of Artetxe et al. (2020) (BASELARGE with 12 layers and BASESMALL with 4 layers), and 2 variants of our proposed approach where we set N = 4 (MINIJOINT, which jointly trains a 12-layer primary model and a 4-layer mini-model from scratch, and MINIPOST, which starts from a regular 12-layer primary model and constructs a 6-layer minimodel post-hoc).
- BASELARGE is an upper-bound on performance, as it is the original 12-layer model that is used for adaptation.
- BASESMALL is a lowerbound, demonstrating performance of the standard approach utilizing an adaptation model of similar size as ours.
- Models are trained for 125,000 steps with a global batch size of 2048, sequence length of 512, and learning rate of 7e-4 with 10,000 warmup updates and linear decay, both for the original pretraining (Step 1), and cross-lingual extension into each language (Step 2).
- As such, models see 131.1 billion training tokens per language.
- Step 1b in MINI-POST uses the same training hyperparameters.
- Evaluation.
- We also report XQuAD (Artetxe et al., 2020) results in §A.2.
- In all cases, the model is finetuned using the corresponding training data in English (Step 3), and zero-shot transferred into the rest of languages (Step 4).
- We perform 5 independent finetuning runs with different random seeds, and report average results.
- During finetuning, we use a peak learning rate of 1e-5 for XNLI and PAWS-X, and 3e-5 for MLQA and XQuAD.
- Each uses a warmup ratio of 0.06 and linear decay, and is finetuned for 3 epochs.
- We compare the training efficiency of different approaches using floating point operations (FLOPs).
- To calculate FLOPs, we estimate analytically using an adaptation of the formula from Narayanan et al. (2021), detailed in §A.1.
- When doing so, we exclusively consider the cost of expanding the model to a new language (Step 2), which is the most significant in the crosslingual lifelong learning setup that our work addresses.
- 2 n addition to FLOPs, we report NVIDIA V100 GPU training days as a more interpretable number, which we also estimate analytically using the empirical estimated throughput of 30 TFLOP/s, or 1 V100 day = 2.592 EFLOPs.

## Main Results

### Performance at Training Completion
- MINIPOST retains 99.3% of BASELARGE's performance at nearly half of its cost
- BASESMALL performs substantially worse than our proposed approach

### GPU days to Near-Maximal Performance
- While standard adaptation requires 5 times more compute than mini-model adaptation, MINIJOINT achieves a 95% speedup
- MINIPOST is faster than standard adaptation in all languages, but has a higher variance
- PAWS-X is the only task where mini-model adaptation achieves a higher speedup than standard adaptation

## Analysis

### Training Curves
- MINIJOINT is usually the leftmost curve-signifying the most rapid adaptation
- In contrast, BASELARGE is by far the slowest system approaching its peak performance
- All methods adapt rapidly in PAWS-X

### Mini-Model Depth
- The mini-model in MINIJOINT has 4 layers, whereas the one in MINIPOST has 6 (the bottom 4 taken from the primary model + 2 additional ones trained on top).
- In this section, we more systematically study the effect of mini-model depth on efficiency and performance.
- To that end, we build models with the same architecture as MINIJOINT, but placing the secondary MLM head after layers 2, 6, 10, or 12.
- We experiment with Arabic, German and Turkish due to compute constraints.
- Figure 4 shows the XNLI training curve averaged over languages.
- We observe more rapid adaptation with shallower attachment of the secondary head, with a cost to final performance.
- For PAWS-X, high performance was rapidly achieved by all models.
- Table 4 reports estimated V100 days to achieve near-maximal performance as defined in §4.2, and upper and lower estimates are in §A.3.
- We find that the optimal depth of the mini-model is largely language-dependent.
- More concretely, Arabic and Turkish never hit the target performance with 2 layers, whereas German does so quickly.
- For Arabic, 4 layers provides the most rapid adaptation, while 7 Attaching after layer 12 means that both heads are at the final layer, making it virtually equivalent to BASELARGE.
- Turkish requires at least 6.

### English Performance
- While all of our results so far correspond to the target languages, we next look into the source language performance.
- MINI-POST uses BASELARGE as the primary model, so their English performance is exactly the same.
- However, MINIJOINT jointly pretrains the primary Layer: 2 4 6 8 10 12 XNLI 0.5 0.7 1.0 1.4 1.6 2.1 MLQA 1.0 0.8 0.9 1.5 1.5 2.1 de PAWS 0.3 0.3 0.6 0.8 1.0 1.5 XNLI ∞ 1.3 1.3 1.9 2.7 3.5 ar MLQA ∞ 1.4 1.5 3.2 3.0 3.6 model and its aligned mini-model.
- To understand the effect of the joint pretraining in the monolingual quality of the model, we compare the full MINI-JOINT model and its corresponding mini-model with BASELARGE and BASESMALL.
- As shown in Table 5, we find that dual-head training does not damage performance: the full MINIJOINT model performs on-par with the 12-layer baseline, and the 4-layer extracted mini-model performs on-par with the 4-layer baseline.

### Variance Across Languages
- While the proposed approach achieves strong results across the board, there are 3 languages that prove challenging for the proposed approach: Hindi, Turkish and Urdu.
- For instance, as shown in Table 3, MINIJOINT takes more than 7 V100 days to achieve near-maximal performance on XNLI for these languages, whereas the rest of the languages require at most 1.3.
- As seen in §5.2 this can be greatly mitigated by using a deeper mini-model in the case of Turkish. However, we observe that even BASELARGE struggles with Urdu and, to a less extent, Hindi.
- This suggests that there is something making these languages particularly challenging for cross-lingual adaptation, affecting not only our method but also the standard approach from Artetxe et al. (2020).
- One plausible hypothesis is that this is due to the high linguistic distance between these languages and English.
- As shown in Table 1, these are the languages that are the most distant from English according to lang2vec,8 and we also observe that they are the only ones with a pure SOV word order.
- In the future, we would like to explore starting with a multilingual model covering a few diverse languages akin to Pfeiffer et al. (2022), which could facilitate adapting to languages that are distant from English but might share features with some of the other languages.
- Another potential factor is that Hindi, Turkish and Urdu, along with Swahili, have the smallest training corpora.

## Related Work
- Multilingual LM models are large and expensive to train
- Multilingual LMs suffer from the curse of multilinguality
- Multilingual LMs can benefit from languagespecific tokenization
- Adapters are a parameter-efficient way to extend LMs by training a small number of parameters that can be swapped-in for on-the-fly adaptation at test time
- BERT adaptation is slow for some languages
- SOV languages are underrepresented in the training data for multilingual LMs

## Conclusion and Future Work
- Our work shows that it is possible to extend pretrained models to new languages using only a fraction of their parameters.
- We achieve this by learning a new embedding layer over a shallow minimodel aligned with the primary model.
- We explore two approaches to learn mini-models. MINIJOINT augments a transformer with a second MLM head during pretraining, adapting with an average 2.4x speedup over the standard method from Artetxe et al. (2020). MINIPOST builds a mini-model by extracting a small number of layers from a pretrained model, providing an average 1.6x speedup.
- Our analysis reveals that shallower mini-models converge faster but plateau at a lower performance. Given this, we would like to explore combining multiple mini-models of different sizes, using the shallowest one at the beginning of the cross-lingual adaptation and switching to deeper ones as training progresses.
- In addition, we would like to explore other potential applications of mini-model adaptation beyond the multilingual scenario.
- Finally, we average results over 5 finetuning runs, but computational restrictions prevented us from also averaging over multiple pretraining runs.
- As discussed in §A.5, we observed a non-negligible variance over pretraining runs in a preliminary experiment, but a more systematic exploration is necessary to better understand its impact.
