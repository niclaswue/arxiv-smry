---
title: "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"
date: 2022-12-20T18:26:34.000Z
author: "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10509v1.webp" # image path/url
    alt: "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10509)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10509).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/interleaving-retrieval-with-chain-of-thought).

# Abstract
- Recent work has shown that large language models are capable of generating natural language reasoning steps or Chains-of-Thoughts (CoT) to answer a multi-step question when prompted to do so.
- A straightforward approach to address this is to retrieve text from an external knowledge source using the question as a query and prepend it as context to the model's input. This, however, is also insufficient for multi-step QA where what to retrieve depends on what has already been derived.
- To address this issue we propose IRCoT, a new approach that interleaves retrieval with CoT for multi-step QA, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Our experiments with GPT3 show substantial improvements in retrieval (up to 22 points) and downstream QA (up to 16 points) over the baselines on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. Notably, our method also works well for much smaller models such as T5-Flan-large (0.7B) without any additional training.

# Paper Content

## Introduction
- Large language models are capable of answering complex questions by generating step-by-step natural language reasoning steps-so called chains of thoughts (CoT)-when prompted appropriately.
- This approach has been successfully applied in settings where all the information needed to answer the question is either provided as context (e.g., algebra questions) or assumed to be present in the parameters of the models (e.g., commonsense reasoning).
- However, for many open-domain questions, all required knowledge is not always available or up-to-date in models' parameters and it's beneficial to retrieve knowledge from external sources.
- We propose an interleaving approach to this problem, where the idea is to use retrieval to guide the chain-of-thought (CoT) reasoning steps and use CoT reasoning to guide the retrieval.
- Our system is effective on 4 multi-step reasoning datasets under an open-domain setting.
- Our work is comparable to contemporaneous works on few-shot prompting for open-domain QA.

## Related Work
- Large Language Models (LLMs) have been shown to learn various tasks by just using a few examples as prompts (Brown et al., 2020).
- They've also been shown to answer complex questions by producing step-by-step reasoning (chain-of-thoughts, or CoT) when prompted with a few demonstrations (Wei et al., 2022) or in many cases even without any demonstrations (Kojima et al., 2022).
- Despite the recent progress in CoT-based prompting for reasoning, their value in improving the retrieval and QA for multi-step open-domain questions has been relatively underexplored.
- There have been few recent works that have also explored this particular problem.
- Nakano et al. (2021) used GPT-3 to answer long-form questions by first interacting with a browser over multiple steps to retrieve evidence and then generating the answer.
- This approach also relies on human annotations of the interactions with the browser, i.e., not few-shot.
- Lazaridou et al. (2022) proposed augmenting fewshot QA prompting with the context retrieved from Google Search results. They do not employ multistep retrieval though and so is closer to our one-step retrieval baseline.
- Contemporaneous to our work, three new approaches have been proposed for mutli-step opendomain QA.
- SelfAsk (Press et al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by a call to Google Search API.
- DecomP (Khot et al., 2022) is a general framework that decomposes a task and delegates sub-tasks to appropriate sub-models. Similar to SelfAsk, they decompose questions and delegate retrieval to a BM25-based retriever. Both these approaches are not developed for CoT reasoning or focus on the retrieval problem for multi-step QA. Additionally, they require a single-hop QA model to answer their decomposed questions.
- Recently, the ReAct (Yao et al., 2022) system was proposed that frames the problem as generating a sequence of reasoning and action steps. Their reasoning and action steps are more complex than simple CoT and BM25 retrieval steps in our system.
- They also rely on prompting much larger models (PaLM-540B) and require fine-tuning for smaller models.
- In general, unlike our work, none of these contemporaneous works have been shown to be effective for smaller models without any training.

### Interleaving Retrieval with Chain-of-Thought Reasoning
- IRCoT is a retrieval-guided reasoning method that can be instantiated from three ingredients: a base retriever, a language model, and a set of annotated questions with reasoning steps explaining how to arrive at the answer in natural language.
- The retrieval-guided reasoning step ("Reason") generates the next CoT sentence using the question, the paragraphs collected thus far, and the CoT sentences generated thus far.
- The prompt template for the task looks like the following: For the paragraphs in the in-context demonstrations, we show the model ground-truth supporting paragraphs and M randomly sampled paragraphs shuffled and concatenated together in the above format. For the test instance, we show all the paragraphs collected thus far across all the previous retrieve-steps.
- If the generated CoT sentence has the "answer is:" string or the maximum number of steps5 has been reached, we terminate the process and return all collected paragraphs as the retrieval result.

### Question Answering Reader
- The QA reader answers the question using retrieved paragraphs taken from the retriever.
- We consider two versions of the QA reader implemented via two prompting strategies: CoT Prompting as proposed by Wei et al. (2022), Direct Prompting as proposed by Brown et al. (2020).
- For CoT prompting, we use the same template as shown in § 3.2, but at test time we ask the model to generate the full CoT from scratch.
- The final sentence of CoT is expected to be of the form "answer is: ...", so that the answer can be extracted programmatically.
- If it's not in that form, the full generation is returned as the answer.

## Experimental Setup

### Datasets
- We evaluate our method on 4 multi-step QA datasets in the open-domain setting: HotpotQA (Yang et al., 2018), 2WikiMulti-hopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022) and IIRC (Ferguson et al., 2020).
- HotpotQA already comes with the associated Wikipedia corpus for the open-domain setting, so we use it directly.
- 2WikiMultihopQA and MuSiQue, however, are originally reading comprehension datasets.
- Questions in 2WikiMulti-hopQA and MuSiQue are associated with 10 and 20 paragraphs respectively, 2-4 of which are supporting and others are non-supporting.
- To turn these datasets into an open-domain setting, we make two corpora, one for each dataset, by combining all supporting and non-supporting paragraphs for all its questions in the train, development, and test sets.
- IIRC is originally a mix between reading comprehension and an open-domain setting.
- Each question is grounded in one main paragraph, which contains links to multiple Wikipedia pages with several paragraphs each.
- We create a corpus out of all the paragraphs from all the Wikipedia pages present in the dataset.
- We do assume the availability 7 Following are the corpus sizes for the datasets: Hot-potQA (5,233,329),2WikiMultihopQA (430,225), MuSiQue has (139,416), and IIRC (1,882,415) of Wikipedia links in the main passage, however, to keep the retrieval problem challenging.
- For each dataset, we randomly sample 100 questions from the original development set and use it as our development set for tuning hyperparameters.
- We sample 500 questions from the remaining development set and use it as our test set.

### Models
- Retriever: We use BM25 (Robertson et al., 2009) implemented in Elasticsearch9 as our base retriever.
- We compare two retriever systems: (i) One-step Retriever (OneR) uses the question as a query to retrieve K paragraphs. We select the optimal K ∈ {5, 7, 9, 11, 13, 15} based on the dev set. (ii) IRCoT Retriever is our method described in §3. We use BM25 as its underlying retriever and experiment with OpenAI GPT3 (code-davinci-002) (Brown et al., 2020;Ouyang et al., 2022;Chen et al., 2021) and T5-Flan (Chung et al., 2022) of different sizes as its underlying CoT generator.
- For demonstrating in-context examples to these language models, we hand-wrote CoTs for 15 questions for all the datasets and used them in all the experiments (see App. §B).
- At test time, we dynamically pack as many demonstrations as possible within the model's context length limit.
- The context limit for GPT3 (code-davinci-002) is 8000 wordpieces. Flan-T5-* doesn't have any hard limit as it uses relative position embeddings. But we limit Flan-T5's context to 6000 wordpieces, which is the maximum we could fit in the memory our GPUs (80G A100).
- IRCoT Retriever has one key hyperparameter: K ∈ {2, 4, 6, 8}, the number of paragraphs to retrieve at each step. Additionally, when creating "training" demonstrations for IRCoT's Reasoner module, we use gold paragraphs and a smaller number M ∈ {1, 2, 3} of distractor paragraphs (see § 3.1).
- Retrieval Metric: We allow a maximum of 15 paragraphs for all the retriever systems and measure the recall of the gold paragraphs among the retrieved set of paragraphs.
- We search for the hyperparameter K (and M for IRCoT) that maximizes the recall on the development set and use it on the test set.
- The reported metric can thus be viewed as the fixed-budget optimal recall for each system considered.

## Results
- IRCoT outperforms one-step retrieval
- IRCoT is effective for smaller models too

### Ablations and Other Findings
- Separate reader in IRCoT QA helps.
- IRCoT, by construction, produces a CoT as a part of its retrieval process.
- So, instead of having a separate Table 1: Answer F1 of IRCoT QA with and without a separate reader for Flan-T5-XXL (top two rows) and GPT3 (bottom two rows).
- When the reader is not used, the answer is extracted from the CoT generated by IRCoT while doing the retrieval.
- Ablating the reader hurts the performance showing its importance.
- post-hoc reader one can also just extract the answer from the CoT generated during retrieval.
- As Table 1 shows, for both Flan-T5-XXL and GPT3, this doesn't work as well.
- This illustrates the importance of having a separate QA reader after the retrieval step.

## Conclusions
- Chain-of-thought prompting has significantly improved prompting-based large language models' ability to perform multi-step reasoning.
- In this work, we leveraged this ability to improve retrieval, and in turn improve QA performance for complex knowledge-intensive open-domain tasks in a fewshot setting.
- One-step question based retrieval is insufficient for such tasks as what information is needed for later steps is not evident from the question alone.
- To address this, we introduced IRCoT, which uses interleaved chain-of-thought reason- ing and retrieval steps that guide each other stepby-step.
- Empirical evaluations on four datasets showed that IRCoT significantly improves retrieval performance as well as few-shot open-domain QA performance when compared to one-step retrieval, for both large and relatively smaller scale language models.
- Table 3 compares reader choice (Direct vs CoT Prompting) for Flan-T5-XXL and GPT3.
- We find that Flan-T5-XXL works better with Direct Prompting as a reader and GPT3 works better with CoT Prompting as a reader.
- Therefore, for the experiments in the main paper, we go with this choice.
- Note though that the trends discussed in § 5 (IRCoT QA > OneR QA > ZeroR QA) hold regardless of the choice of the reader.
