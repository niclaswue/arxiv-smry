---
title: "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"
date: 2022-07-28T17:59:02.000Z
author: "Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, Jiwen Lu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2207-14284v3.webp" # image path/url
    alt: "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2207.14284)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2207.14284).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/hornet-efficient-high-order-spatial).

# Abstract
- Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial modeling mechanism based on dot-product self-attention.
- In this paper, we show that the key ingredients behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can also be efficiently implemented with a convolution-based framework.
- We present the Recursive Gated Convolution ($\textit{g}^\textit{n}$Conv) that performs high-order spatial interactions with gated convolutions and recursive designs. The new operation is highly flexible and customizable, which is compatible with various variants of convolution and extends the two-order interactions in self-attention to arbitrary orders without introducing significant extra computation.
- $\textit{g}^\textit{n}$Conv can serve as a plug-and-play module to improve various vision Transformers and convolution-based models. Based on the operation, we construct a new family of generic vision backbones named HorNet.
- Extensive experiments on ImageNet classification, COCO object detection and ADE20K semantic segmentation show HorNet outperform Swin Transformers and ConvNeXt by a significant margin with similar overall architecture and training configurations. HorNet also shows favorable scalability to more training data and larger model sizes.

# Paper Content

## Introduction
- Convolutional neural networks (CNNs) have driven remarkable progress in deep learning and computation vision since the introduction of AlexNet in the last decade
- Translation equivariance introduces useful inductive biases to major vision tasks and enables transferability across different input resolutions
- The highly optimized implementation makes it efficient on both high-performance GPUs and edge devices
- Evolution of architectures [32,31,49,50,22,24,51] further increases its popularity on various vision tasks
- Transformer-based architectures [16,52,42] greatly challenge the dominance of CNNs. By combining some successful designs in CNN architectures and the new self-attention mechanism, vision Transformers have shown leading performance on various vision tasks
- What makes vision Transformers more powerful than CNNs?
- Some efforts have been made to improve the CNN architectures by learning from the new designs in vision
- Figure 1: Illustration of our main idea. We show representative spatial modeling operations that perform different orders of interactions. In this paper, we focus on studying explicit spatial interactions between a feature (red) and its neighboring region (light gray).

## Related Work

### Model Architectures
- HorNet is a drop-in replacement for the spatial mixing layer in vision Transformers or modern CNNs
- HorNet-T/S/B/L 7×7 and HorNet-T/S/B/L GF have the same number of blocks as Swin Transformers-S/B/L but with an extra block in stage 2
- HorFPN is a replacement for spatial convolutions in the FPN that considers higher-order spatial interactions

## Experiments
- We conduct extensive experiments
- We present the main results on ImageNet
- We compare them with various architectures
- We also test our models on the downstream dense prediction tasks on commonly used semantic segmentation benchmark ADE20K
- Lastly, we provide ablation studies of our designs and analyze the effectiveness of g n Conv on a wide range of models.

### ImageNet Classification
- Conduct image classification experiments on the widely used ImageNet dataset
- Train HorNet-T/S/B models using the standard ImageNet-1K dataset
- Compare models with state-of-the-art vision Transformers and CNNs
- HorNet surpasses Swin Transformers and ConvNeXt which have similar overall architectures and training configurations by a healthy margin on various model sizes and settings

### Dense Prediction Tasks
- HorNet for semantic segmentation outperforms Swin and ConvNeXt models with similar model sizes and FLOPs
- HorNet GF models achieve better results than HorNet 7×7 and ConvNeXt series by large margins in single-scale mIoU, indicating the global interactions captured by the global filter are helpful for semantic segmentation
- HorFPN for dense prediction can outperform standard FPN by 30% in terms of both box AP and mask AP

### Analysis
- Detailed ablation studies of the g n Conv and our HorNet in Table 6
- Both SE [25] and our g n Conv with n = 1 (g {1,1,1,1} Conv) can improve over the baseline model [*], and g {1,1,1,1} Conv is slightly better
- Evaluations on isotropic architectures (with constant spatial resolutions) show that g n Conv can better realize the functions of self-attention compared to plain convolutions and have better ability to model the complex spatial interactions
- HorNet can achieve better trade-offs than the representative vision Transformers and modern CNNs with regards to model size, FLOPs and GPU latency

## Conclusion
- g n Conv is an efficient, extendable, and translation-equivariant high-order spatial interactions with gated convolutions and recursive designs
- g n Conv can serve as a drop-in replace of the spatial mixing layer in various vision Transformers and convolution-based models
- extensive experiments demonstrate the effectiveness of g n Conv and HorNet on commonly used visual recognition benchmarks
- our implementation of g n Conv is more extendable to achieve arbitrary higher-order spatial interactions under a controllable computational budget
