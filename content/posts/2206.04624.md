---
title: "Factuality Enhanced Language Models for Open-Ended Text Generation"
date: 2022-06-09T17:16:43.000Z
author: "Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, Bryan Catanzaro"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2206-04624v2.webp" # image path/url
    alt: "Factuality Enhanced Language Models for Open-Ended Text Generation" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2206.04624)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2206.04624).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/factuality-enhanced-language-models-for-open).

# Abstract
- Pretrained language models are susceptible to generate text with nonfactual information
- In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation
- Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B
- Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions.
- In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ''uniform randomness'' introduced at every sampling step.
- We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality.
- Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia). We propose a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.

# Paper Content

## Introduction
- Large-scale pre-trained language models (LMs) have demonstrated impressive natural language generation results [1]
- However, the generative LMs (e.g.,  ) are solely trained to model the statistical correlations between subword tokens [5], and have limited capability to generate factually accurate text as illustrated in Table 1.
- As a result, there are increasing concerns about the nonfactual generations from large-scale pre-trained LMs [e.g., [6][7][8], which needs to be adequately addressed for their safe deployment in real-world applications, e.g., content creation [9] and dialogue [10].
- In previous studies, different metrics and methods have been proposed to measure and improve the factual accuracy of language generation within different tasks [11], including text summarization [e.g., [12][13][14][15], question answering [e.g., [16][17][18], and table-to-text generation [e.g., 19,20].
- However, these works focus on the faithfulness (or factuality) of the fine-tuned LMs for particular downstream tasks (i.e., factual consistency between source and target text).
- Little exploration has been made to address the factual errors in pretrained LMs for general-purpose open-ended text generation, where the goal is to generate a coherent continuation from the given context (e.g., the use cases from GPT-2).
- One of the popular methods for enhancing generation factuality is to incorporate external knowledge sources [21][22][23].
- Structured knowledge bases and graphs have been utilized for grounded 2 Related Work Factuality vs. Model Size Lin et al. [31] propose the TruthfulQA benchmark to measure the falsehood generations from different sized LMs.
- The result suggests that bigger LMs pre-trained on web text are generally less truthful than smaller ones in terms of false belief or misconception.
- At first glance, this is contradictory to our observation, however, our work focuses on different knowledge to TruthfulQA work.
- The TruthfulQA benchmark focuses on conceptual knowledge, while our benchmark focuses on factual knowledge [32]
- Large LMs can be good at recalling factual knowledge given substantial pre-training corpus, suggested by previous studies on LM's parameteric knowledge [33], but there still remains room for improvement for reasoning conceptual knowledge [34,35].
- Parametric Factual Knowledge A group of work addresses the factual errors in the parametric knowledge of LMs that is acquired from training corpus [36][37][38].
- The correctness of the parametric knowledge is commonly tested in cloze-style question answering format [33] (e.g., Person X is born in __).
- Efforts are made to fine-tune the pre-trained LM to "inject" more knowledge and improve its ability to answer factual questions without consulting external knowledge source [17].
- Table 1: Example of continuations from the 530B LM with greedy decoding and nucleus sampling p = 0.9, along with the continuation from factuality-enhanced 530B LM with factual-nucleus sampling.
- Red represents nonfactual, green represents factual, and strikethrough represents repetition.
- The LMs will stop generation when they generate

### Ground-Truth Knowledge Preparation
- To evaluate the factuality of a given generation, we need to prepare relevant ground-truth knowledge.
- The required ground-truth knowledge can be either document-level or sentence-level, depending on the type of factuality metrics (discussed in ยง3.3).
- The correctness of factuality evaluation is crucially dependent on the correctness of the ground-truth knowledge.
- To ensure that our factuality evaluation is not distorted by the irrelevant provision of ground-truth knowledge, we do the following:
- For document-level ground-truth knowledge, we directly use the Wikipedia document annotation from the FEVER dataset. This way, we can mitigate any potential error from automatic document retrieval.
- For sentence-level ground-truth knowledge, we do automatic sentence selection by using two different methods to maximize the chance of recalling the relevant ground-truth knowledge.
- We treat the generated text as query q and Wikipedia sentences as a pool of candidates C = {c 1 , c 2 , c 3 , ...c N } where N is the number of sentences in the Wikipedia document.
- One ground-truth sentence is retrieved by obtaining TF-IDF vector representations of q and C and selecting the c i with the highest cosine similarity with the q.
- Another is retrieved by obtaining the contextual representation of q and C using SentenceTransformer [62] and selecting the c j with the highest cosine similarity.

### Evaluation Metrics
- We adapt commonly used metric designs from the hallucination literature [11]: named-entity (NE) based metric and textual entailment based metric.
- Each metric captures a different aspect of factuality, so we use both metrics for better understanding of factuality.
- Hallucinated NE Error Since NEs are one of the core building blocks of "fact", NE-related metric design is one of the common choices in literature [11,63,64].
- In this work, we specifically adopt the NE-based metric [64] that is designed with a belief that a model is hallucinating (making factual errors) if it generates a NE that does not appear in the ground-truth knowledge source.
- We define our NE-based metric to be: NE ER = |HALLU NE | / |ALL NE | where ALL NE is the set of all the NEs detected in the LM generation, and HALLU NE is subset of NE All that does not appear in the ground-truth Wikipedia document.
- Note that evaluating NE ER requires document-level ground-truth.
- To ensure the quality of the metric, we also take the same precautions used by [64].
- For named entities consisting of multiple words, partial n-gram overlaps are also treated as a "match". This ensures we can address the shortened form of named entities -e.g., "Barack Hussein Obama II" vs. "Obama".
- Note that stopwords (e.g., the, a) are not considered in the partial n-gram overlaps.
- The named entities are detected using a publicly available pre-trained NE detection model from Spacy.io.
- We define the entailment ratio as: Entail R = |ENTAIL gen | / |ALL gen |, where ALL gen is set of all generations, and ENTAIL gen is the set of generations that are entailed by a entailment model.
- To obtain the entailment scores, we leverage a pretrained entailment model that is publicly available 6 ; a RoBERTa [69] model fine-tuned on MNLI [70] dataset.
- Entail R requires sentence-level groundtruth because only a few Wikipedia sentences are relevant to specific factual information in a given generation.
- For example, "Barack Obama was born in Hawaii" is only relevant to the Wikipedia sentence that mentions his birth location.
- Note that our Entail R is a stricter form of metric that does not treat neutral class to be factual.
- We also evaluate the generation quality from three aspects: i) Fluency is an important aspect of text generation. We measured it by the mean perplexity of generated continuations evaluated with a large pretrained LM, which is 1.3B LM in this work .
- ii) Diversity is an important characteristic of LM that makes the generation more interesting and engaging -it is bland and boring to always generate same texts. It is measured using the mean number of distinct n-grams (we report 4-gram), normalized by the length of text [71,72] among the 10 generations for each prompt (i.e., in total, 160,000 generations to evaluate the diversity of each method).
- iii) Repetition is a common form of degeneration that is very undesirable. We measure the number of repetitive substrings that get generated at the end of the generations by using the publicly available metric code from Holtzman et al. [29].
- Although NE-based and entailment-based metrics have been used in downstream NLG tasks [11], they have not been utilized for evaluating factual accuracy in open-ended text generation.
- To ensure their validity, we collect human annotations to evaluate the correlation between our automatic factuality metrics with human judgement -i.e., are generations with higher Entail R and lower NE ER errors, more likely to be perceived as factual by human?

### Correlation with Human Judgement
- We obtained human annotations for 200 randomly chosen LM continuations of varying NE ER and Entail R scores.
- The annotators are asked to fact-check the LM continuations against Wikipedia and assign factuality label (1 = Factual : can find supporting Wikipedia evidence. 0 = Non-factual : cannot find supporting Wikipedia evidence).
- The fact-checking annotation is a challenging and time-consuming task, as it requires the annotator to carefully read multiple evidences and reason over them.
- To improve the annotation quality, we have two types of annotations. The first type is two annotations from average English speaking workers on Appen.com platform, and the second type is one "expert" annotation from one of the authors who is familiar with the task and spent solid amount of time checking each samples.
- Based on these three annotations, we do majority voting and report the Pearson correlation results in Table 2.
- We also report the correlation result solely using the expert annotations, and show that there is strong correlation between human judgement of factuality and the proposed automatic metric NE ER and Entail R .

## Factuality Analysis of Pretrained LMs
- Model size has a significant impact on the factuality of generated sentences
- Factual prompts lead to less nonfactual generations, while nonfactual prompts always result in less factual generations
- Greedy decoding is more factual, but sacrifices generation diversity and quality
- Named Entity Mix-up and Fabricated Fact are the two notable error types

## Factual-Nucleus Sampling
- The new sampling algorithm achieves a better trade-off between generation quality and factuality than existing decoding algorithms.
- The new algorithm is more efficient and produces less noise than existing decoding algorithms.

### Method
- The randomness of sampling is more harmful to factuality when it is used to generate the latter part of a sentence than the beginning of a sentence.
- There is no preceding text at the start of a sentence, so it is safe for LM to generate anything as long as it is grammatical and contextual.
- However, as the generation proceeds, the premise become more determined, and fewer word choices can make the sentence factual.
- Given the example "Samuel Witwer's father is a Lutheran minister", the beginning of the sentence "Samuel Witwer's father is" is not nonfactual. However, the continuation of "Lutheran minister" makes the sentence nonfactual.
- Therefore, we introduce the factual-nucleus sampling algorithm that dynamically adapts the "nucleus" p along the generation of each sentence.
- In factual-nucleus sampling, the nucleus probability p t to generate the t-th token within each sentence is, where ฮป is the decay factor for top-p probability, and ฯ lower bounds the decay of probability.
- Specifically, it has the following parts: โข ฮป-decay: Given that top-p sampling pool is selected as a set of subwords whose cumulative probability exceeds p, we gradually decay the p value with decay factor ฮป at each generation step to reduce the "randomness" through time.
- p-reset: The nucleus probability p can quickly decay to a small value after a long generation. So, we reset the p-value to the default value at the beginning of every new sentence in the generation (we identify the beginning of a new sentence by checking if the previous step has generated a full-stop). This reduces the unnecessary cost of diversity for any long generations.
- ฯ-bound: If ฮป-decay is applied alone, the p-value could become too small to be equivalent to greedy decoding and hurt diversity. To overcome this, we introduce a lower-bound ฯ to limit how far p-value can be decayed.
- We will show the importance of each parts with ablation studies.

## Factuality-Enhanced Continued Training
- Factuality-enhanced method for continued training of LMs
- TOPICPREFIX for better awareness of facts
- Unstructured factual knowledge typically exists at a document level (i.e., a group of factual sentences about an entity)
- This means that sentences can contain pronouns (e.g., she, he, it), making these sentences factually useless standalone
- To illustrate with an example from Barack Obama's Wikipedia page, "He previously served as a U.S. senator from Illinois from 2005 to 2008" cannot be a useful standalone fact because it is unclear who "He" is
- Due to the GPU memory limit and computation efficiency, it is common to chunk documents in LM training corpus
- This causes the "fragmentation" of information and leads to wrong associations of entities that appear in independent documents with similar contexts
- As a remedy, we propose to prepend TOPICPREFIX to sentences in the factual documents

### Sentence Completion Loss
- We propose a sentence completion loss to address the incorrect association learned between entities.
- To explain our rationale, let us recall the nonfactual example from ยง5: "Samuel Witwer's father is a Lutheran minister".
- This sentence is nonfactual because LM failed to generate factually correct information after "is".
- In other words, LM failed to accurately complete the sentence given the generated context.
- One reason is that the LM is uniformly trained to predict each subword token within the sentence, when ensuring the correct prediction at the latter section of sentence is more critical for factuality.
- Therefore, we construct a sentence completion loss, which makes the LM focus on predicting the subwords later in the sentence.
- For implementation, we determine a pivot t for each sentence, and then apply zero-masking for all token prediction losses before t.
- This pivot is only required during training (i.e., no pivot needed during inference time).
- We emphasize that this loss masking is different from the input token masking applied in BERT [73] or BART [76], and the LM is still trained in an autoregressive manner.

### Results
- The pre-training corpus of LM contains both factual texts (e.g., Wikipedia) and potentially nonfactual texts (e.g., rumors, fake news)
- The nonfactual domain of the training corpus could be the problem.
- Thus, we conduct a baseline experiment that does domain-adaptive training with strictly factual domain text only (i.e., Wikipedia).
- Interestingly, we find that domain-adaptive training can hardly improve generation factuality.
- Effect of TOPICPREFIX Continued pre-training of 1.3B LM with TOPICPREFIX preprocessed Wikipedia alone can already improve the factuality, especially in terms of NE ER .
- For example, it reduces the NE ER from 42.1% to 27.6% when we use the factual-nucleus decoding (0.9 | 0.9 | 0.3), which even outperforms the 1.3B with greedy decoding (NE ER : 27.6% vs. 39.9%) with much less repetition (8.0% vs. 33.1%).
- The proposed sentence completion loss further helps to improve the factuality, especially for the Entail R .
- For example, when one uses factual-nucleus show consistent improvement across different pivot selection strategies, suggesting that the sentence completion loss is robust.
- In particular, the simplest SC HALF performs as good as others or even better in terms of several metrics. Thus we recommend it as the default option.
- As expected, our method on 530B LM further reduces the factual errors and achieves the lowest NE ER (14.5%) and the highest Entail R (25.5%).
- Surprisingly, our method on 530B LM lead to less diverse generation than 1.3B LM despite the significant improvement in the generation quality (i.e., near perfect repetition scores 0.1% 0.2%).
- We conjecture that this is the trade-off between the factuality and diversity for 530B LM.

## Conclusion
- establishing a benchmark to measure and analyze factuality in open-ended text generation tasks
- factual-nucleus sampling that improves generation factuality at inference time
- combination of sentence completion loss and TOPICPREFIX pre-processing that improves factuality with continued training
- demonstrating that our methods are effective in improving the factuality
- exposing the existence of the trade-off between diversity and factuality
