---
title: "Fairness in Language Models Beyond English: Gaps and Challenges"
date: 2023-02-24T11:25:50.000Z
author: "Krithika Ramesh, Sunayana Sitaram, Monojit Choudhury"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2302-12578v2.webp" # image path/url
    alt: "Fairness in Language Models Beyond English: Gaps and Challenges" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2302.12578)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2302.12578).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/fairness-in-language-models-beyond-english).

# Abstract
- Language models are not treating diverse demographic groups fairly.
- Most research on fairness has been focused on English.
- This paper looks at fairness in multilingual and non-English contexts.
- Current research is limited and cannot be scaled across languages and cultures.

# Paper Content

## Introduction
- Language models are susceptible to spurious correlations and encoding biases
- Representational harms refer to groups being misrepresented or underrepresented
- Allocational harms refer to inequitable distribution of resources and opportunities
- Bias can occur in multiple steps of the pipeline
- Most work on fairness in NLP is Anglo-centric
- NLP systems can reinforce and reproduce social and racial hierarchies
- Insufficient documentation of harms from unfair models
- Interplay between privacy, efficiency, and fairness in NLP is understudied

### Metrics for measurement
- Monolingual systems have biases and challenges
- Bias in NLP models can be quantified with intrinsic and extrinsic measures
- WEAT and SEAT are commonly used metrics
- StereoSet and CrowS-Pair measure stereotypical proclivity
- Blodgett et al. (2021) points out flaws in data quality
- Other intrinsic measures have been proposed
- Recent studies compare metrics and provide insights into their potential drawbacks

### Intrinsic vs extrinsic evaluation
- Intrinsic measures indicate the existence of representational bias in systems.
- Little work has been done on addressing bias in extrinsic evaluation.
- Extrinsic evaluation measures are important for bias mitigation objectives.
- Evaluating fairness on downstream task's outputs allows us to gauge how a particular demographic may be affected by biases.
- Monolingual studies have unanswered questions and inconclusive results.
- Template-based data may be unreliable and natural sentence prompts should be used.

### Fairness from the lens of multiple social dimensions
- Existing literature focuses on gender bias, not other dimensions like race and religion.
- Evaluation metrics should be able to evaluate harms over intersectional identities.
- Previous research has emphasized the importance of fairness evaluation and mitigation over intersectional identities.
- Most fairness measures do not account for intersectionality of identities.
- Need more work in benchmarking debiasing techniques to assess their effectiveness.

## Linguistic aspects
- Linguistic variations between languages pose additional problems in multilingual NLP
- Gender has multiple definitions in linguistic terms
- Grammatical gender can affect bias in multilingual and monolingual spaces
- Referential gender deals with terms that address a person's gender
- Lexical gender deals with terms that describe gender
- Bio-social gender involves a mixture of phenotypic traits, gender expression, and identity
- Variations in pronoun complexity and linguistic forms can lead to pronoun changes based on social dynamic

### Grammatically gendered languages
- Linguistics recognizes multiple forms of gender
- Grammatically gendered languages have 2-20 forms of gender divisions
- English is not a grammatically gendered language
- Zhou et al. (2019) examines bias from the view of grammatically gendered languages
- Gonen et al. (2019) show that grammatical gender affects word representations
- Cultural context must be incorporated when curating data for languages
- Hovy and Yang (2021) claim NLP literature oversimplifies information content
- Espa침a-Bonet and Barr칩n-Cede침o (2022) advocate for culturally-sensitive datasets
- Kaneko et al. (2022) proposes a way to generate parallel corpora for other languages

## Multilingual models
- Multilingual spaces allow words to be mapped to their equivalents in other languages
- Training multilingual language models can improve cross-lingual performance on low-resource languages
- Performance across languages begins to dip as the number of languages increases
- Few studies explore the impact of multilingual training on biases

### An outline of fairness evaluation in the context of multilinguality
- Several datasets have been created for multilingual evaluation
- Zhao et al. (2020) quantified biases in multilingual spaces
- Factors that influence bias include language's linguistic properties, target language used for alignment, and transfer learning
- Huang et al. (2020) released first multilingual Twitter corpus for hate speech detection
- Liang et al. (2020) proposed a novel debiasing method
- C칙mara et al. (2022) studied gender and ethnicity in multilingual language models
- Kaneko et al. (2022) proposed a method to use parallel corpora to evaluate bias
- Wolfe and Caliskan (2022) and Wolfe et al. (2022) studied fairness in multimodal contexts
- Wang et al. (2022) studied fairness from a multilingual view in multimodal representations
- Talat et al. (2022) expressed criticism over primary data source for multilingual large language models being English

### An outline of fairness mitigation in the context of multilinguality
- Multilingual spaces are composed of different languages with different properties
- Balancing corpus and transferring to a gendered language's embedding space can reduce bias
- FEDA domain adaption technique can be used to mitigate bias in multilingual text classification
- Adversarial training, token masking, and instance weighting can reduce impact of data instances that lead to bias
- Zero-shot debiasing may be beneficial, but further study is needed to confirm

### Problems in multilingual evaluation and mitigation
- Lack of datasets and literature for evaluation across tasks
- Monolingual research not replicated in multilingual setting
- Datasets neglect less-represented demographics
- Variations in annotator information can result in bias
- No work on evaluating bias in language contact settings
- Models may not learn useful representations of marginalized identities

## Culture
- Language and culture are intrinsically linked.
- NLP research has historically focused on information content, not contextual information.
- Hovy and Yang (2021) proposed a taxonomy of 7 social factors to incorporate into models.
- Variations in language can be controlled by the speaker and receiver.
- Systems can discriminate against marginalized communities.
- Cultural taboos and stereotypes can be highly localized.
- Gender is often treated as a binary variable, but this does not reflect real-world settings.

## Moving towards inclusive systems in all languages
- Multilingualism presents challenges in terms of fairness
- Current solutions may not be viable
- Language and social aspects are ever-evolving
- Social interactions from diverse linguistic backgrounds need to be studied
- Models can exhibit bias based on linguistic variations
- LLMs are often biased towards certain demographic strata
- Interdisciplinary and multicultural teams are needed to identify and mitigate bias
- Better evaluation benchmarks and data collection are needed
- Zero-shot techniques should be used with care in fairness-critical applications
- Multilingual models need to incorporate shared value systems

## Limitations
- Examines fairness literature in languages other than English
- Identifies bias measurement and mitigation strategies
- Notes that authors have a heterogeneous background which may lead to missing diverse perspectives on linguistic and cultural aspects of bias
