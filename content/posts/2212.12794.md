---
title: "GraphCast: Learning skillful medium-range global weather forecasting"
date: 2022-12-24T18:15:39.000Z
author: "Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, Oriol Vinyals, Shakir Mohamed, Peter Battaglia"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-12794v1.webp" # image path/url
    alt: "GraphCast: Learning skillful medium-range global weather forecasting" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.12794)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.12794).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range).

# Abstract
- GraphCast is a machine-learning-based weather simulator that outperforms the most accurate deterministic operational medium-range weather forecasting system in the world.
- GraphCast is an autoregressive model, based on graph neural networks and a novel high-resolution multi-scale mesh representation.
- GraphCast was trained on historical weather data from the European Centre for Medium-Range Weather Forecasts' (ECMWF) ERA5 reanalysis archive.
- GraphCast can make 10-day forecasts, at 6-hour time intervals, of five surface variables and six atmospheric variables, each at 37 vertical pressure levels, on a 0.25-degree latitude-longitude grid, which corresponds to roughly 25 x 25 kilometer resolution at the equator.
- Our results show GraphCast is more accurate than ECMWF's deterministic operational forecasting system, HRES, on 90.0% of the 2760 variable and lead time combinations we evaluated.
- GraphCast also outperforms the most accurate previous ML-based weather forecasting model on 99.2% of the 252 targets it reported.
- GraphCast can generate a 10-day forecast (35 gigabytes of data) in under 60 seconds on Cloud TPU v4 hardware.

# Paper Content

## Introduction
- People rely on medium-range weather forecasts, which are provided up to four times a day by weather bureaus
- Medium-range weather forecasts are generated by simulations run on large high-performance computing (HPC) clusters
- The first component of medium-range weather forecasts is "data assimilation", which is the process of inferring and tracking the weather, based on recent and past observations from satellites, weather stations, ships, etc.
- The second component is a forecast model, traditionally based on "numerical weather prediction" (NWP), which predicts the future temporal evolution of variables that represent the state of the weather
- For decades, there has been tremendous investment in NWP systems, but ML-based methods have only recently begun to be competitive with traditional NWP
- Today there are vast archives of weather and climatological observations, but traditionally there have been few direct means for these data to improve the quality of forecast models

## ERA5 dataset
- For training and evaluating GraphCast, we built our datasets from a subset of ECMWF's ERA5 (Hersbach et al., 2020) 2 reanalysis archive, for the years 1979-2018, at 6 hour time intervals (corresponding to 00z, 06z, 12z, 18z each day), at 0.25°horizontal latitude-longitude resolution, and 37 vertical atmospheric pressure levels.
- Reanalysis means performing data assimilation on historical weather observations, to estimate the full state of the weather globally over time, and ERA5 is regarded as the most comprehensive and accurate reanalysis archive in the world.
- Our model predicts a total of 227 target variables, which includes 5 surface variables, plus 6 atmospheric variables at each of 37 pressure levels (several other static and/or external variables were also provided as input context for our model-see Appendix Table A.1).
- These variables are uniquely identified by their short name (and the pressure level, for atmospheric variables).
- For example, the surface variable "2 metre temperature" is denoted 2 ; the atmospheric variable "Geopotential" at pressure level 500 hPa is denoted 5 0 0.
- The modeled surface variables were: 2 metre temperature (2 ), 10 metre u wind component (1 0 ), 10 metre v wind component (1 0 ), mean sea level pressure ( ), and total precipitation ( ) (accumulated over the previous 6 hours).
- The atmospheric variables, which were represented at each of the 37 pressure levels, were: geopotential ( ), specific humidity ( ), temperature ( ), u component of wind ( ), v component of wind ( ), and vertical velocity ( ).
- The static/external variables include information such as the geometry of the grid/mesh, orography and radiation at the top of the atmosphere. See Appendix Table A.1 for more information about the variables, and Appendix A.1 and Appendix A.2.4 for full details.
- As shown in Figure 1a, we represent the weather state at time index, , as   .
- The grid encircling the Earth corresponds to the variables at every latitude, longitude, and pressure level.
- The surface and atmospheric variables are illustrated by the yellow and blue boxes in the magnified view, respectively.
- We refer to the subset of variables in   that correspond to a particular grid point  (1,038,240 in total) as x   , and to each variable  of the 227 target variables as   .

### Generating a forecast
- GraphCast takes as input two weather states, (  ,  −1 ), which correspond to the current time, , and the immediately preceding time,  − 1, and predicts the weather state at the next time step (as depicted in Figure 1b),
- To generate a -step forecast, X+1:+ = ( X+1 , . . . , X+ ), GraphCast iteratively applies Equation (1) in an autoregressive fashion, feeding its own predictions back in as input to predict later steps (i.e., to predict step  + 2, the input is ( X+1 ,   ); to predict step  + 3, the input is ( X+2 , X+1 )). Figure 1b,c depicts this process, and see Appendix Equation A.5 for details.

### Architecture
- GNNs are effective at learning complex physical dynamics
- GNNs are restricted to local interactions
- GraphCast uses a multi-mesh representation to allow long-range interactions
- The encoder and decoder do not require the raw data to be arranged in a regular rectilinear grid

### Training procedure
- GraphCast was trained to minimize an objective function over 12-step forecasts (3 days) against ERA5 targets
- The objective function was, which averages the squared errors over forecast date-times, lead times, spatial locations, variables and levels, where • 0 ∈  batch represent forecast initialization date-times in a batch of forecasts in the training set, • ∈ 1 :  train are the lead times that correspond to the  train autoregressive steps during training, • ∈  0.25°a re the spatial latitude and longitude coordinates in the grid, • ∈  indexes the variable and level, e.g.,  = { 1 0 0 0, 8 5 0, . . . and   0 + , are predicted and target values for some variable-level, location, and lead time, •   is the per-variable-level inverse variance of single-timestep differences, •   is the per-variable-level loss weight, •   is the normalized area of the latitude-longitude grid cell, which varies with latitude. are per-variable-level inverse variance estimates of the time differences.
- The   are per-variable-level loss weights we specified in a simple way, to control how heavily different target variables are weighed during optimization. The   weight depends on latitude, and weights the errors proportionally to the area of their corresponding grid cells.
- In principle, GraphCast can be retrained (or fine-tuned) regularly, on the most recent weather data, to potentially reap these benefits.
- Training GraphCast took about 3 weeks on 32 Cloud TPU v4 devices using batch parallelism.

## Model evaluation
- GraphCast is skillful at predicting future weather conditions
- The skill of GraphCast is measured using the RMSE and ACC
- RMSE measures the magnitude of the differences between forecasts and ground truth, while ACC measures how well the model forecasts differences from climatology
- For skill scores, we use the normalized RMSE difference between model and baseline as (RMSE − RMSE)/RMSE
- HRES uses HRES analysis as input, so a separate dataset, termed "HRES-fc0", was built to serve as ground truth
- The pressure levels at which GraphCast was evaluated were the 13 used in WeatherBench (Rasp et al., 2020)
- Figure 2 and Figure 3 show sequences of states from ERA5, an HRES forecast, and a GraphCast forecast (top three rows), and the fourth row shows the absolute value of the difference between HRES's forecasted state and the HRES-fc0 ground truth
- The fifth row shows the absolute value of the difference between GraphCast's forecasted state and the ERA5 ground truth

## Results
- GraphCast comprehensively outperforms HRES's weather forecasting skill across 10-day forecasts, at 0.25°horizontal resolution.
- Figure 4 shows how GraphCast (blue lines) clearly outperforms HRES (black lines) for our 10 headline surface and atmospheric variables (see Section 3 for evaluation protocol), chosen from the ECMWF Scorecard for pressure levels closest to the surface.
- Each subplot corresponds to a variable (and pressure level, for atmospheric variables) and skill (y-axis) is plotted at 6-hour steps over 10 day horizons (x-axis).
- Rows 1 and 3 show absolute RMSE, and rows 2 and 4 show the corresponding normalized differences in RMSE.
- From the initial 6-hour step, through all 40 steps over 10 days, GraphCast almost always has higher skill, and the normalized differences are often 30% lower error than HRES at early lead times, while typically plateauing to around 10 − 15% after 10 days.
- We found similar results when evaluating ACC, as shown in Figure 5.
- We also performed a regional analysis, which indicates that these results are consistent across the entire globe (see Appendix Figure 6 summarizes the normalized differences (i.e., Figure 4's rows 2 and 4) for all variables and pressure levels, across the 10 day forecasts, in a format analogous to the ECMWF Scorecard. Each row represents a single variable (left row label) at a single level (right row label), where each colored square's color is proportional to the normalized difference (scaled between -1 and 1), where blue indicates GraphCast had lower error than HRES (negative normalized difference), with -1 (solid blue) meaning zero error, and red indicates it had higher error (positive normalized differences), with 1 (solid red) meaning twice the error.
- GraphCast outperformed HRES on 90.0% of the 2760 variables, levels, and lead times in our evaluation set (4 surface variables, plus 5 atmospheric variables × 13 levels, over 10 days with 4 steps per day).
- We note that HRES tended to have superior performance than GraphCast on the upper atmospheric levels, especially pressure level 50 hPa, which is not surprising because the total training loss weight applied to pressure levels at or below 50 hPa was only 0.66% of the total loss weight across all variables and levels (see Section 3 for details).
- When excluding the 50 hPa level, the percentage of the 2240 targets on which GraphCast outperforms HRES is 96.6%; when excluding levels 50 and 100 hPa, the percentage of the 1720 targets is 99.2%.

### How autoregressive training affects forecast skill
- Figure 7 shows how the forecast performance varies with the number of autoregressive (AR) steps used to train the model.
- When trained with fewer autoregressive steps, the model performs better at short lead times, and worse over longer lead times.
- As the number of autoregressive steps is increased, the performance becomes worse at short lead times, but better at longer ones.
- These results suggest potential for combining multiple models with varying numbers of AR steps, e.g., for short, medium and long lead times, to capitalize on their respective advantages across the entire forecast period.
- Of the three most recent ML-based forecasting models in the past year, Keisler (2022)'s model, FCN (Kurth et al., 2022;Pathak et al., 2022), and Pangu-Weather (Bi et al., 2022), the most recent, Pangu-Weather reports results that are categorically stronger than the former two.
- Thus we focus our evaluation of GraphCast on Pangu-Weather, as it represents the state-of-the-art of ML-based weather forecasting.

## Discussion
- We show that our GraphCast model outperforms the most accurate deterministic operational system, ECMWF's HRES, on 10-day forecasts, at 6-hour steps, and 0.25°latitude-longitude resolution.
- We evaluated GraphCast's skill on a comprehensive set of 2760 variable, pressure level, and lead time combinations, and our results showed our model had lower RMSE than HRES on 90.0% of the metrics.
- When we excluded the upper atmosphere fields from 100 hPa and above, GraphCast outperformed HRES on 99.2% of the 1760 targets.
- GraphCast also outperformed the best previous ML baseline, Pangu-Weather (Bi et al., 2022), on 99.2% of the 252 metrics which Bi et al. (2022) reported.
- A key innovation of GraphCast was its novel "multi-mesh" representation, which allows it to capture much longer-range spatial interactions than in traditional NWP methods, and thus support much coarser native timesteps.
- This is part of why GraphCast can generate an accurate 10-day weather forecast, at 6 hour steps, in under 60 seconds on a single Cloud TPU v4 device.
- One important caveat to our work is that we focused on deterministic forecasts, and compared GraphCast's skill only to HRES. While HRES is ECMWF's highest fidelity single forecast, the other pillar of IFS, the ensemble forecasting system, ENS, is of comparable importance, especially for forecast horizons in the 5-15 day range.
- Because weather dynamics are highly nonlinear, and the analysis provided as input to weather models has inherent uncertainty, as a forecast's lead time increases, it becomes increasingly difficult to make accurate point-wise predictions of weather trajectories, and thus modeling the uncertainty becomes increasingly important.
- Over longer time horizons, we notice GraphCast's forecasts become somewhat more blurry than HRES's. This is to be expected, because it was trained to optimize a weighted mean squared error. Thus, the way it expresses uncertainty over longer lead times is by producing a forecast closer to the mean.
- This contrasts with traditional deterministic NWPs, which will make high-resolution, but sometimes incorrect, predictions. However these predictions may be useful for some purposes, such as predicting temperature extremes, or the chance of storms.
- It also contrasts with ensemble forecast models, which produce multiple forecasts from a sample of initial conditions which approximate the uncertainty in the true initial conditions. Statistical measures of the ensemble forecasts are used to quantify uncertainty.
- Ensemble forecasting offers key advantages, such as estimating the distribution more explicitly, however it comes at the cost of requiring many expensive forecasts to be generated.
- Building models that model uncertainty, and can be comprehensively evaluated against ensemble systems, is a crucial next step.
- Another caveat is that we focused on 0.25°latitude-longitude resolution, while HRES operates on 0.1°. This choice was based on the fact that ERA5 is available only at 0.25°, and because there are substantial engineering challenges in working with large 0.25°state representations.
- Nevertheless, there is no principled or technical reason why our approach will not scale to higher resolution, given sufficient data and engineering advances.
