---
title: "One Embedder, Any Task: Instruction-Finetuned Text Embeddings"
date: 2022-12-19T18:57:05.000Z
author: "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09741v2.webp" # image path/url
    alt: "One Embedder, Any Task: Instruction-Finetuned Text Embeddings" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09741)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09741).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/one-embedder-any-task-instruction-finetuned).

# Abstract
- Introduces INSTRUCTOR, a new method for computing text embeddings given task instructions
- Annotates instructions for 330 diverse tasks and trains INSTRUCTOR on this multitask mixture with a contrastive loss
- Evaluates INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training) and achieves state-of-the-art performance

# Paper Content

## Introduction
- Text embeddings represent discrete text inputs (e.g., sentences, documents, and code) as fixed-sized vectors that can be used in many downstream tasks.
- Recently, we have seen dramatic advances in learning text embeddings (Kiros et al., 2015;Conneau et al., 2017;Logeswaran and Lee, 2018;Reimers and Gurevych, 2019;Gao et al., 2021;Ni et al., 2021Ni et al., , 2022) that perform well on their intended tasks or datasets.
- However, most existing embeddings can have significantly degraded performance when applied to new tasks or domains (Thakur et al., 2021;Muennighoff et al., 2022).
- For example, DPR (Karpukhin et al., 2020) is stronger for retrieval than text similarity tasks, and vice versa for SimCSE (Gao et al., 2021). Moreover, existing embeddings usually perform poorly when applied to the same type of task but in different domains such as medicine and finance.
- A common method to address this issue is to further finetune the embed- dings on datasets in downstream tasks and domains, which often requires a lot of annotated data (Gururangan et al., 2020).
- In this paper, we hypothesize that text embeddings (even for the same text input) can be adjusted to different downstream applications using task and domain descriptions, without further task-or domain-specific finetuning.
- Omnifarious Representations), a single multitask model that generates task-and domain-aware embeddings given a text input and its task instructions. It achieves state-of-the-art performance on massively many downstream embedding tasks without any training.
- At the core of our approach is instruction-based finetuning (Zhong et al., 2021;Min et al., 2022;Sanh et al., 2022;Wei et al., 2022): we embed every input together with its end task and domain instruction, departing from prior approaches to embeddings that only take text input.
- INSTRUCTOR embeds the same input into different vectors for different end goals (e.g., Who sings the song "Love Story"? is embedded into three different vectors for different tasks in Fig. 1).
- As shown in Fig. 2, INSTRUCTOR is trained on MEDI, our new collection of 330 text embedding datasets newly annotated with human-written task instructions ( §2.3).
- We train INSTRUCTOR with a contrastive loss over all datasets that maximizes the similarity between semantically related text pairs while minimizing unrelated pairs. We extensively evaluate INSTRUCTOR on diverse domains (e.g., finance, medicine, and news) and a variety of downstream applications (a total of 70 embedding evaluation datasets, including 66 not seen during training), spanning classification, semantic textual similarity, information retrieval, text generation evaluation, and prompt retrieval for incontext learning.
- INSTRUCTOR significantly outperforms prior state-of-the-art embedding models by an average of 3.4% over the 70 diverse datasets.
- INSTRUCTOR also outperforms a variant that is trained without task instructions ( §4), demonstrating the importance of instructions to create taskaware embeddings.
- Our analysis shows that instruction finetuning addresses the challenge of training a single model on diverse datasets ( §4.1).
- Further, we demonstrate that the task diversity of MEDI makes the performance of INSTRUCTOR particularly robust to paraphrases in instructions ( §4.2). Overall, these results strongly suggest that instruction finetuning should be adopted broadly for text embeddings, which we support by sharing all of our models and code.

### Embedding Architecture
- We build an INSTRUCTOR model that is based on the single encoder architecture (Izacard and Grave, 2021;Ni et al., 2021Ni et al., , 2022)).
- Following prior work (Ni et al., 2021(Ni et al., , 2022)), we use GTR models as the backbone encoder (GTR-Base for INSTRUCTOR-Base, GTR-Large for INSTRUCTOR, GTR-XL for INSTRUC-TOR-XL).
- The GTR models are initialized from T5 models, pretrained on a web corpus, and finetuned on information search datasets.
- The availability of different sizes in the GTR model family allows us to explore the scaling behaviors of instructionfinetuned embedding models.
- Given an input text x and a task instruction I x , INSTRUCTOR encodes their concatenation I x ⊕ x.
- We then generate a fixed-sized, task-specific embedding E I (I x , x) by applying mean pooling to the last hidden representations over the tokens in x.

### Training Objective
- The instructor is trained to form a wide variety of tasks as a text-to-text problem of distinguishing good/bad candidate outputs given an input.
- For a retrieval task, x is a query and good/bad y is a relevant/irrelevant document from some document collection.
- For a textual similarity task, the input and output have a similar form and typically come from the same source collection.
- For a classification task, training samples can be formed by choosing y as text sequences associated with the same vs. different classes for good vs. bad examples.
- The goodness of candidate y for input x is given by similarity, which is the cosine between their INSTRUCTOR embeddings.

### Data Construction

## Experiments
- We use the MTEB benchmark to evaluate the effectiveness of an instructor on 70 downstream tasks
- In all three settings, the instructor achieves the state-of-the-art performance
- This research has potential implications for future machine learning

### Embedding Evaluations
- 66 out of 70 evaluation datasets are unseen during training
- MTEB (Muennighoff et al., 2022) is a comprehensive embedding evaluation benchmark
- Following Muennighoff et al., we also report the average performance over 56 datasets
- For each task family, we briefly describe the task objective, evaluation metric, and how embeddings are used
- Retrieval aims to find the most rel- ible documents p i in D for query q
- The embedding model is used to embed q and p 1 ...p n into fixed-sized vectors, and then the similarity between q and p i is measured by their embedding cosine similarity
- There are 14 diverse datasets (e.g., Natural Questions, Scifact, and NFCorpus) together with the community question-answering (CQA) benchmark
- We use NDCG@10 (Normalized Discounted cumulative gain at rank position 10) to measure the performance
- Reranking ranks a list of documents based on their relevance to a query
- Given a query q and a list of documents D = {p 1 , p 2 ...p n }, the embedding model computes embeddings of both the query and documents, which are then used to rank the documents based on their cosine similarities
- Clustering aims to group similar documents into meaningful clusters
- Pair classification tasks aim to predict a binary label for a pair of texts
- Classification is a popular way to evaluate the quality of embeddings
- STS Semantic textual similarity (STS) tasks evaluate the similarity between two sentences
- Given a sentence pair (t 1 , t 2 ), the embedding model maps t 1 and t 2 into embeddings separately, and then the similarity between t 1 and t 2 is measured by their embedding cosine similarity
- The evaluation metric is Spearman's rank correlation, which measures the correlation between the similarity scores and human judgements
- Automatic summarization evaluation aims to evaluate the quality of a machinegenerated summary given a reference summary
- While human evaluations are considered more accurate, automatic evaluations allow for fast, inexpensive development cycles
- Large language models have demonstrated the ability of in-context learning, where the model can perform downstream tasks by conditioning generation on a few task demonstrations

### Baselines
- We use the official MTEB benchmark for comparisons
- The first class of baselines is embedding models specializing in information retrieval: Contriever-MS (Izacard et al., 2022), GTR (Ni et al., 2021), and coCondenser-MS (Gao and Callan, 2022)
- The second class of baselines focuses on semantic textual similarity: SimCSE (Gao et al., 2021), Sent-T5 (Ni et al., 2022), and SGPT-NLI (Muennighoff, 2022)
- All of these baselines are based on pretrained language models

### Main Results
- INSTRUCTOR outperforms all other models on average
- INSTRUCTOR is more efficient than GTR-Large
- INSTRUCTOR is more efficient than Sent-T5-XXL

## Analysis and Ablations
- INSTRUCTOR enables universal text embeddings for many diverse tasks
- by default, we report average performance across all categories

### Instructions Enable Diverse Training
- Symmetric and asymmetric data can be used for training the model
- If the data is only symmetric or asymmetric, the model will perform similarly to or better than the original GTR model
- However, if the data is also heterogeneous, the model will benefit from instructions

### Instruction Robustness
- Previous work (Sanh et al., 2022;Zhou et al., 2022) shows that instruction-finetuned language models are not robust to paraphrased instructions
- Here we measure INSTRUCTOR's robustness to variation in human-written instructions. Specifically, we  write five paraphrased instructions for all evaluation datasets (Table 4 in Appendix) and measure INSTRUCTOR's performance gap between the bestperforming and the worst-performing instructions.
- Fig. 4 shows that inclusion of 300 super-NI datasets is critical to the robustness of INSTRUCTOR.

### Complexity of Instructions
- INSTRUCTOR outperforms the original GTR model, illustrating the effectiveness of instructions for diverse training
- As more information is provided in the instruction (from tag to simple and from simple to detail), we observe consistent improvements
- Fig. 5 shows their average performances across all task categories
- As the encoder transformer model scales up, the performance continues to increase for both GTR and INSTRUCTOR
- Nonetheless, the improvement in INSTRUCTOR is more pronounced, perhaps because embeddings with instructions benefit from larger capacities

### Instructions Mitigate Domain Shifts
- INSTRUCTOR helps models generalize better to unseen domains
- Instruction-based finetuning improves models' ability to generalize

### Qualititive analysis
- Without instructions, pairs with the same sentiment are closer together in the embedding space
- With instructions, our method (INSTRUCTOR) successfully encodes the red dot pairs into close embeddings and correctly classifies the pairs

## Related Work
- Text embeddings are useful in many applications such as information retrieval
- Different from Sentence-T5 trained only on symmetric data or GTR trained only on asymmetric data, we combine both groups of datasets and build MEDI
- Muennighoff et al. (2022) introduced the massive text embedding benchmark, which can be used to evaluate embedding models on a variety of embedding tasks, spanning reranking, classification, information retrieval, bitext mining, pair classification, STS, and summarization
- Their benchmark shows that models performing well on one task may not perform well on other tasks
- The poor zero-shot transfer abilities of existing embedding models make it difficult to use them in applications where only few labeled data are available
- This motivates us to develop a single embedding model that is applicable to a variety of tasks and has better generalization to unseen tasks
- Instruction Finetuning Recent work demonstrated that instruction-finetuned language models could perform new tasks given a natural language instruction
- In this work, we explore finetuning embedding models to follow human instructions where the instruction specifies eventual use cases
- Concurrent work demonstrated that instructions could facilitate information retrieval

## Conclusion
- We introduced INSTRUCTOR, a single model that creates broadly-applicable text embeddings using natural language instructions.
- We constructed MEDI, a collection of diverse datasets, to finetune INSTRUCTOR with instructions. Our extensive experiments showed that INSTRUCTOR achieves state-of-the-art performance on text embedding benchmarks, as well as prompt retrieval for fewshot in-context learning.
- We hope that researchers and practitioners will benefit from our embeddings or our datasets for tasks of their interest.
- We list all instructions for each dataset in MEDI in Table 7.
- Asymmetric 25,000 stackexchange (Silva et al., 2018)
- Asymmetric 25,000 eli5_question_answer (Fan et al., 2019)
- Asymmetric 25,000 squad_pairs (Rajpurkar et al., 2016)
- Asymmetric 25,000 NQ * (Kwiatkowski et al., 2019)
- Asymmetric 50,000 amazon-qa (Gupta et al., 2019)
- Asymmetric 100,000 WikiAnswers (Fader et al., 2014)
- Symmetric 25,000 agnews (Zhang et al., 2015)
- Asymmetric 45,000 AllNLI (Bowman et al., 2015)
- Symmetric 50,000 npr (Li, 2020)
- Asymmetric 25,000 specter_train_triples (Cohan et al., 2020)
- Symmetric 50,000 ccnews_title_text (Hamborg et al., 2017)
- Asymmetric 25,000 triviaqa (Joshi et al., 2017)
- Asymmetric 50,000 zero_shot_re (Levy et al., 2017)
- Asymmetric 15,000 flickr30k_captions (Young et al., 2014)
- Symmetric 25,000 xsum (Narayan et al., 2018)
- Asymmetric 10,000 code_search (Husain et al., (He and McAuley, 2016)
- Asymmetric 100,000 S2ORC_title_abstract (Lo et al., 2020)
- Asymmetric 100,000 PAQ_pairs (Lewis et al., 2021)
- Asymmetric 25,000 wow (Dinan et al., 2019)
- Asymmetric 30,000 trex (Elsahar et al., 2018)
- Asymmetric 30,000 pubmed (Sen et al., 2008)
- Asymmetric 30,000 medmcqa (Pal et al., 2022)
- Asymmetric 30,000 wikihow (Koupaee and Wang, 2018)
- Asymmetric 5,000 simple_wiki (Coster and Kauchak, 2011)
- Asymmetric 5,000 Super-NI (300 datasets) (Wang et al., 2022)
- Symmetric 180,000
