---
title: "Perceptual-Neural-Physical Sound Matching"
date: 2023-01-07T16:17:48.000Z
author: "Han Han, Vincent Lostanlen, Mathieu Lagrange"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-02886v1.webp" # image path/url
    alt: "Perceptual-Neural-Physical Sound Matching" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.02886)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.02886).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/perceptual-neural-physical-sound-matching).

# Abstract
- Sound matching algorithms seek to approximate a target waveform by parametric audio synthesis
- Deep neural networks have achieved promising results in matching sustained harmonic tones
- However, the task is more challenging when targets are nonstationary and inharmonic, e.g., percussion
- We attribute this problem to the inadequacy of loss function. On one hand, mean square error in the parametric domain, known as "P-loss", is simple and fast but fails to accommodate the differing perceptual significance of each parameter. On the other hand, mean square error in the spectrotemporal domain, known as "spectral loss", is perceptually motivated and serves in differentiable digital signal processing (DDSP)
- Yet, spectral loss has more local minima than P-loss and its gradient may be computationally expensive; hence a slow convergence. Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the optimal quadratic approximation of spectral loss while being as fast as P-loss during training. We instantiate PNP with physical modeling synthesis as decoder and joint time-frequency scattering transform (JTFS) as spectral representation. We demonstrate its potential on matching synthetic drum sounds in comparison with other loss functions.

# Paper Content

## INTRODUCTION
- Sound matching is the task of retrieving the parameter setting θ that "matches" a target sound x
- Sound matching has applications in automatic music transcription, virtual reality, and audio engineering
- Of particular interest is the case where g(θ) solves a known partial differential equation (PDE) whose coefficients are contained in the vector θ
- In sound matching, the paradigm of differentiable digital signal processing (DDSP) has brought a principled methodology to address this issue
- The key idea behind DDSP is to chain the learnable encoder f W with the known decoder g and a non-learnable but differentiable feature map Φ
- The PNP encoder is penalized in terms of a first-order Taylor expansion of the spectral loss Φ(x) − Φ(x) 2

## METHODS

## Approximating spectral loss with Riemannian geometry
- The synthesizer g and the feature map Φ are continuously differentiable
- The spectral loss L DDSP associated to the triplet (Φ, f W , g) is:
- The perceptual-neural-physical loss (PNP) associated to (Φ, f W , g) is the linearization of spectral loss at θ

## Damped least squares
- The principal components of the Jacobian ∇ (Φ•g) (θ) are the eigenvectors of M(θ).
- We denote them by vj and the corresponding eigenvalues by σ 2 j : for each of them, we have M(θ)vj = σ 2 j vj.
- The vj's form an orthonormal basis of R J , in which we can decompose the parameter deviation ( θ − θ).
- Recalling Equation 5, we obtain an alternative formula for PNP loss:
- The eigenvalues σ 2 j stretch and compress the error vector along their associated direction vj, analogous to the magnification and suppression of perceptually relevant and irrelevant parameter deviations.
- In practice however, when σ 2 j cover drastic ranges or contain zeros, as presented below in Section 4.3, the error vector is subject to extreme distortion and potential instability due to numerical precision errors.
- These scenarios, commonly referred to as M being ill-conditioned, can lead to intractable learning objective L PNP θ .
- Reminiscent of the damping mechanism introduced in Levenberg-Marquardt algorithm when solving nonlinear optimization problems, we update Equation 5as
- The damping term λI up-shifts all eigenvalues of M by a constant positive amount λ, thereby changing its condition number.
- When λ is huge, M (θ) + λI is close to an identity matrix with uniform eigenvalues, L PNP θ is optimizing in parameter loss regime.
- On the other hand when λ is small, small correctional effects keeps L PNP θ in the spectral loss regime.
- Alternatively, Equation 8 may also be viewed as a L2 regularization with coefficient λ, which allows smooth transition between spectral and parameter loss regimes.
- To further address potential convergence issues, λ may be scheduled or adaptively changed according to epoch validation loss.

## APPLICATION TO DRUM SOUND MATCHING

## Perceptual: Joint time-frequency scattering (JTFS)
- The joint time-frequency scattering transform (JTFS) is a nonlinear convolutional operator which extracts spectrotemporal modulations in the constant-Q scalogram [9].
- Its kernels proceed from a separable product between two complex-valued wavelet filterbanks, defined over the time axis and over the log-frequency axis respectively.
- After convolution, we apply pointwise complex modulus and temporal averaging to each JTFS coefficient.
- These coefficients are known as scattering "paths" p.
- We apply a logarithmic transformation to the feature vector JTFS(xn) corresponding to each sound xn, yielding where we have set the hyperparameter ε = 10 −3 of the order of the median value of JTFS across all examples xn and paths p.
- The multiresolution structure of JTFS is reminiscent of spectrotemporal receptive fields (STRF), and thus may serve as a biologically plausible predictor of neurophysiological responses in the primary auditory cortex [10].
- At a higher level of music cognition, a recent study has shown that Euclidean distances in Φ space predict auditory judgments of timbre similarity within a large vocabulary of instrumental playing techniques, as collected from a group of professional composers and non-expert music listeners [11].

## Neural: Deep convolutional network (convnet)
- EfficientNet is a convolutional neural network architecture that balances the scaling of the depth, width and input resolution of consecutive convolutional blocks
- Achieving state-of-the-art performance on image classification with significantly less trainable parameters, its most light-weight version EfficientNet-B0 also succeeded in benchmarking audio classification tasks
- We adopt EfficientNet-B0 as our encoder f W , resulting in 4M learnable parameters
- We append a linear dense layer of J = dim θ neurons and a 1D batch normalization before tanh activation.

## Physical: Functional transformation method (FTM)
- ω1 < ω2 < ω3
- τ1 < τ2 < τ3
- D > 0
- p > 0
- θ = {log ω1, τ1, log p, log D, α}
- sonically-plausible ranges for each parameter in θ

## RESULTS

## Baselines
- We train fW with 3 different losses -multi-scale spectral loss [19], parameter loss, and PNP loss.
- We use a batch size of 64 samples for spectral loss, and 256 samples for parameter and PNP loss.
- The training proceeds for 70 epochs, where around 20% of the training set is seen at each epoch.
- We use Adam optimizer with learning rate 10 −3 .
- Table 1 reports the training time per epoch on a single Tesla V100 16GB GPU.

## Evaluation with JTFS-based spectral loss
- We propose to use the L2 norm of JTFS coefficients error averaged over test set for evaluation
- As a point of reference, we also include the average multi-scale spectral error, implemented as in Section 4.1.
- One of the key distinctions between Euclidean JTFS distance and multi-scale spectral error is the former's inclusion of spectrotemporal modulations information.
- Meanwhile unlike mean squared parameter error, both metrics reflect the perceptual closeness instead of parametric retrieval accuracy for each proposed model.

## Discussion
- Despite being the optimal quadratic approximation of spectral loss, it is nontrivial to apply the bear PNP loss form as Equation 5 experimental settings.
- On one hand, Φ • g potentially has undesirable property that exposes the Riemannian metric calculations to numerical precision errors.
- On the other hand, extreme deformation of the optimization landscape may lead to the same numerical instability facing stochastic gradient descent with spectral loss.
- We report on a few remedies that helped stabilize learning with PNP loss, and offer insights on future directions to take.
- First and foremost, our preliminary experiments show that training PNP loss without damping λ = 0 subjects to serious convergence issues.
- Recalling Section 2.2, indeed our empirical Ms suffer from high condition numbers.
- Fig. 2 shows the sorted eigenvalue distribution of all Ms in test set, where Ms are rank-2,3 or 4 matrices with eigenvalues ranging from 0 to 10 20 .
- This could be an implication that entries of θ contain implicit linear dependencies in generator g, or that local variations of certain θ fail to linearize differences in the output of g or Φ • g.
- As an example, the aspect ratio α influences the modal frequencies and decay rates via [18,, where in fact its variant 1/α + 1/α 2 could be a better choice of variable that linearizes g.
- To address Ms' ill conditions we attempted at numerous damping mechanisms to update λ, namely constant λ, scheduled λ decay, and adaptive λ decay.
- The intuition is to have L PNP θ start in the parameter loss regime and move towards the spectral loss regime while training.
- The best performing model is achieved with adaptive λ decay, as described in Section 2.2.
- We initialize λ to be 10 20 to match the largest empirical σ 2 j , which later gets adaptively decayed to 3 × 10 14 in 20 epochs, breaking records 7 times. This indicates that f W is able to learn with damped PNP loss, under the condition that λ being large enough to simulate parameter loss regime and compensate for deficiency in M.
- The diagonal elements of M(θ) can be regarded as both the applied weights' magnitudes and proxies for θ's perceptual significance.
- To gain further insights on how each model regresses different parameters, we visualize in Fig. 3 pairs of (| θ − θ| 2 j , M(θ)j,j).
- Three trends can be observed: First, τ and ω are regressed with the best accuracy across all learning objectives.
- Second, spectral loss particularly struggles in pitch ω and inharmonicity p retrievals.
- Third, we may interpret x-axis as describing from left to right samples with increasing perceptual significance.
- We observe that in Fig. 3(b), PNP loss is able to suppress more errors in samples with high M(θ)j,j than parameter loss, by a nonnegligible margin.
- We believe that more of PNP loss' mathematical potential can be exploited in the future, notably its ability to interpolate between various loss regimes and its use in hybrid optimization schemes.

## CONCLUSION
- Perceptual-Neural-Physical (PNP) autoencoding is a bilinear form learning objective for sound matching task
- In our application, PNP optimizes the retrieval of physical parameters from sounds in a perceptually-motivated metric space
- We demonstrated PNP's mathematical proximity to spectral loss and its generalizability to parameter loss
- Using this formulation, we motivated and established one way of enabling smooth transition between optimizing in parameter and spectral loss regimes
- Damping mechanisms are used to facilitate its learning under ill-conditioned empirical settings
- The best performing models with known and unknown pitch are P-loss and PNP loss respectively
