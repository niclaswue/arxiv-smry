---
title: "Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes"
date: 2023-01-04T18:34:25.000Z
author: "Justin Reppert, Ben Rachbach, Charlie George, Luke Stebbing, Jungwon Byun, Maggie Appleton, Andreas Stuhlmüller"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "/home/niclas/arxiv-smry/arxiv-smry/static/thumbnails/2301-01751v2.webp" # image path/url
    alt: "Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01751)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01751).


# Abstract
- Language models can perform complex reasoning either end-to-end, with hidden latent state, or compositionally, with transparent intermediate state
- Composition offers benefits for interpretability and safety, but may need workflow support and infrastructure to remain competitive
- We describe iterated decomposition, a human-in-the-loop workflow for developing and refining compositional LM programs
- We improve the performance of compositions by zooming in on failing components and refining them through decomposition, additional context, chain of thought, etc.
- To support this workflow, we develop ICE, an open-source tool for visualizing the execution traces of LM programs
- We apply iterated decomposition to three real-world tasks and improve the accuracy of LM programs over less compositional baselines: describing the placebo used in a randomized controlled trial (25% to 65%), evaluating participant adherence to a medical intervention (53% to 70%), and answering NLP questions on the Qasper dataset (38% to 69%).

# Paper Content

## Introduction
- Language models are often trained using feedback on outcomes, leveraging reward models that imitate human evaluations provided as pairwise comparisons (Christiano et al., 2017;Ziegler et al., 2020).
- This works well for basic question-answering, summarization, simple code generation, and general short-form instruction-following (Stiennon et al., 2020;Wu et al., 2021;Ouyang et al., 2022).
- However, as model capabilities and task complexities scale up, outcome-based evaluation may run into alignment problems: First, for some important applications the process used to generate the output matters as much as the output itself. Consider long-range forecasting, and policy decisions informed by such forecasts: The quality of a forecast or decision depends on the assumptions, evidence, and reasoning used to produce it.
- If feedback doesn't inform the process used to generate outputs, we may get results that look good (because they are optimized for favorable evaluation), and may even look systematic, but are worse in exactly the ways that can't easily be measured (Stuhlmüller and Byun, 2022).
- Second, outcome-based feedback may create incentives for language models to deceive or manipulate their users by exploiting gaps or biases in the feedback signal (Amodei et al., 2016). In extreme cases, this could lead to models behaving as intended during training, hiding their true intentions and capabilities, but defecting at deployment (Cotra, 2022;Ngo et al., 2022).
- Process supervision is an alternative to outcome-based training that uses language models to execute humanunderstandable task decompositions, either by imitating human steps or by rewarding human-endorsed steps (Christiano, 2016;Stuhlmüller and Byun, 2022;Uesato et al., 2022).
- This paradigm promises increased interpretability, trust, and alignment by reducing the reliance on black-box computation and enabling users to inspect and intervene in the model's reasoning process.
- Right now, process supervision is ahead of outcome-based training: Language model capabilities are weak, so complex tasks require a composition of multiple calls.
- Indeed, engineered multi-step pipelines have been a staple of NLP for a long time. However, without scalable infrastructure to support it, we expect that outcome-based training will eventually crush process supervision performance-wise, leading to the alignment problems above.
- By sharing our workflow and tooling now when even basic tasks are still challenging, we hope to accelerate a future where LM deployments are controllable and interpretable.

## Process Supervision
- Process supervision refers to approaches to LM training and deployment that rely on human-understandable intermediate steps.
- We review the literature and highlight gaps, then explain the iterated decomposition workflow and ICE visualizer.

### Prior work on process supervision
- Over the past year, there have been significant advances in techniques for process supervision
- At the same time, real-world use cases are still rare
- We review prior work and highlight gaps in the literature that may be contributing to this
- This is a rapidly growing field, so we are only able to review a sample of the work (Table 1)
- Table 1: A sample of prior work on process supervision, categorized into work that primarily contributes (1) new task decompositions, (2) training and finetuning techniques, (3) workflows and tutorials, (4) surveys and frameworks for organizing prior work, (5) tools, and (6) theory.
- While work on decompositions and training techniques is rapidly growing, there is little investigation of workflows, tooling, and theory
- There is a quickly growing literature on how to compose multiple language model calls to improve performance or accomplish more difficult tasks
- • Creswell et al. (2022) and Creswell and Shanahan (2022) generate reasoning steps by alternating between selection and inference. They show that their approach outperforms other prompting methods on multi-step logical deduction and scientific QA tasks, and generates interpretable reasoning traces.
- • Kazemi et al. ( 2022) apply backward-chaining to simple logic tasks, starting with a goal proposition and recursively decomposing it into sub-goals until the sub-goals can be proved or disproved.
- • Wu et al. (2021) and Saha et al. (2022) apply recursive summarization to generate summaries of long texts, such as books or articles. They use LMs to summarize small sections of the text and then recursively summarize these summaries to produce a summary of the entire text. They show that recursive summarization improves the quality and coherence of the summaries, and enables human feedback and evaluation.
- • Yang et al. (2022) generate long stories by first creating a story plan, generating passages b yprompting a model with contextual information from the plan and the current story state, and then revising the passages by reranking and editing them.
- • ReAct (Yao et al., 2022) interleaves generating chain-of-thought reasoning and actions pertaining to a task (e.g., search, lookup).
- • WebGPT (Nakano et al., 2021) finetunes LMs to answer long-form questions using a text-based web-browsing environment.
- • Gao et al. (2022) offload computation to Python interpreters.
- • Trivedi et al. (2022) and Khattab et al. (2022) interleave chain-of-thought with knowledge retrieval steps.
- • Khot et al. (2022) study decomposition in general, letting the language model push subtasks to task-specific handlers.
- • Ozturkler et al. (2022) aggregate language model probabilities using mathematical combinators like sum and product.
- • Jung et al. (2022) recursively generate a tree of explanations for a statement, then determine the truth of the statement by treating the inference as a satisfiability problem over these explanations and their logical relations.
- • Various works, including Fu et al. (2021), Dua et al. (2022), Guo et al. (2022), andZhou et al. (2022), explore decomposition of questions into subquestions, often under the name multi-hop question-answering.
- The most relevant work is
- 1 trains small models from scratch for particular composition steps. Wies et al. (2022) show some benefits of process supervision: When concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable.

### Iterated decomposition
- Iterated decomposition is a human-in-the-loop workflow that incrementally improves a task decomposition through error diagnosis and amendment
- The decomposition is applied to multiple test inputs with gold standard answers
- Evaluation is automatic or manual
- Failures are identified using the ICE trace visualizer or other tools

### Interactive Composition Explorer
- ICE provides a decorator that can be applied to any async function that should be recorded
- ICE provides utilities for recording all top-level async functions defined in a given module, any custom values of interest within a function, and the structure of each interpolated string
- ICE provides three views into this data: 1. An expandable tree of function calls shown in order of execution 2. A sortable, filterable table of function calls and their custom values 3. A function call detail pane containing inputs, custom values, outputs, and source code
- The tree can be browsed to a particular function call which can then be inspected in the detail pane. Often, it is helpful to compare many calls to the same function, so ICE provides a dropdown menu of all recorded functions in the execution trace and their respective call counts.
- If a function is selected, all calls to it will be shown in the table and highlighted in the tree.

## Real-World Context of Case Studies
- We used iterated decomposition and ICE to improve performance over simple baselines on three real-world case studies
- The Placebo Classification & Description task is designed to help researchers evaluate risk of bias
- Information about the placebo helps researchers assess the risk of bias in the study and decide how much to trust the results -comparing an intervention to a placebo control is usually stronger evidence than comparing to a no-treatment control, especially if the placebo successfully blinded the participants
- The first step of the Participant Flow in RCTs task is to identify the trials and trial arms within each study
- This helps the user understand what the researchers did in the study, to determine whether it addresses their research question
- The second step, evaluating participant adherence to an intervention, helps the researcher assess risk of bias and contextualize the study's findings: Was a small effect driven by poor adherence to the treatment?
- Elicit currently supports about 20 pre-specified questions, but also allows users to enter their own questions
- Ultimately, we want to find highly generalizable strategies to improve performance.

## Case Study: Placebo Classification & Description
- Domain-specific decompositions can improve the accuracy of generated placebo descriptions
- The accuracy of generated placebo descriptions can be improved through iteration on the classification and description steps

### Setup
- The QASPER dataset comes from a ranked list of relevant documents
- The QASPER dataset is used to answer information-seeking questions asked by regular readers of NLP papers
- The QASPER dataset is used to answer questions by separate NLP experts
- The QASPER dataset is used to answer questions about NLP papers
- The QASPER dataset is used to answer questions about the efficacy of a placebo
- The QASPER dataset is used to answer questions about the efficacy of a treatment
- The QASPER dataset is used to answer questions about the efficacy of a strategy
- The QASPER dataset is used to answer questions about the efficacy of a technique
- The QASPER dataset is used to answer questions about the efficacy of a document
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence
- The QASPER dataset is used to answer questions about the efficacy of a word
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph fragment
- The QASPER dataset is used to answer questions about the efficacy of a sentence
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a sentence
- The QASPER dataset is used to answer questions about the efficacy of a paragraph fragment
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a sentence
- The QASPER dataset is used to answer questions about the efficacy of a paragraph fragment
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment
- The QASPER dataset is used to answer questions about the efficacy of a paragraph
- The QASPER dataset is used to answer questions about the efficacy of a sentence fragment

## Case Study: Participant Flow in Randomized Controlled Trials
- Iteration on selection and generation steps improved accuracy on all subtasks
- Extracting experiments, trial arms, and adherence all improved substantially on a held-out test set

### Evaluation
- Experiments and arms are relatively easy to evaluate
- Adherence tends to require a narrative answer, and to be more subjective
- This means that it may benefit more from a nuanced decomposition, and approaches that work well for adherence may generalize better to other fuzzy tasks
- Each arm has an adherence answer, so there are a total of 135 adherence answers in our test set
- Often, no information about adherence is available in the text of the paper-information about adherence is only available for 56/135 arms (41%) in our test set

### Iterations
- We tested both our select-then-generate baselines and our best-performing decompositions (perplexity selection, with and without few-shot examples from demonstrations) on a sample of QASPER questions.
- Table 4 shows the results.
- We found that, of the approaches we tested, the best approach on the participant flow questions was also the best approach on QASPER.
- This approach is using a perplexity classifier followed by a few-shot prompt generated from demonstrations (in this case just other, randomly selected QASPER question-supporting paragraph-answer tuples) and scored 69% accuracy on our sample.
- This approach significantly outperformed both the baseline approach of using only the top-1 paragraph from the classifier, then generating the answer (38%) and using the Elicit-like perplexity classifier then generating without few-shot demonstrations (55%).
- These findings provide evidence that this decomposition is somewhat robust to domain transfer.

## Conclusion
- ICE is an open-source debugger for language model programs
- We built ICE to support our iterated decomposition workflow
- To support this workflow, we expect to be in the world where human programmers can automate novel complex tasks by improving a task decomposition with many iterations per day
- Essentially all decompositions in prior work and to some extent also in our case studies are fairly simple, even when they make hundreds of language model calls
- To accomplish 1 and 2, more sophisticated developer tools that make it faster to go from overall result to the sources of errors, to proposals for how to address them
- Making the dev tools in 3 machine-accessible so that the diagnosis and improvement steps can be learned by LMs
- Reducing the boundary between task design and execution, so that decomposition, debugging, and iteration can happen at runtime as models execute complex tasks
- Distillation and other ways to reduce cost of complex decompositions, so that decompositions are more cost-competitive with end-to-end execution

### Author contributions
- Justin Reppert and Charlie George ran the participant flow and QASPER case studies
- Ben Rachbach selected and designed the tasks
- Luke Stebbing implemented ICE features
- Maggie Appleton created figures
- Andreas Stuhlmüller ran the placebo case study and wrote the literature review
- Andreas Stuhlmüller, Justin Reppert, and Ben Rachbach wrote the paper

### Acknowledgements

### A taxonomy of process supervision
- The process of reasoning that a computer model performs to reach an answer can vary along several dimensions, such as:
- Causality: How the reasoning steps are connected and used by the model. For example, chain-of-thought reasoning is a process that model follows on the way to its final answer, but we cannot verify that the reasoning was causally necessary for the final answer. On the other hand, if we first generate reasoning, then feed that output to another call that solves the task without other information that it could use to directly solve the task, we know that the reasoning was part of the causal chain that led to the final answer.
- Content: What the reasoning steps are doing. For example, the model might reason to incorporate more context (such as more sections of a long paper), to use external tools (such as a web search between model calls), or to apply multiple criteria or lines of reasoning to a question.
- Designer: Who decides how to compose reasoning steps. For example, the human developer, the human end user, the language model, or some combination of them (such as the developer providing a set of possible subcompositions and the language model choosing among them).
- Purpose: What the decomposition aims to achieve. For example, the decomposition might aim to improve the performance on the main task, to reflect an ideal or normative reasoning process, or to make the reasoning process more supervisable.
- Supervision: Who performs the supervision? The developer, the language model, or the user?
- Availability of ground truth: Do we know the correct answer to the task (so we can compare the process outputs to it), or do we not know the correct answer (so we can only judge the process quality by its own logic and consistency)? In this paper, we supervise the process to improve the task decomposition, the supervisor is the developer, and we always have access to the correct answer.

### Data collection
- Placebo was nasal spray with all ingredients except oxytocin
- Oxytocin was given to the control group but not mentioned
