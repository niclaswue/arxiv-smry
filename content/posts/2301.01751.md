---
title: "Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes"
date: 2023-01-04T18:34:25.000Z
author: "Justin Reppert, Ben Rachbach, Charlie George, Luke Stebbing, Jungwon Byun, Maggie Appleton, Andreas Stuhlmüller"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-01751v2.webp" # image path/url
    alt: "Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.01751)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.01751).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/iterated-decomposition-improving-science-q-a).

# Abstract
- Language models can perform complex reasoning either end-to-end, with hidden latent state, or compositionally, with transparent intermediate state.
- Composition offers benefits for interpretability and safety, but may need workflow support and infrastructure to remain competitive.
- We describe iterated decomposition, a human-in-the-loop workflow for developing and refining compositional LM programs.
- We improve the performance of compositions by zooming in on failing components and refining them through decomposition, additional context, chain of thought, etc.
- To support this workflow, we develop ICE, an open-source tool for visualizing the execution traces of LM programs.
- We apply iterated decomposition to three real-world tasks and improve the accuracy of LM programs over less compositional baselines: describing the placebo used in a randomized controlled trial (25% to 65%), evaluating participant adherence to a medical intervention (53% to 70%), and answering NLP questions on the Qasper dataset (38% to 69%).

# Paper Content

## Introduction
- Language models are often trained using feedback on outcomes, leveraging reward models that imitate human evaluations provided as pairwise comparisons (Christiano et al., 2017;Ziegler et al., 2020).
- This works well for basic question-answering, summarization, simple code generation, and general short-form instruction-following (Stiennon et al., 2020;Wu et al., 2021;Ouyang et al., 2022).
- However, as model capabilities and task complexities scale up, outcome-based evaluation may run into alignment problems: First, for some important applications the process used to generate the output matters as much as the output itself. Consider long-range forecasting, and policy decisions informed by such forecasts: The quality of a forecast or decision depends on the assumptions, evidence, and reasoning used to produce it.
- Second, outcome-based feedback may create incentives for language models to deceive or manipulate their users by exploiting gaps or biases in the feedback signal (Amodei et al., 2016). In extreme cases, this could lead to models behaving as intended during training, hiding their true intentions and capabilities, but defecting at deployment (Cotra, 2022;Ngo et al., 2022).
- Process supervision is an alternative to outcome-based training that uses language models to execute humanunderstandable task decompositions, either by imitating human steps or by rewarding human-endorsed steps (Christiano, 2016;Stuhlmüller and Byun, 2022;Uesato et al., 2022).
- This paradigm promises increased interpretability, trust, and alignment by reducing the reliance on black-box computation and enabling users to inspect and intervene in the model's reasoning process.
- Right now, process supervision is ahead of outcome-based training: Language model capabilities are weak, so complex tasks require a composition of multiple calls.
- However, without scalable infrastructure to support it, we expect that outcome-based training will eventually crush process supervision performance-wise, leading to the alignment problems above.
- This paper describes our experience applying process supervision to academic question-answering in the context of Elicit2 , the AI research assistant developed by Ought.
- Our contributions are:
- A review of the literature on process supervision, highlighting gaps in workflows and tooling that contribute to making real-world use cases rare
- 2. Iterated decomposition, a human-in-the-loop workflow for developing compositional language model programs
- 3. ICE, an open-source visualizer for language model execution traces
- 4. Case studies that use this workflow to improve performance over baselines on three real-world tasks: (a) extracting placebo information from randomized controlled trials (RCTs), (b) analyzing participant flow in RCTs, and (c) answering questions about natural language processing papers.

## Process Supervision
- Process supervision refers to approaches to LM training and deployment that rely on human-understandable intermediate steps.
- We review the literature and highlight gaps, then explain the iterated decomposition workflow and ICE visualizer.

### Prior work on process supervision
- Over the past year, there have been significant advances in techniques for process supervision as well as frameworks, interfaces, and libraries for implementing it.
- At the same time, real-world use cases are still rare.
- We review prior work and highlight gaps in the literature that may be contributing to this.
- This is a rapidly growing field, so we are only able to review a sample of the work (Table 1).
- Table 1: A sample of prior work on process supervision, categorized into work that primarily contributes (1) new task decompositions, (2) training and finetuning techniques, (3) workflows and tutorials, (4) surveys and frameworks for organizing prior work, (5) tools, and (6) theory.
- While work on decompositions and training techniques is rapidly growing, there is little investigation of workflows, tooling, and theory.
- There is a quickly growing literature on how to compose multiple language model calls to improve performance or accomplish more difficult tasks:
- • Creswell et al. (2022) and Creswell and Shanahan (2022) generate reasoning steps by alternating between selection and inference. They show that their approach outperforms other prompting methods on multi-step logical deduction and scientific QA tasks, and generates interpretable reasoning traces.
- • Kazemi et al. ( 2022) apply backward-chaining to simple logic tasks, starting with a goal proposition and recursively decomposing it into sub-goals until the sub-goals can be proved or disproved.
- • Wu et al. (2021) and Saha et al. (2022) apply recursive summarization to generate summaries of long texts, such as books or articles. They use LMs to summarize small sections of the text and then recursively summarize these summaries to produce a summary of the entire text. They show that recursive summarization improves the quality and coherence of the summaries, and enables human feedback and evaluation.
- • Yang et al. (2022) generate long stories by first creating a story plan, generating passages b yprompting a model with contextual information from the plan and the current story state, and then revising the passages by reranking and editing them.
- • ReAct (Yao et al., 2022) interleaves generating chain-of-thought reasoning and actions pertaining to a task (e.g., search, lookup).
- • WebGPT (Nakano et al., 2021) finetunes LMs to answer long-form questions using a text-based web-browsing environment.
- • Gao et al. (2022) offload computation to Python interpreters.
- • Trivedi et al. (2022) and Khattab et al. (2022) interleave chain-of-thought with knowledge retrieval steps.
- • Khot et al. (2022) study decomposition in general, letting the language model push subtasks to task-specific handlers.
- • Ozturkler et al. (2022) aggregate language model probabilities using mathematical combinators like sum and product.
- • Jung et al. (2022) recursively generate a tree of explanations for a statement, then determine the truth of the statement by treating the inference as a satisfiability problem over these explanations and their logical relations.
- • Various works, including Fu et al. (2021), Dua et al. (2022), Guo et al. (2022), andZhou et al. (2022), explore decomposition of questions into subquestions, often under the name multi-hop question-answering.
- The most relevant...

### Iterated decomposition
- Iterated decomposition is a human-in-the-loop workflow that incrementally improves a task decomposition through error diagnosis and amendment
- The decomposition is applied to multiple test inputs with gold standard answers
- Evaluation is automatic or manual
- Failed components are identified and improved

### Interactive Composition Explorer
- ICE provides a decorator that can be applied to any async function that should be recorded
- ICE provides utilities for recording all top-level async functions defined in a given module, any custom values of interest within a function, and the structure of each interpolated string
- ICE provides three views into this data: an expandable tree of function calls shown in order of execution, a sortable, filterable table of function calls and their custom values, and a function call detail pane containing inputs, custom values, outputs, and source code
- The tree can be browsed to a particular function call which can then be inspected in the detail pane. Often, it is helpful to compare many calls to the same function, so ICE provides a dropdown menu of all recorded functions in the execution trace and their respective call counts.

## Real-World Context of Case Studies
- We used iterated decomposition and ICE to improve performance over simple baselines on three real-world case studies
- The Placebo Classification & Description task is designed to help researchers evaluate risk of bias
- Information about the placebo helps researchers assess the risk of bias in the study and decide how much to trust the results -comparing an intervention to a placebo control is usually stronger evidence than comparing to a no-treatment control, especially if the placebo successfully blinded the participants
- The first step of the Participant Flow in RCTs task is to identify the trials and trial arms within each study
- This helps the user understand what the researchers did in the study, to determine whether it addresses their research question
- The second step, evaluating participant adherence to an intervention, helps the researcher assess risk of bias and contextualize the study's findings: Was a small effect driven by poor adherence to the treatment?
- Elicit currently supports about 20 pre-specified questions, but also allows users to enter their own questions
- Ultimately, we want to find highly generalizable strategies to improve performance

## Case Study: Placebo Classification & Description
- Domain-specific decompositions can improve the accuracy of generated placebo descriptions.
- This case study focused on domain-specific decompositions.

### Setup
- The QASPER paper and benchmark measures F 1 scores of generated answers versus the expert answers.
- The goal of the QASPER case study is to answer about NLP papers.
- QASPER is a dataset of 5049 information-seeking questions asked by regular readers of NLP papers and answered by separate NLP experts.
- Apart from finetuning, we explored the same methods as for participant flow to learn how they generalize.
- For adherence, 80% of errors were false negatives, i.e. saying that adherence was not mentioned when it in fact was.
- Further, all (51/51) of these false negatives resulted from errors at the selection stage-the answer really was not mentioned in the top-1 paragraph from the monoT5 ranker.
- By using an oracle for selection with the same generation approach, accuracy on the adherence task rose to 77%.
- So, the baseline fails primarily by failing to make good use of the long context of the paper.
- For this reason, we started by iterating on selection.
- We noticed that the information required to describe the placebo is often dispersed throughout the paper.
- So, we rank the most relevant paragraphs in the paper and then use them to generate an answer.
- We re-use the same Rank and Answer technique that we used for classification.
- This generalizable subtask decomposition works well for both purposes.
- This approach results in a big improvement on the Elicit baseline (25% correct → 65%; p = 0.025-see Table 2).
- Diagnosis 14 of the 15 trials where the baseline failed to describe the placebo correctly are ones that it failed to classify correctly.
- The baseline does not even attempt to describe the placebo if the classification is "no placebo", so it fails all of these by Accuracy F 1 .
- We performed light data cleaning before testing, removing special tokens indicating cross-references with natural language equivalents (e.g., converting 'BIBREF0' to '[1]') and, as in the original QASPER paper, omitting questions that required tables or figures to answer.
- The strong paragraph-based classification results raise the question whether can we encode our understanding of how to figure out whether a trial used a placebo in a much simpler algorithm.
- We created a simple regex keyword-matching algorithm:
- 1. Classify as no placebo if the paper contains words like "open-label"
- 2. Then classify as placebo is the paper contains words like "placebo", and take the first matching sentence as a description
- 3. Then classify as no placebo if none of these words are present.

## Case Study: Participant Flow in Randomized Controlled Trials
- Iteration on selection and generation steps improved accuracy on all subtasks
- Accuracy on all subtasks improved substantially on a held-out test set

### Evaluation
- Experiments and arms are relatively easy to evaluate
- Adherence tends to require a narrative answer, and to be more subjective
- This means that it may benefit more from a nuanced decomposition, and approaches that work well for adherence may generalize better to other fuzzy tasks
- Each arm has an adherence answer, so there are a total of 135 adherence answers in our test set
- Often, no information about adherence is available in the text of the paper-information about adherence is only available for 56/135 arms (41%) in our test set

### Iterations
- We tested both our select-then-generate baselines and our best-performing decompositions (perplexity selection, with and without few-shot examples from demonstrations) on a sample of QASPER questions.
- Table 4 shows the results. We found that, of the approaches we tested, the best approach on the participant flow questions was also the best approach on QASPER.
- This approach is using a perplexity classifier followed by a few-shot prompt generated from demonstrations (in this case just other, randomly selected QASPER question-supporting paragraph-answer tuples) and scored 69% accuracy on our sample.
- This approach significantly outperformed both the baseline approach of using only the top-1 paragraph from the classifier, then generating the answer (38%) and using the Elicit-like perplexity classifier then generating without few-shot demonstrations (55%).
- These findings provide evidence that this decomposition is somewhat robust to domain transfer.

## Conclusion
- Iterated decomposition is a human-in-the-loop workflow for process supervision
- To support this workflow, we built ICE, an open-source debugger for language model programs
- Based on our experience with the case studies, the work needed to get there looks like this: 1. Increased iteration speed. Before full automation, we expect to be in the world where human programmers can automate novel complex tasks by improving a task decomposition with many iterations per day.
- Complex decompositions are essentially all decompositions in prior work and to some extent also in our case studies are fairly simple, even when they make hundreds of language model calls.
- To accomplish 1 and 2, more sophisticated developer tools that make it faster to go from overall result to the sources of errors, to proposals for how to address them.
- Making the dev tools in 3 machine-accessible so that the diagnosis and improvement steps can be learned by LMs.
- Reducing the boundary between task design and execution, so that decomposition, debugging, and iteration can happen at runtime as models execute complex tasks.
- Distillation and other ways to reduce cost of complex decompositions, so that decompositions are more cost-competitive with end-to-end execution.
- Our hope is that we can develop sufficient infrastructure for scalable process supervision so that humans can always inprinciple understand how models are solving complex tasks even if eventually almost all decomposition and supervision is done by machines.
