---
title: "High-Resolution Image Synthesis with Latent Diffusion Models"
date: 2021-12-20T18:55:25.000Z
author: "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2112-10752v2.webp" # image path/url
    alt: "High-Resolution Image Synthesis with Latent Diffusion Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2112.10752)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2112.10752).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/high-resolution-image-synthesis-with-latent).

# Abstract
- Decomposing image formation process into sequential application of denoising autoencoders
- Formulation allows for guiding mechanism to control image generation process without retraining
- However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations
- To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders
- Cross-attention layers turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes
- LDM achieves a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs

# Paper Content

## Introduction
- Democratizing high-resolution image synthesis by reducing the computational demands of DMs without impairing their performance.
- Departing to latent space to reduce the complexity of the training process.

## Related Work
- Generative models for image synthesis have high dimensional challenges
- GANs allow for efficient sampling of high resolution images with good perceptual quality
- In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved
- Variational autoencoders (VAE) and flow-based models enable efficient synthesis of high resolution images
- However, sample quality is not on par with GANs.
- While autoregressive models (ARM) achieve strong performance in density estimation, computationally demanding architectures and a sequential sampling process limit them to low resolution images.
- To scale to higher resolutions, several two-stage approaches use ARMs to model a compressed latent image space instead of raw pixels.
- Recently, Diffusion Probabilistic Models (DM) have achieved state-of-the-art results in density estimation and in sample quality.
- The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet.
- Evaluating and optimizing these models in pixel space has the downside of low inference speed and very high training costs.
- Our work prevents such tradeoffs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone.

## Method
- Diffusion models require costly function evaluations in pixel space
- We propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase
- This approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture, which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches. (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis.

### Perceptual Image Compression
- The perceptual compression model is based on previous work
- The encoder downscales the image by a factor of f = H/h = W/w
- The model can be interpreted as a VQGAN but with the quantization layer absorbed by the decoder
- The compression model preserves details of the input better

### Latent Diffusion Models
- Diffusion models are probabilistic models designed to learn a data distribution
- The most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching
- These models can be interpreted as an equally weighted sequence of denoising autoencoders θ (x t , t); t = 1 . . . T , which are trained to predict a denoised variant of their input x t , where x t is a noisy version of the input x.
- The corresponding objective can be simplified to (Sec. B) with t uniformly sampled from {1, . . . , T }.

### Conditioning Mechanisms
- Diffusion models are in principle capable of modeling conditional distributions of the form p(z|y).
- This can be implemented with a conditional denoising autoencoder θ (z t , t, y) and paves the way to controlling the synthesis process through inputs y such as text [68], semantic maps [33,61] or other image-to-image translation tasks [34].
- In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels [15] or blurred variants of the input image [72] is so far an under-explored area of research.
- We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36].
- To pre-process y from various modalities (such as language prompts) we introduce a domain specific encoder τ θ that projects y to an intermediate representation τ θ (y) ∈ R M ×dτ , which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing Attention Here, .
- Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and classconditional ImageNet [12], each with a resolution of 256 × 256. Best viewed when zoomed in.

## Experiments
- LDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities
- Empirically, we find that LDMs trained in VQregularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of VQregularized first stage models slightly fall behind those of their continuous counterparts
- In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section

### On Perceptual Compression Tradeoffs
- Tab. 8: Hyperparameters and reconstruction performance of the first stage models used for the LDMs compared in this section.
- Fig. 6: Sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset.
- LDM-{1,2} result in slow training progress, whereas LDM-{4-16} strike a good balance between efficiency and perceptually faithful results.
- LDM-{4-8} outperform models with unsuitable ratios of perceptual and conceptual compression.

### Image Generation with Latent Diffusion
- We train a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs.
- We also outperform LSGM where a latent diffusion model is trained jointly together with the first stage.
- In contrast, we train diffusion models in a fixed space.
- Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION [78] database, are better than those generated with 200 DDIM steps and η = 1.0.
- We use unconditional guidance with s = 10.0. and avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space.
- We outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM [15].
- LDMs we open them up for various conditioning modalities previously unexplored for diffusion models.
- For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M [78].
- We employ the BERT-tokenizer [14] and implement τ θ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) crossattention (Sec. 4.4).
- This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts.
- For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17,66] and GAN-based [109] methods, cf . Tab. 2.
- We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count.

### Super-Resolution with Latent Diffusion
- direct conditioning on low resolution images
- uses concatenation to condition on low resolution images
- outperforms SR3 in FID
- good performance in human preference studies

### Inpainting with Latent Diffusion
- Inpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image.
- We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task.
- Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8].
- The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2.
- We first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQ-LDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions.
- For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution 256 2 and 512 2 , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7× between pixel-and latent-based diffusion models while improving FID scores by a factor of at least 1.6×.
- The comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of [88].
- LPIPS between the unmasked images and our samples is slightly higher than that of [88]. We attribute this to [88] only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf .
- Additionally in a user study (Tab. 4) human subjects favor our results over those of [88]. Based on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the VQregularized first stage without attention.

## Limitations & Societal Impact

## Conclusion
- We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B parameters).
- This also includes a new comparison to very recent competing methods on this task that were published on arXiv at the same time as ( [59,109]) or after ( [26]) the publication of our work.
- We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also updated.
- Both the updated text-to-image and the class-conditional model now use classifier-free guidance [32] as a measure to increase visual fidelity.
- We conducted a user study (following the scheme suggested by Saharia et al [72]) which provides additional evaluation for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.6).
- Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.
- Diffusion models can be specified in terms of a signal-to-noise ratio SNR(t) = consisting of sequences (α t ) T t=1 and (σ t ) T t=1 which, starting from a data sample x 0 , define a forward diffusion process q as with the Markov structure for s < t:
- Denoising diffusion models are generative models p(x 0 ) which revert this process with a similar Markov structure running backward in time, i.e. they are specified as
- The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as
- The prior p(x T ) is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on the final signal-to-noise ratio SNR(T ). To minimize the remaining terms, a common choice to parameterize p(x t−1 |x t ) is to specify it in terms of the true posterior q(x t−1 |x t , x 0 ) but with the unknown x 0 replaced by an estimate x θ (x t , t) based on the current step x t . This gives
- [45] p where the mean can be expressed as
- In this case, the sum of the ELBO simplify to Following [30], we use the reparameterization to express the reconstruction term as a denoising objective, and the reweighting, which assigns each of the terms the same weight and results in Eq. ( 1).
- Samples
- 256 2 Guided Convolutional Samples 512 2 Convolutional Samples 512 2
- An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15,82,85]. In particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset with a classifier log p Φ (y|x t ), trained on each x t of the diffusion process.
- We directly build on this formulation and introduce post-hoc image-guiding: For an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in [15] reads:
- This can be interpreted as an update correcting the "score" θ with a conditional distribution log p Φ (y|z t ). So far, this scenario has only been applied to single-class classification models.
- We re-interpret the guiding distribution p Φ (y|T (D(z 0 (z t )))) as a general purpose image-to-image translation...
