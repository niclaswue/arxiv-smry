---
title: "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning"
date: 2022-12-15T15:52:39.000Z
author: "Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-07919v1.webp" # image path/url
    alt: "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.07919)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.07919).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/roscoe-a-suite-of-metrics-for-scoring-step-by).

# Abstract

# Paper Content

## INTRODUCTION
- Scaling language models has improved state-of-the-art performance on nearly every NLP benchmark (Brown et al., 2020), with large language models (LLMs) performing impressively as few-shot learners (Brown et al., 2020).
- Despite these achievements, even the largest of these models still struggle with tasks including math word problems (Hendrycks et al., 2021), symbolic manipulation (Rytting & Wingate, 2021), and commonsense reasoning (West et al., 2022).
- Recent work has shown that prompting (Wei et al., 2022;Wang et al., 2022) or fine-tuning (Lampinen et al., 2022) LLMs to generate step-by-step rationales can lead to improvements on reasoning tasks.
- Some of these include small-scale analysis of specific error types within step-by-step rationales (Lewkowycz et al., 2022;Chowdhery et al., 2022), as shown in Table 1.
- However, existing works primarily focus on end-task performance.
- Although text generation evaluation metrics sometimes offer fine-grained quality evaluations (e.g., adequacy, fluency) against human scores (Opitz & Frank, 2021;Leiter et al., 2022), these metrics generally treat the output as a whole, and many of these generative metrics operate on tasks such as summarization or machine-translation rather than reasoning.
- Table 1: Step-by-step reasoning for logical inference task generated from 2-shot GPT-3 (text-davinci-002) (Brown et al., 2020). Error types are highlighted: logical or factual issues, partially correct, somewhat-relevant. Full 2-shot exemplars are in Appendix B.
- Context: Shelley is from Virginia, but is visiting that city with that famous market where they throw the fish. Going home next Tuesday!
- Question: Is it likely that Shelly will be near the Pacific Ocean this weekend?
- Step-by-step reasoning: • No, it is not likely that Shelley will be near the Pacific Ocean this weekend. • Shelley is from Virginia, which is on the East Coast of the United States. • The Pacific Ocean is on the West Coast. • Shelley is visiting a city with a famous market, which could be anywhere in the world. • Tuesday is her travel day, so she will likely not be traveling this weekend.

## Contributions.
- We propose a new taxonomy for reasoning errors.
- Using our taxonomy, we propose a new suite of metrics that focus on sequence and step level analysis of step-by-step reasoning.
- We present extensive comparative analysis on 11 datasets of varied complex reasoning problems demonstrating the strengths of each metric, especially in terms of interpretability relative to baselines, and considerations for use.

## RELATED WORK
- NL explanations should enable accurate representation of the reasoning process and degree of plausibility
- A qualitative assessment of NL explanations with correctness labels collected from human judges was presented in (Camburu et al., 2018)
- Recent work has also investigated automatic metrics for natural language generation (NLG) evaluation including word overlap or embedding based similarly with human written explanations (Clinciu et al., 2021)
- Though fast and cost-effective, automatic metrics for NLG are not equipped to measure the logical inconsistencies or information gain with thinking steps
- Explanations have also been evaluated by collecting datasets, and running correlation analysis to investigate the degree to which an automatic metric correlates with human judgements of clarity, relevance and informativeness
- Although reliable, human evaluation is an expensive, domain specific, and time-consuming process
- ROSCOE provides generic automatic evaluation procedures that are domain and task specific

## Error Type Definition
- Grammar Faulty
- Unconventional
- Controversial
- Factuality
- Information about an object (i.e. quantity, characteristics) or a named entity doesn't match with the input context.

## Hallucination
- Information is not provided in the problem statement
- Redundancy Explanation contains redundant information
- Explanation contains information that is irrelevant or wrong
- Repetition Step paraphrases information already mentioned in previous reasoning steps

## Missing step
- The generated reasoning is incomplete and lacks required information to produce the correct answer.
- Alignment scores from hypothesis to context are highlighted, and alignment scores from context to hypothesis are underscored.
- The reasoning alignment combines token and step level similarities where each alignment value (cell) is the cosine similarity and explicitly measures the grounding of the token and step-wise reasoning with respect to the source text.
- The variation of scorers of the ROSCOE shares some similarities, thus we explain them here:
- BARTScore (Yuan et al., 2021) claims that more high level text can be generated using sequence to sequence model.
- It can support different evaluation perspectives such as factuality (by evaluating from source to hypothesis) or informativeness (by evaluating from both directions between reference and hypothesis).
- BARTScore is used to measure the probability of generated text from a source text x to a target set y:
- BART Score = m t=1 w t log p(y t |y <t , x, θ)
- BARTScoreintroduce two variations: (1) finetuning, in which the BART model is finetuned on the task specific dataset to make the pre-training domain closer to the evaluation domain.
- (2) prompting, in which a task specific textual prompt is appended to the source x to get the y.
- In our experiments we compare the the BARTScorebaseline and one with the prompting variant BARTScore+to compare in the experiments.

## Coherency
- Steps contradict each other or do not follow a cohesive story
- Commonsense Model lacks relations that should be known from general world
- Arithmetic Error in math calculations et al., 2021) and BARTScore (Yuan et al., 2021), as both introduce a set of interpretable metrics to evaluate the similarity between two texts. However, ROSCOE is unique in providing fine-grained interpretations of reasoning steps, determining contradictions, and identifying ordering issues in the reasoning narrative.
- Self-Consistency with LLMs. Recent work on improving LLMs performance on complex reasoning tasks uses an ensemble strategy called self-consistency (Wang et al., 2022). This method samples a diverse set of reasoning paths from a language model via reasoning traces prompting and returns the most consistent final answer in the set. Other work evaluates the diversity of a reasoning path (Li et al., 2022), or the consistency of an inference step (Creswell et al., 2022) or finetune LLMs (Zelikman et al., 2022) to improve on difficult NLP tasks. In contrast to these works, we present a suit of metrics that focus on determining the type of the error (e.g., commonsense or logical inconsistency) in a reasoning path, if one exists.

## SEMANTIC ALIGNMENT METRICS (ROSCOE-SA)
- The reasoning alignment vector r-align(h → s) is a normalized cosine similarity between hypothesis step and most similar sentence in a context.
- The alignment score r-align(h → s) is a measure of grounding of the step-wise reasoning with respect to the source text.
- The error perturbation rules require complex problem solving skills.
- The 12 error perturbation rules are used to generate the diagnostics datasets.
- The human judged datasets include GSM8K (arithmetic reasoning), DROP (discrete reasoning), ESNLI (deductive and commonsense reasoning), COSMOS-QA (commonsense reasoning), and SemEVAL (commonsense reasoning).
- Wei et al. (2022) provide model generated chain of thought reasoning steps for GSM8K.
- We used chains produced by the 175b_verification model to annotate for reasoning errors.
- For other datasets, we prompt GPT-3 LLM (Brown et al., 2020) with few-shot in-context examples to obtain step-by-step reasoning sequences.
- We use the error types in our taxonomy in Table 2 as human evaluation perspectives of reasoning errors.

## EXPERIMENTAL RESULTS
- ROSCOE can outperform all other reference-free methods on all six diagnostic datasets
- the gains for ROSCOE-SS are more pronounced in four out of six diagnostics datasets
- finetuning SimCSE gives highest improvements on the ASDIV dataset
- ASDIV is a 1-step reasoning dataset (see App. Table 12), where step is represented by an equation with one of the arithmetic perturbations added
- we hypothesize that including these patterns in finetuning helped the model to better learn relationships between context and equations, and resulted in higher scores
- on EQASC dataset, Repetition* scores are able to catch all duplicated steps in a chain, i.e., we can separate perturbed and non-perturbed chains based on the given threshold value for the Repetition* scores

## ANALYSIS
- Metrics correlate better when the level of errors is high
- Baseline metrics perform better on EntailmentBank
- Perturbations applied on the MATH dataset (e.g., RandomOperation, or ShuffleNumbers) are harder to detect with language model based (BARTScore) and NLI model based (ROSCOE-LC) metrics
- Ideal scorer based on ease of use is not possible at the moment

## CONCLUSION
- ROSCOE is a suite of interpretable, unsupervised metrics that enables evaluation of step-by-step reasoning generations of LMs when no golden reference generation exists
- Metrics are generic enough, work on natural language rationales, and consider the alignment with the input context and the generated explanation
- Left some follow-up questions, such as the application of these metrics for improving downstream task performance, for future exploration
