---
title: "A Measure-Theoretic Characterization of Tight Language Models"
date: 2022-12-20T18:17:11.000Z
author: "Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, Ryan Cotterell"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-10502v1.webp" # image path/url
    alt: "A Measure-Theoretic Characterization of Tight Language Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.10502)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.10502).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/a-measure-theoretic-characterization-of-tight).

# Abstract
- Language modeling is the process of estimating a probability distribution over strings
- In most cases, the estimated distribution sums to 1 over all finite strings, but in some pathological cases, probability mass can "leak" onto the set of infinite sequences
- In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling
- We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense
- We also generalize characterizations of tightness proposed in previous works

# Paper Content

## Introduction
- Language modeling is a core task in natural language processing
- Language models are a distribution over the set of strings over a given alphabet
- If the alphabet is the set of words in a language,1 then a language model can be thought of as a distribution over the language's sentences
- Language models are essential for computational linguistics research
- Further, it is also central to a wide range of natural language processing applications
- Properly treating a distribution over this sample space requires a modicum of measure theory
- To clarify the situation, we review the measure-theoretic treatment of distributions over infinite sequences
- We then make use of a termination symbol EOS to define a random variable whose value can be a string, i.e., an element of Σ * .
- In a tight language model, this random variable has probability 1 of being a string and hence finite.
- Beyond offering a measure-theoretic formalization, our paper also demonstrates how tightness relates to the Borel-Cantelli lemmata, simplifying a recent result by Meister et al. (2022).
- We analyze several language modeling architectures and give conditions on their tightness. We demonstrate that n-gram language modelsand more generally, language models defined by stochastic finite-state automata-can be non-tight, and we give a simple necessary and sufficient condition for tightness in terms of the inverse of the automaton's transition matrix.
- We also discuss when neural language models become non-tight. We prove that Transformer-based language modelsare always tight and that recurrent neural language models are always tight when they employ a bounded activation function. However, we also exhibit a recurrent neural network (RNN) language model with a ReLU activation (Nair and Hinton, 2010) that is non-tight in a simpler construction than the one offered by Chen et al. (2018). As a byproduct, we also generalize and strengthen the results given by Welleck et al. (2020), who give a sufficient condition for tightness of recurrent neural language models in terms of the norm of the hidden state.

## Motivating Examples
- A string of length n ≥ 0 is a finite sequence x = x 1 x 2 . . . x n where each x t ∈ Σ.
- The EOS probability is always strictly positive but, as we will prove later in Theorem 4.7, this language model is non-tight, and, in fact, the probability of all (finite) strings is ≈ 0.702.
- An ASM may be tight after all if the probability of EOS decays more slowly-even when it still approaches 0.702.

## The Language Model Measure
- An ASM can lose probability mass to the set of infinite sequences, Σ ∞ .
- However, Σ ∞ , unlike Σ * , is uncountable, and it is due to this fact that we need to work explicitly with the measure-theoretic formulation of probability.

### Measure-Theoretic Background
- The goal of measure-theoretic probability is to assign probability to subsets of an outcome space Ω.
- For many common Ω, it is impossible to assign probabilities in a way that satisfies a set of reasonable desiderata.
- Consequently, probabilists (and analysts alike) commonly resort to only assigning probability to certain "nice" subsets of Ω, which are referred to as measurable subsets or events.
- The set of measurable subsets is commonly denoted as F (Def. 3.1) and a probability measure P : F → [0, 1] is the function that assigns a probability to measurable subsets.
- As it turns out, the following simple and reasonable requirements imposed on F and P are enough to rigorously discuss probability.

### Language Models as a Measure
- The ASM defines a probability space over the uncountable set Σ * ∪ Σ ∞
- This type of distribution is different from a language model, which only operates over finite strings Σ *
- To distinguish the two, we refer to a distribution over Σ * ∪ Σ ∞ as a sequence model, which we formally define below

### Pre-Measure
- It is impossible to assign a probability measure to every subset of Ω.
- The Axiom of Choice and the Continuum Hypothesis imply that there exists no probability measure P over ({H, T} ∞ , P({H, T} ∞ )) such that P({ω}) = 0 for all ω ∈ {H, T} ∞ .
- There is a natural way to construct a probability space for sequences that behaves in accordance with our intuition, and we will do so in Proposition C.2.
- P 0 is a pre-measure over C.

### Extension of Pre-Measure
- We have gathered all the ingredients needed to state Carathéodory's theorem.
- Theorem 3.11 (Carathéodory's Extension Theorem). Given an algebra A over some set Ω and a probability pre-measure P 0 : A → [0, 1], there exists a probability space (Ω, F, P) such that A ⊂ F and P| A = P 0 .
- Furthermore, the σ-algebra F depends only on A and is minimal and unique, which we will also denote by σ(A), and the probability measure P is unique.
- Proof Sketch. See App. C.2.1.
- Applying Carathéodory's extension theorem to our cylinder algebra C and pre-measure P 0 , we see that there exists a probability space (Σ ∞ , σ(C), P) over Σ ∞ that agrees with the autoregressive sequence model p's probabilities.

### A String-valued Random Variable
- The probability space Σ * ∪ Σ ∞ can be induced from the probability space Σ by constructing a σ-algebra over Σ * ∪ Σ ∞ .
- The function X Eq. ( 12) is a measurable mapping and takes values in Σ * ∪Σ ∞ .
- The EOS probability in an autoregressive sequence model emerges as the conditional probability that, given that we must generate a string with a prefix x ∈ Σ * , the string is exactly x.

## Characterizing Tightness
- The paper discusses the measure-theoretic formalization of tightness in autoregressive sequence models
- The paper generalizes Lemma 3.2 from Welleck et al. (2020)
- The paper defines tightness in terms of the event consisting of all finite strings and the event consisting of all infinite strings, which are both measurable
- P (∪ ∞ k=1 A k ) and P (∩ ∞ k=1 A c k ) are well defined.

### A Lower Bound Result
- The probability of an event is tight when it is lower bounded by a function that depends only on the length of the prefix.
- This condition is useful when the probability of an event is lower bounded by a function that depends only on the length of the prefix.

### The Borel-Cantelli Lemmata
- The Borel-Cantelli lemmata are statements about the event where i.o. stands for "infinitely often."
- Lemma 4.2 can be viewed as a parallel statement to the Borel-Cantelli lemmata and one can prove the lemmata using a very similar proof (cf. proof of Theorem 2.3.7 in Durrett, 2019).
- Concretely, given a sequence of events {A n } ∞ n=1 in some probability space, the Borel-Cantelli lemmata are statements about the event where i.o. stands for "infinitely often."
- Intuitively, {A n i.o.} is the set of outcomes that appear in infinitely many sets in the collection {A n } ∞ n=1 (hence the name).
- We will not use Borel-Cantelli directly, but they offer a probabilistic proof of a key result (Corollary 4.6) which will in turn lead to the desired statement about tightness.
- We formally state the first and second Borel-Cantelli lemmata below.
- Lemma 4.5 (Borel-Cantelli II). Assume {A n } is a sequence of independent events, then
- Using the Borel-Cantelli lemmata, we can prove the following useful fact.
- Corollary 4.6. Given a sequence {p n } where p n ∈ [0, 1). Then,
- Proof. See App. D.3.
- We now turn to proving a more general version of Proposition 4.3, which would imply its converse.
- First, we define the following quantity which can be viewed as the EOS probability at step t, given that EOS was not generated at any earlier step.
- In Eq. (45a) in App. D.2, we show that, when p EOS (t) is defined, it has the same value as
- We can now completely characterize the tightness of an ASM with the following theorem.
- Theorem 4.7 (Proposition 2.4 in Meister et al., 2022). An autoregressive sequence model is tight if and only if p EOS (t) = 1 for some t or If the first case of the condition does not hold, the second case can be checked since its summands will be well-defined (the conditional probabilities in Eq. ( 21) will not divide by 0).
- We remark that Theorem 4.7 is a generalization of Proposition 4.3 since if p EOS (t) is lower-bounded by f (t) whose series diverges, its own series would also diverge. However, since p EOS (t) involves the computation of a partition function in its denominator, it is most likely intractable to calculate (Lin et al., 2021;Lin and McCarthy, 2022). Hence, Proposition 4.3 will be the main tool for determining tightness.
- Finally, we note that Theorem 4.7 generalizes claims in previous work. For example, Welleck et al. (2020) require f (t) = c > 0 for some c not dependent on t to determine tightness. Hence, their bound is not helpful in determining the tightness in either Example 2.4 or Example 2.5, because the EOS probability can be arbitrarily small in both cases.
- Applying Proposition 4.3, we see that (1) the autoregressive sequence model in Example 2.4 is non-tight, because the series ∞ t=1 1 e t +1 is convergent, and (2) the autoregressive sequence model in Example 2.5 is tight, since the series ∞ t=1 1 t+1 is divergent.

## Analysis of Common Language Models
- We discuss the tightness of several classes of autoregressive sequence models.
- We use the bootstrap to measure the tightness of the models.

### Stochastic Finite-State Language Models
- Language modeling based on n-grams has been historically influential in NLP
- Tight SFSSMs are those where all accessible states are also co-accessible
- Trimming an SFSSM removes its non-useful (useless) states to obtain a substochastic finite-state sequence model
- The well-known matrix inversion formula used above finds the total weight of all accepting paths in any weighted graph

### Transformer Language Models
- The theorem states that a transformer is a function that takes a finite number of input vectors and outputs a vector for each input.
- The lemma states that there exists a compact set K ⊂ R d such that for any ∈ Z >0 , f K ⊆ K .
- The theorem states that a sequence model as defined by a transformer is tight.

### Recurrent Neural Language Models
- The hidden states of an RNN are typically defined by the following recurrence
- The conditional probabilities are usually defined in the same way as Eq. (25)
- Using Theorem 5.6 and the same strategy of proof as in Theorem 5.8, one can also easily prove the tightness of any RNN language model with bounded activations
- However, as we saw in Example 2.4, unbounded activation can indeed lead to non-tightness when the probability of EOS decays too fast
- The condition derived in Theorem 4.7 precisely determines how fast such decay can be without losing the tightness of the language model
- Proposition 5.9 states that given an RNN language model over Σ, if the output vectors of token x are u x ∈ R d and k def = sup x∈Σ u x − u EOS 2 , then the model is tight.

## Conclusion
- Language modeling is studied in measure-theoretic terms
- Termination probabilities of a language model can be computed
- A language model is a distribution over finite strings
- A model is said to be tight if it doesn't leak probability mass to infinite-length strings
