---
title: "Deep Learning and Computational Physics (Lecture Notes)"
date: 2023-01-03T03:56:19.000Z
author: "Deep Ray, Orazio Pinti, Assad A. Oberai"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-00942v1.webp" # image path/url
    alt: "Deep Learning and Computational Physics (Lecture Notes)" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.00942)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.00942).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/deep-learning-and-computational-physics).

# Abstract
- Deep learning algorithms are based on the principles of computational physics
- Deep learning algorithms can be used to solve problems in computational physics
- These algorithms can be used to model physical phenomena

# Paper Content

## Computational physics
- Computational physics plays a fundamental role in solving many problems in fields of science and engineering
- To gain an understanding of this concept, we briefly outline the key steps involved in solving a physical problem: 1. Consider a physical phenomena and collect measurements of some observable of interest.
- In certain situations an exact analytical form of the solution can be obtained.
- In most scenarios, exact expressions of the solution cannot be obtained and must be suitable approximated using a numerical algorithm.
- Once the algorithm to evaluate the solution (exactly or approximately) is designed, use it to validate the mathematical model, i.e., see if the predictions are in tune with the data collected.

## Machine learning
- ML does not require the postulation of a physical law
- The general steps involved are: 1. Collect data

### Examples of ML

### Types of ML algorithms based leaning task
- Supervised learning: Given the data S = {(x i , y i ) : 1 ≤ i ≤ N }, predict ŷ for some new x, such that ( x, ŷ) / ∈ S.
- Unsupervised learning: Given the data S = {x i ∈ Ω x : 1 ≤ i ≤ N }, find a relation among different regions of Ω x .
- Re-inforcement learning: The methods belonging to this family learn driven by rewards or penalties for decisions taken.

## Artificial Intelligence, Machine Learning and Deep Learning
- AI refers to a system with human-like intelligence
- ML is a key component of an AI system, but there are other ingredients involved
- A self-driving car is a prototypical example of AI
- In order for a self-driving car to work, an ML algorithm is used to perform a semantic segmentation
- DL is a subset of ML algorithms that focuses on the transmission of signals in the central nervous system

## Machine learning and computational physics
- MLP is a simple network architecture that is available
- MLP is a network architecture that is used for learning
- MLP is a network architecture that is used for prediction
- MLP is a network architecture that is used for classification

## MLP architecture
- An MLP is a type of neural network that uses a number of stacked artificial neurons to approximate a function.
- An artificial neuron in an MLP is a unit that is only responsible for providing an input to the network, and does not do any actual computation.
- The width of a layer in an MLP is the number of neurons in that layer.
- An MLP with 2 hidden layers is shown in Figure 2.1.
- The activation function in an MLP is a function that helps the network to represent non-linear complex functions.
- The parameters of an MLP are all the weights and biases, which we will represent as where N θ denotes the total number of parameters of the network.

## Activation functions
- The sigmoid function is a popular activation function because it has a nice linearity and is easy to optimize.
- The hyperbolic tangent function is a more recent activation function that has better performance on deep networks.
- The tanh function is a less popular activation function that has better performance on shallow networks.
- The activation function is perhaps the most important component of an MLP.
- A large number of activations are available in literature, each with its own advantages and disadvantages.
- Let us take a look at a few of these options (also see Figure 2.2).
- The sigmoid function is a popular activation function because it has a nice linearity and is easy to optimize.
- The hyperbolic tangent function is a more recent activation function that has better performance on deep networks.
- The tanh function is a less popular activation function that has better performance on shallow networks.

### Linear activation
- The activation function is σ(ξ) = ξ.
- The activation function is infinitely smooth and all derivatives beyond the second derivative are zero.
- The range of the activation function is (−∞, ∞).
- Using the linear activation function (in all layers) will reduce the entire network to a single affine transformation of the input x.

### Rectified linear unit (ReLU)
- The function is piecewise linear
- The function is defined as
- The function has a derivative that is piecewise constant
- The higher-order derivatives of the function are not well-defined

### Leaky ReLU
- The ReLU activation leads to a null output if the affine transformation of the neuron is negative.
- This can lead to the phenomena of dying neurons while training a neural network.
- To overcome this challenge, a leaky version ReLU was designed where α becomes a network hyper-parameter.
- Some features of this function are:
- The derivates of Leaky ReLU behave in the same way as those for ReLU.
- The range of the function is (−∞, ∞).

### Logistic function
- The Logistic or Sigmoid activation function is given by
- The function is infinitely smooth and monotonic
- The range of the function is (0, 1), i.e., the function is bounded
- Since the derivative quickly decays to zero away from ξ = 0, this activation function can lead to slow convergence of the network while training.

### Tanh
- The tanh function is a symmetric extension of the logistic function
- The range of the tanh function is (−1, 1), which means it is bounded
- The derivative of tanh quickly decays to zero away from ξ = 0, which means the tanh function can lead to slow convergence while training networks

### Sine
- The sine function is an efficient activation function
- The sine function has the best features of all the activation function discussed above

## Expressivity of a network
- The MLP with L = 1,W = 2 has one kink in the output.
- The MLP with L = 2,W = 3 has two kinks in the output.

### Universal approximation results
- There exists an MLP with a single hidden layer, arbitrary width H and a non-polynomial continuous activation σ such that Theorem 2.3.1 is true.
- There exists an MLP with arbitrary number of hidden layers L, each having width H ≥ d + D + 2, a continuous activation σ (with some additional mild conditions), such that Theorem 2.3.3 is true.

## Training, validation and testing of neural networks
- Now that we have a better understanding of the architecture of MLPs, we would now like to discuss how the parameters of these networks are set to approximate some target function.
- We restrict our discussions to the framework of supervised learning.
- Let us assume that we are given a dataset of pairwise samples S = {(x i , y i ) : 1 ≤ i ≤ N } corresponding to a target function f : x → y.
- We wish to approximate this function using the neural network F(x; θ, Θ) where θ are the network parameters defined before, while Θ corresponds to the hyper-parameters of the network such as the depth L + 1, width H, type of activation function σ, etc.
- The strategy to design a robust network involves three steps:
- 1. Find the optimal values of θ (for a fixed Θ) in the training phase.
- 2. Find the optimal values of Θ in the validation phase.
- 3. Test the performance of the network on unseen data on the testing phase.
- To accomplish these three tasks, it is first customary to split the dataset S into three distinct parts: a training set with N train samples, a validation set with N val samples and test set with N test samples, with N = N train + N val + N test .
- Typically, one uses around 60% of the samples as training samples, 20% as validation samples and the remaining 20% for testing.
- Splitting the dataset is necessary as neural networks are heavily over-parameterized functions.
- The large number of degrees of freedom available to model the data can lead to over-fitting the data. This happens when the error or noise present in the data drives the behavior of the network more than the underlying input-output relation itself. Thus, a part of the data is used to determine θ, and another part to determine the hyper-parameters Θ.
- The remainder of the data is kept aside for testing the performance of the trained network on unseen data, i.e., the network's ability to generalize well.

## Generalizability
- The question is addressed by studying the generalizability of the trained network
- If the network is trained to overfit the training data, the network will typically lead to poor predictions on test data
- Typically, if S train , S val and S test are chosen from the same distribution of data, then a small value of Π train , Π val can lead to small values of Π test .

### Regularization
- Neural networks are almost always over-parametrized, which can lead to a highly non-linear network model.
- To nudge the choice of θ * in a more favorable direction, a regularization technique can be employed. The simplest method of regularization involves augmenting a penalty term to the loss function.
- Using a proper regularization would help avoid over fitting.

## Gradient descent
- Gradient descent is a optimization algorithm that uses a step size to adjust the direction of the gradient.
- The minimization problem can be solved using gradient descent if the loss function is convex.
- If there are more than one minima, then the gradient descent algorithm will pick the minima that is closer to the starting point.
- For convergence, the local curvature of the loss function must be smaller than a.

## Some advanced optimization algorithms
- Gradient descent: This is a simple and popular optimization technique that makes use of the following update formula:
- Stochastic gradient descent: This is a more sophisticated optimization technique that makes use of the following update formula:
- Momentum: This is a more sophisticated optimization technique that makes use of the following update formula:
- conjugate gradient: This is a more sophisticated optimization technique that makes use of the following update formula:
- Levenberg-Marquardt: This is a more sophisticated optimization technique that makes use of the following update formula:

### Momentum methods
- momentum methods use a weighted moving average of the gradient
- β 1 is commonly used to adjust the weighting of the moving average
- this is supposed to smooth out the zig-zagging seen in Figure 2.6

### Adam
- The Adam optimization was introduced by Kingma and Ba [10]
- For an initial learning rate η, the updates are given by where g k and G k are the weighted running averages of the gradients and the square of the gradients, respectively
- The recommended values for the hyper-parameters are β 1 = 0.9, β 2 = 0.999 and = 10 −8
- Referring back to the example in Figure 2.6, this would mean a smaller learning rate for θ 2 in comparison to θ 1 , and therefore will help alleviate the zig-zag path of the optimization algorithm

### Stochastic optimization
- The gradient of the loss function is
- The update formula for the GD method is
- Stochastic gradient descent can be circumvented by using the following update formula
- The more homogeneous the minima, the better they generalize

## Calculating gradients using back-propagation
- The final piece of the training algorithm that we need to understand is how the gradients are actually evaluated while training the network.
- Recall the output x (l+1) of layer l + 1 is given by Affine transform: ξ Non-linear transform:
- Given a training sample (x, y), set x (0) = x. The value of the loss/objective function (for this particular sample) can be evaluated using the forward pass:
- For l = 1, ..., L + 1 (a) Evaluate ξ (l) using (2.13). (b) Evaluate x (l) using (2.14).
- This operation can be written succinctly in the form of a computational graph as shown in Figure 2.8.
- In this figure, the lower portion of the graph represents the evaluation of the loss function Π. We would of course need to repeat this step for all samples in the training set (or a mini-batch for stochastic optimization).
- For simplicity, we restrict the discussion to the evaluation of the loss and its gradient for a single sample.
- In order to update the network parameters, we need ∂Π ∂θ , or more precisely ∂Π ∂W (l) , ∂Π ∂b (l) for 1 ≤ l ≤ L + 1. We will derive expressions for these derivatives by first deriving expressions for ∂Π ∂ξ (l) and ∂Π ∂x (l) .
- From the computational graph it is easy to see how each hidden variable in the network is transformed to the next. Recognizing this, and applying the chain rule repeatedly yields
- In order to evaluate this expression we need to evaluate the following terms: where the last two relations hold for any network layer l, H l is the width of that particular layer, and σ denotes the derivative of the activation with respect to its argument.
- Using these relations in (2.15), we arrive at, Taking the transpose, and recognizing that Σ (l) is diagonal and therefore symmetric, we finally arrive at
- This evaluation can also be represented as a computational graph. In fact, as shown in Figure 2.8, it can be appended to the original graph, where this part of the computation appear in the upper row of the graph.
- Note that we are now traversing in the backward direction. Hence the name back propagation.
- P S (1)  S (l) S (l+1) s

## Regression versus classification
- Neural networks with the above losses can be used to solve various regression problems where the underlying function is highly nonlinear and the inputs/outputs are multi-dimensional.
- Till now, given the labelled dataset S = {(x i , y i ) : 1 ≤ i ≤ N }, we have considered two types of losses • The mean square error (MSE) • The mean absolute error (MAE)
- Neural networks with the above losses can be called "ResNets" or "Residual networks" and were introduced by He et al. in 2015.
- The main difference between ResNets and traditional neural networks is that ResNets are designed to solve problems where the output is not a single number, but a distribution.
- Residual networks are related to ODEs and can be used to solve problems where the input is not a single number, but a distribution.

## Vanishing gradients in deep networks
- Gradients become small when the network becomes deep.
- Adding skip connections between the hidden layers helps to prevent the gradients from becoming small.

## ResNets
- If all weights (and biases) were null, then x (5) = x (1)
- The computational graph for forward and back-propagation of a ResNet is shown in Figure 3.3
- Looking at this graph, it is clear that the expression for ∂x (l+1) ∂x (l) now involves traversing two branches and adding their sum. Therefore, we have ∂Π ∂ξ (L+1)
- If we assume that |W (m) | 1 via regularization, we have W (m) T Σ (m) + higher order terms ∂Π ∂ξ (L+1)

## Connections with ODEs
- A ResNet is a descritization of a nonlinear system of ODEs
- In order to solve a system of ODEs, we can uniformly divide the temporal domain with a time-step ∆t and temporal nodes t (l) = l∆t, 0 ≤ l ≤ L + 1
- Given x(0) and the weights of a network, we can approximate the solution x(L+1) using a time-integrator
- Training the ResNet means determining the parameters θ of the network so that x (L+1) is as close as possible to y i when x(0) is predicted
- When viewed from the analogous ODE point of view, training means determining the right hand side V (x, t) by requiring x(T ) to be as close as possible to y i when x(0) is predicted

## Neural ODEs
- Finite difference methods: This is a method that uses a series of small differences between the current and previous values of the equation to solve it. It is computationally expensive, and can only solve problems of moderate size.
- Finite element methods: This is a more computationally expensive method that uses a series of small calculations to solve the equation. It is also able to solve problems of moderate size, but can be more accurate than finite difference methods.
- Spectral Galerkin and collocation methods: These are more computationally expensive methods that use a series of large calculations to solve the equation. They are able to solve problems of very large size, but can be more accurate than finite difference methods.

## Finite difference method
- Discretize the domain into a grid of points
- Approximate the derivates with finite difference approximations at these points
- Solve this system using a suitable algorithm to find the solution
- In practice, we never actually invert K as it is computationally expensive. We instead use smart numerical algorithms to solve the system

## Spectral collocation method
- Spectral collocation methods seek a solution written as an expansion in terms of a set of smooth and global basis functions.
- The basis functions are chosen a priori, whereas the coefficients of the expansion are unknowns, and are computed by requiring that the numerical solution of the PDE is exact at a set of so-called collocation points.
- More specifically, this approach involves the following steps.
- 1. Select a set of global basis functions with the following properties: (a) It forms complete basis in the space of functions being considered. (b) Is smooth enough so that derivatives can be evaluated. (c) Easy to evaluate. (d) Derivatives that are easy to evaluate.
- For instance, one can use the Chebyshev polynomials defined on ξ ∈ (−1, 1), given by the following recurrence relation
- The first few Chebyshev polynomials are shown in
- 3. Evaluate the derivates for the PDE, which for our toy problem will be
- 4. Use the boundary conditions of the PDE. For the specific case of (4.1), which leads to
- 2 linear equations for N + 1 coefficients.
- We then consider a set of (suitably chosen) nodes x i , 1 ≤ i ≤ N − 1 in the interior of the domain, i.e. the collocation points, and use the derivatives found in step 3.
- in the PDE evaluated at these N − 1 nodes
- This leads to an additional N − 1 equations for the N + 1 coefficients.
- Combining (4.7) and (4.8) leads to the following linear system where
- We need to choose the collocation/quadrature points x i properly, so that K has desirable properties that make the linear system (4.9) easier to solve. These include invertibility, positivedefiniteness, sparseness, etc.
- Remark 4.2.1. The method is called a collocation method as the PDE is evaluated at the specific collocation/quadrature points x i .
- Remark 4.2.2. When working with a non-linear PDE, we will end up with a non-linear systems of algebraic equations for the coefficients u 0 , ..., u N , which is typically solved by Newton's method.
- Let us look at a least-square variant for finding the coefficients of the expansion of the spectral methods.
- As done earlier, we still represent the solution using (4.5) and compute its derivates. Then, the coefficients u are found by minimizing the following loss function
- This can be solved using any of the gradient-based methods we have seen in Chapter 2.
- This approach is especially useful when treating non-linear PDEs. In fact, in those cases it is not be possible to write a linear system in terms of the coefficients such as (4.9).

## Physics-informed neural networks (PINNs)
- Neural networks are a function representation of the PDE solution
- To find θ such that the PDE is satisfied in some suitable form, we compare this with spectral collocation approximation given by (4.5), where we need to determine the coefficients u n
- There are various ways to improve the accuracy of PINNs, such as increasing the number of collocation points, changing the hyper-parameter λ b weighting the boundary loss, or increasing the size of the network

## Extending PINNs to a more general PDE
- Consider a general PDE: Find the solution u : where L is the differential operator, f is the known forcing term, B is the boundary operator, and g is the non-homogeneous part of boundary condition (also prescribed).
- As an example, we can consider the three-dimensional incompressible Navier-Stokes equation solving for the velocity field
- Here Ω S is the three dimensions spatial domain and [0, T ] is the time interval of interest.
- The equation is given by ∂v ∂t
- The first equation above is the balance of linear moment. The second equation enforces the conservation of mass. The third equation is the no-slip boundary condition which is used when the boundary is rigid and fixed. The fourth equation is the prescription of the initial velocity field.
- To design a PINN for (4.13), the input to the network should be the independent variables x and the output should be the solution vector u.
- For the specific case of the Navier-Stokes system (4.14), the input to the network would be [s 1 , s 2 , s 3 , t] ∈ R 4 , while the output vector would be u 4 .
- The steps would be the following: • Select suitable N v collocation points in the interior of the domain and N b points on the domain boundary to evaluate the residuals. These could be chosen as based on quadrature rules, such as Gaussian, Lobatto, Uniform, Random, etc. Then the loss function is
- We make some remarks here: • It is implicitly assumed that a weight regularization term is also added to the loss Π(θ). • Is u * (x) the exact solution to the PDE? The answer is No! -Firstly, Π(θ * ) may not be zero. -Even if Π(θ * ) is identically zero, it only means that the residuals vanishes at the collocation points. However, that does not guarantee that the residuals will vanish everywhere in the domain. For that N v , N b → ∞. -Also, with a fixed network (N θ fixed) we cannot represent all functions. For that, we will need N θ → ∞.
- In practice, we only compute Π(θ * ). Is the solution error e = u * − u related to this loss value? And if it is, can we say that this error will be small as long as the loss is small? This is what we try to answer in next section.

## Error analysis for PINNs
- The error e = u * − u can only be estimated if the exact solution u is known
- The error can be controlled by reducing the loss functions and by increasing the number of interior and boundary collocation points
- The error can be controlled by increasing the number of interior and boundary collocation points if the residuals of the MLP solution are small

## Data assimilation using PINNs
- The problem of data assimilation is often encountered in the science and engineering.
- In this problem, we are able to make a few sparse measurements of a quantity, and using these we wish to evaluate it everywhere on a fine grid.
- We are also given a physical principal (in the form of a PDE) that the variable of interest must adhere to.
- Let us assume that we are given a set of sparse measurements of some quantity u on the domain Ω
- Furthermore, we are given that u satisfies some constraint R(u) = 0 on Ω.
- Then, data assimilation corresponds to using this information to find the value of u at any x ∈ Ω.
- We can solve this problem using PINNs.
- First, we represent u using a neural network F(x, θ).
- Next, we define a loss function where x i , M + 1 ≤ i ≤ M + N v are some collocation points chosen to evaluate the residual, while λ I , λ are hyper-parameters.
- Then we train the network by finding θ * = arg min θ Π(θ), and set the PINNs solution as u * = F (x; θ * ).

## Functions and images
- Consider a function u(x) where the image in (5.1) defines a grayscale image
- If we work with color images, then it would be a three-dimensional tensor, with the third dimension corresponding to the red, blue and green channels
- In other words, we can use the convolution operator to address all these issues

## Convolutions of functions
- The convolution operator maps functions to functions.
- Consider the function u(x), x ∈ R d , and a sufficiently smooth kernel function g(x) which decays as |x| → ∞.
- Then the convolution operator is given by
- We can interpret the convolution operator as sampling u by varying x.
- For example, in 1D, let u(x) and g(x) be as shown in Figure 5.1, and
- Consider a point x 0 . Then g(y − x 0 ) shifts the kernel to the location x 0 which will sample the function u in the orange shaded region. Similarly, for another point x 1 , g(y − x 1 ) shifts the kernel to the location x 1 which will sample the function u in the green shaded region. So as the kernel moves, it samples u in different windows.
- Lets now consider a few typical kernel functions.

### Example 1
- The Gaussian kernel is a popular choice for smoothing/blurring images
- It is isotropic, has a unity integral over the domain, and is parameterized by σ, which represents a frequency cut-off
- The Gaussian kernel is shown in Figure 5.1(a) for σ = 0.2

### Example 2
- This kernel is shown in both 1D and 2D in Figure 5.3
- The region to the left of the center of the kernel is weighted by a negative value and the region to the right is weighted by a positive value

## Discrete convolutions
- The discrete convolution is evaluated by considering the limit as the kernel width gets smaller and smaller
- The kernel width is reduced by excluding all the pixels over which the convolution will be zero

## Connection to finite differences
- Convolution is closely related to the stencil of a finite difference scheme
- The convolution operation approximates a derivative or second derivative
- The convolution operation is the same as the discrete convolution operation, but with weights given by a Taylor series expansion

## Convolution layers
- Each convolution layer consists of several discrete convolutions, each with its own kernel.
- The weights of the kernel, which determine its action (smoothing, first derivative, second derivative etc.), are learnable parameters and are determined when training the network.
- Thus, the way to think about the learning process is that the network learns the operations (convolutions) that are appropriate for its task.
- The task can be a classification problem, for example.
- Let us assume we have an N 1 ×N 2 image as an input. Then, we will have multiple convolutions in a convolution layer, each of which will generate a different image, as shown in Figure 5.4(a).
- The trainable parameters of this layer are the weights of each convolution kernel. Assuming the width of the kernels is N in each direction, and there are P kernels, then the number of trainable weights will be P × (2 N + 1) 2 .
- Next let us consider the size of the output image after applying a single kernel operation. Note that we will not be able to apply the kernel on the boundary pixels since there are no pixel-values available beyond the image boundary (see Figure 5.4(b)).
- Thus, we will have to skip N pixels at each boundary when applying the kernel, leading to an output image of shape (N 1 − N + 1) × (N 2 − N + 1).
- One way to overcome this is by padding the image with N pixels with zero value on each edge.
- Now we can apply the kernel on the boundary pixels and the output image will be the same size as the input image, as can be seen in Figure 5.4(c).
- Another feature of convolutions is known as the stride which determines the number of pixels by which the kernel is shifted as we move over the image.
- In the examples above, the stride was 1 in both directions.
- In practice, we can choose a stride > 1 which will further shrink the size of the output image.
- For instance, if stride was taken as S in each direction (with zero-padding applied), then the output image size would reduce by a factor of S in each direction (see Figure 5.4(d)).

### Average and Max Pooling
- Pooling operations reduce the size of an image
- Pooling operations allow you to step through different scales of the image
- Pooling operations are typically used to reduce the size of an image, and allow you to move through different scales of the image

### Convolution for inputs with multiple channels
- The input to a convolution layer is of size N × N × C, where C is the number of channels in the input image.
- A convolution layer apply P convolutions on this input and give an output of size M × M × P .
- The output of each convolution are stacked together to give the final output of the convolution layer.

## Convolution Neural Network (CNN)
- Convolution layers are a type of layer that takes an input image and produces a new image.
- A convolution layer is a type of layer that is used to reduce the resolution of an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a type of layer that is used to reduce the size of an image.
- A convolution layer is a type of layer that is used to reduce the number of pixels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of pixels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a type of layer that is used to reduce the size of an image.
- A convolution layer is a type of layer that is used to reduce the number of pixels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a type of layer that is used to reduce the size of an image.
- A convolution layer is a type of layer that is used to reduce the number of pixels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a type of layer that is used to reduce the size of an image.
- A convolution layer is a type of layer that is used to reduce the number of pixels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a type of layer that is used to reduce the size of an image.
- A convolution layer is a type of layer that is used to reduce the number of pixels in an image.
- A convolution layer is a type of layer that is used to reduce the number of features in an image.
- A convolution layer is a type of layer that is used to reduce the number of dimensions in an image.
- A convolution layer is a type of layer that is used to reduce the number of channels in an image.
- A convolution layer is a...

## Image-to-image transformations
- Image-to-image transformations can be seen as function-to-function transformations
- U-nets are a particular type of network that is used for image-to-image transformations
- U-nets have a downward branch that downscales images and an upward branch that scales up images
- U-nets also have skip connections that combine information at a particular scale in the downward branch to the information in the upward branch at the same scale

## The problem with PINNs
- A typical MLP y = F (x; θ) is a function that takes as input x ∈ R d and gives an output y ∈ R d with trainable weights θ
- A PINN is a network of the form u(x) = F (x; θ) taking as input the independent variable x of the underlying PDE and giving the solution u(x) (of the PDE) as output
- The network is trained by minimizing the weighted sum of the PDE and boundary residual

## Parametrized PDEs
- Assume the the source term f in (6.1) is given as a parametric function f (x; α).
- For instance, we could have Then we could train a PINN that accommodates for the parametrization by considering a network that takes as input both x and α, i.e., F(x, α; θ).
- This is shown in Figure 6.1
- This network can be trained by minimizing the loss function
- Note that we have to also consider collocation points for the parameter α while constructing the loss function.
- If θ * = arg min θ Π(θ), then the solution to the parameterized PDE would be u(x, α) = F(x, α; θ * ).
- Further, for any new value of α = α we could find the solution by evaluating F(x, α; θ * ).

## Operators
- The operator N maps the boundary condition g and the conductivity κ to the solution u of the PDE.
- In this case the input to the operator are two functions (g, κ) and the output is a single function.
- Therefore Ω X = Ω, while Ω Y = Ω × ∂Ω.
- The input and the output are related through the solution to the PDE above where it is assumed that f is given and fixed.
- Now consider the equations of linear isotropic elasticity posed on a three-dimensional domain
- Consider the operator defined by u(x) = N (f )(x). Here the input function, f : Ω → R 3 , and the output function u : Ω → R 3 . The two are related by the equations above where λ, µ, g are given and fixed.
- Now, consider a different PDE. In particular, the advection-diffusion-reaction equation,
- We want to find the operator N maps the initial condition u 0 to the solution u at the final time T , i.e., u(x, T ) = N (u 0 )(x). In this case Ω X = Ω Y = Ω. Further the input and the output functions are related to each other via the solution of the PDE above with a, κ, f, g given and fixed.

## Deep Operator Network (DeepONet) Architecture
- Operator networks were first proposed by Chen and Chen [5], where they considered only shallow networks with a single hidden layer.
- This idea was rediscovered and extended to deep architectures more recently in [14] and were called DeepONets.
- A standard DeepONet comprises two neural networks.
- We describe below its construction to approximate an operator N : A → U , where A is a set of functions of the form a : Ω Y ⊂ R d → R while U consists of functions of the form u : Ω X ⊂ R D → R. Furthermore, we assume that point-wise evaluations of both class of functions is possible.
- The architecture for the DeepONet for this operator is illustrated in Figure 6.2.
- It is explained below:
- • Fix M distinct sensor points y (1) , ..., y (M ) in Ω Y .
- • Sample a function a ∈ A at these sensor points to get the vector a = [a(y (1) ), ..., a(y • Supply a as the input to a sub-network, called the branch net B(.; θ B ) : R M → R p , whose output would be the vector β = [β 1 (a), ..., β p (a)] ∈ R p . Here θ B are the trainable parameters of the branch net. The dimension of the output of the branch is relatively small, say p ≈ 100.
- • Supply x as an input to a second sub-network, called the trunk net T (.; θ T ) : R D → R p , whose output would be the vector τ = [τ 1 (x), ..., τ p (x)] ∈ R p . Here θ T are the trainable parameters of the trunk net.
- • Take a dot product of the outputs of the branch and trunk nets to get the final output of the DeepONet N (., .; θ) : R D × R M → R which will approximate the value of u(x) where the trainable parameters of the DeepONet will be the combined parameters of the branch and trunk nets, i.e., θ = [θ T , θ M ].
- In the above construction, once the DeepONet is trained (we will discuss the training in the following section), it will approximate the underlying operator N , and allow us to approximate the value of any N (a)(x) for any a ∈ A and any x ∈ Ω X .
- Note that in the construction of the DeepONet, the M sensor points need to be pre-defined and cannot change during the training and evaluation phases.

## Training DeepONets
- Evaluate the values of these functions at the M sensor points, i.e., a j = a (i) (y (j) ) for 1 ≤ j ≤ M .
- For each a (i) , determine (numerically or analytically) the corresponding functions u (i) given by the operator N .
- Sample the function u (i) at N 2 points in Ω X , i.e., u (i) (x (k) ) for 1 ≤ k ≤ N 2 .

## Error Analysis for DeepONets
- There is a universal approximation theorem for a shallow version of DeepONets
- The theorem states that given a nonlinear, continuous operator mapping V ⊂ C(Ω Y ) into C(Ω X ), there exists a DeepONet with M sensors and a single hidden layer of width P
- This result has been extended to a deeper version of the network in [14], and generalized further by removing the compactness assumptions on the spaces [12]
