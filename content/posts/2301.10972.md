---
title: "On the Importance of Noise Scheduling for Diffusion Models"
date: 2023-01-26T07:37:22.000Z
author: "Ting Chen"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2301-10972v1.webp" # image path/url
    alt: "On the Importance of Noise Scheduling for Diffusion Models" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2301.10972)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2301.10972).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/on-the-importance-of-noise-scheduling-for).

# Abstract
- Noise scheduling is important for performance
- Optimal noise scheduling depends on task
- Increasing image size shifts optimal noise scheduling to noisier one
- Scaling input data with fixed noise schedule is a good strategy across image sizes

# Paper Content

## Why is noise scheduling important for diffusion models?
- Diffusion models define a noising process of data.
- Training of diffusion models involves sampling a continuous number between 0 and 1 and training a denoising network.
- Noise schedule determines the distribution of noise levels the neural network is trained on.
- Redundancy of information in data increases with image size, making it easier to recover the original signal.

## Strategies to adjust noise scheduling
- Noise scheduling is a concept related to computer science
- Two different noise scheduling strategies for diffusion models are studied

### Strategy 1: changing noise schedule functions
- Parameterized noise schedule with a one-dimensional function
- Cosine and sigmoid functions proposed in [11] and [8]
- Linear noise schedule function proposed in [6]
- Algorithm 1 presents code for instantiations of noise schedule function
- Figure 3 visualizes noise schedule functions and corresponding logSNR

### Strategy 2: adjusting input scaling factor
- Noise scheduling can be adjusted indirectly by scaling the input x 0 by a constant factor b.
- Reducing the scaling factor b increases the noise levels.
- Variance of x t can change even if x 0 has the same mean and variance.
- To ensure the variance is kept fixed, x t can be scaled by a factor of 1 (b 2 −1)γ(t)+1.
- Inputs can be normalized by their variance to make sure they have unit variance before feeding them to the denoising network.
- Input scaling shifts the logSNR along the y-axis without changing its shape.
- Cosine and sigmoid functions put most emphasis on where t is closer to 1.

### Putting it together: a simple compound noise scheduling strategy
- Combining two strategies by having a single noise schedule function
- During training and inference, variance normalization should be used
- Inference schedule does not need to be the same as training schedule
- During inference, time is discretized into a given number of steps
- Cosine schedule works well for sampling

## Experiments

### Setup
- Conducted experiments on class-conditional ImageNet image generation
- Used FID and Inception Score as metrics
- Used smaller models and shorter training steps
- Improved noise scheduling
- Used LAMB optimizer with specific hyper-parameters
- Summarized major hyper-parameters in Table 1 and 2

### The effect of strategy 1 (noise schedule functions)
- Different noise schedule functions can be used to obtain the best performance when input scaling is fixed to 1.
- Different image resolutions require different noise schedule functions to get the best performance.
- It is difficult to find the optimal schedule due to several hyper-parameters involved.

### The effect of strategy 2 (input scaling)

### Visualization of generated samples
- Label dropout not used for images at resolution of 512x512
- Classifier-free guidance during sampling improves fidelity of generated samples
- Visualization samples generated with guidance weight of 3
- Global structure well preserved across various resolutions

## Conclusion
- Noise scheduling is important for image generation and other tasks such as panoptic segmentation
- A simple strategy of adjusting input scaling factor works well across different image resolutions
- When combined with RIN architecture, noise scheduling enables single-stage generation of high resolution images
- Practitioners should select a proper noise scheduling scheme when training diffusion models
- Examples of random samples generated by single-stage end-to-end model at different resolutions
- Noised images with the same noise level show higher degree of redundancy in higher resolution images
- Cosine and sigmoid functions put most emphasis on where t is closer to 1
- Algorithm 2 shows how to incorporate combined noise scheduling strategy into training of diffusion models
- Comparison of state-of-the-art class-conditional pixel-based image generation models on ImageNet
