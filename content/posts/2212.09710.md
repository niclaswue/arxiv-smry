---
title: "Continual Learning for Instruction Following from Realtime Feedback"
date: 2022-12-19T18:39:43.000Z
author: "Alane Suhr, Yoav Artzi"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-09710v1.webp" # image path/url
    alt: "Continual Learning for Instruction Following from Realtime Feedback" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.09710)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.09710).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/continual-learning-for-instruction-following).

# Abstract
- The problem of continually training an instruction-following agent is studied
- Learning is cast as a contextual bandit problem
- The approach is evaluated through multiple rounds of human-agent interactions
- The feedback signal is roughly equivalent to the learning signal of supervised demonstration data

# Paper Content

## Introduction
- The dynamics that arise in situated language interactions between human users and automated agents expose a plethora of language learning signals.
- A prime example of such a signal is explicit feedback: when users convey their intent via natural language instructions for an agent to follow, they are well positioned to provide feedback, for example, through a stream of binary signals as the agent acts.
- Learning from this type of signal has significant potential. It shifts the burden of learning from annotated data to learning through interaction with users, which not only reduces data costs, but also enables continual improvement through interaction with users.
- This signal also fundamentally differs from gold-standard annotated data: it directly targets current agent behavior, rather than reflecting optimal human behavior that may be of less relevance to the agent's current policy.
- This approach stands in contrast with most methods for learning to follow instructions, where the use of demonstration data entails high data costs, and the clear separation between training and deployment passes over We iterate between deployment and training.
- In each round, during deployment, we sample and execute actions conditioned on user-written instructions, while users provide binary feedback as they observe instruction execution.
- We train each subsequent policy using a contextual bandit objective using the feedback data.
- This paper, we study the problem of continually improving an automated instruction-following agent1 by learning from user feedback in humanagent interactions.
- We situate the interaction in a collaborative environment, which provides conditions necessary for this type of learning: users are incentivized to improve the agent's capabilities by providing feedback, and are continuously present as the agent executes their instructions.
- We use the CEREALBAR scenario (Suhr et al., 2019), where two participants collaborate towards a common goal in a shared world, using natural language instructions to coordinate their actions.
- During interaction, human users write instructions for an agent, and provide feedback to the agent as it executes them via a stream of binary signals.
- Figure 1 illustrates the setup and our learning process.
- A key challenge is the complexity of the learning signal. Users provide feedback at their discretion and under time pressure as the agent executes their instructions. This results in an unpredictable, and often noisy, learning signal.
- While the concurrency of agent actions and user feedback provides a form of weak alignment, delays in human response and the lack of clarity about which action or actions they critique make attributing the learning signal to specific actions challenging.
- Further complicating learning is the feedback loop created by the continually changing agent behavior and constant adaptation by human users. These factors create a scenario that is particularly difficult for complex, even if expressive methods, as they often make various assumptions about the learning signal and the interaction dynamics (Knox and Stone, 2009;Mac-Glashan et al., 2017).
- We opt for a simple approach with minimal assumptions and formulate learning as a contextual bandit scenario.
- We alternate between deployment, where the agent interacts with users that provide feedback, and training, where we compute immediate rewards from feedback signals, and weigh examples using the current policy to account for the continually changing agent behavior.
- We experiment with our approach through interactions with humans users, where we repeatedly deploy, train, and re-deploy the agent.
- We study the effects of human adaptation and process design decisions on the efficacy of learning.
- We design our experiments to tease apart genuine agent improvements from human user adaptation, and observe dramatic improvements in agent behavior beyond what user adaptation explains, 15.4% absolute improvement in instruction execution accuracy in our longer running experiment.
- Our code and data will be released upon publication.

## Technical Overview
- Situates user-agent interactions in a collaborative scenario where two participants, a leader and a follower, collect sets of matching cards together in a shared, 3D environment.
- The leader plans what the pair should do, acts according to the plan, and describes the follower's part of the plan using natural language instructions.
- The follower's role is to follow the instructions.
- The pair receives a point for each set of cards they successfully collect. Their goal is to maximize their score.
- We add to the original CEREALBAR setting the ability for the leader to provide binary feedback signals to the follower as they execute instructions.
- In our study, the agent controls the follower and the leader is always a human user.
- The agent's task is to map natural language instructions and observations to follower actions.
- Actions include moving FORWARD or BACKWARD, turning LEFT or RIGHT, and completing instruction execution with STOP.
- The agent observation is a partial view of the world state from the first-person perspective of the follower.
- Formally,2 given an instruction x and an initial observation o 1 , our goal is to generate a sequence (o 1 , a 1 ), . . . , (o m , a m ) , where o i an agent observation, a i is the action taken, and a m = STOP.
- We define the follower agent as a policy π(x, o; θ) parameterized by θ that maps instructions x and state observations o to a distribution over actions.
- During interaction with users, for each user-written instruction x, we sample and execute actions a i ∼ π(• | x, o i ; θ) until we sample the instruction completion action.
- Concurrently, the user may provide positive and negative feedback signals f . Feedback signals are given in realtime, and are not timed to coincide with a specific action.
- Between any two agent steps, there could be any number of feedback signals.
- We optimize parameters through rounds of continual learning (Figure 1).
- The execution of an instruction x during interaction creates a trace made of two sequences ( (o i , a i , w a i ) m i=1 , (f j , w f j ) n j=1 ), where each action a i and feedback signal f j are paired with wall time w a i or w f j .
- At each round ρ, we deploy the policy with parameters θ ρ to interact with users, and then estimate parameters θ ρ+1 .
- During deployment, we collect agent instruction execution and user feedback.
- We compute reward from the feedback to construct training examples (x, o, a, r), where r is a reward.
- We optimize the policy parameters at each round by solving a contextual bandit problem, maximizing immediate expected reward.
- Our main metric is instruction execution accuracy for user-written instructions. Because we have no access to ground-truth demonstrations, but only to executions of our agent, we cannot compute accuracy automatically.
- We instead use acquire human judgments of follower accuracy after each round of deployment through separate crowdsourcing.
- We also evaluate performance through trends in user feedback and ratings over time.

## Continual Learning
- We estimate the policy parameters from user feedback during interaction with users.
- The processes progress in rounds.
- Each round ρ includes: (a) deploying the agent policy parameterized by θ ρ to interact with users (Section 3.1), (b) computing rewards from user feedback to construct training data D ρ (Section 3.2), and (b) optimizing the policy parameters using all data observed so far ρ ρ =0 D ρ to compute θ ρ+1 (Section 3.3).

### Deployment Interactions
- During deployment, users collaborate with the agent and delegate tasks to it by typing natural language instructions.
- For each instruction, the agent samples a sequence of actions from the policy and executes them in the environment.
- The agent records the wall time of the user when they start seeing the follower start to execute the action.
- When the user provides binary (positive or negative) feedback signals, the agent creates a stream of (f j , w f j ) n j=1.
- The user can also manually reboot the follower at any point during an instruction execution.

### Dataset Construction
- We use all traces collected in round ρ to construct a training dataset D ρ .
- Each example in D ρ is a tuple (x, o, a, r), where x is an instruction written by the user in round ρ, o is the agent observation when action a was selected during the execution x in the interaction, and r is a numerical reward we compute from the feedback stream.
- For each instruction written during round ρ we may create no examples if no feedback was given, or multiple examples if computing the rewards from feedback resulted in attributing rewards to multiple actions.
- We consider the value of each positive feedback as f = +1 and each negative feedback as f = −1.
- We compute the reward r i for an action a i as the sign of sum of feedback signals that occur between it and the next action after correcting for human response delay time (Mac-Glashan et al., 2017): where w a i is the wall time of action a i , w f j is the wall time of feedback signal f j , and d is a human response delay constant, which we set to 0.2 seconds.
- If the set is empty because there are no feedback signals within the time window, or if r i = 0, we do not set a reward or create an example for the corresponding action.
- Heuristically Propagated Reward
- The simple reward computation is likely to result in relatively sparse reward assignment, with many actions not receiving any learning signal. This is because users are not required to follow any particular feedback schedule. Indeed, in our experiments, we observe that roughly only 63.1% of actions receive feedback. However, human feedback many not be about the most recent action only, but can potentially be attributed to multiple recent actions.
- For example, the user may not be certain if to discourage a behavior, such as turning and going in a certain direction, until the trajectory is clear after several steps.
- We address this by taking inspiration from the use of eligibility traces for credit assignment (Sutton and Barto, 1998), and create heuristics to propagate the reward to actions that otherwise receive no reward.
- If an action is assigned no reward, we propagate to it a reward from a later action by assigning to it the reward of the next action that is assigned one, if one exists within the next eight actions.
- The only exception is that we do not propagate reward from a STOP action that receives a negative reward.
- We additionally prevent noisy feedback from being propagated by considering the scenario dynam-ics.
- We remove all training examples for actions following an action that results in card interaction (i.e., selecting or de-selecting a card) that receives a negative reward.
- If an action results in an invalid set (i.e., the cards selected cannot make a valid set), we remove it from the training data and do not propagate its award.
- These scenario-specific heuristics were developed by identifying common feedback errors in the data.
- Following propagation, we observe that 82.2% of the actions have reward assigned to them, up from 63.1, without increasing the reward error rate of about 7.0.

### Parameter Optimization
- We use a contextual bandit policy gradient objective to maximize the expected immediate reward
- At the end of each round, we train from scratch using all the data observed so far
- We process the initial human demonstration data D 0 to the same form as the reward data we get from user interactions, so we can use it seamlessly with the data we get from the rounds
- For each action a and input instruction x and observation o in D 0 , we create an example (x, o, a, +1) by setting the action reward to +1
- During round ρ, the gradient for an example (x, o, a, r) from dataset D ρ , ρ ≤ ρ is: where c(•) computes a clipped inverse propensity scoring (IPS) coefficient of the policy gradient (Horvitz and Thompson, 1952), and θ ρ are parameters with which the examples in D ρ were generated. IPS is commonly used for debiasing policies during off-policy learning.
- We also use IPS to avoid the problem of exploding gradients while training on negative examples (r < 0)

## Experimental Setup

## Results and Analysis
- We conduct an 11-round experiment to observe long-term learning and user behavior trends
- We compare several learning design decisions in a 5-round experiment

### Long-Term Experiment
- Collects 3368 games and 46,573 instructions
- Uses the heuristically propagated reward
- Improves accuracy from 66.7 ± 1.5 to 82.1 ± 1.1
- User adaptation results in increased interaction success
- User perception of agent behavior improves over time
- Language change results in decreased instructions specifying no cards to interact with and increased instructions specifying a single card

### Comparison of Learning Design Choices
- We conduct a second deployment experiment to study the impact of the amount of initial demonstration data, negative feedback, and reward propagation heuristics.
- We compare the feedback learning signal to supervised demonstration data.
- We design and deploy five system variations: REWARDPROP uses the exactly same process as in the long-term experiment (Section 5.1), including reward propagation (Section 3.2). SIMPLEREWARD measures the effect densifying and de-noising user feedback with heuristics by using only the simple reward (Section 3.2). NONEGATIVE simulates using positive feedback only.
- We discard all negative feedback, including reboots, from the data, and then apply the reward propagation heuristics (Section 3.2).
- FEWERDEMO uses only 144 human-human interactions (2,114 instructions) for initialization, about 25% of the supervised data used in RE-WARDPROP.
- SUPONLY compares training with human demonstrations vs. feedback data given the same number of interactions. SUPONLY does not use data from the human-agent interactions, even though we deploy it for evaluation. Instead, after each round, we add a fixed number of additional human-human interactions to the training set, equivalent to the amount of data collected with the other variants.
- Due to the limited amount of demonstration data, we are only able to deploy this approach for three rounds.
- We deploy the five variations for five rounds. In each rounds, all variations are deployed concurrently, each for 200 user interactions. To ensure that each worker interacts with all agents and to reduce ordering bias, we assign each worker a unique, consistent, and random ordering of agents; as they play multiple games, they cycle through agents in this order.
- Because all variants except FEW-ERDEMO are trained on the same initial data, we deploy a single agent for all but FEWERDEMO in the first round.
- This experiment cost $16,769.46.
- Figure 5 shows results for the five agent variants over five rounds. Overall, learning from feedback is robust to the different design choices.
- REWARDPROP has minor benefit relative to the simple reward (SIMPLEREWARD). RE-WARDPROP shows slightly faster learning, which means SIMPLEREWARD gives a poorer user experience early on, even though it catches up quickly.
- We observe that negative feedback is important: compared to REWARDPROP and SIMPLEREWARD, NONEGATIVE learns slower, receives less positive feedback, and more negative feedback and reboots. This difference does not show in early rounds, but only appears around round three.
- It particularly stands out in the fourth round, when NONEGATIVE shows an accuracy of 76.8 ± 1.0, compared to 81.0 ± 0.9 of REWARDPROP.
- The experiment illustrates that we can likely start with a much weaker agent.
- FEWERDEMO uses roughly a quarter of the demonstration initialization data available to the other variants, and indeed starts with much lower performance, 34.8 ± 1.8 accuracy, compared to 70.6 ± 1.1 for the others.
- Even this weak model is sufficient for effective for training and analysis by preferring earlier games played by each worker.
- We discard the other games.

## Related Work
- Learning for instruction following commonly relies on data with varying levels of supervision, such as gold-standard human demonstrations (e.g., Chen and Mooney, 2011;Tellex et al., 2011;Duvallet et al., 2013;Misra et al., 2017;Anderson et al., 2018;Blukis et al., 2018a,b;Suhr et al., 2019;Chen et al., 2019;Shridhar et al., 2020), or goal annotations (e.g., Artzi and Zettlemoyer, 2013;Misra et al., 2018;Suhr et al., 2018).
- We collapse the distinction between learning and deployment, shifting both training and evaluation into human-agent interactions, relying only on a small amount of human demonstration data to kickstart the process.
- There has been limited work on continual learning for language-related tasks.
- The most technically related work to ours is Kojima et al. (2021), who focuses on instruction generation and continual learning using implicit feedback.
- Similar to this work and the machine translation work of Kreutzer et al. (2018a,b), we cast learning as a contextual bandit problem.
- Human feedback through annotation or selection of intended output was studied for semantic parsing (Wang et al., 2016;Iyer et al., 2017) and summarization (Stiennon et al., 2020).
- This is related to soliciting partial annotations through dialogue (Artzi and Zettlemoyer, 2011;Thomason et al., 2015;Yao et al., 2020).
- Post-interaction feedback was studied in the context of dialogue (Liu et al., 2018).
- In contrast to existing work, we focus on sequential execution of instructions with realtime feedback.
- We use the collaborative CEREALBAR sce-nario (Suhr et al., 2019), where the user and the agent both act in the world and coordinate using natural language.
- Existing work with collaborative focus has primarily focused on reference games (Krauss andWeinheimer, 1964, 1966;Clark and Wilkes-Gibbs, 1986;Hawkins et al., 2017;Udagawa and Aizawa, 2019).
- We focus on dynamic environments and instructional language.
- CEREALBAR is related to the collaborative Cards environment (Djalali et al., 2011(Djalali et al., , 2012;;Potts, 2012), which was used for linguistic analysis.
- Continual learning through human-agent interaction has been studied more extensively outside of a language context.
- Our work is inspired by TAMER (Knox and Stone, 2009;Knox et al., 2013;Warnell et al., 2018) andCOACH (MacGlashan et al., 2017;Arumugam et al., 2018), where humans provide binary feedback to an agent acting in a world.

## Discussion
- The proposed approach is able to learn to follow instructions through interaction with human users that provide realtime feedback.
- The proposed approach is effective through multiple rounds of training and deployment, including thousands of interactions.
- The proposed approach is robust and can even start from a very weak initial agent.
- There is significant potential for learning from implicit signals that do not require any additional effort from users, such as Kojima et al. (2021) shows for instruction generation.
- Another promising avenue for research is more expressive feedback mechanisms, such as through natural language and even complete bidirectional dialogue.

## C Evaluation
- We randomly sample instruction execution traces
- E c contains all instructions marked as complete by the agent, and E contains instructions that were either marked as complete or rebooted
- We assume all rebooted instructions are incorrect executions
- The adjusted correctness rate is:
- e is the trace of the agent's execution of an instruction
- e * is the human demonstration
- e −1 ≡ e * −1 only if e results in the same set of cards selected as in e * . || e −1 − e * −1 || is the hex distance between stopping positions

## D Crowdsourcing Details
- Qualify workers through a tutorial and a short quiz
- Pay workers bonuses per point earned in each game
- Split workers into two pools: expert and novice
- Only expert workers are qualified to provide posthoc instruction execution judgments

## E Additional Results
- Figure 6 shows the Likert distribution for the three post-interaction statements users are asked about for the experiments comparing learning design choices.
- The three post-interaction statements are "I found the system easy to use," "I found the system helpful," and "I found the system confusing."
- The five systems were "A system using a text-based user interface," "A system using a graphical user interface," "A system using both a text-based and graphical user interface," "A system using a voice-based user interface," and "A system using a touch-based user interface."

### E.2 Evaluation on Static Data
- Evaluation through human-agent interaction is the main focus of our work. However, we also evaluate instruction-following agents against held-out, static data from Suhr et al. (2019).
- This evaluation does not take into account how the actual data distribution shifts over the agent's lifetime, because of the dynamics between the agent and human users.
- Figure 7 shows average SWSD for the models deployed in each round. SWSD begins at 39.7 for the initial model, and peaks at 46.4. This improvement is due entirely to adding training data acquired from human-agent interactions.
- Figure 1: Illustration of our continual learning process.We iterate between deployment and training. In each round, during deployment, we sample and execute actions conditioned on user-written instructions, while users provide binary feedback as they observe instruction execution. We train each subsequent policy using a contextual bandit objective using the feedback data.
- Figure 2: Mean estimated instruction execution accuracy and game scores across 11 rounds. The x-axis shows the number of instructions observed. Dashed lines show round boundaries. We mark with × the accuracies and scores from the post-hoc study of user adaptation for θ 1 and θ 11 . The dotted lines illustrate the share of the improvement due to user adaptation.
- Figure 3: Proportion of actions receiving positive and negative feedback from users over 11 rounds, and the ratio of frequency between positive and negative feedback. The x-axis shows the number of instructions observed. Dashed lines show round boundaries.
- Figure 5: Results for five agent variants over five rounds. Left: mean estimated instruction execution accuracy. Right: rate of positive ( ) and negative ( ) feedback, and instruction reboot rate ( ). x, e) is user judgment of execution e = (o i , a i , w a i ) m i=1 for instruction x.
- Static Evaluation Data We also evaluate on the development split from Suhr et al. (2019), with success weighted by stopping distance (SWSD). SWSD is computed per instruction execution: SWSD(e , e * ) = 1 e −1 ≡e * −1 1+ || e −1 − e * −1 ||
