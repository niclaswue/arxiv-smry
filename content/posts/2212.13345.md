---
title: "The Forward-Forward Algorithm: Some Preliminary Investigations"
date: 2022-12-27T02:54:46.000Z
author: "Geoffrey Hinton"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "https://ik.imagekit.io/smryai/2212-13345v1_1r_6EJBk3.jpg" # image path/url
    alt: "The Forward-Forward Algorithm: Some Preliminary Investigations" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13345)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13345).


# Abstract
- The aim of this paper is to introduce a new learning procedure for neural networks
- The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself
- Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data
- The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities
- If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives

# Paper Content

## The Forward-Forward Algorithm
- The Forward-Forward algorithm is a greedy multi-layer learning procedure inspired by Boltzmann machines (Hinton and Sejnowski, 1986) and Noise Contrastive Estimation (Gutmann and Hyvärinen, 2010).
- The idea is to replace the forward and backward passes of backpropagation by two forward passes that operate in exactly the same way as each other, but on different data and with opposite objectives.
- The positive pass operates on real data and adjusts the weights to increase the goodness in every hidden layer. The negative pass operates on "negative data" and adjusts the weights to decrease the goodness in every hidden layer.
- This paper explores two different measures of goodness -the sum of the squared neural activities and the negative sum of the squared activities, but many other measures are possible.
- Let us suppose that the goodness function for a layer is simply the sum of the squares of the activities of the rectified linear neurons in that layer.
- The aim of the learning is to make the goodness be well above some threshold value for real data and well below that value for negative data. More specifically, the aim is to correctly classify input vectors as positive data or negative data when the probability that an input vector is positive (i.e. real) is given by applying the logistic function, σ to the goodness, minus some threshold, θ where y j is the activity of hidden unit j before layer normalization.
- The negative data may be predicted by the neural net using top-down connections, or it may be supplied externally.

### The backpropagation baseline
- The MNIST dataset is used for most of the experiments
- The MNIST dataset is well studied and the performance of simple neural networks trained with backpropagation is well known
- Sensibly-engineered convolutional neural nets with a few hidden layers typically get about 0.6% test error
- In the "permutation-invariant" version of the task, the neural net is not given any information about the spatial layout of the pixels so it would perform equally well if all of the training and test images were subjected to the same random permutation of the pixels before training started.
- For the permutation-invariant version of the task, feed-forward neural networks with a few fully connected hidden layers of Rectified Linear Units (ReLUs) typically get about 1.4% test error.

### A simple unsupervised example of FF
- FF can be used to learn a linear transformation of representation vectors into vectors of logits
- The learning of the linear transformation to the logits is supervised, but does not involve learning any hidden layers
- FF can be used to perform this kind of representation learning by using real data vectors as the positive examples and corrupted data vectors as the negative examples
- After training for 60 epochs it gave 1.16% test error.

### A simple supervised example of FF
- Learning hidden representations without using any label information is quite sensible
- The unsupervised learning extracts a smorgasbord of features and the individual tasks can use whichever features are helpful
- But if we are only interested in one task and we want to use a small model that does not have the capacity to model the full distribution of the input data, it makes more sense to use supervised learning
- One way to achieve this with FF is to include the label in the input
- After training with FF, it is possible to classify a test digit by doing a single forward pass through the net starting from an input that consists of the test digit and a neutral label composed of ten entries of 0.1.

### Using FF to model top-down effects in perception
- All image classification algorithms so far have had a major weakness in that what is learned in later layers cannot affect what is learned in earlier layers
- To overcome this weakness, a recurrent neural network is used that updates the hidden layers in an alternating fashion
- This approach works better than feed-forward neural networks on the test data

### Using predictions from the spatial context as a teacher
- The recurrent net is a type of neural network that tries to have good agreement between the input from the layer above and the input from the layer below for positive data and bad agreement for negative data.
- In a network with spatially local connectivity, this has a very desirable property: The top-down input will be determined by a larger region of the image and will be the result of more stages of processing so it can be viewed as a contextual prediction for what should be produced by the bottom-up input which is based on a more local region of the image.
- If the input is changing over time, the top-down input will be based on older input data so it will have to learn to predict the representations of the bottom-up input.
- If we reverse the sign of the objective function and aim for low squared activities for positive data, the top-down input should learn to cancel out the bottom-up input on positive data which looks very like predictive coding (Rao and Ballard, 1999;van den Oord et al., 2018).
- The layer normalization means that plenty of information gets sent to the next layer even when the cancellation works pretty well.
- If all the prediction errors are small, they get exaggerated by the normalization thus making them more resistant to noise in transmission.
- The idea of learning by using a contextual prediction as a teaching signal for local feature extraction has been around for a long time but it has been difficult to make it work in neural networks using the spatial context as opposed to the one-sided temporal context.
- The obvious method of using the consensus of the top-down and bottom-up inputs as the teaching signal for both the top-down and bottom-up weights leads to collapse.
- This problem can be reduced by using the contextual predictions from other images to create the negative pairs, but the use of negative data rather than any negative internal representation seems more robust and does not require any restrictions such as only using the past to predict the future.
- For FF, hidden units in the last hidden layer After training with FF, a single forward pass through the net is a quick but sub-optimal way to classify an image. It is better to run the net with a particular label as the top-level input and record the average goodness over the middle iterations. After doing this for each label separately, the label with the highest goodness is chosen.
- For a large number of labels, a single forward pass could be used to get a candidate list of which labels to evaluate more thoroughly.

## Sleep
- FF is easier to implement than other contrastive learning techniques
- FF is more effective than other contrastive learning techniques
- FF is more efficient than other contrastive learning techniques
- FF is more biologically plausible than other contrastive learning techniques
- FF is more efficient than reinforcement learning

### Relationship to Boltzmann Machines
- backpropagation was a promising learning procedure for deep neural networks in the early 1980s
- Boltzmann Machines are a network of stochastic binary neurons with pairwise connections that have the same weight in both directions
- when it is running freely with no external input, a Boltzmann machine repeatedly updates each binary neuron by setting it to the on state with a probability equal to the logistic of the total input it receives from other active neurons
- the negative energy is simply the sum of the weights between all pairs of neurons that are on in that configuration
- a subset of the neurons in a Boltzmann Machine is "visible" and binary data vectors are presented to the network by clamping them on the visible neurons and then letting it repeatedly update the states of the remaining, hidden neurons
- the aim of Boltzmann machine learning is to make the distribution of binary vectors on the visible neurons when the network is running freely match the data distribution
- very surprisingly, the Kullback-Liebler divergence between the data distribution and the model distribution exhibited on the visible neurons by the freely running Boltzmann machine at thermal equilibrium has a very simple derivative w.r.t. any weight, w ij in the network: where the angle brackets denote an expectation over the stochastic fluctuations at thermal equilibrium and also the data for the first term
- the exciting thing about this result is that it gives derivatives for weights deep within the network without ever propagating error derivatives explicitly
- instead, it propagates neural activities in two different phases which were intended to correspond to wake and sleep
- unfortunately, the mathematical simplicity of the learning rule comes at a very high price
- it requires a deep Boltzmann Machine to approach its equilibrium distribution and this makes it impractical as a machine learning technique and implausible as a model of cortical learning

### Relationship to Generative Adversarial Networks
- A GAN (Goodfellow et al., 2014) uses a multi-layer neural network to generate data and it trains its generative model by using a multi-layer discriminative network to give it derivatives w.r.t. the output of the generative model of the probability that the output is real data as opposed to generated data.
- GANs are tricky to train because the discriminative and generative models are competing.
- In practice they generate very nice images but suffer from mode collapse: There can be large regions of image space in which they never generate examples.
- Also they use backpropagation to adapt each network so it is hard to see how to implement them in cortex.
- FF can be viewed as a special case of a GAN in which every hidden layer of the discriminative network makes its own greedy decision about whether the input is positive or negative so there is no need to backpropagate to learn the discriminative model.
- There is also no need to backpropagate to learn the generative model because instead of learning its own hidden representations, it just reuses the representations learned by the discriminative model.

### Relationship to contrastive methods that compare representations of two different image crops
- There is a family of self-supervised contrastive methods that learn by optimizing an objective function that favors agreement between the representations of two different crops from the same image and disagreement between the representations of crops from two different images.
- These methods generally use many layers to extract the representations of the crops and they train these layers by backpropagating the derivatives of the objective function.
- They do not work if the two crops always overlap in exactly the same way because then they can simply report the intensities of the shared pixels and get perfect agreement.
- In a real neural network, it would be non-trivial to measure the agreement between two different representations
- There is no obvious way to extract the representations of two crops at the same time using the same weights.
- FF uses a different way to measure agreement, which seems easier for a real neural network.
- Many different sources of information provide input to the same set of neurons.
- If the sources agree on which neurons to activate there will be positive interference resulting in high squared activities and if they disagree the squared activities will be lower
- Measuring agreement by using positive interference is a lot more flexible than comparing two different representation vectors because there is no need to arbitrarily divide the input into two separate sources.
- A major weakness of SimCLR-like methods is that a lot of computation is used to derive the representations of two image crops, but the objective function only provides a modest amount of constraint on the representations and this limits the rate at which information about the domain can be injected into the weights.
- To make the representation of a crop be closer to its correct mate than to a million possible alternatives only takes 20 bits of information.
- This problem seems even worse for FF since it only takes one bit to distinguish a positive case from a negative one.
- The solution to this poverty of constraint is to divide each layer into many small blocks and to force each block separately to use the length of its pre-normalized activity vector to decide between positive and negative cases.
- The information required to satisfy the constraints then scales linearly with the number of blocks which is much better than the logarithmic scaling achieved by using a larger contrast set in SimCLR-like methods.

### A problem with stacked contrastive learning
- An obvious unsupervised way to learn multiple layers of representation is to first learn one hidden layer that captures some of the structure in the data and to then treat the activity vectors in this layer as data and apply the same unsupervised learning algorithm again.
- This is how multiple layers of representation are learned using Restricted Boltzmann machines (RBMs) (Hinton et al., 2006b) or stacked autoencoders (Bengio et al., 2007).
- But it has a fatal flaw. Suppose we map some random noise images through a random weight matrix. The resulting activity vectors will have correlational structure that is created by the weight matrix and has nothing to do with the data.
- When unsupervised learning is applied to these activity vectors it will discover some of this structure, but that tells the system nothing about the external world.
- The original Boltzmann Machine learning algorithm was designed to avoid this flaw (Hinton and Sejnowski, 1986) (page 297) by contrasting statistics caused by two different external boundary conditions.
- This cancels out all the structure that is just the result of other parts of the network.
- When contrasting positive and negative data, there is no need to restrict the wiring or have random spatial relationships between crops to prevent the network from cheating.
- This makes it easy to have a large number of interconnected groups of neurons, each with its own objective of distinguishing positive from negative data.
- It is possible to measure agreement much more sharply if the inputs are spikes which arrive at particular times and the post-synaptic neurons only fire if they get many spikes within a small time window.
- Stacked RBMs can deal with this issue, though not very well, by initializing each RBM to have the transpose of the weights of the previous RBM. This means that the initial weights have already captured the structure induced by the previous weight matrix and changes to those initial weights may capture structure due to the external world, but a lot of the capacity is used up just modeling the previous weight matrix.

## Learning fast and slow
- The weight updates used to change the goodness function of a layer for a particular input vector have no effect on the layer-normalized output of that layer when the input vector is x.
- The vector of increments of the incoming weights for hidden neuron j is given by: where y j is the activity of the ReLU before layer normalization, w j is the vector of incoming weights of neuron j and is the learning rate.
- After the weight update has occurred, the change in the activity of neuron j is simply the scalar product ∆w j x.
- The fact that the weight update does not change the layer normalized output for that input vector means that it is possible to perform simultaneous online weight updates in many different layers.
- It is possible to change all the weights in one step so that every layer exactly achieves a desired goodness of S * for input vector x.
- Assuming that the input vector and all of the layer-normalized hidden vectors are of length 1, the learning rate that achieves this is given by: where S L is the current sum of squared activities of layer L before layer normalization.
- Currently, we do not exploit this interesting property of FF because we still use mini-batches, but the ability of a deep neural net to absorb a lot of information from a single training case by jumping to a set of weights that handles that case perfectly could be of interest to psychologists who are tired of creeping down gradients.

## Mortal Computation
- General purpose digital computers were designed to faithfully follow instructions because it was assumed that the only way to get a general purpose computer to perform a specific task was to write a program that specified exactly what to do in excruciating detail.
- This is no longer true, but the research community has been slow to comprehend the long-term implications of deep learning for the way computers are built.
- More specifically the community has clung to the idea that the software should be separable from the hardware so that the same program or the same set of weights can be run on a different physical copy of the hardware. This makes the knowledge contained in the program or the weights immortal: The knowledge does not die when the hardware dies.
- The separation of software from hardware is one of the foundations of Computer Science and it has many benefits. It makes it possible to study the properties of programs without worrying about electrical engineering. It makes it possible to write a program once and copy it to millions of computers. It makes it possible to compute derivatives on huge data-sets by using many copies of the same model running in parallel.
- If, however, we are willing to abandon immortality it should be possible to achieve huge savings in the energy required to perform a computation and in the cost of fabricating the hardware that executes the computation.
- We can allow large and unknown variations in the connectivity and non-linearities of different instances of hardware that are intended to perform the same task and then rely on a learning procedure to discover parameter values that make effective use of the unknown properties of each particular instance of the hardware. These parameter values are only useful for that specific hardware instance, so the computation they perform is mortal: it dies with the hardware.
- Even though it makes no sense to copy the parameter values to a different piece of hardware that works differently, there is a more biological way of transferring what has been learned by one piece of hardware to a different piece of hardware.
- For a task like classification of objects in images, what we are really interested in is the function relating pixel intensities to class labels, not the parameter values that implement that function in a particular piece of hardware.
- The function itself can be transferred (approximately) to a different piece of hardware by using distillation (Hinton et al., 2014): The new hardware is trained not only to give the same answers as the old hardware but also to output the same probabilities for incorrect answers. These probabilities are a much richer indication of how the old model generalizes than just the label it thinks is most likely.
- So by training the new model to match the probabilities of incorrect answers we are training it to generalize in the same way as the old model. This is a rare example of neural net training actually optimizing what we care about: generalization.
- Distillation works best when the teacher gives highly informative outputs that reveal a lot about the teacher's internal representations. This may be one of the major functions of language.
- Rather than viewing a descriptive sentence as a piece of symbolic knowledge that needs to be stored in some unambiguous internal language we could view it as a way of providing information about the speaker's internal vector representations that allows the hearer to learn similar vector representations (Raghu et al., 2020).
- From this perspective, a tweet that is factually incorrect could nevertheless be a very effective way to transfer features used for interpreting the world to a set of followers whose immediate intuitions would then be the same as the teacher's.
- If language has evolved to facilitate the learning of internal representation vectors by the members of a culture, it is not surprising that training a large neural network on massive amounts of language is such an effective way to capture the world view of that culture.
