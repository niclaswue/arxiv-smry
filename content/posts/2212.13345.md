---
title: "The Forward-Forward Algorithm: Some Preliminary Investigations"
date: 2022-12-27T02:54:46.000Z
author: "Geoffrey Hinton"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-13345v1.webp" # image path/url
    alt: "The Forward-Forward Algorithm: Some Preliminary Investigations" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.13345)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.13345).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/the-forward-forward-algorithm-some-1).

# Abstract
- introduces a new learning procedure for neural networks
- demonstrates that it works well enough on a few small problems to be worth further investigation
- forward-forward algorithm replaces forward and backward passes of backpropagation by two forward passes, one with positive data and the other with negative data which could be generated by the network itself
- each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data
- the sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities

# Paper Content

## The Forward-Forward Algorithm
- The Forward-Forward algorithm is a greedy multi-layer learning procedure inspired by Boltzmann machines (Hinton and Sejnowski, 1986) and Noise Contrastive Estimation (Gutmann and Hyvärinen, 2010).
- The idea is to replace the forward and backward passes of backpropagation by two forward passes that operate in exactly the same way as each other, but on different data and with opposite objectives.
- The positive pass operates on real data and adjusts the weights to increase the goodness in every hidden layer. The negative pass operates on "negative data" and adjusts the weights to decrease the goodness in every hidden layer.
- This paper explores two different measures of goodness -the sum of the squared neural activities and the negative sum of the squared activities, but many other measures are possible.
- Let us suppose that the goodness function for a layer is simply the sum of the squares of the activities of the rectified linear neurons in that layer.
- The aim of the learning is to make the goodness be well above some threshold value for real data and well below that value for negative data. More specifically, the aim is to correctly classify input vectors as positive data or negative data when the probability that an input vector is positive (i.e. real) is given by applying the logistic function, σ to the goodness, minus some threshold, θ where y is the activity of hidden unit before layer normalization.
- The negative data may be predicted by the neural net using top-down connections, or it may be supplied externally.

### The backpropagation baseline
- The MNIST dataset is used for most of the experiments
- The MNIST dataset is well studied and the performance of simple neural networks trained with backpropagation is well known
- Sensibly-engineered convolutional neural nets with a few hidden layers typically get about 0.6% test error
- In the "permutation-invariant" version of the task, the neural net is not given any information about the spatial layout of the pixels so it would perform equally well if all of the training and test images were subjected to the same random permutation of the pixels before training started.
- For the permutation-invariant version of the task, feed-forward neural networks with a few fully connected hidden layers of Rectified Linear Units (ReLUs) typically get about 1.4% test error.

### A simple unsupervised example of FF
- FF can be used to learn a linear transformation of representation vectors into vectors of logits which is used in a softmax to determine a probability distribution over labels
- The learning of the linear transformation to the logits is supervised, but does not involve learning any hidden layers so it does not require backpropagation of derivatives
- FF can be used to perform this kind of representation learning by using real data vectors as the positive examples and corrupted data vectors as the negative examples
- To force FF to focus on the longer range correlations in images that characterize shapes, we need to create negative data that has very different long range correlations but very similar short range correlations
- This can be done by creating a mask containing fairly large regions of ones and zeros
- After training for 60 epochs it gave 1.16% test error.

### A simple supervised example of FF
- Learning hidden representations without using any label information is quite sensible
- The unsupervised learning extracts a smorgasbord of features and the individual tasks can use whichever features are helpful
- But if we are only interested in one task and we want to use a small model that does not have the capacity to model the full distribution of the input data, it makes more sense to use supervised learning
- One way to achieve this with FF is to include the label in the input
- The positive data consists of an image with the correct label and the negative data consists of an image with the incorrect label
- After training with FF, it is possible to classify a test digit by doing a single forward pass through the net starting from an input that consists of the test digit and a neutral label composed of ten entries of 0.1.
- The hidden activities in all but the first hidden layer are then used as the inputs to a softmax that has been learned during training.

### Using FF to model top-down effects in perception
- All of the examples of image classification so far have used feed-forward neural networks that were learned greedily one layer at a time.
- This means that what is learned in later layers cannot affect what is learned in earlier layers.
- The key to overcoming this apparent limitation of FF is to treat a static image as a rather boring video that is processed by a multi-layer recurrent neural network.
- FF runs forwards in time for both the positive and negative data, but, as figure 3 shows, the activity vector at each layer is determined by the normalized activity vectors at both the layer above and the layer below at the previous time-step.
- As an initial check that this approach actually works, we can use "video" input that consists of a static MNIST image which is simply repeated for each time-frame.
- The bottom layer is the pixel image and the top layer is a one-of-N representation of the digit class.
- There are two or three intermediate layers each of which has 2000 neurons.
- In a preliminary experiment, the recurrent net was run for 10 time-steps and at each time-step the even layers were updated based on the normalized activities in the odd layers and then the odd layers were updated based on the new normalized activities in the even layers.
- This alternating update was designed to avoid biphasic oscillations, but it appears to be unnecessary: With a little damping, synchronous updates of all hidden layers based on the normalized states at the previous time step actually learn slightly better which is good news for recurrent neural networks.
- The net shown in figure 3 was trained on MNIST for 60 epochs.
- For each image, the hidden layers are initialized by a single bottom-up pass. After this the network is run for 8 synchronous iterations with damping.
- The performance of the network on the test data is evaluated by running it for 8 iterations with each of the 10 labels and picking the label that has the highest goodness averaged over iterations 3 to 5.
- It gets 1.31% test error.
- Negative data is generated by doing a single forward pass through the net to get probabilities for all the classes and then choosing between the incorrect classes in proportion to their probabilities.

### Using predictions from the spatial context as a teacher
- The recurrent net is a type of neural network that tries to have good agreement between the input from the layer above and the input from the layer below for positive data and bad agreement for negative data.
- In a network with spatially local connectivity, this has a very desirable property: The top-down input will be determined by a larger region of the image and will be the result of more stages of processing so it can be viewed as a contextual prediction for what should be produced by the bottom-up input which is based on a more local region of the image.
- If the input is changing over time, the top-down input will be based on older input data so it will have to learn to predict the representations of the bottom-up input.
- If we reverse the sign of the objective function and aim for low squared activities for positive data, the top-down input should learn to cancel out the bottom-up input on positive data which looks very like predictive coding (Rao and Ballard, 1999;van den Oord et al., 2018).
- The layer normalization means that plenty of information gets sent to the next layer even when the cancellation works pretty well.
- If all the prediction errors are small, they get exaggerated by the normalization thus making them more resistant to noise in transmission.
- The idea of learning by using a contextual prediction as a teaching signal for local feature extraction has been around for a long time but it has been difficult to make it work in neural networks using the spatial context as opposed to the one-sided temporal context.
- The obvious method of using the consensus of the top-down and bottom-up inputs as the teaching signal for both the top-down and bottom-up weights leads to collapse.
- This problem can be reduced by using the contextual predictions from other images to create the negative pairs, but the use of negative data rather than any negative internal representation seems more robust and does not require any restrictions such as only using the past to predict the future.
- For FF, hidden units in the last hidden layer After training with FF, a single forward pass through the net is a quick but sub-optimal way to classify an image. It is better to run the net with a particular label as the top-level input and record the average goodness over the middle iterations. After doing this for each label separately, the label with the highest goodness is chosen.
- For a large number of labels, a single forward pass could be used to get a candidate list of which labels to evaluate more thoroughly.

## Sleep
- FF is easier to implement than other contrastive learning techniques
- FF is less effective than other contrastive learning techniques when the data is negative
- FF is more effective than other contrastive learning techniques when the data is positive
- It is possible to separate the positive and negative phases of learning with FF
- Eliminating the negative phase updates for a while, leads to effects that mimic the devastating effects of severe sleep deprivation.

### Relationship to Boltzmann Machines
- Backpropagation and Boltzmann Machines were two promising learning procedures for deep neural networks in the early 1980s
- The Kullback-Liebler divergence between the data distribution and the model distribution exhibited on the visible neurons by the freely running Boltzmann machine at thermal equilibrium has a very simple derivative w.r.t. any weight, w ij in the network: where the angle brackets denote an expectation over the stochastic fluctuations at thermal equilibrium and also the data for the first term.
- Boltzmann machine learning remains intellectually interesting because it replaces the forward and backward passes of backpropagation with two iterative settlings that work in exactly the same way, but with different boundary conditions on the visible neurons -clamped to data versus free.
- FF combines the contrastive learning from Boltzmann machines with a simple, local goodness function that is far more tractable than the free energy of a network of binary stochastic neurons.

### Relationship to Generative Adversarial Networks
- A GAN (Goodfellow et al., 2014) uses a multi-layer neural network to generate data and it trains its generative model by using a multi-layer discriminative network to give it derivatives w.r.t. the output of the generative model of the probability that the output is real data as opposed to generated data.
- GANs are tricky to train because the discriminative and generative models are competing.
- In practice they generate very nice images but suffer from mode collapse: There can be large regions of image space in which they never generate examples.
- Also they use backpropagation to adapt each network so it is hard to see how to implement them in cortex.
- FF can be viewed as a special case of a GAN in which every hidden layer of the discriminative network makes its own greedy decision about whether the input is positive or negative so there is no need to backpropagate to learn the discriminative model.
- There is also no need to backpropagate to learn the generative model because instead of learning its own hidden representations, it just reuses the representations learned by the discriminative model.

### Relationship to contrastive methods that compare representations of two different image crops
- There is a family of self-supervised contrastive methods that learn by optimizing an objective function that favors agreement between the representations of two different crops from the same image and disagreement between the representations of crops from two different images.
- These methods generally use many layers to extract the representations of the crops and they train these layers by backpropagating the derivatives of the objective function.
- They do not work if the two crops always overlap in exactly the same way because then they can simply report the intensities of the shared pixels and get perfect agreement.
- In a real neural network, it would be non-trivial to measure the agreement between two different representations and there is no obvious way to extract the representations of two crops at the same time using the same weights.
- FF uses a different way to measure agreement, which seems easier for a real neural network.
- Many different sources of information provide input to the same set of neurons.
- If the sources agree on which neurons to activate there will be positive interference resulting in high squared activities and if they disagree the squared activities will be lower.
- Measuring agreement by using positive interference is a lot more flexible than comparing two different representation vectors because there is no need to arbitrarily divide the input into two separate sources.
- A major weakness of SimCLR-like methods is that a lot of computation is used to derive the representations of two image crops, but the objective function only provides a modest amount of constraint on the representations and this limits the rate at which information about the domain can be injected into the weights.
- To make the representation of a crop be closer to its correct mate than to a million possible alternatives only takes 20 bits of information.
- This problem seems even worse for FF since it only takes one bit to distinguish a positive case from a negative one.
- The solution to this poverty of constraint is to divide each layer into many small blocks and to force each block separately to use the length of its pre-normalized activity vector to decide between positive and negative cases.
- The information required to satisfy the constraints then scales linearly with the number of blocks which is much better than the logarithmic scaling achieved by using a larger contrast set in SimCLR-like methods.

### A problem with stacked contrastive learning
- An obvious unsupervised way to learn multiple layers of representation is to first learn one hidden layer that captures some of the structure in the data and to then treat the activity vectors in this layer as data and apply the same unsupervised learning algorithm again.
- This is how multiple layers of representation are learned using Restricted Boltzmann machines (RBMs) (Hinton et al., 2006b) or stacked autoencoders (Bengio et al., 2007).
- But it has a fatal flaw. Suppose we map some random noise images through a random weight matrix. The resulting activity vectors will have correlational structure that is created by the weight matrix and has nothing to do with the data.
- When unsupervised learning is applied to these activity vectors it will discover some of this structure, but that tells the system nothing about the external world.
- The original Boltzmann Machine learning algorithm was designed to avoid this flaw (Hinton and Sejnowski, 1986) (page 297) by contrasting statistics caused by two different external boundary conditions.
- This cancels out all the structure that is just the result of other parts of the network.
- When contrasting positive and negative data, there is no need to restrict the wiring or have random spatial relationships between crops to prevent the network from cheating.
- This makes it easy to have a large number of interconnected groups of neurons, each with its own objective of distinguishing positive from negative data.
- But it is possible to measure agreement much more sharply if the inputs are spikes which arrive at particular times and the post-synaptic neurons only fire if they get many spikes within a small time window.
- Stacked RBMs can deal with this issue, though not very well, by initializing each RBM to have the transpose of the weights of the previous RBM.

## Learning fast and slow
- If there is full connectivity between layers, the weight updates used to change the goodness function of a layer for a particular input vector have no effect on the layer-normalized output of that layer when the input vector is x.
- The vector of increments of the incoming weights for hidden neuron j is given by: where y j is the activity of the ReLU before layer normalization, w j is the vector of incoming weights of neuron j and is the learning rate.
- After the weight update has occurred, the change in the activity of neuron j is simply the scalar product ∆w j x.
- The fact that the weight update does not change the layer normalized output for that input vector means that it is possible to perform simultaneous online weight updates in many different layers.
- It is possible to change all the weights in one step so that every layer exactly achieves a desired goodness of S * for input vector x.
- Assuming that the input vector and all of the layer-normalized hidden vectors are of length 1, the learning rate that achieves this is given by: where S L is the current sum of squared activities of layer L before layer normalization.
- Currently, we do not exploit this interesting property of FF because we still use mini-batches, but the ability of a deep neural net to absorb a lot of information from a single training case by jumping to a set of weights that handles that case perfectly could be of interest to psychologists who are tired of creeping down gradients.

## Mortal Computation
