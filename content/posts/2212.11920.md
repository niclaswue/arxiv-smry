---
title: "Beyond SOT: It's Time to Track Multiple Generic Objects at Once"
date: 2022-12-22T17:59:19.000Z
author: "Christoph Mayer, Martin Danelljan, Ming-Hsuan Yang, Vittorio Ferrari, Luc Van Gool, Alina Kuznetsova"
weight: 2
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Important disclaimer: the following content is AI-generated, please make sure to fact check the presented information by reading the full paper."
disableHLJS: true # to disable highlightjs
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: false
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: false
UseHugoToc: false
cover:
    image: "thumbnails/2212-11920v1.webp" # image path/url
    alt: "Beyond SOT: It's Time to Track Multiple Generic Objects at Once" # alt text
    caption: "The full paper is available [here](https://arxiv.org/abs/2212.11920)." # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
---

# Link to paper
The full paper is available [here](https://arxiv.org/abs/2212.11920).

You can also find the paper on PapersWithCode [here](https://paperswithcode.com/paper/beyond-sot-it-s-time-to-track-multiple).

# Abstract
- Generic Object Tracking (GOT) is the problem of tracking target objects, specified by bounding boxes in the first frame of a video
- While the task has received much attention in the last decades, researchers have almost exclusively focused on the single object setting
- Multi-object GOT benefits from a wider applicability, rendering it more attractive in real-world applications
- We attribute the lack of research interest into this problem to the absence of suitable benchmarks
- In this work, we introduce a new large-scale GOT benchmark, LaGOT, containing multiple annotated target objects per sequence
- Our benchmark allows researchers to tackle key remaining challenges in GOT, aiming to increase robustness and reduce computation through joint tracking of multiple objects simultaneously
- TaMOs achieves a 4x faster run-time in case of 10 concurrent objects compared to tracking each object independently and outperforms existing single object trackers on our new benchmark
- TaMOs achieves highly competitive results on single-object GOT datasets, setting a new state-of-the-art on TrackingNet with a success rate AUC of 84.4%.

# Paper Content

## Introduction
- Visual object tracking is a fundamental problem in computer vision
- Over the years the research effort has been directed mainly to two different task definitions: Generic Object Tracking (GOT) [2,4,12,23,28,30,54] and Multiple Object Tracking (MOT) [5,15,21,48,[64][65][66]. MOT aims at detecting and tracking all objects corresponding to a set of predefined classes.
- Thus, MOT trackers typically require a detector separately trained on objects of all these classes. Such setups not only require additional training data and an-* Work done while interning at Google Research.  .
- Generic Object Trackers (GOT) have only focused on the Single Object Tracking (SOT) scenario. Thus, when encountered with multiple objects, which frequently occurs in applications, such methods must resort to independent tracking of each present object. This leads to a directly linear increase in computation.
- In contrast, our tracker TaMOs jointly tracks multiple objects, leading to substantial computational savings and the opportunity to exploit inter-object information for improved robustness.
- To promote new research in the direction of multiple object tracking, we introduce the new GOT benchmark LaGOT. In particular, we extend LaSOT [18] annotations with up to 10 tracks per sequence with an average of 2.8 tracks per video. We choose LaSOT because it features challenging characteristics such as fast moving objects, frequent occlusions, camera motion, camouflage and often many visually similar objects are present.
- In total we almost triple the total length of all tracks, from 381 to 975 minutes. We also increase the diversity of objects by including 31 additional object classes.
- Tracking multiple target objects in the same video sequences poses key challenges and research questions that are typically overlooked by GOT methods. A generic multiple object tracker needs to jointly produce multiple target models using the first-frame annotations. This allows the tracker to exploit annotations of potential distractor objects to improve the robustness of each target model. Furthermore, a joint localization step opens the opportunity for global reasoning across all tracks in order to reduce the risk of confusing similar objects. Finally, operating on targetspecific search areas [7,38,59], complicates re-detection of lost objects and becomes cumbersome and inefficient in case of multiple objects.
- We tackle these challenges by introducing a new multiple object GOT tracker. In order to track all desired target objects at once it operates globally on the entire frame. Furthermore, we propose a new generic multiple object encoding that allows us to encode multiple targets within the same training sample. We achieve this by learning a fixed size pool of different object embeddings, each representing a different target. Thus, we query the proposed model predictor with these object embeddings to produce all target models. In addition, we introduce a Feature Pyramidal Network (FPN) to increase the overall tracking accuracy while operating on full-frame inputs.
- Contributions. Our main contributions are: (i) We propose a novel multiple object GOT evaluation benchmark LaGOT consisting of densely annotated multiple generic object tracks with an average of 2.8 tracks per sequence and an overall evaluation length of 879 minutes in single object mode. We evaluate 8 existing GOT trackers on the proposed LaGOT benchmark. (ii) We propose TaMOs a GOT tracker that tracks multiple generic objects at the same time efficiently. To achieve this we propose a new multiple object encoding, introduce an FPN and apply the tracker globally on the full video frame. (iii) We analyze the proposed tracker by assessing the impact of its different components and evaluate it on the proposed multiple object GOT benchmark as well as on popu-lar single object GOT benchmarks where it achieves competitive results. TaMOs outperforms recent trackers, such as MixFormer [7] and ToMP [62] on LaGOT, while achieving excellent results on LaSOT [18] and setting a new state of the art on TrackingNet [41]. Lastly, TaMOs demonstrates almost constant run-time when increasing the number of targets and operates at over 4× faster run-times compared to the baseline when tracking 10 objects.

## Related Work
- Generic object tracking is a well explored topic
- There are specialized datasets and challenges that focus on short-term [19,25,26,28,41,54] or long-term tracking [17,18,26,40,50]
- However, all of these benchmarks and datasets share the same setup of only providing a single user-specified bounding box such that only one target is tracked in each video sequence
- Recently, GMOT-40 [1] focused on Generic Multi Object Tracking (GMOT), where a single bounding box is provided in the first video frame and all objects that correspond to the same class as the annotated object should be tracked
- In contrast to GMOT, we focus on the setting where multiple targets are given, potentially from different classes
- MOT focuses on tracking multiple objects corresponding to different but predefined classes and thus requires a pretrained detector on these classes
- MOT research mainly aims at tracking pedestrians [15,48] or on autonomous driving settings, where only a handful of different classes are considered [5,21,64]
- To overcome this limitation TAO [14] focuses on tracking objects of a long-tailed class distributions, but provides sparse annotations due to the costly annotation process
- Another related task is open world tracking [33] that aims at detecting and tracking any object in a video sequence
- However similar to long-tailed MOT datasets, objects of interest might not be detected nor tracked in such setups. Especially if the desired objects correspond to rare classes, parts of an object or patches, tracking them without providing a user-specified bounding box is challenging
- In the Video Object Segmentation (VOS) domain, DAVIS [44] and YouTubeVOS [56] provide multiobject annotations. However, their videos are extremely short (2.9 and 4.5 seconds on average respectively), and therefore not suitable for tracking
- Moreover, the VOS domain provides less challenges for trackers, instead focusing on large objects and a short-term nature, where the predominant challenge is the prediction of accurate segmentation masks
- Global Generic Object Tracking. Global tracking operates on the whole video frame, rather than in a restricted search area near the object location in the previous frame. This is not only beneficial when tracking multiple objects in the same scene but also facilitates re-detecting lost objects.
- GlobalTrack [24] and Siam R-CNN [52] track the target by using global RPNs that retrieve target-specific proposals.
- Another group of methods such as MetaUpdater [8] and SPLT [60] operate on local search areas but use a redetector to re-localize the target if it disappeared from the search area. In contrast, our proposed tracker always operates on the entire frame and uses a Transformer-based model predictor to generate target specific correlation filters instead of producing target-specific proposals.
- UTT [37] tackles both GOT and MOT by tracking one generic object per sequence and multiple objects belonging to a set of predefined classes such as pedestrians. UTT matches test frame features with reference features of the detected objects in the initial (SOT) or previous (MOT) frame using a Transformer.
- Unicorn [58] performs SOT and MOT with the same model by varying the input data type. In contrast, our tracker tracks multiple generic objects at the same time instead of only one object or multiple objects of predefined classes.
- Transformers for Generic Object Tracking. Tracking has seen a a tremendous progress in recent years with the advent of Transformers [51]. Most such trackers share the idea of fusing the search area and the template image features by using a Transformer [6,7,38,59,62,63].
- Mix-Former [7] and OSTrack [62] employ a Transformer to jointly extract and fuse the template and search area features.
- TransT [6], STARK [59] and ToMP [38] use a backbone to extract visual features and employ cross attention to fuse the feature representations. However, none of these trackers can easily be extended to jointly track multiple objects, which is addressed in this work.

## Benchmark
- The existing datasets that seem more suitable for multi-object GOT evaluation are TAO and Ima-geNetVID, but they both have limitations.
- TAO is annotated at only 1fps on the evaluation set, and has many short tracks (21 seconds on average, or 21 frames).
- Ima-geNetVID is densely annotated but contains mostly fairly simple videos, only 30 object classes, and very short tracks (8 seconds on average) on the validation set.
- We opt for creating a new dataset for multi-object GOT evaluation, which is based on the widely used LaGOT evaluation set.
- The new dataset has 294 video sequences with 837 tracks, which is almost triple the number of tracks compared to the original LaGOT validation set.
- We add 31 additional generic object classes, e.g. pool queue, propeller, tires or fabric bag.

## Method
- TaMOs employs a Transformer to jointly model and track a set of arbitrary objects
- ToMP is a recent Transformerbased generic single object tracker that serves as our starting point
- The proposed Transformerbased multi-object tracking architecture is introduced in Sec. 4.3

### Background -Transforming Model Prediction
- We use the recent ToMP [38] as our baseline tracker
- ToMP uses a backbone to extract visual features f train , f test ∈ R h×w×c for the training and the test image crops
- The training image crop contains substantial background information, in order to enhance target-background discrimination
- The target location is thus encoded with a foreground embedding e fg ∈ R 1×c . It is combined with the Gaussian score map y i ∈ R h×w×1 that represents the center location of the target object i and the LTRB [49,57] bounding box encoding b ltrb i ∈ R h×w×4 .
- The encoding is given by, where φ is a Multi-Layer Perceptron (MLP)

### Generic Multi-Object Tracker -Overview
- The proposed generic multi-object tracker TaMOs operates on the full train and test images instead of crops
- The target object encoder uses a pool of learnable object embeddings to encode the location and extent of each target object within a single shared feature map
- To track small objects we propose an FPN-based feature fusion of the test frame features produced by the Transformer with the higher resolution backbone features
- In order to track efficiently a growing number of target objects, we propose a novel object encoding that allows to encode multiple objects in a shared feature map without requiring multiple templates
- We process the entire video frame instead of operating on a separate search region for each target
- In particular, we extend the single object encoding formulation presented in Eq. (1) to be applicable for multiple objects
- The idea is to replace the aforementioned single object embedding with multiple embeddings, each representing a different object
- We produce a correlation filter for target classification and adopt the bounding box regression branch of ToMP [38]
- But instead of applying the target classifier and box regressor on the lowresolution test features h test of the Transformer encoder, we use high resolution features generated with an FPN: where f high test ∈ R 2h×2w×c are the high resolution test features extracted at an earlier stage of the backbone, and ψ(•) denotes the FPN.

### Training
- During training, we employ a classification and a bounding box regression loss.
- The classification loss is given by where L GIoU denotes the generalized IoU-Loss [46].
- The overall training loss is then defined as where λ cls and λ bbreg are scalars weighting the contribution of each loss component.

## Experiments
- We evaluate our proposed tracking architecture along with 8 recent trackers on our GOT benchmark LaGOT
- We compare our method to recent trackers on several SOT benchmarks
- We present an ablation study, evaluating the impact of different components of our tracker.

### Implementation Details
- Uses PyTracking to implement the method
- Uses a training sequence of LaSOT, GOT10k, TrackingNet, MS-COCO, ImageNet-Vid, TAO, and YoutubeVOS
- Uses a memory updating approach to improve performance
- Uses 4 Nvidia A100 GPUs to train the tracker

### State-of-the-Art Evaluation on LaGOT
- Our tracker achieves the best AUC, even outperforming the state-of-the-art MixFormerLarge-22k [7] by 1 point.
- Our tracker with ResNet-50 achieves the highest AUC among all trackers with that backbone, and even outperforming STARK101 [59] that uses ResNet-101.
- Our approach achieves almost a constant run-time even when increasing the number of targets.

### State-of-the-Art Comparison on SOT Datasets
- Our tracker TaMOs achieves the highest success rate and precision AUC on popular large-scale GOT benchmarks
- Our tracker is second only or spatial windowing considered standard in tracking
- Our tracker with SwinBase sets the new state of the art on TrackingNet

### Ablation Study
- All ablation experiments use Resnet-50 as backbone unless specified otherwise.
- Generic Multiple Object Encoding.
- To better assess the contribution of each object encoding component, we train different variations of our model.
- Tab. 4 shows the effect of the Gaussian score map encoding, the LTRB bounding box encoding and the total number of object embeddings m stored in the pool E.
- The first two rows in Tab. 4 show that LTRB bounding box encoding is more important than the Gaussian score map encoding (as removing the former decreases all results more significantly).
- Another key factor in the proposed generic multi object encoding is the number of different object embeddings that sets an upper limit on the number of objects that can be tracked.
- LaGOT requires at least 10 embeddings and our tracker achieves the best results when using a pool size of 10.
- Increasing the number of embeddings decreases the overall tracking performance.
- A possible explanation why using 10 embeddings works best could be that all training datasets except MS-COCO contain at most 10 objects per training sequence.
- Thus, our final tracker uses both encodings and a pool size of ten.
- Architecture.
- In contrast to the baseline ToMP [38], we either keep the Resnet-50 backbone or replace it with Swin-Base.
- Tab. 5 shows that using SwinBase clearly increases the tracking performance on LaSOT and LaGOT (1 st vs. 3 rd row).
- Similarly, adding an FPN improves results on both LaSOT and LaGOT, showing even greater benefits on the latter.
- Thus, we employ an FPN in our final version and report all results for both backbone types.
- As explained in Sec 4.3 we add a second training frame during inference that corresponds to the most recent video frame where the tracker produced bounding boxes with a high confidence.
- Tab. 6 shows the tracking performance of our tracker when using only the initial training frame or when adding the most recent training frame with predicted annotations.
- We conclude that including a second training frame substantially improves the results, especially on LaSOT.

## Conclusion
- We propose a novel multiple object GOT tracking benchmark, LaGOT, that allows to evaluate GOT methods that can jointly track multiple targets in the same sequence.
- We further propose a Transformer-based tracker capable of processing multiple targets at the same time.
- Our approach integrates a novel generic multi object encoding and an FPN in order to achieve full frame tracking.
- Our method outperforms recent trackers on the LaGOT benchmark, while operating 4× faster than the SOT baseline when tracking 10 objects.
- Lastly, our approach also achieves excellent results on large-scale SOT benchmarks.
